[
  {
    "entry_id": "http://arxiv.org/abs/2402.08682v1",
    "updated": "2024-02-13T18:59:51+00:00",
    "published": "2024-02-13T18:59:51+00:00",
    "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
    "authors": [
      {
        "name": "Luke Melas-Kyriazi"
      },
      {
        "name": "Iro Laina"
      },
      {
        "name": "Christian Rupprecht"
      },
      {
        "name": "Natalia Neverova"
      },
      {
        "name": "Andrea Vedaldi"
      },
      {
        "name": "Oran Gafni"
      },
      {
        "name": "Filippos Kokkinos"
      }
    ],
    "summary": "Most text-to-3D generators build upon off-the-shelf text-to-image models\ntrained on billions of images. They use variants of Score Distillation Sampling\n(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation\nis to fine-tune the 2D generator to be multi-view aware, which can help\ndistillation or can be combined with reconstruction networks to output 3D\nobjects directly. In this paper, we further explore the design space of\ntext-to-3D models. We significantly improve multi-view generation by\nconsidering video instead of image generators. Combined with a 3D\nreconstruction algorithm which, by using Gaussian splatting, can optimize a\nrobust image-based loss, we directly produce high-quality 3D outputs from the\ngenerated views. Our new method, IM-3D, reduces the number of evaluations of\nthe 2D generator network 10-100x, resulting in a much more efficient pipeline,\nbetter quality, fewer geometric inconsistencies, and higher yield of usable 3D\nassets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08682v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08682v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08682v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08680v1",
    "updated": "2024-02-13T18:59:05+00:00",
    "published": "2024-02-13T18:59:05+00:00",
    "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
    "authors": [
      {
        "name": "Linxi Zhao"
      },
      {
        "name": "Yihe Deng"
      },
      {
        "name": "Weitong Zhang"
      },
      {
        "name": "Quanquan Gu"
      }
    ],
    "summary": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.",
    "comment": "27 pages, 20 figures, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08680v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08680v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08680v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08679v1",
    "updated": "2024-02-13T18:58:48+00:00",
    "published": "2024-02-13T18:58:48+00:00",
    "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
    "authors": [
      {
        "name": "Xingang Guo"
      },
      {
        "name": "Fangxu Yu"
      },
      {
        "name": "Huan Zhang"
      },
      {
        "name": "Lianhui Qin"
      },
      {
        "name": "Bin Hu"
      }
    ],
    "summary": "Jailbreaks on Large language models (LLMs) have recently received increasing\nattention. For a comprehensive assessment of LLM safety, it is essential to\nconsider jailbreaks with diverse attributes, such as contextual coherence and\nsentiment/stylistic variations, and hence it is beneficial to study\ncontrollable jailbreaking, i.e. how to enforce control on LLM attacks. In this\npaper, we formally formulate the controllable attack generation problem, and\nbuild a novel connection between this problem and controllable text generation,\na well-explored topic of natural language processing. Based on this connection,\nwe adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a\nstate-of-the-art, highly efficient algorithm in controllable text generation,\nand introduce the COLD-Attack framework which unifies and automates the search\nof adversarial LLM attacks under a variety of control requirements such as\nfluency, stealthiness, sentiment, and left-right-coherence. The controllability\nenabled by COLD-Attack leads to diverse new jailbreak scenarios which not only\ncover the standard setting of generating fluent suffix attacks, but also allow\nus to address new controllable attack settings such as revising a user query\nadversarially with minimal paraphrasing, and inserting stealthy attacks in\ncontext with left-right-coherence. Our extensive experiments on various LLMs\n(Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad\napplicability, strong controllability, high success rate, and attack\ntransferability. Our code is available at\nhttps://github.com/Yu-Fangxu/COLD-Attack.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08679v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08679v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08679v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08678v1",
    "updated": "2024-02-13T18:58:17+00:00",
    "published": "2024-02-13T18:58:17+00:00",
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "authors": [
      {
        "name": "Ali Behrouz"
      },
      {
        "name": "Farnoosh Hashemi"
      }
    ],
    "summary": "Graph Neural Networks (GNNs) have shown promising potential in graph\nrepresentation learning. The majority of GNNs define a local message-passing\nmechanism, propagating information over the graph by stacking multiple layers.\nThese methods, however, are known to suffer from two major limitations:\nover-squashing and poor capturing of long-range dependencies. Recently, Graph\nTransformers (GTs) emerged as a powerful alternative to Message-Passing Neural\nNetworks (MPNNs). GTs, however, have quadratic computational cost, lack\ninductive biases on graph structures, and rely on complex Positional/Structural\nEncodings (SE/PE). In this paper, we show that while Transformers, complex\nmessage-passing, and SE/PE are sufficient for good performance in practice,\nneither is necessary. Motivated by the recent success of State Space Models\n(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general\nframework for a new class of GNNs based on selective SSMs. We discuss and\ncategorize the new challenges when adopting SSMs to graph-structured data, and\npresent four required and one optional steps to design GMNs, where we choose\n(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of\nBidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE\nand SE. We further provide theoretical justification for the power of GMNs.\nExperiments demonstrate that despite much less computational cost, GMNs attain\nan outstanding performance in long-range, small-scale, large-scale, and\nheterophilic benchmark datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08678v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08678v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08678v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08676v1",
    "updated": "2024-02-13T18:56:55+00:00",
    "published": "2024-02-13T18:56:55+00:00",
    "title": "A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification",
    "authors": [
      {
        "name": "Burak \u00c7akmak"
      },
      {
        "name": "Yue M. Lu"
      },
      {
        "name": "Manfred Opper"
      }
    ],
    "summary": "Motivated by the recent application of approximate message passing (AMP) to\nthe analysis of convex optimizations in multi-class classifications [Loureiro,\net. al., 2021], we present a convergence analysis of AMP dynamics with\nnon-separable multivariate nonlinearities. As an application, we present a\ncomplete (and independent) analysis of the motivated convex optimization\nproblem.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08676v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08676v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08676v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08674v1",
    "updated": "2024-02-13T18:55:27+00:00",
    "published": "2024-02-13T18:55:27+00:00",
    "title": "Human Curriculum Effects Emerge with In-Context Learning in Neural Networks",
    "authors": [
      {
        "name": "Jacob Russin"
      },
      {
        "name": "Ellie Pavlick"
      },
      {
        "name": "Michael J. Frank"
      }
    ],
    "summary": "Human learning is sensitive to rule-like structure and the curriculum of\nexamples used for training. In tasks governed by succinct rules, learning is\nmore robust when related examples are blocked across trials, but in the absence\nof such rules, interleaving is more effective. To date, no neural model has\nsimultaneously captured these seemingly contradictory effects. Here we show\nthat this same tradeoff spontaneously emerges with \"in-context learning\" (ICL)\nboth in neural networks trained with metalearning and in large language models\n(LLMs). ICL is the ability to learn new tasks \"in context\" - without weight\nchanges - via an inner-loop algorithm implemented in activation dynamics.\nExperiments with pretrained LLMs and metalearning transformers show that ICL\nexhibits the blocking advantage demonstrated in humans on a task involving\nrule-like structure, and conversely, that concurrent in-weight learning\nreproduces the interleaving advantage observed in humans on tasks lacking such\nstructure.",
    "comment": "7 pages, 4 figures, under review at CogSci 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.NE",
    "categories": [
      "cs.NE",
      "cs.LG",
      "q-bio.NC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08674v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08674v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08674v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08672v1",
    "updated": "2024-02-13T18:54:08+00:00",
    "published": "2024-02-13T18:54:08+00:00",
    "title": "Model Assessment and Selection under Temporal Distribution Shift",
    "authors": [
      {
        "name": "Elise Han"
      },
      {
        "name": "Chengpiao Huang"
      },
      {
        "name": "Kaizheng Wang"
      }
    ],
    "summary": "We investigate model assessment and selection in a changing environment, by\nsynthesizing datasets from both the current time period and historical epochs.\nTo tackle unknown and potentially arbitrary temporal distribution shift, we\ndevelop an adaptive rolling window approach to estimate the generalization\nerror of a given model. This strategy also facilitates the comparison between\nany two candidate models by estimating the difference of their generalization\nerrors. We further integrate pairwise comparisons into a single-elimination\ntournament, achieving near-optimal model selection from a collection of\ncandidates. Theoretical analyses and numerical experiments demonstrate the\nadaptivity of our proposed methods to the non-stationarity in data.",
    "comment": "24 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "62G05 (Primary), 62J02 (Secondary)"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08672v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08672v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08672v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08671v1",
    "updated": "2024-02-13T18:53:13+00:00",
    "published": "2024-02-13T18:53:13+00:00",
    "title": "Are Semi-Dense Detector-Free Methods Good at Matching Local Features?",
    "authors": [
      {
        "name": "Matthieu Vilain"
      },
      {
        "name": "R\u00e9mi Giraud"
      },
      {
        "name": "Hugo Germain"
      },
      {
        "name": "Guillaume Bourmaud"
      }
    ],
    "summary": "Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among\nthe most popular image matching methods. While SDF methods are trained to\nestablish correspondences between two images, their performances are almost\nexclusively evaluated using relative pose estimation metrics. Thus, the link\nbetween their ability to establish correspondences and the quality of the\nresulting estimated pose has thus far received little attention. This paper is\na first attempt to study this link. We start with proposing a novel structured\nattention-based image matching architecture (SAM). It allows us to show a\ncounter-intuitive result on two datasets (MegaDepth and HPatches): on the one\nhand SAM either outperforms or is on par with SDF methods in terms of\npose/homography estimation metrics, but on the other hand SDF approaches are\nsignificantly better than SAM in terms of matching accuracy. We then propose to\nlimit the computation of the matching accuracy to textured regions, and show\nthat in this case SAM often surpasses SDF methods. Our findings highlight a\nstrong correlation between the ability to establish accurate correspondences in\ntextured regions and the accuracy of the resulting estimated pose/homography.\nOur code will be made available.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08671v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08671v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08671v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08670v1",
    "updated": "2024-02-13T18:51:18+00:00",
    "published": "2024-02-13T18:51:18+00:00",
    "title": "Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models",
    "authors": [
      {
        "name": "Yuqing Liu"
      },
      {
        "name": "Yu Wang"
      },
      {
        "name": "Lichao Sun"
      },
      {
        "name": "Philip S. Yu"
      }
    ],
    "summary": "The development of large vision-language models (LVLMs) offers the potential\nto address challenges faced by traditional multimodal recommendations thanks to\ntheir proficient understanding of static images and textual dynamics. However,\nthe application of LVLMs in this field is still limited due to the following\ncomplexities: First, LVLMs lack user preference knowledge as they are trained\nfrom vast general datasets. Second, LVLMs suffer setbacks in addressing\nmultiple image dynamics in scenarios involving discrete, noisy, and redundant\nimage sequences. To overcome these issues, we propose the novel reasoning\nscheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large\nvision-language models for multimodal recommendation. We utilize user history\nas in-context user preferences to address the first challenge. Next, we prompt\nLVLMs to generate item image summaries and utilize image comprehension in\nnatural language space combined with item titles to query the user preferences\nover candidate items. We conduct comprehensive experiments across four datasets\nwith three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results\nindicate the efficacy of VST.",
    "comment": "under review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08670v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08670v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08670v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08667v1",
    "updated": "2024-02-13T18:48:28+00:00",
    "published": "2024-02-13T18:48:28+00:00",
    "title": "Target Score Matching",
    "authors": [
      {
        "name": "Valentin De Bortoli"
      },
      {
        "name": "Michael Hutchinson"
      },
      {
        "name": "Peter Wirnsberger"
      },
      {
        "name": "Arnaud Doucet"
      }
    ],
    "summary": "Denoising Score Matching estimates the score of a noised version of a target\ndistribution by minimizing a regression loss and is widely used to train the\npopular class of Denoising Diffusion Models. A well known limitation of\nDenoising Score Matching, however, is that it yields poor estimates of the\nscore at low noise levels. This issue is particularly unfavourable for problems\nin the physical sciences and for Monte Carlo sampling tasks for which the score\nof the clean original target is known. Intuitively, estimating the score of a\nslightly noised version of the target should be a simple task in such cases. In\nthis paper, we address this shortcoming and show that it is indeed possible to\nleverage knowledge of the target score. We present a Target Score Identity and\ncorresponding Target Score Matching regression loss which allows us to obtain\nscore estimates admitting favourable properties at low noise levels.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08667v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08667v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08667v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08662v1",
    "updated": "2024-02-13T18:46:10+00:00",
    "published": "2024-02-13T18:46:10+00:00",
    "title": "Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback",
    "authors": [
      {
        "name": "Jenny Zhang"
      },
      {
        "name": "Steve Heim"
      },
      {
        "name": "Se Hwan Jeon"
      },
      {
        "name": "Sangbae Kim"
      }
    ],
    "summary": "We present a minimal phase oscillator model for learning quadrupedal\nlocomotion. Each of the four oscillators is coupled only to itself and its\ncorresponding leg through local feedback of the ground reaction force, which\ncan be interpreted as an observer feedback gain. We interpret the oscillator\nitself as a latent contact state-estimator. Through a systematic ablation\nstudy, we show that the combination of phase observations, simple phase-based\nrewards, and the local feedback dynamics induces policies that exhibit emergent\ngait preferences, while using a reduced set of simple rewards, and without\nprescribing a specific gait. The code is open-source, and a video synopsis\navailable at https://youtu.be/1NKQ0rSV3jU.",
    "comment": "ICRA 2024, 8 pages 7 Figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08662v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08662v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08662v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08658v1",
    "updated": "2024-02-13T18:39:36+00:00",
    "published": "2024-02-13T18:39:36+00:00",
    "title": "The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting",
    "authors": [
      {
        "name": "David Haag"
      },
      {
        "name": "Devender Kumar"
      },
      {
        "name": "Sebastian Gruber"
      },
      {
        "name": "Mahdi Sareban"
      },
      {
        "name": "Gunnar Treff"
      },
      {
        "name": "Josef Niebauer"
      },
      {
        "name": "Christopher Bull"
      },
      {
        "name": "Jan David Smeddinck"
      }
    ],
    "summary": "We explored the viability of Large Language Models (LLMs) for triggering and\npersonalizing content for Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. JITAIs are being explored as a key mechanism for sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. However, traditional rule-based and machine learning models for JITAI\nimplementation face scalability and reliability limitations, such as lack of\npersonalization, difficulty in managing multi-parametric systems, and issues\nwith data sparsity. To investigate JITAI implementation via LLMs, we tested the\ncontemporary overall performance-leading model 'GPT-4' with examples grounded\nin the use case of fostering heart-healthy physical activity in outpatient\ncardiac rehabilitation. Three personas and five sets of context information per\npersona were used as a basis of triggering and personalizing JITAIs.\nSubsequently, we generated a total of 450 proposed JITAI decisions and message\ncontent, divided equally into JITAIs generated by 10 iterations with GPT-4, a\nbaseline provided by 10 laypersons (LayPs), and a gold standard set by 10\nhealthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs\ngenerated by GPT-4 were superior to those by HCPs and LayPs over all assessed\nscales: i.e., appropriateness, engagement, effectiveness, and professionality.\nThis study indicates that LLMs have significant potential for implementing\nJITAIs as a building block of personalized or \"precision\" health, offering\nscalability, effective personalization based on opportunistically sampled\ninformation, and good acceptability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "J.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08658v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08658v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08658v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08653v1",
    "updated": "2024-02-13T18:33:45+00:00",
    "published": "2024-02-13T18:33:45+00:00",
    "title": "SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds",
    "authors": [
      {
        "name": "Wuxinlin Cheng"
      },
      {
        "name": "Chenhui Deng"
      },
      {
        "name": "Ali Aghdaei"
      },
      {
        "name": "Zhiru Zhang"
      },
      {
        "name": "Zhuo Feng"
      }
    ],
    "summary": "Modern graph neural networks (GNNs) can be sensitive to changes in the input\ngraph structure and node features, potentially resulting in unpredictable\nbehavior and degraded performance. In this work, we introduce a spectral\nframework known as SAGMAN for examining the stability of GNNs. This framework\nassesses the distance distortions that arise from the nonlinear mappings of\nGNNs between the input and output manifolds: when two nearby nodes on the input\nmanifold are mapped (through a GNN model) to two distant ones on the output\nmanifold, it implies a large distance distortion and thus a poor GNN stability.\nWe propose a distance-preserving graph dimension reduction (GDR) approach that\nutilizes spectral graph embedding and probabilistic graphical models (PGMs) to\ncreate low-dimensional input/output graph-based manifolds for meaningful\nstability analysis. Our empirical evaluations show that SAGMAN effectively\nassesses the stability of each node when subjected to various edge or feature\nperturbations, offering a scalable approach for evaluating the stability of\nGNNs, extending to applications within recommendation systems. Furthermore, we\nillustrate its utility in downstream tasks, notably in enhancing GNN stability\nand facilitating adversarial targeted attacks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08653v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08653v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08653v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08648v1",
    "updated": "2024-02-13T18:27:53+00:00",
    "published": "2024-02-13T18:27:53+00:00",
    "title": "Generating Universal Adversarial Perturbations for Quantum Classifiers",
    "authors": [
      {
        "name": "Gautham Anil"
      },
      {
        "name": "Vishnu Vinod"
      },
      {
        "name": "Apurva Narayan"
      }
    ],
    "summary": "Quantum Machine Learning (QML) has emerged as a promising field of research,\naiming to leverage the capabilities of quantum computing to enhance existing\nmachine learning methodologies. Recent studies have revealed that, like their\nclassical counterparts, QML models based on Parametrized Quantum Circuits\n(PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of\nUniversal Adversarial Perturbations (UAPs) in the quantum domain has been\ndemonstrated theoretically in the context of quantum classifiers. In this work,\nwe introduce QuGAP: a novel framework for generating UAPs for quantum\nclassifiers. We conceptualize the notion of additive UAPs for PQC-based\nclassifiers and theoretically demonstrate their existence. We then utilize\ngenerative models (QuGAP-A) to craft additive UAPs and experimentally show that\nquantum classifiers are susceptible to such attacks. Moreover, we formulate a\nnew method for generating unitary UAPs (QuGAP-U) using quantum generative\nmodels and a novel loss function based on fidelity constraints. We evaluate the\nperformance of the proposed framework and show that our method achieves\nstate-of-the-art misclassification rates, while maintaining high fidelity\nbetween legitimate and adversarial samples.",
    "comment": "Accepted at AAAI 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08648v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08648v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08648v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08646v1",
    "updated": "2024-02-13T18:24:23+00:00",
    "published": "2024-02-13T18:24:23+00:00",
    "title": "Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data",
    "authors": [
      {
        "name": "Hiroyuki Kido"
      }
    ],
    "summary": "Inspired by empirical work in neuroscience for Bayesian approaches to brain\nfunction, we give a unified probabilistic account of various types of symbolic\nreasoning from data. We characterise them in terms of formal logic using the\nclassical consequence relation, an empirical consequence relation, maximal\nconsistent sets, maximal possible sets and maximum likelihood estimation. The\ntheory gives new insights into reasoning towards human-like machine\nintelligence.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08646v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08646v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08646v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08645v1",
    "updated": "2024-02-13T18:24:10+00:00",
    "published": "2024-02-13T18:24:10+00:00",
    "title": "Peeking Behind the Curtains of Residual Learning",
    "authors": [
      {
        "name": "Tunhou Zhang"
      },
      {
        "name": "Feng Yan"
      },
      {
        "name": "Hai Li"
      },
      {
        "name": "Yiran Chen"
      }
    ],
    "summary": "The utilization of residual learning has become widespread in deep and\nscalable neural nets. However, the fundamental principles that contribute to\nthe success of residual learning remain elusive, thus hindering effective\ntraining of plain nets with depth scalability. In this paper, we peek behind\nthe curtains of residual learning by uncovering the \"dissipating inputs\"\nphenomenon that leads to convergence failure in plain neural nets: the input is\ngradually compromised through plain layers due to non-linearities, resulting in\nchallenges of learning feature representations. We theoretically demonstrate\nhow plain neural nets degenerate the input to random noise and emphasize the\nsignificance of a residual connection that maintains a better lower bound of\nsurviving neurons as a solution. With our theoretical discoveries, we propose\n\"The Plain Neural Net Hypothesis\" (PNNH) that identifies the internal path\nacross non-linear layers as the most critical part in residual learning, and\nestablishes a paradigm to support the training of deep plain neural nets devoid\nof residual connections. We thoroughly evaluate PNNH-enabled CNN architectures\nand Transformers on popular vision benchmarks, showing on-par accuracy, up to\n0.3% higher training throughput, and 2x better parameter efficiency compared to\nResNets and vision Transformers.",
    "comment": "Arxiv Preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08645v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08645v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08645v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08644v1",
    "updated": "2024-02-13T18:24:08+00:00",
    "published": "2024-02-13T18:24:08+00:00",
    "title": "Tandem Transformers for Inference Efficient LLMs",
    "authors": [
      {
        "name": "Aishwarya P S"
      },
      {
        "name": "Pranav Ajit Nair"
      },
      {
        "name": "Yashas Samaga"
      },
      {
        "name": "Toby Boyd"
      },
      {
        "name": "Sanjiv Kumar"
      },
      {
        "name": "Prateek Jain"
      },
      {
        "name": "Praneeth Netrapalli"
      }
    ],
    "summary": "The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08644v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08644v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08644v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08643v1",
    "updated": "2024-02-13T18:20:04+00:00",
    "published": "2024-02-13T18:20:04+00:00",
    "title": "Learned Image Compression with Text Quality Enhancement",
    "authors": [
      {
        "name": "Chih-Yu Lai"
      },
      {
        "name": "Dung Tran"
      },
      {
        "name": "Kazuhito Koishida"
      }
    ],
    "summary": "Learned image compression has gained widespread popularity for their\nefficiency in achieving ultra-low bit-rates. Yet, images containing substantial\ntextual content, particularly screen-content images (SCI), often suffers from\ntext distortion at such compressed levels. To address this, we propose to\nminimize a novel text logit loss designed to quantify the disparity in text\nbetween the original and reconstructed images, thereby improving the perceptual\nquality of the reconstructed text. Through rigorous experimentation across\ndiverse datasets and employing state-of-the-art algorithms, our findings reveal\nsignificant enhancements in the quality of reconstructed text upon integration\nof the proposed loss function with appropriate weighting. Notably, we achieve a\nBjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and\n-28.03% for Word Error Rate (WER) on average by applying the text logit loss\nfor two screenshot datasets. Additionally, we present quantitative metrics\ntailored for evaluating text quality in image compression tasks. Our findings\nunderscore the efficacy and potential applicability of our proposed text logit\nloss function across various text-aware image compression contexts.",
    "comment": "Submitted to ICIP 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08643v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08643v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08643v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08640v1",
    "updated": "2024-02-13T18:09:38+00:00",
    "published": "2024-02-13T18:09:38+00:00",
    "title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs",
    "authors": [
      {
        "name": "Xuemei Gu"
      },
      {
        "name": "Mario Krenn"
      }
    ],
    "summary": "The exponential growth in scientific publications poses a severe challenge\nfor human researchers. It forces attention to more narrow sub-fields, which\nmakes it challenging to discover new impactful research ideas and\ncollaborations outside one's own field. While there are ways to predict a\nscientific paper's future citation counts, they need the research to be\nfinished and the paper written, usually assessing impact long after the idea\nwas conceived. Here we show how to predict the impact of onsets of ideas that\nhave never been published by researchers. For that, we developed a large\nevolving knowledge graph built from more than 21 million scientific papers. It\ncombines a semantic network created from the content of the papers and an\nimpact network created from the historic citations of papers. Using machine\nlearning, we can predict the dynamic of the evolving network into the future\nwith high accuracy, and thereby the impact of new research directions. We\nenvision that the ability to predict the impact of new ideas will be a crucial\ncomponent of future artificial muses that can inspire new impactful and\ninteresting scientific ideas.",
    "comment": "8 pages, 6 figures, Comments welcome!",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DL",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08640v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08640v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08640v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08637v1",
    "updated": "2024-02-13T18:03:56+00:00",
    "published": "2024-02-13T18:03:56+00:00",
    "title": "Strategizing against No-Regret Learners in First-Price Auctions",
    "authors": [
      {
        "name": "Aviad Rubinstein"
      },
      {
        "name": "Junyao Zhao"
      }
    ],
    "summary": "We study repeated first-price auctions and general repeated Bayesian games\nbetween two players, where one player, the learner, employs a no-regret\nlearning algorithm, and the other player, the optimizer, knowing the learner's\nalgorithm, strategizes to maximize its own utility. For a commonly used class\nof no-regret learning algorithms called mean-based algorithms, we show that (i)\nin standard (i.e., full-information) first-price auctions, the optimizer cannot\nget more than the Stackelberg utility -- a standard benchmark in the\nliterature, but (ii) in Bayesian first-price auctions, there are instances\nwhere the optimizer can achieve much higher than the Stackelberg utility.\n  On the other hand, Mansour et al. (2022) showed that a more sophisticated\nclass of algorithms called no-polytope-swap-regret algorithms are sufficient to\ncap the optimizer's utility at the Stackelberg utility in any repeated Bayesian\ngame (including Bayesian first-price auctions), and they pose the open question\nwhether no-polytope-swap-regret algorithms are necessary to cap the optimizer's\nutility. For general Bayesian games, under a reasonable and necessary\ncondition, we prove that no-polytope-swap-regret algorithms are indeed\nnecessary to cap the optimizer's utility and thus answer their open question.\nFor Bayesian first-price auctions, we give a simple improvement of the standard\nalgorithm for minimizing the polytope swap regret by exploiting the structure\nof Bayesian first-price auctions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.DS",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08637v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08637v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08637v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08631v1",
    "updated": "2024-02-13T17:59:34+00:00",
    "published": "2024-02-13T17:59:34+00:00",
    "title": "Knowledge Editing on Black-box Large Language Models",
    "authors": [
      {
        "name": "Xiaoshuai Song"
      },
      {
        "name": "Zhengyang Wang"
      },
      {
        "name": "Keqing He"
      },
      {
        "name": "Guanting Dong"
      },
      {
        "name": "Jinxu Zhao"
      },
      {
        "name": "Weiran Xu"
      }
    ],
    "summary": "Knowledge editing (KE) aims to efficiently and precisely modify the behavior\nof large language models (LLMs) to update specific knowledge without negatively\ninfluencing other knowledge. Current research primarily focuses on white-box\nLLMs editing, overlooking an important scenario: black-box LLMs editing, where\nLLMs are accessed through interfaces and only textual output is available. To\naddress the limitations of existing evaluations that are not inapplicable to\nblack-box LLM editing and lack comprehensiveness, we propose a\nmulti-perspective evaluation framework, incorporating the assessment of style\nretention for the first time. To tackle privacy leaks of editing data and style\nover-editing in current methods, we introduce a novel postEdit framework,\nresolving privacy concerns through downstream post-processing and maintaining\ntextual style consistency via fine-grained editing to original responses.\nExperiments and analysis on two benchmarks demonstrate that postEdit\noutperforms all baselines and achieves strong generalization, especially with\nhuge improvements on style retention (average $+20.82\\%\\uparrow$).",
    "comment": "Work in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08631v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08631v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08631v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08621v1",
    "updated": "2024-02-13T17:42:27+00:00",
    "published": "2024-02-13T17:42:27+00:00",
    "title": "A Generalized Approach to Online Convex Optimization",
    "authors": [
      {
        "name": "Mohammad Pedramfar"
      },
      {
        "name": "Vaneet Aggarwal"
      }
    ],
    "summary": "In this paper, we analyze the problem of online convex optimization in\ndifferent settings. We show that any algorithm for online linear optimization\nwith fully adaptive adversaries is an algorithm for online convex optimization.\nWe also show that any such algorithm that requires full-information feedback\nmay be transformed to an algorithm with semi-bandit feedback with comparable\nregret bound. We further show that algorithms that are designed for fully\nadaptive adversaries using deterministic semi-bandit feedback can obtain\nsimilar bounds using only stochastic semi-bandit feedback when facing oblivious\nadversaries. We use this to describe general meta-algorithms to convert first\norder algorithms to zeroth order algorithms with comparable regret bounds. Our\nframework allows us to analyze online optimization in various settings, such\nfull-information feedback, bandit feedback, stochastic regret, adversarial\nregret and various forms of non-stationary regret. Using our analysis, we\nprovide the first efficient projection-free online convex optimization\nalgorithm using linear optimization oracles.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08621v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08621v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08621v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08616v1",
    "updated": "2024-02-13T17:32:59+00:00",
    "published": "2024-02-13T17:32:59+00:00",
    "title": "Adjustment Identification Distance: A gadjid for Causal Structure Learning",
    "authors": [
      {
        "name": "Leonard Henckel"
      },
      {
        "name": "Theo W\u00fcrtzen"
      },
      {
        "name": "Sebastian Weichwald"
      }
    ],
    "summary": "Evaluating graphs learned by causal discovery algorithms is difficult: The\nnumber of edges that differ between two graphs does not reflect how the graphs\ndiffer with respect to the identifying formulas they suggest for causal\neffects. We introduce a framework for developing causal distances between\ngraphs which includes the structural intervention distance for directed acyclic\ngraphs as a special case. We use this framework to develop improved\nadjustment-based distances as well as extensions to completed partially\ndirected acyclic graphs and causal orders. We develop polynomial-time\nreachability algorithms to compute the distances efficiently. In our package\ngadjid (open source at https://github.com/CausalDisco/gadjid), we provide\nimplementations of our distances; they are orders of magnitude faster than the\nstructural intervention distance and thereby provide a success metric for\ncausal discovery that scales to graph sizes that were previously prohibitive.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08616v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08616v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08616v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08609v1",
    "updated": "2024-02-13T17:18:56+00:00",
    "published": "2024-02-13T17:18:56+00:00",
    "title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL",
    "authors": [
      {
        "name": "Johan Obando-Ceron"
      },
      {
        "name": "Ghada Sokar"
      },
      {
        "name": "Timon Willi"
      },
      {
        "name": "Clare Lyle"
      },
      {
        "name": "Jesse Farebrother"
      },
      {
        "name": "Jakob Foerster"
      },
      {
        "name": "Gintare Karolina Dziugaite"
      },
      {
        "name": "Doina Precup"
      },
      {
        "name": "Pablo Samuel Castro"
      }
    ],
    "summary": "The recent rapid progress in (self) supervised learning models is in large\npart predicted by empirical scaling laws: a model's performance scales\nproportionally to its size. Analogous scaling laws remain elusive for\nreinforcement learning domains, however, where increasing the parameter count\nof a model often hurts its final performance. In this paper, we demonstrate\nthat incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs\n(Puigcerver et al., 2023), into value-based networks results in more\nparameter-scalable models, evidenced by substantial performance increases\nacross a variety of training regimes and model sizes. This work thus provides\nstrong empirical evidence towards developing scaling laws for reinforcement\nlearning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08609v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08609v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08609v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08606v1",
    "updated": "2024-02-13T17:12:01+00:00",
    "published": "2024-02-13T17:12:01+00:00",
    "title": "Arbitrary Polynomial Separations in Trainable Quantum Machine Learning",
    "authors": [
      {
        "name": "Eric R. Anschuetz"
      },
      {
        "name": "Xun Gao"
      }
    ],
    "summary": "Recent theoretical results in quantum machine learning have demonstrated a\ngeneral trade-off between the expressive power of quantum neural networks\n(QNNs) and their trainability; as a corollary of these results, practical\nexponential separations in expressive power over classical machine learning\nmodels are believed to be infeasible as such QNNs take a time to train that is\nexponential in the model size. We here circumvent these negative results by\nconstructing a hierarchy of efficiently trainable QNNs that exhibit\nunconditionally provable, polynomial memory separations of arbitrary constant\ndegree over classical neural networks in performing a classical sequence\nmodeling task. Furthermore, each unit cell of the introduced class of QNNs is\ncomputationally efficient, implementable in constant time on a quantum device.\nThe classical networks we prove a separation over include well-known examples\nsuch as recurrent neural networks and Transformers. We show that quantum\ncontextuality is the source of the expressivity separation, suggesting that\nother classical sequence learning problems with long-time correlations may be a\nregime where practical advantages in quantum machine learning may exist.",
    "comment": "35 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "quant-ph",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08606v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08606v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08606v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08595v1",
    "updated": "2024-02-13T16:57:06+00:00",
    "published": "2024-02-13T16:57:06+00:00",
    "title": "Homomorphism Counts for Graph Neural Networks: All About That Basis",
    "authors": [
      {
        "name": "Emily Jin"
      },
      {
        "name": "Michael Bronstein"
      },
      {
        "name": "Ismail Ilkan Ceylan"
      },
      {
        "name": "Matthias Lanzinger"
      }
    ],
    "summary": "Graph neural networks are architectures for learning invariant functions over\ngraphs. A large body of work has investigated the properties of graph neural\nnetworks and identified several limitations, particularly pertaining to their\nexpressive power. Their inability to count certain patterns (e.g., cycles) in a\ngraph lies at the heart of such limitations, since many functions to be learned\nrely on the ability of counting such patterns. Two prominent paradigms aim to\naddress this limitation by enriching the graph features with subgraph or\nhomomorphism pattern counts. In this work, we show that both of these\napproaches are sub-optimal in a certain sense and argue for a more fine-grained\napproach, which incorporates the homomorphism counts of all structures in the\n\"basis\" of the target pattern. This yields strictly more expressive\narchitectures without incurring any additional overhead in terms of\ncomputational complexity compared to existing approaches. We prove a series of\ntheoretical results on node-level and graph-level motif parameters and\nempirically validate them on standard benchmark datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08595v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08595v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08595v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08593v1",
    "updated": "2024-02-13T16:53:48+00:00",
    "published": "2024-02-13T16:53:48+00:00",
    "title": "Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs",
    "authors": [
      {
        "name": "Jovan Blanu\u0161a"
      },
      {
        "name": "Maximo Cravero Baraja"
      },
      {
        "name": "Andreea Anghel"
      },
      {
        "name": "Luc von Niederh\u00e4usern"
      },
      {
        "name": "Erik Altman"
      },
      {
        "name": "Haris Pozidis"
      },
      {
        "name": "Kubilay Atasu"
      }
    ],
    "summary": "In this paper, we present \"Graph Feature Preprocessor\", a software library\nfor detecting typical money laundering and fraud patterns in financial\ntransaction graphs in real time. These patterns are used to produce a rich set\nof transaction features for downstream machine learning training and inference\ntasks such as money laundering detection. We show that our enriched transaction\nfeatures dramatically improve the prediction accuracy of\ngradient-boosting-based machine learning models. Our library exploits multicore\nparallelism, maintains a dynamic in-memory graph, and efficiently mines\nsubgraph patterns in the incoming transaction stream, which enables it to be\noperated in a streaming manner. We evaluate our library using highly-imbalanced\nsynthetic anti-money laundering (AML) and real-life Ethereum phishing datasets.\nIn these datasets, the proportion of illicit transactions is very small, which\nmakes the learning process challenging. Our solution, which combines our Graph\nFeature Preprocessor and gradient-boosting-based machine learning models, is\nable to detect these illicit transactions with higher minority-class F1 scores\nthan standard graph neural networks. In addition, the end-to-end throughput\nrate of our solution executed on a multicore CPU outperforms the graph neural\nnetwork baselines executed on a powerful V100 GPU. Overall, the combination of\nhigh accuracy, a high throughput rate, and low latency of our solution\ndemonstrates the practical value of our library in real-world applications.\nGraph Feature Preprocessor has been integrated into IBM mainframe software\nproducts, namely \"IBM Cloud Pak for Data on Z\" and \"AI Toolkit for IBM Z and\nLinuxONE\".",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08593v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08593v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08593v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08592v1",
    "updated": "2024-02-13T16:52:10+00:00",
    "published": "2024-02-13T16:52:10+00:00",
    "title": "Convolutional Neural Networks Towards Facial Skin Lesions Detection",
    "authors": [
      {
        "name": "Reza Sarshar"
      },
      {
        "name": "Mohammad Heydari"
      },
      {
        "name": "Elham Akhondzadeh Noughabi"
      }
    ],
    "summary": "Facial analysis has emerged as a prominent area of research with diverse\napplications, including cosmetic surgery programs, the beauty industry,\nphotography, and entertainment. Manipulating patient images often necessitates\nprofessional image processing software. This study contributes by providing a\nmodel that facilitates the detection of blemishes and skin lesions on facial\nimages through a convolutional neural network and machine learning approach.\nThe proposed method offers advantages such as simple architecture, speed and\nsuitability for image processing while avoiding the complexities associated\nwith traditional methods. The model comprises four main steps: area selection,\nscanning the chosen region, lesion diagnosis, and marking the identified\nlesion. Raw data for this research were collected from a reputable clinic in\nTehran specializing in skincare and beauty services. The dataset includes\nadministrative information, clinical data, and facial and profile images. A\ntotal of 2300 patient images were extracted from this raw data. A software tool\nwas developed to crop and label lesions, with input from two treatment experts.\nIn the lesion preparation phase, the selected area was standardized to 50 * 50\npixels. Subsequently, a convolutional neural network model was employed for\nlesion labeling. The classification model demonstrated high accuracy, with a\nmeasure of 0.98 for healthy skin and 0.97 for lesioned skin specificity.\nInternal validation involved performance indicators and cross-validation, while\nexternal validation compared the model's performance indicators with those of\nthe transfer learning method using the Vgg16 deep network model. Compared to\nexisting studies, the results of this research showcase the efficacy and\ndesirability of the proposed model and methodology.",
    "comment": "6 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08592v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08592v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08592v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08586v1",
    "updated": "2024-02-13T16:44:02+00:00",
    "published": "2024-02-13T16:44:02+00:00",
    "title": "Faster Repeated Evasion Attacks in Tree Ensembles",
    "authors": [
      {
        "name": "Lorenzo Cascioli"
      },
      {
        "name": "Laurens Devos"
      },
      {
        "name": "Ond\u0159ej Ku\u017eelka"
      },
      {
        "name": "Jesse Davis"
      }
    ],
    "summary": "Tree ensembles are one of the most widely used model classes. However, these\nmodels are susceptible to adversarial examples, i.e., slightly perturbed\nexamples that elicit a misprediction. There has been significant research on\ndesigning approaches to construct such examples for tree ensembles. But this is\na computationally challenging problem that often must be solved a large number\nof times (e.g., for all examples in a training set). This is compounded by the\nfact that current approaches attempt to find such examples from scratch. In\ncontrast, we exploit the fact that multiple similar problems are being solved.\nSpecifically, our approach exploits the insight that adversarial examples for\ntree ensembles tend to perturb a consistent but relatively small set of\nfeatures. We show that we can quickly identify this set of features and use\nthis knowledge to speedup constructing adversarial examples.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08586v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08586v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08586v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08583v1",
    "updated": "2024-02-13T16:36:50+00:00",
    "published": "2024-02-13T16:36:50+00:00",
    "title": "Mixture of Link Predictors",
    "authors": [
      {
        "name": "Li Ma"
      },
      {
        "name": "Haoyu Han"
      },
      {
        "name": "Juanhui Li"
      },
      {
        "name": "Harry Shomer"
      },
      {
        "name": "Hui Liu"
      },
      {
        "name": "Xiaofeng Gao"
      },
      {
        "name": "Jiliang Tang"
      }
    ],
    "summary": "Link prediction, which aims to forecast unseen connections in graphs, is a\nfundamental task in graph machine learning. Heuristic methods, leveraging a\nrange of different pairwise measures such as common neighbors and shortest\npaths, often rival the performance of vanilla Graph Neural Networks (GNNs).\nTherefore, recent advancements in GNNs for link prediction (GNN4LP) have\nprimarily focused on integrating one or a few types of pairwise information. In\nthis work, we reveal that different node pairs within the same dataset\nnecessitate varied pairwise information for accurate prediction and models that\nonly apply the same pairwise information uniformly could achieve suboptimal\nperformance. As a result, we propose a simple mixture of experts model Link-MoE\nfor link prediction. Link-MoE utilizes various GNNs as experts and\nstrategically selects the appropriate expert for each node pair based on\nvarious types of pairwise information. Experimental results across diverse\nreal-world datasets demonstrate substantial performance improvement from\nLink-MoE. Notably, Link-MoE achieves a relative improvement of 18.82\\% on the\nMRR metric for the Pubmed dataset and 10.8\\% on the Hits@100 metric for the\nogbl-ppa dataset, compared to the best baselines.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08583v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08583v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08583v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08582v1",
    "updated": "2024-02-13T16:36:21+00:00",
    "published": "2024-02-13T16:36:21+00:00",
    "title": "FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis",
    "authors": [
      {
        "name": "Charulkumar Chodvadiya"
      },
      {
        "name": "Navyansh Mahla"
      },
      {
        "name": "Kinshuk Gaurav Singh"
      },
      {
        "name": "Kshitij Sharad Jadhav"
      }
    ],
    "summary": "Medical image segmentation is a critical process in the field of medical\nimaging, playing a pivotal role in diagnosis, treatment, and research. It\ninvolves partitioning of an image into multiple regions, representing distinct\nanatomical or pathological structures. Conventional methods often grapple with\nthe challenge of balancing spatial precision and comprehensive feature\nrepresentation due to their reliance on traditional loss functions. To overcome\nthis, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that\nintegrates the benefits of contrastive learning (which extracts intricate\nfeatures, particularly in the nuanced domain of medical imaging) with the\nspatial accuracy inherent in the Dice loss. The objective is to augment both\nspatial precision and feature-based representation in the segmentation of\nmedical images. FESS Loss signifies a notable advancement, offering a more\naccurate and refined segmentation process, ultimately contributing to\nheightened precision in the analysis of medical images. Further, FESS loss\ndemonstrates superior performance in limited annotated data availability\nscenarios often present in the medical domain.",
    "comment": "5 Pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08582v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08582v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08582v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08578v1",
    "updated": "2024-02-13T16:30:30+00:00",
    "published": "2024-02-13T16:30:30+00:00",
    "title": "FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing",
    "authors": [
      {
        "name": "Yongzhe Jia"
      },
      {
        "name": "Xuyun Zhang"
      },
      {
        "name": "Amin Beheshti"
      },
      {
        "name": "Wanchun Dou"
      }
    ],
    "summary": "Federated Learning (FL) has emerged as a promising solution in Edge Computing\n(EC) environments to process the proliferation of data generated by edge\ndevices. By collaboratively optimizing the global machine learning models on\ndistributed edge devices, FL circumvents the need for transmitting raw data and\nenhances user privacy. Despite practical successes, FL still confronts\nsignificant challenges including constrained edge device resources, multiple\ntasks deployment, and data heterogeneity. However, existing studies focus on\nmitigating the FL training costs of each single task whereas neglecting the\nresource consumption across multiple tasks in heterogeneous FL scenarios. In\nthis paper, we propose Heterogeneous Federated Learning with Local Parameter\nSharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer\nlearning to facilitate the deployment of multiple tasks on a single device by\ndividing the local model into a shareable encoder and task-specific encoders.\nTo further reduce resource consumption, a channel-wise model pruning algorithm\nthat shrinks the footprint of local models while accounting for both data and\nsystem heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous\nmodel aggregation algorithm is proposed to aggregate the heterogeneous\npredictors in FedLPS. We implemented the proposed FedLPS on a real FL platform\nand compared it with state-of-the-art (SOTA) FL frameworks. The experimental\nresults on five popular datasets and two modern DNN models illustrate that the\nproposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88%\nand reduces the computational resource consumption by 21.3%. Our code is\navailable at:https://github.com/jyzgh/FedLPS.",
    "comment": "Accepted by AAAI 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08578v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08578v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08578v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08577v1",
    "updated": "2024-02-13T16:28:28+00:00",
    "published": "2024-02-13T16:28:28+00:00",
    "title": "Test-Time Backdoor Attacks on Multimodal Large Language Models",
    "authors": [
      {
        "name": "Dong Lu"
      },
      {
        "name": "Tianyu Pang"
      },
      {
        "name": "Chao Du"
      },
      {
        "name": "Qian Liu"
      },
      {
        "name": "Xianjun Yang"
      },
      {
        "name": "Min Lin"
      }
    ],
    "summary": "Backdoor attacks are commonly executed by contaminating training data, such\nthat a trigger can activate predetermined harmful effects during the test\nphase. In this work, we present AnyDoor, a test-time backdoor attack against\nmultimodal large language models (MLLMs), which involves injecting the backdoor\ninto the textual modality using adversarial test images (sharing the same\nuniversal perturbation), without requiring access to or modification of the\ntraining data. AnyDoor employs similar techniques used in universal adversarial\nattacks, but distinguishes itself by its ability to decouple the timing of\nsetup and activation of harmful effects. In our experiments, we validate the\neffectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4,\nInstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.\nNotably, because the backdoor is injected by a universal perturbation, AnyDoor\ncan dynamically change its backdoor trigger prompts/harmful effects, exposing a\nnew challenge for defending against backdoor attacks. Our project page is\navailable at https://sail-sg.github.io/AnyDoor/.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08577v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08577v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08577v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08576v1",
    "updated": "2024-02-13T16:24:57+00:00",
    "published": "2024-02-13T16:24:57+00:00",
    "title": "Regret Minimization in Stackelberg Games with Side Information",
    "authors": [
      {
        "name": "Keegan Harris"
      },
      {
        "name": "Zhiwei Steven Wu"
      },
      {
        "name": "Maria-Florina Balcan"
      }
    ],
    "summary": "In its most basic form, a Stackelberg game is a two-player game in which a\nleader commits to a (mixed) strategy, and a follower best-responds. Stackelberg\ngames are perhaps one of the biggest success stories of algorithmic game theory\nover the last decade, as algorithms for playing in Stackelberg games have been\ndeployed in many real-world domains including airport security, anti-poaching\nefforts, and cyber-crime prevention. However, these algorithms often fail to\ntake into consideration the additional information available to each player\n(e.g. traffic patterns, weather conditions, network congestion), a salient\nfeature of reality which may significantly affect both players' optimal\nstrategies. We formalize such settings as Stackelberg games with side\ninformation, in which both players observe an external context before playing.\nThe leader then commits to a (possibly context-dependent) strategy, and the\nfollower best-responds to both the leader's strategy and the context. We focus\non the online setting in which a sequence of followers arrive over time, and\nthe context may change from round-to-round. In sharp contrast to the\nnon-contextual version, we show that it is impossible for the leader to achieve\ngood performance (measured by regret) in the full adversarial setting (i.e.,\nwhen both the context and the follower are chosen by an adversary). However, it\nturns out that a little bit of randomness goes a long way. Motivated by our\nimpossibility result, we show that no-regret learning is possible in two\nnatural relaxations: the setting in which the sequence of followers is chosen\nstochastically and the sequence of contexts is adversarial, and the setting in\nwhich the sequence of contexts is stochastic and the sequence of followers is\nchosen by an adversary.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08576v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08576v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08576v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08573v1",
    "updated": "2024-02-13T16:21:18+00:00",
    "published": "2024-02-13T16:21:18+00:00",
    "title": "Two Tales of Single-Phase Contrastive Hebbian Learning",
    "authors": [
      {
        "name": "Rasmus Kj\u00e6r H\u00f8ier"
      },
      {
        "name": "Christopher Zach"
      }
    ],
    "summary": "The search for \"biologically plausible\" learning algorithms has converged on\nthe idea of representing gradients as activity differences. However, most\napproaches require a high degree of synchronization (distinct phases during\nlearning) and introduce substantial computational overhead, which raises doubts\nregarding their biological plausibility as well as their potential utility for\nneuromorphic computing. Furthermore, they commonly rely on applying\ninfinitesimal perturbations (nudges) to output units, which is impractical in\nnoisy environments. Recently it has been shown that by modelling artificial\nneurons as dyads with two oppositely nudged compartments, it is possible for a\nfully local learning algorithm named ``dual propagation'' to bridge the\nperformance gap to backpropagation, without requiring separate learning phases\nor infinitesimal nudging. However, the algorithm has the drawback that its\nnumerical stability relies on symmetric nudging, which may be restrictive in\nbiological and analog implementations. In this work we first provide a solid\nfoundation for the objective underlying the dual propagation method, which also\nreveals a surprising connection with adversarial robustness. Second, we\ndemonstrate how dual propagation is related to a particular adjoint state\nmethod, which is stable regardless of asymmetric nudging.",
    "comment": "18 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08573v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08573v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08573v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08570v1",
    "updated": "2024-02-13T16:14:32+00:00",
    "published": "2024-02-13T16:14:32+00:00",
    "title": "Online Foundation Model Selection in Robotics",
    "authors": [
      {
        "name": "Po-han Li"
      },
      {
        "name": "Oyku Selin Toprak"
      },
      {
        "name": "Aditya Narayanan"
      },
      {
        "name": "Ufuk Topcu"
      },
      {
        "name": "Sandeep Chinchali"
      }
    ],
    "summary": "Foundation models have recently expanded into robotics after excelling in\ncomputer vision and natural language processing. The models are accessible in\ntwo ways: open-source or paid, closed-source options. Users with access to both\nface a problem when deciding between effective yet costly closed-source models\nand free but less powerful open-source alternatives. We call it the model\nselection problem. Existing supervised-learning methods are impractical due to\nthe high cost of collecting extensive training data from closed-source models.\nHence, we focus on the online learning setting where algorithms learn while\ncollecting data, eliminating the need for large pre-collected datasets. We thus\nformulate a user-centric online model selection problem and propose a novel\nsolution that combines an open-source encoder to output context and an online\nlearning algorithm that processes this context. The encoder distills vast data\ndistributions into low-dimensional features, i.e., the context, without\nadditional training. The online learning algorithm aims to maximize a composite\nreward that includes model performance, execution time, and costs based on the\ncontext extracted from the data. It results in an improved trade-off between\nselecting open-source and closed-source models compared to non-contextual\nmethods, as validated by our theoretical analysis. Experiments across\nlanguage-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open\nX-Embodiment demonstrate real-world applications of the solution. The results\nshow that the solution significantly improves the task success rate by up to\n14%.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08570v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08570v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08570v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08567v1",
    "updated": "2024-02-13T16:06:17+00:00",
    "published": "2024-02-13T16:06:17+00:00",
    "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
    "authors": [
      {
        "name": "Xiangming Gu"
      },
      {
        "name": "Xiaosen Zheng"
      },
      {
        "name": "Tianyu Pang"
      },
      {
        "name": "Chao Du"
      },
      {
        "name": "Qian Liu"
      },
      {
        "name": "Ye Wang"
      },
      {
        "name": "Jing Jiang"
      },
      {
        "name": "Min Lin"
      }
    ],
    "summary": "A multimodal large language model (MLLM) agent can receive instructions,\ncapture images, retrieve histories from memory, and decide which tools to use.\nNonetheless, red-teaming efforts have revealed that adversarial images/prompts\ncan jailbreak an MLLM and cause unaligned behaviors. In this work, we report an\neven more severe safety issue in multi-agent environments, referred to as\ninfectious jailbreak. It entails the adversary simply jailbreaking a single\nagent, and without any further intervention from the adversary, (almost) all\nagents will become infected exponentially fast and exhibit harmful behaviors.\nTo validate the feasibility of infectious jailbreak, we simulate multi-agent\nenvironments containing up to one million LLaVA-1.5 agents, and employ\nrandomized pair-wise chat as a proof-of-concept instantiation for multi-agent\ninteraction. Our results show that feeding an (infectious) adversarial image\ninto the memory of any randomly chosen agent is sufficient to achieve\ninfectious jailbreak. Finally, we derive a simple principle for determining\nwhether a defense mechanism can provably restrain the spread of infectious\njailbreak, but how to design a practical defense that meets this principle\nremains an open question to investigate. Our project page is available at\nhttps://sail-sg.github.io/Agent-Smith/.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08567v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08567v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08567v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08565v1",
    "updated": "2024-02-13T16:05:51+00:00",
    "published": "2024-02-13T16:05:51+00:00",
    "title": "Artificial Intelligence for Literature Reviews: Opportunities and Challenges",
    "authors": [
      {
        "name": "Francisco Bolanos"
      },
      {
        "name": "Angelo Salatino"
      },
      {
        "name": "Francesco Osborne"
      },
      {
        "name": "Enrico Motta"
      }
    ],
    "summary": "This manuscript presents a comprehensive review of the use of Artificial\nIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous\nand organised methodology that assesses and integrates previous research on a\ngiven topic. Numerous tools have been developed to assist and partially\nautomate the SLR process. The increasing role of AI in this field shows great\npotential in providing more effective support for researchers, moving towards\nthe semi-automatic creation of literature reviews. Our study focuses on how AI\ntechniques are applied in the semi-automation of SLRs, specifically in the\nscreening and extraction phases. We examine 21 leading SLR tools using a\nframework that combines 23 traditional features with 11 AI features. We also\nanalyse 11 recent tools that leverage large language models for searching the\nliterature and assisting academic writing. Finally, the paper discusses current\ntrends in the field, outlines key research challenges, and suggests directions\nfor future research.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08565v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08565v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08565v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08563v1",
    "updated": "2024-02-13T16:04:41+00:00",
    "published": "2024-02-13T16:04:41+00:00",
    "title": "Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator",
    "authors": [
      {
        "name": "Amartya Mukherjee"
      },
      {
        "name": "Melissa M. Stadt"
      },
      {
        "name": "Lena Podina"
      },
      {
        "name": "Mohammad Kohandel"
      },
      {
        "name": "Jun Liu"
      }
    ],
    "summary": "Diffusion models have emerged as a promising class of generative models that\nmap noisy inputs to realistic images. More recently, they have been employed to\ngenerate solutions to partial differential equations (PDEs). However, they\nstill struggle with inverse problems in the Laplacian operator, for instance,\nthe Poisson equation, because the eigenvalues that are large in magnitude\namplify the measurement noise. This paper presents a novel approach for the\ninverse and forward solution of PDEs through the use of denoising diffusion\nrestoration models (DDRM). DDRMs were used in linear inverse problems to\nrestore original clean signals by exploiting the singular value decomposition\n(SVD) of the linear operator. Equivalently, we present an approach to restore\nthe solution and the parameters in the Poisson equation by exploiting the\neigenvalues and the eigenfunctions of the Laplacian operator. Our results show\nthat using denoising diffusion restoration significantly improves the\nestimation of the solution and parameters. Our research, as a result, pioneers\nthe integration of diffusion models with the principles of underlying physics\nto solve PDEs.",
    "comment": "29 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV",
      "math.AP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08563v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08563v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08563v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08562v1",
    "updated": "2024-02-13T16:04:21+00:00",
    "published": "2024-02-13T16:04:21+00:00",
    "title": "Higher Layers Need More LoRA Experts",
    "authors": [
      {
        "name": "Chongyang Gao"
      },
      {
        "name": "Kezhen Chen"
      },
      {
        "name": "Jinmeng Rao"
      },
      {
        "name": "Baochen Sun"
      },
      {
        "name": "Ruibo Liu"
      },
      {
        "name": "Daiyi Peng"
      },
      {
        "name": "Yawen Zhang"
      },
      {
        "name": "Xiaoyuan Guo"
      },
      {
        "name": "Jie Yang"
      },
      {
        "name": "VS Subrahmanian"
      }
    ],
    "summary": "Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA)\noffer training efficiency on Large Language Models, but their impact on model\nperformance remains limited. Recent efforts integrate LoRA and\nMixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite\npromising results, research on improving the efficiency of LoRA with MoE is\nstill in its early stages. Recent studies have shown that experts in the MoE\narchitecture have different strengths and also exhibit some redundancy. Does\nthis statement also apply to parameter-efficient MoE? In this paper, we\nintroduce a novel parameter-efficient MoE method,\n\\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert\n\\textbf{A}llocation (MoLA)} for Transformer-based models, where each model\nlayer has the flexibility to employ a varying number of LoRA experts. We\ninvestigate several architectures with varying layer-wise expert\nconfigurations. Experiments on six well-known NLP and commonsense QA benchmarks\ndemonstrate that MoLA achieves equal or superior performance compared to all\nbaselines. We find that allocating more LoRA experts to higher layers further\nenhances the effectiveness of models with a certain number of experts in total.\nWith much fewer parameters, this allocation strategy outperforms the setting\nwith the same number of experts in every layer. This work can be widely used as\na plug-and-play parameter-efficient tuning approach for various applications.\nThe code is available at https://github.com/GCYZSL/MoLA.",
    "comment": "The code is available at https://github.com/GCYZSL/MoLA",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08562v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08562v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08562v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08552v1",
    "updated": "2024-02-13T15:55:41+00:00",
    "published": "2024-02-13T15:55:41+00:00",
    "title": "Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases",
    "authors": [
      {
        "name": "Ziyi Zhang"
      },
      {
        "name": "Sen Zhang"
      },
      {
        "name": "Yibing Zhan"
      },
      {
        "name": "Yong Luo"
      },
      {
        "name": "Yonggang Wen"
      },
      {
        "name": "Dacheng Tao"
      }
    ],
    "summary": "Bridging the gap between diffusion models and human preferences is crucial\nfor their integration into practical generative workflows. While optimizing\ndownstream reward models has emerged as a promising alignment strategy,\nconcerns arise regarding the risk of excessive optimization with learned reward\nmodels, which potentially compromises ground-truth performance. In this work,\nwe confront the reward overoptimization problem in diffusion model alignment\nthrough the lenses of both inductive and primacy biases. We first identify the\ndivergence of current methods from the temporal inductive bias inherent in the\nmulti-step denoising process of diffusion models as a potential source of\noveroptimization. Then, we surprisingly discover that dormant neurons in our\ncritic model act as a regularization against overoptimization, while active\nneurons reflect primacy bias in this setting. Motivated by these observations,\nwe propose Temporal Diffusion Policy Optimization with critic active neuron\nReset (TDPO-R), a policy gradient algorithm that exploits the temporal\ninductive bias of intermediate timesteps, along with a novel reset strategy\nthat targets active neurons to counteract the primacy bias. Empirical results\ndemonstrate the superior efficacy of our algorithms in mitigating reward\noveroptimization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08552v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08552v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08552v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08547v1",
    "updated": "2024-02-13T15:53:09+00:00",
    "published": "2024-02-13T15:53:09+00:00",
    "title": "Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting",
    "authors": [
      {
        "name": "Simina Br\u00e2nzei"
      },
      {
        "name": "MohammadTaghi Hajiaghayi"
      },
      {
        "name": "Reed Phillips"
      },
      {
        "name": "Suho Shin"
      },
      {
        "name": "Kun Wang"
      }
    ],
    "summary": "We consider the setting of repeated fair division between two players,\ndenoted Alice and Bob, with private valuations over a cake. In each round, a\nnew cake arrives, which is identical to the ones in previous rounds. Alice cuts\nthe cake at a point of her choice, while Bob chooses the left piece or the\nright piece, leaving the remainder for Alice. We consider two versions:\nsequential, where Bob observes Alice's cut point before choosing left/right,\nand simultaneous, where he only observes her cut point after making his choice.\nThe simultaneous version was first considered by Aumann and Maschler (1995).\n  We observe that if Bob is almost myopic and chooses his favorite piece too\noften, then he can be systematically exploited by Alice through a strategy akin\nto a binary search. This strategy allows Alice to approximate Bob's preferences\nwith increasing precision, thereby securing a disproportionate share of the\nresource over time.\n  We analyze the limits of how much a player can exploit the other one and show\nthat fair utility profiles are in fact achievable. Specifically, the players\ncan enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every\ntrajectory of play, by keeping the other player's utility to approximately\n$1/2$ on average while guaranteeing they themselves get at least approximately\n$1/2$ on average. We show this theorem using a connection with Blackwell\napproachability.\n  Finally, we analyze a natural dynamic known as fictitious play, where players\nbest respond to the empirical distribution of the other player. We show that\nfictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a\nrate of $O(1/\\sqrt{T})$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08547v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08547v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08547v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08540v1",
    "updated": "2024-02-13T15:45:20+00:00",
    "published": "2024-02-13T15:45:20+00:00",
    "title": "Generative VS non-Generative Models in Engineering Shape Optimization",
    "authors": [
      {
        "name": "Muhammad Usama"
      },
      {
        "name": "Zahid Masood"
      },
      {
        "name": "Shahroz Khan"
      },
      {
        "name": "Konstantinos Kostas"
      },
      {
        "name": "Panagiotis Kaklis"
      }
    ],
    "summary": "In this work, we perform a systematic comparison of the effectiveness and\nefficiency of generative and non-generative models in constructing design\nspaces for novel and efficient design exploration and shape optimization. We\napply these models in the case of airfoil/hydrofoil design and conduct the\ncomparison on the resulting design spaces. A conventional Generative\nAdversarial Network (GAN) and a state-of-the-art generative model, the\nPerformance-Augmented Diverse Generative Adversarial Network (PaDGAN), are\njuxtaposed with a linear non-generative model based on the coupling of the\nKarhunen-Lo\\`eve Expansion and a physics-informed Shape Signature Vector\n(SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding\nand a physics-augmented design space, non-generative models have the potential\nto cost-effectively generate high-performing valid designs with enhanced\ncoverage of the design space. In this work, both approaches are applied to two\nlarge foil profile datasets comprising real-world and artificial designs\ngenerated through either a profile-generating parametric model or deep-learning\napproach. These datasets are further enriched with integral properties of their\nmembers' shapes as well as physics-informed parameters. Our results illustrate\nthat the design spaces constructed by the non-generative model outperform the\ngenerative model in terms of design validity, generating robust latent spaces\nwith none or significantly fewer invalid designs when compared to generative\nmodels. We aspire that these findings will aid the engineering design community\nin making informed decisions when constructing designs spaces for shape\noptimization, as we have show that under certain conditions computationally\ninexpensive approaches can closely match or even outperform state-of-the art\ngenerative models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08540v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08540v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08540v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08539v1",
    "updated": "2024-02-13T15:43:30+00:00",
    "published": "2024-02-13T15:43:30+00:00",
    "title": "Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning",
    "authors": [
      {
        "name": "Mingyang Li"
      },
      {
        "name": "Hongyu Liu"
      },
      {
        "name": "Yixuan Li"
      },
      {
        "name": "Zejun Wang"
      },
      {
        "name": "Yuan Yuan"
      },
      {
        "name": "Honglin Dai"
      }
    ],
    "summary": "This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI)\ndataset and aims to explore early detection and disease progression in\nAlzheimer's disease (AD). We employ innovative data preprocessing strategies,\nincluding the use of the random forest algorithm to fill missing data and the\nhandling of outliers and invalid data, thereby fully mining and utilizing these\nlimited data resources. Through Spearman correlation coefficient analysis, we\nidentify some features strongly correlated with AD diagnosis. We build and test\nthree machine learning models using these features: random forest, XGBoost, and\nsupport vector machine (SVM). Among them, the XGBoost model performs the best\nin terms of diagnostic performance, achieving an accuracy of 91%. Overall, this\nstudy successfully overcomes the challenge of missing data and provides\nvaluable insights into early detection of Alzheimer's disease, demonstrating\nits unique research value and practical significance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08539v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08539v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08539v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08530v1",
    "updated": "2024-02-13T15:35:24+00:00",
    "published": "2024-02-13T15:35:24+00:00",
    "title": "A Distributional Analogue to the Successor Representation",
    "authors": [
      {
        "name": "Harley Wiltzer"
      },
      {
        "name": "Jesse Farebrother"
      },
      {
        "name": "Arthur Gretton"
      },
      {
        "name": "Yunhao Tang"
      },
      {
        "name": "Andr\u00e9 Barreto"
      },
      {
        "name": "Will Dabney"
      },
      {
        "name": "Marc G. Bellemare"
      },
      {
        "name": "Mark Rowland"
      }
    ],
    "summary": "This paper contributes a new approach for distributional reinforcement\nlearning which elucidates a clean separation of transition structure and reward\nin the learning process. Analogous to how the successor representation (SR)\ndescribes the expected consequences of behaving according to a given policy,\nour distributional successor measure (SM) describes the distributional\nconsequences of this behaviour. We formulate the distributional SM as a\ndistribution over distributions and provide theory connecting it with\ndistributional and model-based reinforcement learning. Moreover, we propose an\nalgorithm that learns the distributional SM from data by minimizing a two-level\nmaximum mean discrepancy. Key to our method are a number of algorithmic\ntechniques that are independently valuable for learning generative models of\nstate. As an illustration of the usefulness of the distributional SM, we show\nthat it enables zero-shot risk-sensitive policy evaluation in a way that was\nnot previously possible.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08530v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08530v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08530v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08529v1",
    "updated": "2024-02-13T15:34:39+00:00",
    "published": "2024-02-13T15:34:39+00:00",
    "title": "Approximately Piecewise E(3) Equivariant Point Networks",
    "authors": [
      {
        "name": "Matan Atzmon"
      },
      {
        "name": "Jiahui Huang"
      },
      {
        "name": "Francis Williams"
      },
      {
        "name": "Or Litany"
      }
    ],
    "summary": "Integrating a notion of symmetry into point cloud neural networks is a\nprovably effective way to improve their generalization capability. Of\nparticular interest are $E(3)$ equivariant point cloud networks where Euclidean\ntransformations applied to the inputs are preserved in the outputs. Recent\nefforts aim to extend networks that are $E(3)$ equivariant, to accommodate\ninputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In\npractical settings, however, the partitioning into individually transforming\nregions is unknown a priori. Errors in the partition prediction would\nunavoidably map to errors in respecting the true input symmetry. Past works\nhave proposed different ways to predict the partition, which may exhibit\nuncontrolled errors in their ability to maintain equivariance to the actual\npartition. To this end, we introduce APEN: a general framework for constructing\napproximate piecewise-$E(3)$ equivariant point networks. Our primary insight is\nthat functions that are equivariant with respect to a finer partition will also\nmaintain equivariance in relation to the true partition. Leveraging this\nobservation, we propose a design where the equivariance approximation error at\neach layers can be bounded solely in terms of (i) uncertainty quantification of\nthe partition prediction, and (ii) bounds on the probability of failing to\nsuggest a proper subpartition of the ground truth one. We demonstrate the\neffectiveness of APEN using two data types exemplifying part-based symmetry:\n(i) real-world scans of room scenes containing multiple furniture-type objects;\nand, (ii) human motions, characterized by articulated parts exhibiting rigid\nmovement. Our empirical results demonstrate the advantage of integrating\npiecewise $E(3)$ symmetry into network design, showing a distinct improvement\nin generalization compared to prior works for both classification and\nsegmentation tasks.",
    "comment": "ICLR 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08529v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08529v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08529v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08526v1",
    "updated": "2024-02-13T15:29:50+00:00",
    "published": "2024-02-13T15:29:50+00:00",
    "title": "Concept-1K: A Novel Benchmark for Instance Incremental Learning",
    "authors": [
      {
        "name": "Junhao Zheng"
      },
      {
        "name": "Shengjie Qiu"
      },
      {
        "name": "Qianli Ma"
      }
    ],
    "summary": "Incremental learning (IL) is essential to realize the human-level\nintelligence in the neural network. However, existing IL scenarios and datasets\nare unqualified for assessing forgetting in PLMs, giving an illusion that PLMs\ndo not suffer from catastrophic forgetting. To this end, we propose a\nchallenging IL scenario called instance-incremental learning (IIL) and a novel\ndataset called Concept-1K, which supports an order of magnitude larger IL\nsteps. Based on the experiments on Concept-1K, we reveal that billion-parameter\nPLMs still suffer from catastrophic forgetting, and the forgetting is affected\nby both model scale, pretraining, and buffer size. Furthermore, existing IL\nmethods and a popular finetuning technique, LoRA, fail to achieve satisfactory\nperformance. Our study provides a novel scenario for future studies to explore\nthe catastrophic forgetting of PLMs and encourage more powerful techniques to\nbe designed for alleviating the forgetting in PLMs. The data, code and scripts\nare publicly available at\nhttps://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08526v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08526v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08526v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08522v1",
    "updated": "2024-02-13T15:24:46+00:00",
    "published": "2024-02-13T15:24:46+00:00",
    "title": "Fairness Auditing with Multi-Agent Collaboration",
    "authors": [
      {
        "name": "Martijn de Vos"
      },
      {
        "name": "Akash Dhasade"
      },
      {
        "name": "Jade Garcia Bourr\u00e9e"
      },
      {
        "name": "Anne-Marie Kermarrec"
      },
      {
        "name": "Erwan Le Merrer"
      },
      {
        "name": "Benoit Rottembourg"
      },
      {
        "name": "Gilles Tredan"
      }
    ],
    "summary": "Existing work in fairness audits assumes that agents operate independently.\nIn this paper, we consider the case of multiple agents auditing the same\nplatform for different tasks. Agents have two levers: their collaboration\nstrategy, with or without coordination beforehand, and their sampling method.\nWe theoretically study their interplay when agents operate independently or\ncollaborate. We prove that, surprisingly, coordination can sometimes be\ndetrimental to audit accuracy, whereas uncoordinated collaboration generally\nyields good results. Experimentation on real-world datasets confirms this\nobservation, as the audit accuracy of uncoordinated collaboration matches that\nof collaborative optimal sampling.",
    "comment": "21 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08522v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08522v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08522v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08514v1",
    "updated": "2024-02-13T15:10:30+00:00",
    "published": "2024-02-13T15:10:30+00:00",
    "title": "Counterfactual Influence in Markov Decision Processes",
    "authors": [
      {
        "name": "Milad Kazemi"
      },
      {
        "name": "Jessica Lally"
      },
      {
        "name": "Ekaterina Tishchenko"
      },
      {
        "name": "Hana Chockler"
      },
      {
        "name": "Nicola Paoletti"
      }
    ],
    "summary": "Our work addresses a fundamental problem in the context of counterfactual\ninference for Markov Decision Processes (MDPs). Given an MDP path $\\tau$, this\nkind of inference allows us to derive counterfactual paths $\\tau'$ describing\nwhat-if versions of $\\tau$ obtained under different action sequences than those\nobserved in $\\tau$. However, as the counterfactual states and actions deviate\nfrom the observed ones over time, the observation $\\tau$ may no longer\ninfluence the counterfactual world, meaning that the analysis is no longer\ntailored to the individual observation, resulting in interventional outcomes\nrather than counterfactual ones. Even though this issue specifically affects\nthe popular Gumbel-max structural causal model used for MDP counterfactuals, it\nhas remained overlooked until now. In this work, we introduce a formal\ncharacterisation of influence based on comparing counterfactual and\ninterventional distributions. We devise an algorithm to construct\ncounterfactual models that automatically satisfy influence constraints.\nLeveraging such models, we derive counterfactual policies that are not just\noptimal for a given reward structure but also remain tailored to the observed\npath. Even though there is an unavoidable trade-off between policy optimality\nand strength of influence constraints, our experiments demonstrate that it is\npossible to derive (near-)optimal policies while remaining under the influence\nof the observation.",
    "comment": "12 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08514v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08514v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08514v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08511v1",
    "updated": "2024-02-13T15:05:54+00:00",
    "published": "2024-02-13T15:05:54+00:00",
    "title": "Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the Unknown",
    "authors": [
      {
        "name": "Cedric Derstroff"
      },
      {
        "name": "Jannis Brugger"
      },
      {
        "name": "Jannis Bl\u00fcml"
      },
      {
        "name": "Mira Mezini"
      },
      {
        "name": "Stefan Kramer"
      },
      {
        "name": "Kristian Kersting"
      }
    ],
    "summary": "Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast\namount of applications. It strategically allocates computational resources to\nfocus on promising segments of the search tree, making it a very attractive\nsearch algorithm in large search spaces. However, it often expends its limited\nresources on reevaluating previously explored regions when they remain the most\npromising path. Our proposed methodology, denoted as AmEx-MCTS, solves this\nproblem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the\ndecoupling of value updates, visit count updates, and the selected path during\nthe tree search, thereby enabling the exclusion of already explored subtrees or\nleaves. This segregation preserves the utility of visit counts for both\nexploration-exploitation balancing and quality metrics within MCTS. The\nresultant augmentation facilitates in a considerably broader search using\nidentical computational resources, preserving the essential characteristics of\nMCTS. The expanded coverage not only yields more precise estimations but also\nproves instrumental in larger and more complex problems. Our empirical\nevaluation demonstrates the superior performance of AmEx-MCTS, surpassing\nclassical MCTS and related approaches by a substantial margin.",
    "comment": "10 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08511v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08511v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08511v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08509v1",
    "updated": "2024-02-13T15:04:11+00:00",
    "published": "2024-02-13T15:04:11+00:00",
    "title": "From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)",
    "authors": [
      {
        "name": "Philipp Seifer"
      },
      {
        "name": "Daniel Hern\u00e1ndez"
      },
      {
        "name": "Ralf L\u00e4mmel"
      },
      {
        "name": "Steffen Staab"
      }
    ],
    "summary": "SPARQL CONSTRUCT queries allow for the specification of data processing\npipelines that transform given input graphs into new output graphs. It is now\ncommon to constrain graphs through SHACL shapes allowing users to understand\nwhich data they can expect and which not. However, it becomes challenging to\nunderstand what graph data can be expected at the end of a data processing\npipeline without knowing the particular input data: Shape constraints on the\ninput graph may affect the output graph, but may no longer apply literally, and\nnew shapes may be imposed by the query template. In this paper, we study the\nderivation of shape constraints that hold on all possible output graphs of a\ngiven SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is\nfixed, e.g., being part of a program, whereas the input graphs adhere to input\nshape constraints but may otherwise vary over time and, thus, are mostly\nunknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment\nof SHACL (Simple SHACL). We formally define the problem of deriving the most\nrestrictive set of Simple SHACL shapes that constrain the results from\nevaluating a SCCQ over any input graph restricted by a given set of Simple\nSHACL shapes. We propose and implement an algorithm that statically analyses\ninput SHACL shapes and CONSTRUCT queries and prove its soundness and\ncomplexity.",
    "comment": "19 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DB",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08509v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08509v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08509v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08508v1",
    "updated": "2024-02-13T15:03:02+00:00",
    "published": "2024-02-13T15:03:02+00:00",
    "title": "A PAC-Bayesian Link Between Generalisation and Flat Minima",
    "authors": [
      {
        "name": "Maxime Haddouche"
      },
      {
        "name": "Paul Viallard"
      },
      {
        "name": "Umut Simsekli"
      },
      {
        "name": "Benjamin Guedj"
      }
    ],
    "summary": "Modern machine learning usually involves predictors in the overparametrised\nsetting (number of trained parameters greater than dataset size), and their\ntraining yield not only good performances on training data, but also good\ngeneralisation capacity. This phenomenon challenges many theoretical results,\nand remains an open problem. To reach a better understanding, we provide novel\ngeneralisation bounds involving gradient terms. To do so, we combine the\nPAC-Bayes toolbox with Poincar\\'e and Log-Sobolev inequalities, avoiding an\nexplicit dependency on dimension of the predictor space. Our results highlight\nthe positive influence of \\emph{flat minima} (being minima with a neighbourhood\nnearly minimising the learning problem as well) on generalisation performances,\ninvolving directly the benefits of the optimisation phase.",
    "comment": "We provide novel PAC-Bayesian generalisation bounds involving\n  gradient norms and being interpretable under the lens of flat minima",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08508v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08508v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08508v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08502v1",
    "updated": "2024-02-13T14:59:19+00:00",
    "published": "2024-02-13T14:59:19+00:00",
    "title": "Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea",
    "authors": [
      {
        "name": "Hanna Krasowski"
      },
      {
        "name": "Matthias Althoff"
      }
    ],
    "summary": "Autonomous vehicles have to obey traffic rules. These rules are often\nformalized using temporal logic, resulting in constraints that are hard to\nsolve using optimization-based motion planners. Reinforcement Learning (RL) is\na promising method to find motion plans adhering to temporal logic\nspecifications. However, vanilla RL algorithms are based on random exploration,\nwhich is inherently unsafe. To address this issue, we propose a provably safe\nRL approach that always complies with traffic rules. As a specific application\narea, we consider vessels on the open sea, which must adhere to the Convention\non the International Regulations for Preventing Collisions at Sea (COLREGS). We\nintroduce an efficient verification approach that determines the compliance of\nactions with respect to the COLREGS formalized using temporal logic. Our action\nverification is integrated into the RL process so that the agent only selects\nverified actions. In contrast to agents that only integrate the traffic rule\ninformation in the reward function, our provably safe agent always complies\nwith the formalized rules in critical maritime traffic situations and, thus,\nnever causes a collision.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08502v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08502v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08502v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08496v1",
    "updated": "2024-02-13T14:51:45+00:00",
    "published": "2024-02-13T14:51:45+00:00",
    "title": "A Systematic Review of Data-to-Text NLG",
    "authors": [
      {
        "name": "Chinonso Cynthia Osuji"
      },
      {
        "name": "Thiago Castro Ferreira"
      },
      {
        "name": "Brian Davis"
      }
    ],
    "summary": "This systematic review aims to provide a comprehensive analysis of the state\nof data-to-text generation research, focusing on identifying research gaps,\noffering future directions, and addressing challenges found during the review.\nWe thoroughly examined the literature, including approaches, datasets,\nevaluation metrics, applications, multilingualism, and hallucination mitigation\nmeasures. Our review provides a roadmap for future research in this rapidly\nevolving field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08496v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08496v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08496v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08493v1",
    "updated": "2024-02-13T14:41:28+00:00",
    "published": "2024-02-13T14:41:28+00:00",
    "title": "Sparsity via Sparse Group $k$-max Regularization",
    "authors": [
      {
        "name": "Qinghua Tao"
      },
      {
        "name": "Xiangming Xi"
      },
      {
        "name": "Jun Xu"
      },
      {
        "name": "Johan A. K. Suykens"
      }
    ],
    "summary": "For the linear inverse problem with sparsity constraints, the $l_0$\nregularized problem is NP-hard, and existing approaches either utilize greedy\nalgorithms to find almost-optimal solutions or to approximate the $l_0$\nregularization with its convex counterparts. In this paper, we propose a novel\nand concise regularization, namely the sparse group $k$-max regularization,\nwhich can not only simultaneously enhance the group-wise and in-group sparsity,\nbut also casts no additional restraints on the magnitude of variables in each\ngroup, which is especially important for variables at different scales, so that\nit approximate the $l_0$ norm more closely. We also establish an iterative soft\nthresholding algorithm with local optimality conditions and complexity analysis\nprovided. Through numerical experiments on both synthetic and real-world\ndatasets, we verify the effectiveness and flexibility of the proposed method.",
    "comment": "7 pages, accepted to American Control Conference 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08493v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08493v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08493v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08492v1",
    "updated": "2024-02-13T14:38:12+00:00",
    "published": "2024-02-13T14:38:12+00:00",
    "title": "The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale",
    "authors": [
      {
        "name": "Xiaoqiang Liu"
      },
      {
        "name": "Yubin Wang"
      },
      {
        "name": "Zicheng Huang"
      },
      {
        "name": "Boming Xu"
      },
      {
        "name": "Yilin Zeng"
      },
      {
        "name": "Xinqi Chen"
      },
      {
        "name": "Zilong Wang"
      },
      {
        "name": "Enning Yang"
      },
      {
        "name": "Xiaoxuan Lei"
      },
      {
        "name": "Yisen Huang"
      },
      {
        "name": "Xiaobo Liu"
      }
    ],
    "summary": "Background: Colonoscopy, a crucial diagnostic tool in gastroenterology,\ndepends heavily on superior bowel preparation. ChatGPT, a large language model\nwith emergent intelligence which also exhibits potential in medical\napplications. This study aims to assess the accuracy and consistency of ChatGPT\nin using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment.\nMethods: We retrospectively collected 233 colonoscopy images from 2020 to 2023.\nThese images were evaluated using the BBPS by 3 senior endoscopists and 3\nnovice endoscopists. Additionally, ChatGPT also assessed these images, having\nbeen divided into three groups and undergone specific Fine-tuning. Consistency\nwas evaluated through two rounds of testing. Results: In the initial round,\nChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists'\naccuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and\n0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT\nshows promise in bowel preparation scoring, it currently does not match the\naccuracy and consistency of experienced endoscopists. Future research should\nfocus on in-depth Fine-tuning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08492v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08492v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08492v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08491v1",
    "updated": "2024-02-13T14:36:46+00:00",
    "published": "2024-02-13T14:36:46+00:00",
    "title": "Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming",
    "authors": [
      {
        "name": "Andrzej Mizera"
      },
      {
        "name": "Jakub Zarzycki"
      }
    ],
    "summary": "Cellular reprogramming can be used for both the prevention and cure of\ndifferent diseases. However, the efficiency of discovering reprogramming\nstrategies with classical wet-lab experiments is hindered by lengthy time\ncommitments and high costs. In this study, we develop a~novel computational\nframework based on deep reinforcement learning that facilitates the\nidentification of reprogramming strategies. For this aim, we formulate\na~control problem in the context of cellular reprogramming for the frameworks\nof BNs and PBNs under the asynchronous update mode. Furthermore, we introduce\nthe notion of a~pseudo-attractor and a~procedure for identification of\npseudo-attractor state during training. Finally, we devise a~computational\nframework for solving the control problem, which we test on a~number of\ndifferent models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.MN",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08491v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08491v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08491v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08480v1",
    "updated": "2024-02-13T14:13:17+00:00",
    "published": "2024-02-13T14:13:17+00:00",
    "title": "Revealing Decurve Flows for Generalized Graph Propagation",
    "authors": [
      {
        "name": "Chen Lin"
      },
      {
        "name": "Liheng Ma"
      },
      {
        "name": "Yiyang Chen"
      },
      {
        "name": "Wanli Ouyang"
      },
      {
        "name": "Michael M. Bronstein"
      },
      {
        "name": "Philip H. S. Torr"
      }
    ],
    "summary": "This study addresses the limitations of the traditional analysis of\nmessage-passing, central to graph learning, by defining {\\em\n\\textbf{generalized propagation}} with directed and weighted graphs. The\nsignificance manifest in two ways. \\textbf{Firstly}, we propose {\\em\nGeneralized Propagation Neural Networks} (\\textbf{GPNNs}), a framework that\nunifies most propagation-based graph neural networks. By generating\ndirected-weighted propagation graphs with adjacency function and connectivity\nfunction, GPNNs offer enhanced insights into attention mechanisms across\nvarious graph models. We delve into the trade-offs within the design space with\nempirical experiments and emphasize the crucial role of the adjacency function\nfor model expressivity via theoretical analysis. \\textbf{Secondly}, we propose\nthe {\\em Continuous Unified Ricci Curvature} (\\textbf{CURC}), an extension of\ncelebrated {\\em Ollivier-Ricci Curvature} for directed and weighted graphs.\nTheoretically, we demonstrate that CURC possesses continuity, scale invariance,\nand a lower bound connection with the Dirichlet isoperimetric constant\nvalidating bottleneck analysis for GPNNs. We include a preliminary exploration\nof learned propagation patterns in datasets, a first in the field. We observe\nan intriguing ``{\\em \\textbf{decurve flow}}'' - a curvature reduction during\ntraining for models with learnable propagation, revealing the evolution of\npropagation over time and a deeper connection to over-smoothing and bottleneck\ntrade-off.",
    "comment": "15 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.DG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08480v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08480v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08480v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08473v1",
    "updated": "2024-02-13T14:07:49+00:00",
    "published": "2024-02-13T14:07:49+00:00",
    "title": "Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models",
    "authors": [
      {
        "name": "Shaeke Salman"
      },
      {
        "name": "Md Montasir Bin Shams"
      },
      {
        "name": "Xiuwen Liu"
      },
      {
        "name": "Lingjiong Zhu"
      }
    ],
    "summary": "Transformer-based models have dominated natural language processing and other\nareas in the last few years due to their superior (zero-shot) performance on\nbenchmark datasets. However, these models are poorly understood due to their\ncomplexity and size. While probing-based methods are widely used to understand\nspecific properties, the structures of the representation space are not\nsystematically characterized; consequently, it is unclear how such models\ngeneralize and overgeneralize to new inputs beyond datasets. In this paper,\nbased on a new gradient descent optimization method, we are able to explore the\nembedding space of a commonly used vision-language model. Using the Imagenette\ndataset, we show that while the model achieves over 99\\% zero-shot\nclassification performance, it fails systematic evaluations completely. Using a\nlinear approximation, we provide a framework to explain the striking\ndifferences. We have also obtained similar results using a different model to\nsupport that our results are applicable to other transformer models with\ncontinuous inputs. We also propose a robust way to detect the modified images.",
    "comment": "30 pages, 30 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08473v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08473v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08473v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08472v1",
    "updated": "2024-02-13T14:05:02+00:00",
    "published": "2024-02-13T14:05:02+00:00",
    "title": "Large Language Models for the Automated Analysis of Optimization Algorithms",
    "authors": [
      {
        "name": "Camilo Chac\u00f3n Sartori"
      },
      {
        "name": "Christian Blum"
      },
      {
        "name": "Gabriela Ochoa"
      }
    ],
    "summary": "The ability of Large Language Models (LLMs) to generate high-quality text and\ncode has fuelled their rise in popularity. In this paper, we aim to demonstrate\nthe potential of LLMs within the realm of optimization algorithms by\nintegrating them into STNWeb. This is a web-based tool for the generation of\nSearch Trajectory Networks (STNs), which are visualizations of optimization\nalgorithm behavior. Although visualizations produced by STNWeb can be very\ninformative for algorithm designers, they often require a certain level of\nprior knowledge to be interpreted. In an attempt to bridge this knowledge gap,\nwe have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive\nwritten reports, complemented by automatically generated plots, thereby\nenhancing the user experience and reducing the barriers to the adoption of this\ntool by the research community. Moreover, our approach can be expanded to other\ntools from the optimization community, showcasing the versatility and potential\nof LLMs in this field.",
    "comment": "Submitted to the GECCO 2024 conference",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08472v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08472v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08472v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08470v1",
    "updated": "2024-02-13T14:00:59+00:00",
    "published": "2024-02-13T14:00:59+00:00",
    "title": "Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale",
    "authors": [
      {
        "name": "Yangxin Fan"
      },
      {
        "name": "Raymond Wieser"
      },
      {
        "name": "Laura Bruckman"
      },
      {
        "name": "Roger French"
      },
      {
        "name": "Yinghui Wu"
      }
    ],
    "summary": "We propose a novel Spatio-Temporal Graph Neural Network empowered trend\nanalysis approach (ST-GTrend) to perform fleet-level performance degradation\nanalysis for Photovoltaic (PV) power networks. PV power stations have become an\nintegral component to the global sustainable energy production landscape.\nAccurately estimating the performance of PV systems is critical to their\nfeasibility as a power generation technology and as a financial asset. One of\nthe most challenging problems in assessing the Levelized Cost of Energy (LCOE)\nof a PV system is to understand and estimate the long-term Performance Loss\nRate (PLR) for large fleets of PV inverters. ST-GTrend integrates\nspatio-temporal coherence and graph attention to separate PLR as a long-term\n\"aging\" trend from multiple fluctuation terms in the PV input data. To cope\nwith diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled\ngraph autoencoder array to extract aging and fluctuation terms simultaneously.\nST-GTrend imposes flatness and smoothness regularization to ensure the\ndisentanglement between aging and fluctuation. To scale the analysis to large\nPV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate\nthe training and inference of ST-GTrend. We have evaluated ST-GTrend on three\nlarge-scale PV datasets, spanning a time period of 10 years. Our results show\nthat ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean\nDistances by 34.74% and 33.66% compared to the SOTA methods. Our results\ndemonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We\nfurther verify the generality and effectiveness of ST-GTrend for trend analysis\nusing financial and economic datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08470v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08470v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08470v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08468v1",
    "updated": "2024-02-13T13:54:47+00:00",
    "published": "2024-02-13T13:54:47+00:00",
    "title": "ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System",
    "authors": [
      {
        "name": "Tommaso Puccetti"
      },
      {
        "name": "Simone Nardi"
      },
      {
        "name": "Cosimo Cinquilli"
      },
      {
        "name": "Tommaso Zoppi"
      },
      {
        "name": "Andrea Ceccarelli"
      }
    ],
    "summary": "Most of the intrusion detection datasets to research machine learning-based\nintrusion detection systems (IDSs) are devoted to cyber-only systems, and they\ntypically collect data from one architectural layer. Additionally, often the\nattacks are generated in dedicated attack sessions, without reproducing the\nrealistic alternation and overlap of normal and attack actions. We present a\ndataset for intrusion detection by performing penetration testing on an\nembedded cyber-physical system built over Robot Operating System 2 (ROS2).\nFeatures are monitored from three architectural layers: the Linux operating\nsystem, the network, and the ROS2 services. The dataset is structured as a time\nseries and describes the expected behavior of the system and its response to\nROS2-specific attacks: it repeatedly alternates periods of attack-free\noperation with periods when a specific attack is being performed. Noteworthy,\nthis allows measuring the time to detect an attacker and the number of\nmalicious activities performed before detection. Also, it allows training an\nintrusion detector to minimize both, by taking advantage of the numerous\nalternating periods of normal and attack operations.",
    "comment": "18 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08468v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08468v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08468v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08466v1",
    "updated": "2024-02-13T13:48:54+00:00",
    "published": "2024-02-13T13:48:54+00:00",
    "title": "Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence",
    "authors": [
      {
        "name": "Cary Coglianese"
      },
      {
        "name": "Colton R. Crum"
      }
    ],
    "summary": "Fervent calls for more robust governance of the harms associated with\nartificial intelligence (AI) are leading to the adoption around the world of\nwhat regulatory scholars have called a management-based approach to regulation.\nRecent initiatives in the United States and Europe, as well as the adoption of\nmajor self-regulatory standards by the International Organization for\nStandardization, share in common a core management-based paradigm. These\nmanagement-based initiatives seek to motivate an increase in human oversight of\nhow AI tools are trained and developed. Refinements and systematization of\nhuman-guided training techniques will thus be needed to fit within this\nemerging era of management-based regulatory paradigm. If taken seriously,\nhuman-guided training can alleviate some of the technical and ethical pressures\non AI, boosting AI performance with human intuition as well as better\naddressing the needs for fairness and effective explainability. In this paper,\nwe discuss the connection between the emerging management-based regulatory\nframeworks governing AI and the need for human oversight during training. We\nbroadly cover some of the technical components involved in human-guided\ntraining and then argue that the kinds of high-stakes use cases for AI that\nappear of most concern to regulators should lean more on human-guided training\nthan on data-only training. We hope to foster a discussion between legal\nscholars and computer scientists involving how to govern a domain of technology\nthat is vast, heterogenous, and dynamic in its applications and risks.",
    "comment": "12 pages, 1 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08466v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08466v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08466v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08450v1",
    "updated": "2024-02-13T13:37:13+00:00",
    "published": "2024-02-13T13:37:13+00:00",
    "title": "Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products",
    "authors": [
      {
        "name": "Guy Bar-Shalom"
      },
      {
        "name": "Beatrice Bevilacqua"
      },
      {
        "name": "Haggai Maron"
      }
    ],
    "summary": "In the realm of Graph Neural Networks (GNNs), two exciting research\ndirections have recently emerged: Subgraph GNNs and Graph Transformers. In this\npaper, we propose an architecture that integrates both approaches, dubbed\nSubgraphormer, which combines the enhanced expressive power, message-passing\nmechanisms, and aggregation schemes from Subgraph GNNs with attention and\npositional encodings, arguably the most important components in Graph\nTransformers. Our method is based on an intriguing new connection we reveal\nbetween Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be\nformulated as Message Passing Neural Networks (MPNNs) operating on a product of\nthe graph with itself. We use this formulation to design our architecture:\nfirst, we devise an attention mechanism based on the connectivity of the\nproduct graph. Following this, we propose a novel and efficient positional\nencoding scheme for Subgraph GNNs, which we derive as a positional encoding for\nthe product graph. Our experimental results demonstrate significant performance\nimprovements over both Subgraph GNNs and Graph Transformers on a wide range of\ndatasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08450v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08450v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08450v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08426v1",
    "updated": "2024-02-13T12:53:18+00:00",
    "published": "2024-02-13T12:53:18+00:00",
    "title": "Frequency-aware Graph Signal Processing for Collaborative Filtering",
    "authors": [
      {
        "name": "Jiafeng Xia"
      },
      {
        "name": "Dongsheng Li"
      },
      {
        "name": "Hansu Gu"
      },
      {
        "name": "Tun Lu"
      },
      {
        "name": "Peng Zhang"
      },
      {
        "name": "Li Shang"
      },
      {
        "name": "Ning Gu"
      }
    ],
    "summary": "Graph Signal Processing (GSP) based recommendation algorithms have recently\nattracted lots of attention due to its high efficiency. However, these methods\nfailed to consider the importance of various interactions that reflect unique\nuser/item characteristics and failed to utilize user and item high-order\nneighborhood information to model user preference, thus leading to sub-optimal\nperformance. To address the above issues, we propose a frequency-aware graph\nsignal processing method (FaGSP) for collaborative filtering. Firstly, we\ndesign a Cascaded Filter Module, consisting of an ideal high-pass filter and an\nideal low-pass filter that work in a successive manner, to capture both unique\nand common user/item characteristics to more accurately model user preference.\nThen, we devise a Parallel Filter Module, consisting of two low-pass filters\nthat can easily capture the hierarchy of neighborhood, to fully utilize\nhigh-order neighborhood information of users/items for more accurate user\npreference modeling. Finally, we combine these two modules via a linear model\nto further improve recommendation accuracy. Extensive experiments on six public\ndatasets demonstrate the superiority of our method from the perspectives of\nprediction accuracy and training efficiency compared with state-of-the-art\nGCN-based recommendation methods and GSP-based recommendation methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08426v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08426v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08426v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08425v1",
    "updated": "2024-02-13T12:52:41+00:00",
    "published": "2024-02-13T12:52:41+00:00",
    "title": "Transfer Operators from Batches of Unpaired Points via Entropic Transport Kernels",
    "authors": [
      {
        "name": "Florian Beier"
      },
      {
        "name": "Hancheng Bi"
      },
      {
        "name": "Cl\u00e9ment Sarrazin"
      },
      {
        "name": "Bernhard Schmitzer"
      },
      {
        "name": "Gabriele Steidl"
      }
    ],
    "summary": "In this paper, we are concerned with estimating the joint probability of\nrandom variables $X$ and $Y$, given $N$ independent observation blocks\n$(\\boldsymbol{x}^i,\\boldsymbol{y}^i)$, $i=1,\\ldots,N$, each of $M$ samples\n$(\\boldsymbol{x}^i,\\boldsymbol{y}^i) = \\bigl((x^i_j, y^i_{\\sigma^i(j)})\n\\bigr)_{j=1}^M$, where $\\sigma^i$ denotes an unknown permutation of i.i.d.\nsampled pairs $(x^i_j,y_j^i)$, $j=1,\\ldots,M$. This means that the internal\nordering of the $M$ samples within an observation block is not known. We derive\na maximum-likelihood inference functional, propose a computationally tractable\napproximation and analyze their properties. In particular, we prove a\n$\\Gamma$-convergence result showing that we can recover the true density from\nempirical approximations as the number $N$ of blocks goes to infinity. Using\nentropic optimal transport kernels, we model a class of hypothesis spaces of\ndensity functions over which the inference functional can be minimized. This\nhypothesis class is particularly suited for approximate inference of transfer\noperators from data. We solve the resulting discrete minimization problem by a\nmodification of the EMML algorithm to take addional transition probability\nconstraints into account and prove the convergence of this algorithm.\nProof-of-concept examples demonstrate the potential of our method.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.DS",
      "37A30, 62G07"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08425v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08425v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08425v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08424v1",
    "updated": "2024-02-13T12:52:02+00:00",
    "published": "2024-02-13T12:52:02+00:00",
    "title": "Conditional Neural Expert Processes for Learning from Demonstration",
    "authors": [
      {
        "name": "Yigit Yildirim"
      },
      {
        "name": "Emre Ugur"
      }
    ],
    "summary": "Learning from Demonstration (LfD) is a widely used technique for skill\nacquisition in robotics. However, demonstrations of the same skill may exhibit\nsignificant variances, or learning systems may attempt to acquire different\nmeans of the same skill simultaneously, making it challenging to encode these\nmotions into movement primitives. To address these challenges, we propose an\nLfD framework, namely the Conditional Neural Expert Processes (CNEP), that\nlearns to assign demonstrations from different modes to distinct expert\nnetworks utilizing the inherent information within the latent space to match\nexperts with the encoded representations. CNEP does not require supervision on\nwhich mode the trajectories belong to. Provided experiments on artificially\ngenerated datasets demonstrate the efficacy of CNEP. Furthermore, we compare\nthe performance of CNEP with another LfD framework, namely Conditional Neural\nMovement Primitives (CNMP), on a range of tasks, including experiments on a\nreal robot. The results reveal enhanced modeling performance for movement\nprimitives, leading to the synthesis of trajectories that more accurately\nreflect those demonstrated by experts, particularly when the model inputs\ninclude intersection points from various trajectories. Additionally, CNEP\noffers improved interpretability and faster convergence by promoting expert\nspecialization. Furthermore, we show that the CNEP model accomplishes obstacle\navoidance tasks with a real manipulator when provided with novel start and\ndestination points, in contrast to the CNMP model, which leads to collisions\nwith the obstacle.",
    "comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. Submitted to Robotics and Automation Letters on\n  February 13, 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08424v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08424v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08424v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08423v1",
    "updated": "2024-02-13T12:50:04+00:00",
    "published": "2024-02-13T12:50:04+00:00",
    "title": "Vehicle Behavior Prediction by Episodic-Memory Implanted NDT",
    "authors": [
      {
        "name": "Peining Shen"
      },
      {
        "name": "Jianwu Fang"
      },
      {
        "name": "Hongkai Yu"
      },
      {
        "name": "Jianru Xue"
      }
    ],
    "summary": "In autonomous driving, predicting the behavior (turning left, stopping, etc.)\nof target vehicles is crucial for the self-driving vehicle to make safe\ndecisions and avoid accidents. Existing deep learning-based methods have shown\nexcellent and accurate performance, but the black-box nature makes it\nuntrustworthy to apply them in practical use. In this work, we explore the\ninterpretability of behavior prediction of target vehicles by an Episodic\nMemory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of\neMem-NDT is constructed by hierarchically clustering the text embedding of\nvehicle behavior descriptions. eMem-NDT is a neural-backed part of a\npre-trained deep learning model by changing the soft-max layer of the deep\nmodel to eMem-NDT, for grouping and aligning the memory prototypes of the\nhistorical vehicle behavior features in training data on a neural decision\ntree. Each leaf node of eMem-NDT is modeled by a neural network for aligning\nthe behavior memory prototypes. By eMem-NDT, we infer each instance in behavior\nprediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching\nthe appropriate leaf node and the links to the root node) and top-down Leaf\nLink Aggregation (LLA) (obtaining the probability of future behaviors of\nvehicles for certain instances). We validate eMem-NDT on BLVD and LOKI\ndatasets, and the results show that our model can obtain a superior performance\nto other methods with clear explainability. The code is available at\nhttps://github.com/JWFangit/eMem-NDT.",
    "comment": "Accepted by ICRA2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08423v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08423v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08423v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08422v1",
    "updated": "2024-02-13T12:49:50+00:00",
    "published": "2024-02-13T12:49:50+00:00",
    "title": "Distribution Estimation under the Infinity Norm",
    "authors": [
      {
        "name": "Aryeh Kontorovich"
      },
      {
        "name": "Amichai Painsky"
      }
    ],
    "summary": "We present novel bounds for estimating discrete probability distributions\nunder the $\\ell_\\infty$ norm. These are nearly optimal in various precise\nsenses, including a kind of instance-optimality. Our data-dependent convergence\nguarantees for the maximum likelihood estimator significantly improve upon the\ncurrently known results. A variety of techniques are utilized and innovated\nupon, including Chernoff-type inequalities and empirical Bernstein bounds. We\nillustrate our results in synthetic and real-world experiments. Finally, we\napply our proposed framework to a basic selective inference problem, where we\nestimate the most frequent probabilities in a sample.",
    "comment": "Distribution Estimation, Probability Estimation, Infinity Norm",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.ST",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08422v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08422v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08422v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08421v1",
    "updated": "2024-02-13T12:49:22+00:00",
    "published": "2024-02-13T12:49:22+00:00",
    "title": "Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins",
    "authors": [
      {
        "name": "Eslam Eldeeb"
      },
      {
        "name": "Houssem Sifaou"
      },
      {
        "name": "Osvaldo Simeone"
      },
      {
        "name": "Mohammad Shehab"
      },
      {
        "name": "Hirley Alves"
      }
    ],
    "summary": "Digital twin (DT) platforms are increasingly regarded as a promising\ntechnology for controlling, optimizing, and monitoring complex engineering\nsystems such as next-generation wireless networks. An important challenge in\nadopting DT solutions is their reliance on data collected offline, lacking\ndirect access to the physical environment. This limitation is particularly\nsevere in multi-agent systems, for which conventional multi-agent reinforcement\n(MARL) requires online interactions with the environment. A direct application\nof online MARL schemes to an offline setting would generally fail due to the\nepistemic uncertainty entailed by the limited availability of data. In this\nwork, we propose an offline MARL scheme for DT-based wireless networks that\nintegrates distributional RL and conservative Q-learning to address the\nenvironment's inherent aleatoric uncertainty and the epistemic uncertainty\narising from limited data. To further exploit the offline data, we adapt the\nproposed scheme to the centralized training decentralized execution framework,\nallowing joint training of the agents' policies. The proposed MARL scheme,\nreferred to as multi-agent conservative quantile regression (MA-CQR) addresses\ngeneral risk-sensitive design criteria and is applied to the trajectory\nplanning problem in drone networks, showcasing its advantages.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08421v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08421v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08421v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08412v1",
    "updated": "2024-02-13T12:29:38+00:00",
    "published": "2024-02-13T12:29:38+00:00",
    "title": "Interacting Particle Systems on Networks: joint inference of the network and the interaction kernel",
    "authors": [
      {
        "name": "Quanjun Lang"
      },
      {
        "name": "Xiong Wang"
      },
      {
        "name": "Fei Lu"
      },
      {
        "name": "Mauro Maggioni"
      }
    ],
    "summary": "Modeling multi-agent systems on networks is a fundamental challenge in a wide\nvariety of disciplines. We jointly infer the weight matrix of the network and\nthe interaction kernel, which determine respectively which agents interact with\nwhich others and the rules of such interactions from data consisting of\nmultiple trajectories. The estimator we propose leads naturally to a non-convex\noptimization problem, and we investigate two approaches for its solution: one\nis based on the alternating least squares (ALS) algorithm; another is based on\na new algorithm named operator regression with alternating least squares\n(ORALS). Both algorithms are scalable to large ensembles of data trajectories.\nWe establish coercivity conditions guaranteeing identifiability and\nwell-posedness. The ALS algorithm appears statistically efficient and robust\neven in the small data regime but lacks performance and convergence guarantees.\nThe ORALS estimator is consistent and asymptotically normal under a coercivity\ncondition. We conduct several numerical experiments ranging from Kuramoto\nparticle systems on networks to opinion dynamics in leader-follower models.",
    "comment": "53 pages, 17 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.DS",
      "math.ST",
      "stat.TH",
      "62F12, 82C22"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08412v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08412v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08412v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08409v1",
    "updated": "2024-02-13T12:21:06+00:00",
    "published": "2024-02-13T12:21:06+00:00",
    "title": "Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging",
    "authors": [
      {
        "name": "Kwanseok Oh"
      },
      {
        "name": "Jieun Lee"
      },
      {
        "name": "Da-Woon Heo"
      },
      {
        "name": "Dinggang Shen"
      },
      {
        "name": "Heung-Il Suk"
      }
    ],
    "summary": "Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI,\nprovides superior anatomical details of internal brain structures owing to its\nenhanced signal-to-noise ratio and susceptibility-induced contrast. However,\nthe widespread use of 7T MRI is limited by its high cost and lower\naccessibility compared to low-field (LF) MRI. This study proposes a\ndeep-learning framework that systematically fuses the input LF magnetic\nresonance feature representations with the inferred 7T-like feature\nrepresentations for brain image segmentation tasks in a 7T-absent environment.\nSpecifically, our adaptive fusion module aggregates 7T-like features derived\nfrom the LF image by a pre-trained network and then refines them to be\neffectively assimilable UHF guidance into LF image features. Using\nintensity-guided features obtained from such aggregation and assimilation,\nsegmentation models can recognize subtle structural representations that are\nusually difficult to recognize when relying only on LF features. Beyond such\nadvantages, this strategy can seamlessly be utilized by modulating the contrast\nof LF features in alignment with UHF guidance, even when employing arbitrary\nsegmentation models. Exhaustive experiments demonstrated that the proposed\nmethod significantly outperformed all baseline models on both brain tissue and\nwhole-brain segmentation tasks; further, it exhibited remarkable adaptability\nand scalability by successfully integrating diverse segmentation models and\ntasks. These improvements were not only quantifiable but also visible in the\nsuperlative visual quality of segmentation masks.",
    "comment": "32 pages, 9 figures, and 5 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08409v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08409v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08409v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08406v1",
    "updated": "2024-02-13T12:11:40+00:00",
    "published": "2024-02-13T12:11:40+00:00",
    "title": "Transition Constrained Bayesian Optimization via Markov Decision Processes",
    "authors": [
      {
        "name": "Jose Pablo Folch"
      },
      {
        "name": "Calvin Tsay"
      },
      {
        "name": "Robert M Lee"
      },
      {
        "name": "Behrang Shafei"
      },
      {
        "name": "Weronika Ormaniec"
      },
      {
        "name": "Andreas Krause"
      },
      {
        "name": "Mark van der Wilk"
      },
      {
        "name": "Ruth Misener"
      },
      {
        "name": "Mojm\u00edr Mutn\u00fd"
      }
    ],
    "summary": "Bayesian optimization is a methodology to optimize black-box functions.\nTraditionally, it focuses on the setting where you can arbitrarily query the\nsearch space. However, many real-life problems do not offer this flexibility;\nin particular, the search space of the next query may depend on previous ones.\nExample challenges arise in the physical sciences in the form of local movement\nconstraints, required monotonicity in certain variables, and transitions\ninfluencing the accuracy of measurements. Altogether, such transition\nconstraints necessitate a form of planning. This work extends Bayesian\noptimization via the framework of Markov Decision Processes, iteratively\nsolving a tractable linearization of our objective using reinforcement learning\nto obtain a policy that plans ahead over long horizons. The resulting policy is\npotentially history-dependent and non-Markovian. We showcase applications in\nchemical reactor optimization, informative path planning, machine calibration,\nand other synthetic examples.",
    "comment": "9 pages main, 24 pages total, 13 figures, 1 table, preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08406v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08406v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08406v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08405v1",
    "updated": "2024-02-13T12:09:15+00:00",
    "published": "2024-02-13T12:09:15+00:00",
    "title": "A Novel Approach to Regularising 1NN classifier for Improved Generalization",
    "authors": [
      {
        "name": "Aditya Challa"
      },
      {
        "name": "Sravan Danda"
      },
      {
        "name": "Laurent Najman"
      }
    ],
    "summary": "In this paper, we propose a class of non-parametric classifiers, that learn\narbitrary boundaries and generalize well.\n  Our approach is based on a novel way to regularize 1NN classifiers using a\ngreedy approach. We refer to this class of classifiers as Watershed\nClassifiers. 1NN classifiers are known to trivially over-fit but have very\nlarge VC dimension, hence do not generalize well. We show that watershed\nclassifiers can find arbitrary boundaries on any dense enough dataset, and, at\nthe same time, have very small VC dimension; hence a watershed classifier leads\nto good generalization.\n  Traditional approaches to regularize 1NN classifiers are to consider $K$\nnearest neighbours. Neighbourhood component analysis (NCA) proposes a way to\nlearn representations consistent with ($n-1$) nearest neighbour classifier,\nwhere $n$ denotes the size of the dataset. In this article, we propose a loss\nfunction which can learn representations consistent with watershed classifiers,\nand show that it outperforms the NCA baseline.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08405v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08405v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08405v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08401v1",
    "updated": "2024-02-13T12:02:37+00:00",
    "published": "2024-02-13T12:02:37+00:00",
    "title": "LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection",
    "authors": [
      {
        "name": "Batool Lakzaei"
      },
      {
        "name": "Mostafa Haghir Chehreghani"
      },
      {
        "name": "Alireza Bagheri"
      }
    ],
    "summary": "In the era of widespread social networks, the rapid dissemination of fake\nnews has emerged as a significant threat, inflicting detrimental consequences\nacross various dimensions of people's lives. Machine learning and deep learning\napproaches have been extensively employed for identifying fake news. However, a\nsignificant challenge in identifying fake news is the limited availability of\nlabeled news datasets. Therefore, the One-Class Learning (OCL) approach,\nutilizing only a small set of labeled data from the interest class, can be a\nsuitable approach to address this challenge. On the other hand, representing\ndata as a graph enables access to diverse content and structural information,\nand label propagation methods on graphs can be effective in predicting node\nlabels. In this paper, we adopt a graph-based model for data representation and\nintroduce a semi-supervised and one-class approach for fake news detection,\ncalled LOSS-GAT. Initially, we employ a two-step label propagation algorithm,\nutilizing Graph Neural Networks (GNNs) as an initial classifier to categorize\nnews into two groups: interest (fake) and non-interest (real). Subsequently, we\nenhance the graph structure using structural augmentation techniques.\nUltimately, we predict the final labels for all unlabeled data using a GNN that\ninduces randomness within the local neighborhood of nodes through the\naggregation function. We evaluate our proposed method on five common datasets\nand compare the results against a set of baseline models, including both OCL\nand binary labeled models. The results demonstrate that LOSS-GAT achieves a\nnotable improvement, surpassing 10%, with the advantage of utilizing only a\nlimited set of labeled fake news. Noteworthy, LOSS-GAT even outperforms binary\nlabeled models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08401v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08401v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08401v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08400v1",
    "updated": "2024-02-13T11:59:43+00:00",
    "published": "2024-02-13T11:59:43+00:00",
    "title": "Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing",
    "authors": [
      {
        "name": "Alaa Anani"
      },
      {
        "name": "Tobias Lorenz"
      },
      {
        "name": "Bernt Schiele"
      },
      {
        "name": "Mario Fritz"
      }
    ],
    "summary": "Common certification methods operate on a flat pre-defined set of\nfine-grained classes. In this paper, however, we propose a novel, more general,\nand practical setting, namely adaptive hierarchical certification for image\nsemantic segmentation. In this setting, the certification can be within a\nmulti-level hierarchical label space composed of fine to coarse levels. Unlike\nclassic methods where the certification would abstain for unstable components,\nour approach adaptively relaxes the certification to a coarser level within the\nhierarchy. This relaxation lowers the abstain rate whilst providing more\ncertified semantically meaningful information. We mathematically formulate the\nproblem setup and introduce, for the first time, an adaptive hierarchical\ncertification algorithm for image semantic segmentation, that certifies image\npixels within a hierarchy and prove the correctness of its guarantees. Since\ncertified accuracy does not take the loss of information into account when\ntraversing into a coarser hierarchy level, we introduce a novel evaluation\nparadigm for adaptive hierarchical certification, namely the certified\ninformation gain metric, which is proportional to the class granularity level.\nOur evaluation experiments on real-world challenging datasets such as\nCityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher\ncertified information gain and a lower abstain rate compared to the current\nstate-of-the-art certification method, as well as other non-adaptive versions\nof it.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08400v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08400v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08400v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08384v1",
    "updated": "2024-02-13T11:25:20+00:00",
    "published": "2024-02-13T11:25:20+00:00",
    "title": "Selective Learning: Towards Robust Calibration with Dynamic Regularization",
    "authors": [
      {
        "name": "Zongbo Han"
      },
      {
        "name": "Yifeng Yang"
      },
      {
        "name": "Changqing Zhang"
      },
      {
        "name": "Linjun Zhang"
      },
      {
        "name": "Joey Tianyi Zhou"
      },
      {
        "name": "Qinghua Hu"
      },
      {
        "name": "Huaxiu Yao"
      }
    ],
    "summary": "Miscalibration in deep learning refers to there is a discrepancy between the\npredicted confidence and performance. This problem usually arises due to the\noverfitting problem, which is characterized by learning everything presented in\nthe training set, resulting in overconfident predictions during testing.\nExisting methods typically address overfitting and mitigate the miscalibration\nby adding a maximum-entropy regularizer to the objective function. The\nobjective can be understood as seeking a model that fits the ground-truth\nlabels by increasing the confidence while also maximizing the entropy of\npredicted probabilities by decreasing the confidence. However, previous methods\nlack clear guidance on confidence adjustment, leading to conflicting objectives\n(increasing but also decreasing confidence). Therefore, we introduce a method\ncalled Dynamic Regularization (DReg), which aims to learn what should be\nlearned during training thereby circumventing the confidence adjusting\ntrade-off. At a high level, DReg aims to obtain a more reliable model capable\nof acknowledging what it knows and does not know. Specifically, DReg\neffectively fits the labels for in-distribution samples (samples that should be\nlearned) while applying regularization dynamically to samples beyond model\ncapabilities (e.g., outliers), thereby obtaining a robust calibrated model\nespecially on the samples beyond model capabilities. Both theoretical and\nempirical analyses sufficiently demonstrate the superiority of DReg compared\nwith previous methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08384v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08384v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08384v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08383v1",
    "updated": "2024-02-13T11:22:59+00:00",
    "published": "2024-02-13T11:22:59+00:00",
    "title": "Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution",
    "authors": [
      {
        "name": "Tailin Wu"
      },
      {
        "name": "Willie Neiswanger"
      },
      {
        "name": "Hongtao Zheng"
      },
      {
        "name": "Stefano Ermon"
      },
      {
        "name": "Jure Leskovec"
      }
    ],
    "summary": "Deep learning-based surrogate models have demonstrated remarkable advantages\nover classical solvers in terms of speed, often achieving speedups of 10 to\n1000 times over traditional partial differential equation (PDE) solvers.\nHowever, a significant challenge hindering their widespread adoption in both\nscientific and industrial domains is the lack of understanding about their\nprediction uncertainties, particularly in scenarios that involve critical\ndecision making. To address this limitation, we propose a method that\nintegrates efficient and precise uncertainty quantification into a deep\nlearning-based surrogate model. Our method, termed Latent Evolution of PDEs\nwith Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based\nsurrogate models with robust and efficient uncertainty quantification\ncapabilities for both forward and inverse problems. LE-PDE-UQ leverages latent\nvectors within a latent space to evolve both the system's state and its\ncorresponding uncertainty estimation. The latent vectors are decoded to provide\npredictions for the system's state as well as estimates of its uncertainty. In\nextensive experiments, we demonstrate the accurate uncertainty quantification\nperformance of our approach, surpassing that of strong baselines including deep\nensembles, Bayesian neural network layers, and dropout. Our method excels at\npropagating uncertainty over extended auto-regressive rollouts, making it\nsuitable for scenarios involving long-term predictions. Our code is available\nat: https://github.com/AI4Science-WestlakeU/le-pde-uq.",
    "comment": "Accepted by AAAI 2024 (Oral)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08383v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08383v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08383v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08379v1",
    "updated": "2024-02-13T11:18:27+00:00",
    "published": "2024-02-13T11:18:27+00:00",
    "title": "The Duet of Representations and How Explanations Exacerbate It",
    "authors": [
      {
        "name": "Charles Wan"
      },
      {
        "name": "Rodrigo Belo"
      },
      {
        "name": "Leid Zejnilovi\u0107"
      },
      {
        "name": "Susana Lavado"
      }
    ],
    "summary": "An algorithm effects a causal representation of relations between features\nand labels in the human's perception. Such a representation might conflict with\nthe human's prior belief. Explanations can direct the human's attention to the\nconflicting feature and away from other relevant features. This leads to causal\noverattribution and may adversely affect the human's information processing. In\na field experiment we implemented an XGBoost-trained model as a decision-making\naid for counselors at a public employment service to predict candidates' risk\nof long-term unemployment. The treatment group of counselors was also provided\nwith SHAP. The results show that the quality of the human's decision-making is\nworse when a feature on which the human holds a conflicting prior belief is\ndisplayed as part of the explanation.",
    "comment": null,
    "journal_ref": "In World Conference on Explainable Artificial Intelligence (pp.\n  181-197). Cham: Springer Nature Switzerland (2023)",
    "doi": "10.1007/978-3-031-44067-0_10",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1007/978-3-031-44067-0_10",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08379v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08379v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08379v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08373v1",
    "updated": "2024-02-13T11:10:14+00:00",
    "published": "2024-02-13T11:10:14+00:00",
    "title": "Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting",
    "authors": [
      {
        "name": "Riku Green"
      },
      {
        "name": "Grant Stevens"
      },
      {
        "name": "Telmo de Menezes e Silva Filho"
      },
      {
        "name": "Zahraa Abdallah"
      }
    ],
    "summary": "Multi-step forecasting (MSF) in time-series, the ability to make predictions\nmultiple time steps into the future, is fundamental to almost all temporal\ndomains. To make such forecasts, one must assume the recursive complexity of\nthe temporal dynamics. Such assumptions are referred to as the forecasting\nstrategy used to train a predictive model. Previous work shows that it is not\nclear which forecasting strategy is optimal a priori to evaluating on unseen\ndata. Furthermore, current approaches to MSF use a single (fixed) forecasting\nstrategy.\n  In this paper, we characterise the instance-level variance of optimal\nforecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We\nexperiment using 10 datasets from different scales, domains, and lengths of\nmulti-step horizons. When using a random-forest-based classifier, DyStrat\noutperforms the best fixed strategy, which is not knowable a priori, 94% of the\ntime, with an average reduction in mean-squared error of 11%. Our approach\ntypically triples the top-1 accuracy compared to current approaches. Notably,\nwe show DyStrat generalises well for any MSF task.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08373v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08373v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08373v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08371v1",
    "updated": "2024-02-13T11:02:12+00:00",
    "published": "2024-02-13T11:02:12+00:00",
    "title": "Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization",
    "authors": [
      {
        "name": "A. Esteban"
      },
      {
        "name": "A. Zafra"
      },
      {
        "name": "C. Romero"
      }
    ],
    "summary": "The wide availability of specific courses together with the flexibility of\nacademic plans in university studies reveal the importance of Recommendation\nSystems (RSs) in this area. These systems appear as tools that help students to\nchoose courses that suit to their personal interests and their academic\nperformance. This paper presents a hybrid RS that combines Collaborative\nFiltering (CF) and Content-based Filtering (CBF) using multiple criteria\nrelated both to student and course information to recommend the most suitable\ncourses to the students. A Genetic Algorithm (GA) has been developed to\nautomatically discover the optimal RS configuration which include both the most\nrelevant criteria and the configuration of the rest of parameters. The\nexperimental study has used real information of Computer Science Degree of\nUniversity of Cordoba (Spain) including information gathered from students\nduring three academic years, counting on 2500 entries of 95 students and 63\ncourses. Experimental results show a study of the most relevant criteria for\nthe course recommendation, the importance of using a hybrid model that combines\nboth student information and course information to increase the reliability of\nthe recommendations as well as an excellent performance compared to previous\nmodels.",
    "comment": null,
    "journal_ref": "Knowledge-Based Systems, (2020):194, 105385",
    "doi": "10.1016/j.knosys.2019.105385",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1016/j.knosys.2019.105385",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08371v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08371v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08371v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08369v1",
    "updated": "2024-02-13T11:01:52+00:00",
    "published": "2024-02-13T11:01:52+00:00",
    "title": "One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill",
    "authors": [
      {
        "name": "Sangwoo Shin"
      },
      {
        "name": "Daehee Lee"
      },
      {
        "name": "Minjong Yoo"
      },
      {
        "name": "Woo Kyung Kim"
      },
      {
        "name": "Honguk Woo"
      }
    ],
    "summary": "One-shot imitation is to learn a new task from a single demonstration, yet it\nis a challenging problem to adopt it for complex tasks with the high domain\ndiversity inherent in a non-stationary environment. To tackle the problem, we\nexplore the compositionality of complex tasks, and present a novel skill-based\nimitation learning framework enabling one-shot imitation and zero-shot\nadaptation; from a single demonstration for a complex unseen task, a semantic\nskill sequence is inferred and then each skill in the sequence is converted\ninto an action sequence optimized for environmental hidden dynamics that can\nvary over time. Specifically, we leverage a vision-language model to learn a\nsemantic skill set from offline video datasets, where each skill is represented\non the vision-language embedding space, and adapt meta-learning with dynamics\ninference to enable zero-shot skill adaptation. We evaluate our framework with\nvarious one-shot imitation scenarios for extended multi-stage Meta-world tasks,\nshowing its superiority in learning complex tasks, generalizing to dynamics\nchanges, and extending to different demonstration conditions and modalities,\ncompared to other baselines.",
    "comment": "ICML-2023 Camera Ready Version",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08369v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08369v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08369v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08367v1",
    "updated": "2024-02-13T10:54:43+00:00",
    "published": "2024-02-13T10:54:43+00:00",
    "title": "RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks",
    "authors": [
      {
        "name": "Chengxi Zeng"
      },
      {
        "name": "Tilo Burghardt"
      },
      {
        "name": "Alberto M Gambaruto"
      }
    ],
    "summary": "While many recent Physics-Informed Neural Networks (PINNs) variants have had\nconsiderable success in solving Partial Differential Equations, the empirical\nbenefits of feature mapping drawn from the broader Neural Representations\nresearch have been largely overlooked. We highlight the limitations of widely\nused Fourier-based feature mapping in certain situations and suggest the use of\nthe conditionally positive definite Radial Basis Function. The empirical\nfindings demonstrate the effectiveness of our approach across a variety of\nforward and inverse problem cases. Our method can be seamlessly integrated into\ncoordinate-based input neural networks and contribute to the wider field of\nPINNs research.",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2402.06955",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08367v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08367v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08367v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08365v1",
    "updated": "2024-02-13T10:50:54+00:00",
    "published": "2024-02-13T10:50:54+00:00",
    "title": "NeuRes: Learning Proofs of Propositional Satisfiability",
    "authors": [
      {
        "name": "Mohamed Ghanem"
      },
      {
        "name": "Frederik Schmitt"
      },
      {
        "name": "Julian Siber"
      },
      {
        "name": "Bernd Finkbeiner"
      }
    ],
    "summary": "We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other\nneural SAT solving methods, NeuRes is capable of proving unsatisfiability as\nopposed to merely predicting it. By design, NeuRes operates in a\ncertificate-driven fashion by employing propositional resolution to prove\nunsatisfiability and to accelerate the process of finding satisfying truth\nassignments in case of unsat and sat formulas, respectively. To realize this,\nwe propose a novel architecture that adapts elements from Graph Neural Networks\nand Pointer Networks to autoregressively select pairs of nodes from a dynamic\ngraph structure, which is essential to the generation of resolution proofs. Our\nmodel is trained and evaluated on a dataset of teacher proofs and truth\nassignments that we compiled with the same random formula distribution used by\nNeuroSAT. In our experiments, we show that NeuRes solves more test formulas\nthan NeuroSAT by a rather wide margin on different distributions while being\nmuch more data-efficient. Furthermore, we show that NeuRes is capable of\nlargely shortening teacher proofs by notable proportions. We use this feature\nto devise a bootstrapped training procedure that manages to reduce a dataset of\nproofs generated by an advanced solver by ~23% after training on it with no\nextra guidance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.LO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08365v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08365v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08365v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08349v1",
    "updated": "2024-02-13T10:28:57+00:00",
    "published": "2024-02-13T10:28:57+00:00",
    "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries",
    "authors": [
      {
        "name": "Jonathan F\u00fcrst"
      },
      {
        "name": "Catherine Kosten"
      },
      {
        "name": "Farhard Nooralahzadeh"
      },
      {
        "name": "Yi Zhang"
      },
      {
        "name": "Kurt Stockinger"
      }
    ],
    "summary": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DB",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08349v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08349v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08349v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08344v1",
    "updated": "2024-02-13T10:19:33+00:00",
    "published": "2024-02-13T10:19:33+00:00",
    "title": "Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training",
    "authors": [
      {
        "name": "Tom Sander"
      },
      {
        "name": "Maxime Sylvestre"
      },
      {
        "name": "Alain Durmus"
      }
    ],
    "summary": "Training Deep Neural Networks (DNNs) with small batches using Stochastic\nGradient Descent (SGD) yields superior test performance compared to larger\nbatches. The specific noise structure inherent to SGD is known to be\nresponsible for this implicit bias. DP-SGD, used to ensure differential privacy\n(DP) in DNNs' training, adds Gaussian noise to the clipped gradients.\nSurprisingly, large-batch training still results in a significant decrease in\nperformance, which poses an important challenge because strong DP guarantees\nnecessitate the use of massive batches. We first show that the phenomenon\nextends to Noisy-SGD (DP-SGD without clipping), suggesting that the\nstochasticity (and not the clipping) is the cause of this implicit bias, even\nwith additional isotropic Gaussian noise. We theoretically analyse the\nsolutions obtained with continuous versions of Noisy-SGD for the Linear Least\nSquare and Diagonal Linear Network settings, and reveal that the implicit bias\nis indeed amplified by the additional noise. Thus, the performance issues of\nlarge-batch DP-SGD training are rooted in the same underlying principles as\nSGD, offering hope for potential improvements in large batch training\nstrategies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08344v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08344v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08344v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08341v1",
    "updated": "2024-02-13T10:09:00+00:00",
    "published": "2024-02-13T10:09:00+00:00",
    "title": "Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach",
    "authors": [
      {
        "name": "Airlie Hilliard"
      },
      {
        "name": "Cristian Munoz"
      },
      {
        "name": "Zekun Wu"
      },
      {
        "name": "Adriano Soares Koshiyama"
      }
    ],
    "summary": "Large Language Models (LLMs) are increasingly being utilized by both\ncandidates and employers in the recruitment context. However, with this comes\nnumerous ethical concerns, particularly related to the lack of transparency in\nthese \"black-box\" models. Although previous studies have sought to increase the\ntransparency of these models by investigating the personality traits of LLMs,\nmany of the previous studies have provided them with personality assessments to\ncomplete. On the other hand, this study seeks to obtain a better understanding\nof such models by examining their output variations based on different input\nprompts. Specifically, we use a novel elicitation approach using prompts\nderived from common interview questions, as well as prompts designed to elicit\nparticular Big Five personality traits to examine whether the models were\nsusceptible to trait-activation like humans are, to measure their personality\nbased on the language used in their outputs. To do so, we repeatedly prompted\nmultiple LMs with different parameter sizes, including Llama-2, Falcon,\nMistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined\ntheir personality using classifiers trained on the myPersonality dataset. Our\nresults reveal that, generally, all LLMs demonstrate high openness and low\nextraversion. However, whereas LMs with fewer parameters exhibit similar\nbehaviour in personality traits, newer and LMs with more parameters exhibit a\nbroader range of personality traits, with increased agreeableness, emotional\nstability, and openness. Furthermore, a greater number of parameters is\npositively associated with openness and conscientiousness. Moreover, fine-tuned\nmodels exhibit minor modulations in their personality traits, contingent on the\ndataset. Implications and directions for future research are discussed.",
    "comment": "Manuscript submitted to ACM Facct. Authors One and Two contributed\n  equally to this work",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08341v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08341v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08341v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08324v1",
    "updated": "2024-02-13T09:40:19+00:00",
    "published": "2024-02-13T09:40:19+00:00",
    "title": "Uncertainty Quantification via Stable Distribution Propagation",
    "authors": [
      {
        "name": "Felix Petersen"
      },
      {
        "name": "Aashwin Mishra"
      },
      {
        "name": "Hilde Kuehne"
      },
      {
        "name": "Christian Borgelt"
      },
      {
        "name": "Oliver Deussen"
      },
      {
        "name": "Mikhail Yurochkin"
      }
    ],
    "summary": "We propose a new approach for propagating stable probability distributions\nthrough neural networks. Our method is based on local linearization, which we\nshow to be an optimal approximation in terms of total variation distance for\nthe ReLU non-linearity. This allows propagating Gaussian and Cauchy input\nuncertainties through neural networks to quantify their output uncertainties.\nTo demonstrate the utility of propagating distributions, we apply the proposed\nmethod to predicting calibrated confidence intervals and selective prediction\non out-of-distribution data. The results demonstrate a broad applicability of\npropagating distributions and show the advantages of our method over other\napproaches such as moment matching.",
    "comment": "Published at ICLR 2024, Code @\n  https://github.com/Felix-Petersen/distprop",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08324v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08324v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08324v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08323v1",
    "updated": "2024-02-13T09:38:17+00:00",
    "published": "2024-02-13T09:38:17+00:00",
    "title": "Mapping the Ethics of Generative AI: A Comprehensive Scoping Review",
    "authors": [
      {
        "name": "Thilo Hagendorff"
      }
    ],
    "summary": "The advent of generative artificial intelligence and the widespread adoption\nof it in society engendered intensive debates about its ethical implications\nand risks. These risks often differ from those associated with traditional\ndiscriminative machine learning. To synthesize the recent discourse and map its\nnormative concepts, we conducted a scoping review on the ethics of generative\nartificial intelligence, including especially large language models and\ntext-to-image models. Our analysis provides a taxonomy of 378 normative issues\nin 19 topic areas and ranks them according to their prevalence in the\nliterature. The study offers a comprehensive overview for scholars,\npractitioners, or policymakers, condensing the ethical debates surrounding\nfairness, safety, harmful content, hallucinations, privacy, interaction risks,\nsecurity, alignment, societal impacts, and others. We discuss the results,\nevaluate imbalances in the literature, and explore unsubstantiated risk\nscenarios.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CY",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08323v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08323v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08323v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08321v1",
    "updated": "2024-02-13T09:34:22+00:00",
    "published": "2024-02-13T09:34:22+00:00",
    "title": "Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring",
    "authors": [
      {
        "name": "Taira Tsuchiya"
      },
      {
        "name": "Shinji Ito"
      },
      {
        "name": "Junya Honda"
      }
    ],
    "summary": "Partial monitoring is a generic framework of online decision-making problems\nwith limited observations. To make decisions from such limited observations, it\nis necessary to find an appropriate distribution for exploration. Recently, a\npowerful approach for this purpose, exploration by optimization (ExO), was\nproposed, which achieves the optimal bounds in adversarial environments with\nfollow-the-regularized-leader for a wide range of online decision-making\nproblems. However, a naive application of ExO in stochastic environments\nsignificantly degrades regret bounds. To resolve this problem in locally\nobservable games, we first establish a novel framework and analysis for ExO\nwith a hybrid regularizer. This development allows us to significantly improve\nthe existing regret bounds of best-of-both-worlds (BOBW) algorithms, which\nachieves nearly optimal bounds both in stochastic and adversarial environments.\nIn particular, we derive a stochastic regret bound of $O(\\sum_{a \\neq a^*} k^2\nm^2 \\log T / \\Delta_a)$, where $k$, $m$, and $T$ are the numbers of actions,\nobservations and rounds, $a^*$ is an optimal action, and $\\Delta_a$ is the\nsuboptimality gap for action $a$. This bound is roughly $\\Theta(k^2 \\log T)$\ntimes smaller than existing BOBW bounds. In addition, for globally observable\ngames, we provide a new BOBW algorithm with the first $O(\\log T)$ stochastic\nbound.",
    "comment": "30 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08321v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08321v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08321v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08313v1",
    "updated": "2024-02-13T09:17:20+00:00",
    "published": "2024-02-13T09:17:20+00:00",
    "title": "Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks",
    "authors": [
      {
        "name": "Franz M. Rohrhofer"
      },
      {
        "name": "Stefan Posch"
      },
      {
        "name": "Clemens G\u00f6\u00dfnitzer"
      },
      {
        "name": "Bernhard C. Geiger"
      }
    ],
    "summary": "This paper employs physics-informed neural networks (PINNs) to solve Fisher's\nequation, a fundamental representation of a reaction-diffusion system with both\nsimplicity and significance. The focus lies specifically in investigating\nFisher's equation under conditions of large reaction rate coefficients, wherein\nsolutions manifest as traveling waves, posing a challenge for numerical methods\ndue to the occurring steepness of the wave front. To address optimization\nchallenges associated with the standard PINN approach, a residual weighting\nscheme is introduced. This scheme is designed to enhance the tracking of\npropagating wave fronts by considering the reaction term in the\nreaction-diffusion equation. Furthermore, a specific network architecture is\nstudied which is tailored for solutions in the form of traveling waves. Lastly,\nthe capacity of PINNs to approximate an entire family of solutions is assessed\nby incorporating the reaction rate coefficient as an additional input to the\nnetwork architecture. This modification enables the approximation of the\nsolution across a broad and continuous range of reaction rate coefficients,\nthus solving a class of reaction-diffusion systems using a single PINN\ninstance.",
    "comment": "14 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08313v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08313v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08313v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08310v1",
    "updated": "2024-02-13T09:13:30+00:00",
    "published": "2024-02-13T09:13:30+00:00",
    "title": "One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model",
    "authors": [
      {
        "name": "Thomas P\u00f6llabauer"
      },
      {
        "name": "Julius K\u00fchn"
      },
      {
        "name": "Jiayi Li"
      },
      {
        "name": "Arjan Kuijper"
      }
    ],
    "summary": "Estimating the 3D shape of an object using a single image is a difficult\nproblem. Modern approaches achieve good results for general objects, based on\nreal photographs, but worse results on less expressive representations such as\nhistoric sketches. Our automated approach generates a variety of detailed 3D\nrepresentation from a single sketch, depicting a medieval statue, and can be\nguided by multi-modal inputs, such as text prompts. It relies solely on\nsynthetic data for training, making it adoptable even in cases of only small\nnumbers of training examples. Our solution allows domain experts such as a\ncurators to interactively reconstruct potential appearances of lost artifacts.",
    "comment": null,
    "journal_ref": "21st Eurographics Workshop on Graphics and Cultural Heritage (GCH\n  2023)",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08310v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08310v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08310v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08309v2",
    "updated": "2024-02-14T08:10:38+00:00",
    "published": "2024-02-13T09:12:55+00:00",
    "title": "Prompted Contextual Vectors for Spear-Phishing Detection",
    "authors": [
      {
        "name": "Daniel Nahmias"
      },
      {
        "name": "Gal Engelberg"
      },
      {
        "name": "Dan Klein"
      },
      {
        "name": "Asaf Shabtai"
      }
    ],
    "summary": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include an innovative document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "I.2.7"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08309v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08309v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08309v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08303v2",
    "updated": "2024-02-14T08:09:58+00:00",
    "published": "2024-02-13T09:06:14+00:00",
    "title": "ChatCell: Facilitating Single-Cell Analysis with Natural Language",
    "authors": [
      {
        "name": "Yin Fang"
      },
      {
        "name": "Kangwei Liu"
      },
      {
        "name": "Ningyu Zhang"
      },
      {
        "name": "Xinle Deng"
      },
      {
        "name": "Penghui Yang"
      },
      {
        "name": "Zhuo Chen"
      },
      {
        "name": "Xiangru Tang"
      },
      {
        "name": "Mark Gerstein"
      },
      {
        "name": "Xiaohui Fan"
      },
      {
        "name": "Huajun Chen"
      }
    ],
    "summary": "As Large Language Models (LLMs) rapidly evolve, their influence in science is\nbecoming increasingly prominent. The emerging capabilities of LLMs in task\ngeneralization and free-form dialogue can significantly advance fields like\nchemistry and biology. However, the field of single-cell biology, which forms\nthe foundational building blocks of living organisms, still faces several\nchallenges. High knowledge barriers and limited scalability in current methods\nrestrict the full exploitation of LLMs in mastering single-cell data, impeding\ndirect accessibility and rapid iteration. To this end, we introduce ChatCell,\nwhich signifies a paradigm shift by facilitating single-cell analysis with\nnatural language. Leveraging vocabulary adaptation and unified sequence\ngeneration, ChatCell has acquired profound expertise in single-cell biology and\nthe capability to accommodate a diverse range of analysis tasks. Extensive\nexperiments further demonstrate ChatCell's robust performance and potential to\ndeepen single-cell insights, paving the way for more accessible and intuitive\nexploration in this pivotal field. Our project homepage is available at\nhttps://zjunlp.github.io/project/ChatCell.",
    "comment": "Ongoing work; 15 pages, 6 Tables, 9 Figures; Project homepage:\n  https://zjunlp.github.io/project/ChatCell Code:\n  https://github.com/zjunlp/ChatCell Dataset:\n  https://huggingface.co/datasets/zjunlp/ChatCell-Instructions Demo:\n  https://huggingface.co/spaces/zjunlp/Chatcell",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.HC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08303v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08303v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08303v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08298v1",
    "updated": "2024-02-13T08:53:57+00:00",
    "published": "2024-02-13T08:53:57+00:00",
    "title": "Time to Stop and Think: What kind of research do we want to do?",
    "authors": [
      {
        "name": "Josu Ceberio"
      },
      {
        "name": "Borja Calvo"
      }
    ],
    "summary": "Experimentation is an intrinsic part of research in artificial intelligence\nsince it allows for collecting quantitative observations, validating\nhypotheses, and providing evidence for their reformulation. For that reason,\nexperimentation must be coherent with the purposes of the research, properly\naddressing the relevant questions in each case. Unfortunately, the literature\nis full of works whose experimentation is neither rigorous nor convincing,\noftentimes designed to support prior beliefs rather than answering the relevant\nresearch questions.\n  In this paper, we focus on the field of metaheuristic optimization, since it\nis our main field of work, and it is where we have observed the misconduct that\nhas motivated this letter. Even if we limit the focus of this manuscript to the\nexperimental part of the research, our main goal is to sew the seed of sincere\ncritical assessment of our work, sparking a reflection process both at the\nindividual and the community level. Such a reflection process is too complex\nand extensive to be tackled as a whole. Therefore, to bring our feet to the\nground, we will include in this document our reflections about the role of\nexperimentation in our work, discussing topics such as the use of benchmark\ninstances vs instance generators, or the statistical assessment of empirical\nresults. That is, all the statements included in this document are personal\nviews and opinions, which can be shared by others or not. Certainly, having\ndifferent points of view is the basis to establish a good discussion process.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08298v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08298v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08298v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08296v1",
    "updated": "2024-02-13T08:50:14+00:00",
    "published": "2024-02-13T08:50:14+00:00",
    "title": "Multi-Level GNN Preconditioner for Solving Large Scale Problems",
    "authors": [
      {
        "name": "Matthieu Nastorg"
      },
      {
        "name": "Jean-Marc Gratien"
      },
      {
        "name": "Thibault Faney"
      },
      {
        "name": "Michele Alessandro Bucci"
      },
      {
        "name": "Guillaume Charpiat"
      },
      {
        "name": "Marc Schoenauer"
      }
    ],
    "summary": "Large-scale numerical simulations often come at the expense of daunting\ncomputations. High-Performance Computing has enhanced the process, but adapting\nlegacy codes to leverage parallel GPU computations remains challenging.\nMeanwhile, Machine Learning models can harness GPU computations effectively but\noften struggle with generalization and accuracy. Graph Neural Networks (GNNs),\nin particular, are great for learning from unstructured data like meshes but\nare often limited to small-scale problems. Moreover, the capabilities of the\ntrained model usually restrict the accuracy of the data-driven solution. To\nbenefit from both worlds, this paper introduces a novel preconditioner\nintegrating a GNN model within a multi-level Domain Decomposition framework.\nThe proposed GNN-based preconditioner is used to enhance the efficiency of a\nKrylov method, resulting in a hybrid solver that can converge with any desired\nlevel of accuracy. The efficiency of the Krylov method greatly benefits from\nthe GNN preconditioner, which is adaptable to meshes of any size and shape, is\nexecuted on GPUs, and features a multi-level approach to enforce the\nscalability of the entire process. Several experiments are conducted to\nvalidate the numerical behavior of the hybrid solver, and an in-depth analysis\nof its performance is proposed to assess its competitiveness against a C++\nlegacy solver.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08296v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08296v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08296v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08290v1",
    "updated": "2024-02-13T08:41:32+00:00",
    "published": "2024-02-13T08:41:32+00:00",
    "title": "The Effect of Data Poisoning on Counterfactual Explanations",
    "authors": [
      {
        "name": "Andr\u00e9 Artelt"
      },
      {
        "name": "Shubham Sharma"
      },
      {
        "name": "Freddy Lecu\u00e9"
      },
      {
        "name": "Barbara Hammer"
      }
    ],
    "summary": "Counterfactual explanations provide a popular method for analyzing the\npredictions of black-box systems, and they can offer the opportunity for\ncomputational recourse by suggesting actionable changes on how to change the\ninput to obtain a different (i.e. more favorable) system output. However,\nrecent work highlighted their vulnerability to different types of\nmanipulations. This work studies the vulnerability of counterfactual\nexplanations to data poisoning. We formalize data poisoning in the context of\ncounterfactual explanations for increasing the cost of recourse on three\ndifferent levels: locally for a single instance, or a sub-group of instances,\nor globally for all instances. We demonstrate that state-of-the-art\ncounterfactual generation methods \\& toolboxes are vulnerable to such data\npoisoning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08290v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08290v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08290v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08284v1",
    "updated": "2024-02-13T08:24:32+00:00",
    "published": "2024-02-13T08:24:32+00:00",
    "title": "A Logical Approach to Criminal Case Investigation",
    "authors": [
      {
        "name": "Takanori Ugai"
      },
      {
        "name": "Yusuke Koyanagi"
      },
      {
        "name": "Fumihito Nishino"
      }
    ],
    "summary": "XAI (eXplanable AI) techniques that have the property of explaining the\nreasons for their conclusions, i.e. explainability or interpretability, are\nattracting attention. XAI is expected to be used in the development of forensic\nscience and the justice system. In today's forensic and criminal investigation\nenvironment, experts face many challenges due to large amounts of data, small\npieces of evidence in a chaotic and complex environment, traditional laboratory\nstructures and sometimes inadequate knowledge. All these can lead to failed\ninvestigations and miscarriages of justice. In this paper, we describe the\napplication of one logical approach to crime scene investigation. The subject\nof the application is ``The Adventure of the Speckled Band'' from the Sherlock\nHolmes short stories. The applied data is the knowledge graph created for the\nKnowledge Graph Reasoning Challenge. We tried to find the murderer by inferring\neach person with the motive, opportunity, and method. We created an ontology of\nmotives and methods of murder from dictionaries and dictionaries, added it to\nthe knowledge graph of ``The Adventure of the Speckled Band'', and applied\nscripts to determine motives, opportunities, and methods.",
    "comment": "11 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08284v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08284v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08284v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08280v1",
    "updated": "2024-02-13T08:14:10+00:00",
    "published": "2024-02-13T08:14:10+00:00",
    "title": "Pix2Code: Learning to Compose Neural Visual Concepts as Programs",
    "authors": [
      {
        "name": "Antonia W\u00fcst"
      },
      {
        "name": "Wolfgang Stammer"
      },
      {
        "name": "Quentin Delfosse"
      },
      {
        "name": "Devendra Singh Dhami"
      },
      {
        "name": "Kristian Kersting"
      }
    ],
    "summary": "The challenge in learning abstract concepts from images in an unsupervised\nfashion lies in the required integration of visual perception and generalizable\nrelational reasoning. Moreover, the unsupervised nature of this task makes it\nnecessary for human users to be able to understand a model's learnt concepts\nand potentially revise false behaviours. To tackle both the generalizability\nand interpretability constraints of visual concept learning, we propose\nPix2Code, a framework that extends program synthesis to visual relational\nreasoning by utilizing the abilities of both explicit, compositional symbolic\nand implicit neural representations. This is achieved by retrieving object\nrepresentations from images and synthesizing relational concepts as\nlambda-calculus programs. We evaluate the diverse properties of Pix2Code on the\nchallenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its\nability to identify compositional visual concepts that generalize to novel data\nand concept configurations. Particularly, in stark contrast to neural\napproaches, we show that Pix2Code's representations remain human interpretable\nand can be easily revised for improved performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08280v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08280v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08280v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08277v1",
    "updated": "2024-02-13T08:12:48+00:00",
    "published": "2024-02-13T08:12:48+00:00",
    "title": "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering",
    "authors": [
      {
        "name": "Tobias Schimanski"
      },
      {
        "name": "Jingwei Ni"
      },
      {
        "name": "Mathias Kraus"
      },
      {
        "name": "Elliott Ash"
      },
      {
        "name": "Markus Leippold"
      }
    ],
    "summary": "Advances towards more faithful and traceable answers of Large Language Models\n(LLMs) are crucial for various research and practical endeavors. One avenue in\nreaching this goal is basing the answers on reliable sources. However, this\nEvidence-Based QA has proven to work insufficiently with LLMs in terms of\nciting the correct sources (source quality) and truthfully representing the\ninformation within sources (answer attributability). In this work, we\nsystematically investigate how to robustly fine-tune LLMs for better source\nquality and answer attributability. Specifically, we introduce a data\ngeneration pipeline with automated data quality filters, which can synthesize\ndiversified high-quality training and testing data at scale. We further\nintroduce four test sets to benchmark the robustness of fine-tuned specialist\nmodels. Extensive evaluation shows that fine-tuning on synthetic data improves\nperformance on both in- and out-of-distribution. %Evidence-Based QA cases.\nFurthermore, we show that data quality, which can be drastically improved by\nproposed quality filters, matters more than quantity in improving\nEvidence-Based QA.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08277v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08277v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08277v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08269v1",
    "updated": "2024-02-13T07:49:57+00:00",
    "published": "2024-02-13T07:49:57+00:00",
    "title": "Geometry-induced Implicit Regularization in Deep ReLU Neural Networks",
    "authors": [
      {
        "name": "Joachim Bona-Pellissier"
      },
      {
        "name": "Fran \u00e7ois Malgouyres"
      },
      {
        "name": "Fran \u00e7ois Bachoc"
      }
    ],
    "summary": "It is well known that neural networks with many more parameters than training\nexamples do not overfit. Implicit regularization phenomena, which are still not\nwell understood, occur during optimization and 'good' networks are favored.\nThus the number of parameters is not an adequate measure of complexity if we do\nnot consider all possible networks but only the 'good' ones. To better\nunderstand which networks are favored during optimization, we study the\ngeometry of the output set as parameters vary. When the inputs are fixed, we\nprove that the dimension of this set changes and that the local dimension,\ncalled batch functional dimension, is almost surely determined by the\nactivation patterns in the hidden layers. We prove that the batch functional\ndimension is invariant to the symmetries of the network parameterization:\nneuron permutations and positive rescalings. Empirically, we establish that the\nbatch functional dimension decreases during optimization. As a consequence,\noptimization leads to parameters with low batch functional dimensions. We call\nthis phenomenon geometry-induced implicit regularization.The batch functional\ndimension depends on both the network parameters and inputs. To understand the\nimpact of the inputs, we study, for fixed parameters, the largest attainable\nbatch functional dimension when the inputs vary. We prove that this quantity,\ncalled computable full functional dimension, is also invariant to the\nsymmetries of the network's parameterization, and is determined by the\nachievable activation patterns. We also provide a sampling theorem, showing a\nfast convergence of the estimation of the computable full functional dimension\nfor a random input of increasing size. Empirically we find that the computable\nfull functional dimension remains close to the number of parameters, which is\nrelated to the notion of local identifiability. This differs from the observed\nvalues for the batch functional dimension computed on training inputs and test\ninputs. The latter are influenced by geometry-induced implicit regularization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "math.OC",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08269v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08269v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08269v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08268v1",
    "updated": "2024-02-13T07:47:36+00:00",
    "published": "2024-02-13T07:47:36+00:00",
    "title": "World Model on Million-Length Video And Language With RingAttention",
    "authors": [
      {
        "name": "Hao Liu"
      },
      {
        "name": "Wilson Yan"
      },
      {
        "name": "Matei Zaharia"
      },
      {
        "name": "Pieter Abbeel"
      }
    ],
    "summary": "Current language models fall short in understanding aspects of the world not\neasily described in words, and struggle with complex, long-form tasks. Video\nsequences offer valuable temporal information absent in language and static\nimages, making them attractive for joint modeling with language. Such models\ncould develop a understanding of both human textual knowledge and the physical\nworld, enabling broader AI capabilities for assisting humans. However, learning\nfrom millions of tokens of video and language sequences poses challenges due to\nmemory constraints, computational complexity, and limited datasets. To address\nthese challenges, we curate a large dataset of diverse videos and books,\nutilize the RingAttention technique to scalably train on long sequences, and\ngradually increase context size from 4K to 1M tokens. This paper makes the\nfollowing contributions: (a) Largest context size neural network: We train one\nof the largest context size transformers on long video and language sequences,\nsetting new benchmarks in difficult retrieval tasks and long video\nunderstanding. (b) Solutions for overcoming vision-language training\nchallenges, including using masked sequence packing for mixing different\nsequence lengths, loss weighting to balance language and vision, and\nmodel-generated QA dataset for long sequence chat. (c) A highly-optimized\nimplementation with RingAttention, masked sequence packing, and other key\nfeatures for training on millions-length multimodal sequences. (d) Fully\nopen-sourced a family of 7B parameter models capable of processing long text\ndocuments (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M\ntokens. This work paves the way for training on massive datasets of long video\nand language to develop understanding of both human knowledge and the\nmultimodal world, and broader capabilities.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08268v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08268v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08268v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08267v1",
    "updated": "2024-02-13T07:45:25+00:00",
    "published": "2024-02-13T07:45:25+00:00",
    "title": "Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss",
    "authors": [
      {
        "name": "Kei Iino"
      },
      {
        "name": "Shunsuke Akamatsu"
      },
      {
        "name": "Hiroshi Watanabe"
      },
      {
        "name": "Shohei Enomoto"
      },
      {
        "name": "Akira Sakamoto"
      },
      {
        "name": "Takeharu Eda"
      }
    ],
    "summary": "Image coding for machines (ICM) aims to compress images for machine analysis\nusing recognition models rather than human vision. Hence, in ICM, it is\nimportant for the encoder to recognize and compress the information necessary\nfor the machine recognition task. There are two main approaches in learned ICM;\noptimization of the compression model based on task loss, and Region of\nInterest (ROI) based bit allocation. These approaches provide the encoder with\nthe recognition capability. However, optimization with task loss becomes\ndifficult when the recognition model is deep, and ROI-based methods often\ninvolve extra overhead during evaluation. In this study, we propose a novel\ntraining method for learned ICM models that applies auxiliary loss to the\nencoder to improve its recognition capability and rate-distortion performance.\nOur method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in\nobject detection and semantic segmentation tasks, compared to the conventional\ntraining method.",
    "comment": "copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08267v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08267v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08267v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08256v1",
    "updated": "2024-02-13T07:12:44+00:00",
    "published": "2024-02-13T07:12:44+00:00",
    "title": "Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs",
    "authors": [
      {
        "name": "Hengnian Gu"
      },
      {
        "name": "Zhiyi Duan"
      },
      {
        "name": "Pan Xie"
      },
      {
        "name": "Dongdai Zhou"
      }
    ],
    "summary": "The knowledge concept recommendation in Massive Open Online Courses (MOOCs)\nis a significant issue that has garnered widespread attention. Existing methods\nprimarily rely on the explicit relations between users and knowledge concepts\non the MOOC platforms for recommendation. However, there are numerous implicit\nrelations (e.g., shared interests or same knowledge levels between users)\ngenerated within the users' learning activities on the MOOC platforms. Existing\nmethods fail to consider these implicit relations, and these relations\nthemselves are difficult to learn and represent, causing poor performance in\nknowledge concept recommendation and an inability to meet users' personalized\nneeds. To address this issue, we propose a novel framework based on contrastive\nlearning, which can represent and balance the explicit and implicit relations\nfor knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we\nfirst construct a MOOCs heterogeneous information network (HIN) by modeling the\ndata from the MOOC platforms. Then, we utilize a relation-updated graph\nconvolutional network and stacked multi-channel graph neural network to\nrepresent the explicit and implicit relations in the HIN, respectively.\nConsidering that the quantity of explicit relations is relatively fewer\ncompared to implicit relations in MOOCs, we propose a contrastive learning with\nprototypical graph to enhance the representations of both relations to capture\ntheir fruitful inherent relational knowledge, which can guide the propagation\nof students' preferences within the HIN. Based on these enhanced\nrepresentations, to ensure the balanced contribution of both towards the final\nrecommendation, we propose a dual-head attention mechanism for balanced fusion.\nExperimental results demonstrate that CL-KCRec outperforms several\nstate-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.",
    "comment": "Accepted to WWW 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08256v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08256v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08256v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08255v1",
    "updated": "2024-02-13T07:07:37+00:00",
    "published": "2024-02-13T07:07:37+00:00",
    "title": "Distal Interference: Exploring the Limits of Model-Based Continual Learning",
    "authors": [
      {
        "name": "Heinrich van Deventer"
      },
      {
        "name": "Anna Sergeevna Bosman"
      }
    ],
    "summary": "Continual learning is the sequential learning of different tasks by a machine\nlearning model. Continual learning is known to be hindered by catastrophic\ninterference or forgetting, i.e. rapid unlearning of earlier learned tasks when\nnew tasks are learned. Despite their practical success, artificial neural\nnetworks (ANNs) are prone to catastrophic interference. This study analyses how\ngradient descent and overlapping representations between distant input points\nlead to distal interference and catastrophic interference. Distal interference\nrefers to the phenomenon where training a model on a subset of the domain leads\nto non-local changes on other subsets of the domain. This study shows that\nuniformly trainable models without distal interference must be exponentially\nlarge. A novel antisymmetric bounded exponential layer B-spline ANN\narchitecture named ABEL-Spline is proposed that can approximate any continuous\nfunction, is uniformly trainable, has polynomial computational complexity, and\nprovides some guarantees for distal interference. Experiments are presented to\ndemonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are also\nevaluated on benchmark regression problems. It is concluded that the weaker\ndistal interference guarantees in ABEL-Splines are insufficient for model-only\ncontinual learning. It is conjectured that continual learning with polynomial\ncomplexity models requires augmentation of the training data or algorithm.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "68T07",
      "I.5.1"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08255v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08255v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08255v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08251v1",
    "updated": "2024-02-13T06:40:55+00:00",
    "published": "2024-02-13T06:40:55+00:00",
    "title": "Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles",
    "authors": [
      {
        "name": "Minh Dang Tu"
      },
      {
        "name": "Kieu Trang Le"
      },
      {
        "name": "Manh Duong Phung"
      }
    ],
    "summary": "This work presents a neural network model capable of recognizing small and\ntiny objects in thermal images collected by unmanned aerial vehicles. Our model\nconsists of three parts, the backbone, the neck, and the prediction head. The\nbackbone is developed based on the structure of YOLOv5 combined with the use of\na transformer encoder at the end. The neck includes a BI-FPN block combined\nwith the use of a sliding window and a transformer to increase the information\nfed into the prediction head. The prediction head carries out the detection by\nevaluating feature maps with the Sigmoid function. The use of transformers with\nattention and sliding windows increases recognition accuracy while keeping the\nmodel at a reasonable number of parameters and computation requirements for\nembedded systems. Experiments conducted on public dataset VEDAI and our\ncollected datasets show that our model has a higher accuracy than\nstate-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5,\nSMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that\nour model achieves a real-time computation speed with a stability rate of over\n90%.",
    "comment": "Published in: 2024 IEEE/SICE International Symposium on System\n  Integration (SII)",
    "journal_ref": null,
    "doi": "10.1109/SII58957.2024.10417611",
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/SII58957.2024.10417611",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08251v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08251v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08251v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08250v1",
    "updated": "2024-02-13T06:38:46+00:00",
    "published": "2024-02-13T06:38:46+00:00",
    "title": "A survey of recent methods for addressing AI fairness and bias in biomedicine",
    "authors": [
      {
        "name": "Yifan Yang"
      },
      {
        "name": "Mingquan Lin"
      },
      {
        "name": "Han Zhao"
      },
      {
        "name": "Yifan Peng"
      },
      {
        "name": "Furong Huang"
      },
      {
        "name": "Zhiyong Lu"
      }
    ],
    "summary": "Artificial intelligence (AI) systems have the potential to revolutionize\nclinical practices, including improving diagnostic accuracy and surgical\ndecision-making, while also reducing costs and manpower. However, it is\nimportant to recognize that these systems may perpetuate social inequities or\ndemonstrate biases, such as those based on race or gender. Such biases can\noccur before, during, or after the development of AI models, making it critical\nto understand and address potential biases to enable the accurate and reliable\napplication of AI models in clinical settings. To mitigate bias concerns during\nmodel development, we surveyed recent publications on different debiasing\nmethods in the fields of biomedical natural language processing (NLP) or\ncomputer vision (CV). Then we discussed the methods that have been applied in\nthe biomedical domain to address bias. We performed our literature search on\nPubMed, ACM digital library, and IEEE Xplore of relevant articles published\nbetween January 2018 and December 2023 using multiple combinations of keywords.\nWe then filtered the result of 10,041 articles automatically with loose\nconstraints, and manually inspected the abstracts of the remaining 890 articles\nto identify the 55 articles included in this review. Additional articles in the\nreferences are also included in this review. We discuss each method and compare\nits strengths and weaknesses. Finally, we review other potential methods from\nthe general domain that could be applied to biomedicine to address bias and\nimprove fairness.The bias of AIs in biomedicine can originate from multiple\nsources. Existing debiasing methods that focus on algorithms can be categorized\ninto distributional or algorithmic.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08250v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08250v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08250v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08246v1",
    "updated": "2024-02-13T06:20:37+00:00",
    "published": "2024-02-13T06:20:37+00:00",
    "title": "Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles",
    "authors": [
      {
        "name": "Duy Nam Bui"
      },
      {
        "name": "Thuy Ngan Duong"
      },
      {
        "name": "Manh Duong Phung"
      }
    ],
    "summary": "This paper presents a new swarm intelligence-based approach to deal with the\ncooperative path planning problem of unmanned aerial vehicles (UAVs), which is\nessential for the automatic inspection of infrastructure. The approach uses a\n3D model of the structure to generate viewpoints for the UAVs. The calculation\nof the viewpoints considers the constraints related to the UAV formation model,\ncamera parameters, and requirements for data post-processing. The viewpoints\nare then used as input to formulate the path planning as an extended traveling\nsalesman problem and the definition of a new cost function. Ant colony\noptimization is finally used to solve the problem to yield optimal inspection\npaths. Experiments with 3D models of real structures have been conducted to\nevaluate the performance of the proposed approach. The results show that our\nsystem is not only capable of generating feasible inspection paths for UAVs but\nalso reducing the path length by 29.47\\% for complex structures when compared\nwith another heuristic approach. The source code of the algorithm can be found\nat https://github.com/duynamrcv/aco_3d_ipp.",
    "comment": "Published in: 2024 IEEE/SICE International Symposium on System\n  Integration (SII)",
    "journal_ref": null,
    "doi": "10.1109/SII58957.2024.10417512",
    "primary_category": "eess.SY",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/SII58957.2024.10417512",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08246v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08246v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08246v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08244v1",
    "updated": "2024-02-13T06:18:42+00:00",
    "published": "2024-02-13T06:18:42+00:00",
    "title": "APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks",
    "authors": [
      {
        "name": "Barathi Subramanian"
      },
      {
        "name": "Rathinaraja Jeyaraj"
      },
      {
        "name": "Rakhmonov Akhrorjon Akhmadjon Ugli"
      },
      {
        "name": "Jeonghong Kim"
      }
    ],
    "summary": "Activation function is a pivotal component of deep learning, facilitating the\nextraction of intricate data patterns. While classical activation functions\nlike ReLU and its variants are extensively utilized, their static nature and\nsimplicity, despite being advantageous, often limit their effectiveness in\nspecialized tasks. The trainable activation functions also struggle sometimes\nto adapt to the unique characteristics of the data. Addressing these\nlimitations, we introduce a novel trainable activation function, adaptive\npiecewise approximated activation linear unit (APALU), to enhance the learning\nperformance of deep learning across a broad range of tasks. It presents a\nunique set of features that enable it to maintain stability and efficiency in\nthe learning process while adapting to complex data representations.\nExperiments reveal significant improvements over widely used activation\nfunctions for different tasks. In image classification, APALU increases\nMobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the\nCIFAR10 dataset. In anomaly detection, it improves the average area under the\ncurve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11%\nimprovements with DifferNet, and knowledge distillation, respectively, on the\nMVTech dataset. Notably, APALU achieves 100% accuracy on a sign language\nrecognition task with a limited dataset. For regression tasks, APALU enhances\nthe performance of deep neural networks and recurrent neural networks on\ndifferent datasets. These improvements highlight the robustness and\nadaptability of APALU across diverse deep-learning applications.",
    "comment": "9 pages, 4 figures, Submitted at IJCAI 2024 conference",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08244v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08244v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08244v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08242v1",
    "updated": "2024-02-13T06:13:17+00:00",
    "published": "2024-02-13T06:13:17+00:00",
    "title": "Towards Equitable Agile Research and Development of AI and Robotics",
    "authors": [
      {
        "name": "Andrew Hundt"
      },
      {
        "name": "Julia Schuller"
      },
      {
        "name": "Severin Kacianka"
      }
    ],
    "summary": "Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to\nreplicate and amplify existing biases and prejudices, as do Robots with AI. For\nexample, robots with facial recognition have failed to identify Black Women as\nhuman, while others have categorized people, such as Black Men, as criminals\nbased on appearance alone. A 'culture of modularity' means harms are perceived\nas 'out of scope', or someone else's responsibility, throughout employment\npositions in the 'AI supply chain'. Incidents are routine enough\n(incidentdatabase.ai lists over 2000 examples) to indicate that few\norganizations are capable of completely respecting peoples' rights; meeting\nclaimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and\nthen addressing such failures in their organizations and artifacts. We propose\na framework for adapting widely practiced Research and Development (R&D)\nproject management methodologies to build organizational equity capabilities\nand better integrate known evidence-based best practices. We describe how\nproject teams can organize and operationalize the most promising practices,\nskill sets, organizational cultures, and methods to detect and address\nrights-based fairness, equity, accountability, and ethical problems as early as\npossible when they are often less harmful and easier to mitigate; then monitor\nfor unforeseen incidents to adaptively and constructively address them. Our\nprimary example adapts an Agile development process based on Scrum, one of the\nmost widely adopted approaches to organizing R&D teams. We also discuss\nlimitations of our proposed framework and future research directions.",
    "comment": "15 pages (32 with refs + appendix), 2 figures, 1 table (7 with\n  appendix), incorporates changes based on WeRobot 2023 Draft feedback",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.RO",
      "cs.SE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08242v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08242v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08242v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08236v1",
    "updated": "2024-02-13T06:02:05+00:00",
    "published": "2024-02-13T06:02:05+00:00",
    "title": "BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT",
    "authors": [
      {
        "name": "Siqi Peng"
      },
      {
        "name": "Hongyuan Yang"
      },
      {
        "name": "Akihiro Yamamoto"
      }
    ],
    "summary": "We propose BERT4FCA, a novel method for link prediction in bipartite\nnetworks, using formal concept analysis (FCA) and BERT. Link prediction in\nbipartite networks is an important task that can solve various practical\nproblems like friend recommendation in social networks and co-authorship\nprediction in author-paper networks. Recent research has found that in\nbipartite networks, maximal bi-cliques provide important information for link\nprediction, and they can be extracted by FCA. Some FCA-based bipartite link\nprediction methods have achieved good performance. However, we figured out that\ntheir performance could be further improved because these methods did not fully\ncapture the rich information of the extracted maximal bi-cliques. To address\nthis limitation, we propose an approach using BERT, which can learn more\ninformation from the maximal bi-cliques extracted by FCA and use them to make\nlink prediction. We conduct experiments on three real-world bipartite networks\nand demonstrate that our method outperforms previous FCA-based methods, and\nsome classic methods such as matrix-factorization and node2vec.",
    "comment": "23 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08236v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08236v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08236v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08233v1",
    "updated": "2024-02-13T05:53:00+00:00",
    "published": "2024-02-13T05:53:00+00:00",
    "title": "End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture",
    "authors": [
      {
        "name": "Fabian Krause"
      },
      {
        "name": "Jan-Peter Calliess"
      }
    ],
    "summary": "In Statistical Arbitrage (StatArb), classical mean reversion trading\nstrategies typically hinge on asset-pricing or PCA based models to identify the\nmean of a synthetic asset. Once such a (linear) model is identified, a separate\nmean reversion strategy is then devised to generate a trading signal. With a\nview of generalising such an approach and turning it truly data-driven, we\nstudy the utility of Autoencoder architectures in StatArb. As a first approach,\nwe employ a standard Autoencoder trained on US stock returns to derive trading\nstrategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance\nthis model, we take a policy-learning approach and embed the Autoencoder\nnetwork into a neural network representation of a space of portfolio trading\npolicies. This integration outputs portfolio allocations directly and is\nend-to-end trainable by backpropagation of the risk-adjusted returns of the\nneural policy. Our findings demonstrate that this innovative end-to-end policy\nlearning approach not only simplifies the strategy development process, but\nalso yields superior gross returns over its competitors illustrating the\npotential of end-to-end training over classical two-stage approaches.",
    "comment": "11 pages, 1 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-fin.TR",
    "categories": [
      "q-fin.TR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08233v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08233v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08233v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08229v1",
    "updated": "2024-02-13T05:43:49+00:00",
    "published": "2024-02-13T05:43:49+00:00",
    "title": "Causal Discovery under Off-Target Interventions",
    "authors": [
      {
        "name": "Davin Choo"
      },
      {
        "name": "Kirankumar Shiragur"
      },
      {
        "name": "Caroline Uhler"
      }
    ],
    "summary": "Causal graph discovery is a significant problem with applications across\nvarious disciplines. However, with observational data alone, the underlying\ncausal graph can only be recovered up to its Markov equivalence class, and\nfurther assumptions or interventions are necessary to narrow down the true\ngraph. This work addresses the causal discovery problem under the setting of\nstochastic interventions with the natural goal of minimizing the number of\ninterventions performed. We propose the following stochastic intervention model\nwhich subsumes existing adaptive noiseless interventions in the literature\nwhile capturing scenarios such as fat-hand interventions and CRISPR gene\nknockouts: any intervention attempt results in an actual intervention on a\nrandom subset of vertices, drawn from a distribution dependent on attempted\naction. Under this model, we study the two fundamental problems in causal\ndiscovery of verification and search and provide approximation algorithms with\npolylogarithmic competitive ratios and provide some preliminary experimental\nresults.",
    "comment": "Accepted into AISTATS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ME",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08229v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08229v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08229v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08228v2",
    "updated": "2024-02-14T16:26:09+00:00",
    "published": "2024-02-13T05:38:45+00:00",
    "title": "Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective",
    "authors": [
      {
        "name": "Kai Guo"
      },
      {
        "name": "Hongzhi Wen"
      },
      {
        "name": "Wei Jin"
      },
      {
        "name": "Yaming Guo"
      },
      {
        "name": "Jiliang Tang"
      },
      {
        "name": "Yi Chang"
      }
    ],
    "summary": "Graph neural networks (GNNs) have exhibited remarkable performance under the\nassumption that test data comes from the same distribution of training data.\nHowever, in real-world scenarios, this assumption may not always be valid.\nConsequently, there is a growing focus on exploring the Out-of-Distribution\n(OOD) problem in the context of graphs. Most existing efforts have primarily\nconcentrated on improving graph OOD generalization from two\n\\textbf{model-agnostic} perspectives: data-driven methods and strategy-based\nlearning. However, there has been limited attention dedicated to investigating\nthe impact of well-known \\textbf{GNN model architectures} on graph OOD\ngeneralization, which is orthogonal to existing research. In this work, we\nprovide the first comprehensive investigation of OOD generalization on graphs\nfrom an architecture perspective, by examining the common building blocks of\nmodern GNNs. Through extensive experiments, we reveal that both the graph\nself-attention mechanism and the decoupled architecture contribute positively\nto graph OOD generalization. In contrast, we observe that the linear\nclassification layer tends to compromise graph OOD generalization capability.\nFurthermore, we provide in-depth theoretical insights and discussions to\nunderpin these discoveries. These insights have empowered us to develop a novel\nGNN backbone model, DGAT, designed to harness the robust properties of both\ngraph self-attention mechanism and the decoupled architecture. Extensive\nexperimental results demonstrate the effectiveness of our model under graph\nOOD, exhibiting substantial and consistent enhancements across various training\nstrategies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08228v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08228v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08228v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08225v1",
    "updated": "2024-02-13T05:33:35+00:00",
    "published": "2024-02-13T05:33:35+00:00",
    "title": "Improving Black-box Robustness with In-Context Rewriting",
    "authors": [
      {
        "name": "Kyle O'Brien"
      },
      {
        "name": "Nathan Ng"
      },
      {
        "name": "Isha Puri"
      },
      {
        "name": "Jorge Mendez"
      },
      {
        "name": "Hamid Palangi"
      },
      {
        "name": "Yoon Kim"
      },
      {
        "name": "Marzyeh Ghassemi"
      },
      {
        "name": "Thomas Hartvigsen"
      }
    ],
    "summary": "Machine learning models often excel on in-distribution (ID) data but struggle\nwith unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD\nrobustness are not applicable to settings where the model is effectively a\nblack box, such as when the weights are frozen, retraining is costly, or the\nmodel is leveraged via an API. Test-time augmentation (TTA) is a simple\npost-hoc technique for improving robustness that sidesteps black-box\nconstraints by aggregating predictions across multiple augmentations of the\ntest input. TTA has seen limited use in NLP due to the challenge of generating\neffective natural language augmentations. In this work, we propose LLM-TTA,\nwhich uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA\noutperforms conventional augmentation functions across sentiment, toxicity, and\nnews classification tasks for BERT and T5 models, with BERT's OOD robustness\nimproving by an average of 4.30 percentage points without regressing average ID\nperformance. We explore selectively augmenting inputs based on prediction\nentropy to reduce the rate of expensive LLM augmentations, allowing us to\nmaintain performance gains while reducing the average number of generated\naugmentations by 57.76%. LLM-TTA is agnostic to the task model architecture,\ndoes not require OOD labels, and is effective across low and high-resource\nsettings. We share our data, models, and code for reproducibility.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08225v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08225v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08225v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08219v1",
    "updated": "2024-02-13T05:15:46+00:00",
    "published": "2024-02-13T05:15:46+00:00",
    "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
    "authors": [
      {
        "name": "Haotian Sun"
      },
      {
        "name": "Yuchen Zhuang"
      },
      {
        "name": "Wei Wei"
      },
      {
        "name": "Chao Zhang"
      },
      {
        "name": "Bo Dai"
      }
    ],
    "summary": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini\nfor specific tasks is challenging. Due to the opacity in their parameters,\nembeddings, and even output probabilities, existing fine-tuning adaptation\nmethods are inapplicable. Consequently, adapting these black-box LLMs is only\npossible through their API services, raising concerns about transparency,\nprivacy, and cost. To address these challenges, we introduce BBox-Adapter, a\nnovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target\nand source domain data by treating target data as positive and source data as\nnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to\npromote the likelihood of target domain data while penalizing that of the\nsource domain. Furthermore, it features an online adaptation mechanism, which\nincorporates real-time positive data sampling from ground-truth, human, or AI\nfeedback, coupled with negative data from previous adaptations. Extensive\nexperiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It\nimproves model performance by up to 6.77% across diverse tasks and domains,\nwhile reducing training and inference costs by 31.30x and 1.84x, respectively.",
    "comment": "24 pages, 10 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08219v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08219v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08219v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08211v1",
    "updated": "2024-02-13T04:28:43+00:00",
    "published": "2024-02-13T04:28:43+00:00",
    "title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks",
    "authors": [
      {
        "name": "Aaron Traylor"
      },
      {
        "name": "Jack Merullo"
      },
      {
        "name": "Michael J. Frank"
      },
      {
        "name": "Ellie Pavlick"
      }
    ],
    "summary": "Models based on the Transformer neural network architecture have seen success\non a wide variety of tasks that appear to require complex \"cognitive branching\"\n-- or the ability to maintain pursuit of one goal while accomplishing others.\nIn cognitive neuroscience, success on such tasks is thought to rely on\nsophisticated frontostriatal mechanisms for selective \\textit{gating}, which\nenable role-addressable updating -- and later readout -- of information to and\nfrom distinct \"addresses\" of memory, in the form of clusters of neurons.\nHowever, Transformer models have no such mechanisms intentionally built-in. It\nis thus an open question how Transformers solve such tasks, and whether the\nmechanisms that emerge to help them to do so bear any resemblance to the gating\nmechanisms in the human brain. In this work, we analyze the mechanisms that\nemerge within a vanilla attention-only Transformer trained on a simple sequence\nmodeling task inspired by a task explicitly designed to study working memory\ngating in computational cognitive neuroscience. We find that, as a result of\ntraining, the self-attention mechanism within the Transformer specializes in a\nway that mirrors the input and output gating mechanisms which were explicitly\nincorporated into earlier, more biologically-inspired architectures. These\nresults suggest opportunities for future research on computational similarities\nbetween modern AI architectures and models of the human brain.",
    "comment": "8 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "I.2.6"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08211v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08211v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08211v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08210v1",
    "updated": "2024-02-13T04:19:06+00:00",
    "published": "2024-02-13T04:19:06+00:00",
    "title": "Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS",
    "authors": [
      {
        "name": "Mohammad Ghazi Vakili"
      },
      {
        "name": "Christoph Gorgulla"
      },
      {
        "name": "AkshatKumar Nigam"
      },
      {
        "name": "Dmitry Bezrukov"
      },
      {
        "name": "Daniel Varoli"
      },
      {
        "name": "Alex Aliper"
      },
      {
        "name": "Daniil Polykovsky"
      },
      {
        "name": "Krishna M. Padmanabha Das"
      },
      {
        "name": "Jamie Snider"
      },
      {
        "name": "Anna Lyakisheva"
      },
      {
        "name": "Ardalan Hosseini Mansob"
      },
      {
        "name": "Zhong Yao"
      },
      {
        "name": "Lela Bitar"
      },
      {
        "name": "Eugene Radchenko"
      },
      {
        "name": "Xiao Ding"
      },
      {
        "name": "Jinxin Liu"
      },
      {
        "name": "Fanye Meng"
      },
      {
        "name": "Feng Ren"
      },
      {
        "name": "Yudong Cao"
      },
      {
        "name": "Igor Stagljar"
      },
      {
        "name": "Al\u00e1n Aspuru-Guzik"
      },
      {
        "name": "Alex Zhavoronkov"
      }
    ],
    "summary": "The discovery of small molecules with therapeutic potential is a\nlong-standing challenge in chemistry and biology. Researchers have increasingly\nleveraged novel computational techniques to streamline the drug development\nprocess to increase hit rates and reduce the costs associated with bringing a\ndrug to market. To this end, we introduce a quantum-classical generative model\nthat seamlessly integrates the computational power of quantum algorithms\ntrained on a 16-qubit IBM quantum computer with the established reliability of\nclassical methods for designing small molecules. Our hybrid generative model\nwas applied to designing new KRAS inhibitors, a crucial target in cancer\ntherapy. We synthesized 15 promising molecules during our investigation and\nsubjected them to experimental testing to assess their ability to engage with\nthe target. Notably, among these candidates, two molecules, ISM061-018-2 and\nISM061-22, each featuring unique scaffolds, stood out by demonstrating\neffective engagement with KRAS. ISM061-018-2 was identified as a broad-spectrum\nKRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \\mu M$.\nConcurrently, ISM061-22 exhibited specific mutant selectivity, displaying\nheightened activity against KRAS G12R and Q61H mutants. To our knowledge, this\nwork shows for the first time the use of a quantum-generative model to yield\nexperimentally confirmed biological hits, showcasing the practical potential of\nquantum-assisted drug discovery to produce viable therapeutics. Moreover, our\nfindings reveal that the efficacy of distribution learning correlates with the\nnumber of qubits utilized, underlining the scalability potential of quantum\ncomputing resources. Overall, we anticipate our results to be a stepping stone\ntowards developing more advanced quantum generative models in drug discovery.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "quant-ph",
    "categories": [
      "quant-ph",
      "cs.CE",
      "cs.GT",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08210v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08210v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08210v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08209v1",
    "updated": "2024-02-13T04:17:48+00:00",
    "published": "2024-02-13T04:17:48+00:00",
    "title": "Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits",
    "authors": [
      {
        "name": "Hiroyuki Namba"
      },
      {
        "name": "Shota Horiguchi"
      },
      {
        "name": "Masaki Hamamoto"
      },
      {
        "name": "Masashi Egi"
      }
    ],
    "summary": "Data cleansing aims to improve model performance by removing a set of harmful\ninstances from the training dataset. Data Shapley is a common theoretically\nguaranteed method to evaluate the contribution of each instance to model\nperformance; however, it requires training on all subsets of the training data,\nwhich is computationally expensive. In this paper, we propose an\niterativemethod to fast identify a subset of instances with low data Shapley\nvalues by using the thresholding bandit algorithm. We provide a theoretical\nguarantee that the proposed method can accurately select harmful instances if a\nsufficiently large number of iterations is conducted. Empirical evaluation\nusing various models and datasets demonstrated that the proposed method\nefficiently improved the computational speed while maintaining the model\nperformance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08209v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08209v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08209v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08208v1",
    "updated": "2024-02-13T04:15:26+00:00",
    "published": "2024-02-13T04:15:26+00:00",
    "title": "Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications",
    "authors": [
      {
        "name": "Mandar Pitale"
      },
      {
        "name": "Alireza Abbaspour"
      },
      {
        "name": "Devesh Upadhyay"
      }
    ],
    "summary": "This paper explores the role and challenges of Artificial Intelligence (AI)\nalgorithms, specifically AI-based software elements, in autonomous driving\nsystems. These AI systems are fundamental in executing real-time critical\nfunctions in complex and high-dimensional environments. They handle vital tasks\nlike multi-modal perception, cognition, and decision-making tasks such as\nmotion planning, lane keeping, and emergency braking. A primary concern relates\nto the ability (and necessity) of AI models to generalize beyond their initial\ntraining data. This generalization issue becomes evident in real-time\nscenarios, where models frequently encounter inputs not represented in their\ntraining or validation data. In such cases, AI systems must still function\neffectively despite facing distributional or domain shifts. This paper\ninvestigates the risk associated with overconfident AI models in\nsafety-critical applications like autonomous driving. To mitigate these risks,\nmethods for training AI models that help maintain performance without\noverconfidence are proposed. This involves implementing certainty reporting\narchitectures and ensuring diverse training data. While various\ndistribution-based methods exist to provide safety mechanisms for AI models,\nthere is a noted lack of systematic assessment of these methods, especially in\nthe context of safety-critical automotive applications. Many methods in the\nliterature do not adapt well to the quick response times required in\nsafety-critical edge applications. This paper reviews these methods, discusses\ntheir suitability for safety-critical applications, and highlights their\nstrengths and limitations. The paper also proposes potential improvements to\nenhance the safety and reliability of AI algorithms in autonomous vehicles in\nthe context of rapid and accurate decision-making processes.",
    "comment": "This article is accepted for the SAE WCX 2024 conference proceedings",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08208v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08208v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08208v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08202v1",
    "updated": "2024-02-13T04:03:09+00:00",
    "published": "2024-02-13T04:03:09+00:00",
    "title": "Confronting Discrimination in Classification: Smote Based on Marginalized Minorities in the Kernel Space for Imbalanced Data",
    "authors": [
      {
        "name": "Lingyun Zhong"
      }
    ],
    "summary": "Financial fraud detection poses a typical challenge characterized by class\nimbalance, where instances of fraud are extremely rare but can lead to\nunpredictable economic losses if misidentified. Precisely classifying these\ncritical minority samples represents a challenging task within the\nclassification. The primary difficulty arises from mainstream classifiers,\nwhich often exhibit \"implicit discrimination\" against minority samples in\nevaluation metrics, which results in frequent misclassifications, and the key\nto the problem lies in the overlap of feature spaces between majority and\nminority samples. To address these challenges, oversampling is a feasible\nsolution, yet current classical oversampling methods often lack the necessary\ncaution in sample selection, exacerbating feature space overlap. In response,\nwe propose a novel classification oversampling approach based on the decision\nboundary and sample proximity relationships. This method carefully considers\nthe distance between critical samples and the decision hyperplane, as well as\nthe density of surrounding samples, resulting in an adaptive oversampling\nstrategy in the kernel space. Finally, we test the proposed method on a classic\nfinancial fraud dataset, and the results show that our proposed method provides\nan effective and robust solution that can improve the classification accuracy\nof minorities.",
    "comment": "10 pages, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08202v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08202v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08202v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08201v1",
    "updated": "2024-02-13T03:55:56+00:00",
    "published": "2024-02-13T03:55:56+00:00",
    "title": "Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap",
    "authors": [
      {
        "name": "Mohammad Mehrabi"
      },
      {
        "name": "Stefan Wager"
      }
    ],
    "summary": "Doubly robust methods hold considerable promise for off-policy evaluation in\nMarkov decision processes (MDPs) under sequential ignorability: They have been\nshown to converge as $1/\\sqrt{T}$ with the horizon $T$, to be statistically\nefficient in large samples, and to allow for modular implementation where\npreliminary estimation tasks can be executed using standard reinforcement\nlearning techniques. Existing results, however, make heavy use of a strong\ndistributional overlap assumption whereby the stationary distributions of the\ntarget policy and the data-collection policy are within a bounded factor of\neach other -- and this assumption is typically only credible when the state\nspace of the MDP is bounded. In this paper, we re-visit the task of off-policy\nevaluation in MDPs under a weaker notion of distributional overlap, and\nintroduce a class of truncated doubly robust (TDR) estimators which we find to\nperform well in this setting. When the distribution ratio of the target and\ndata-collection policies is square-integrable (but not necessarily bounded),\nour approach recovers the large-sample behavior previously established under\nstrong distributional overlap. When this ratio is not square-integrable, TDR is\nstill consistent but with a slower-than-$1/\\sqrt{T}$; furthermore, this rate of\nconvergence is minimax over a class of MDPs defined only using mixing\nconditions. We validate our approach numerically and find that, in our\nexperiments, appropriate truncation plays a major role in enabling accurate\noff-policy evaluation when strong distributional overlap does not hold.",
    "comment": "50 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08201v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08201v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08201v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08198v1",
    "updated": "2024-02-13T03:51:10+00:00",
    "published": "2024-02-13T03:51:10+00:00",
    "title": "PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction",
    "authors": [
      {
        "name": "Lirong Wu"
      },
      {
        "name": "Yufei Huang"
      },
      {
        "name": "Cheng Tan"
      },
      {
        "name": "Zhangyang Gao"
      },
      {
        "name": "Bozhen Hu"
      },
      {
        "name": "Haitao Lin"
      },
      {
        "name": "Zicheng Liu"
      },
      {
        "name": "Stan Z. Li"
      }
    ],
    "summary": "Compound-Protein Interaction (CPI) prediction aims to predict the pattern and\nstrength of compound-protein interactions for rational drug discovery. Existing\ndeep learning-based methods utilize only the single modality of protein\nsequences or structures and lack the co-modeling of the joint distribution of\nthe two modalities, which may lead to significant performance drops in complex\nreal-world scenarios due to various factors, e.g., modality missing and domain\nshifting. More importantly, these methods only model protein sequences and\nstructures at a single fixed scale, neglecting more fine-grained multi-scale\ninformation, such as those embedded in key protein fragments. In this paper, we\npropose a novel multi-scale Protein Sequence-structure Contrasting framework\nfor CPI prediction (PSC-CPI), which captures the dependencies between protein\nsequences and structures through both intra-modality and cross-modality\ncontrasting. We further apply length-variable protein augmentation to allow\ncontrasting to be performed at different scales, from the amino acid level to\nthe sequence level. Finally, in order to more fairly evaluate the model\ngeneralizability, we split the test data into four settings based on whether\ncompounds and proteins have been observed during the training stage. Extensive\nexperiments have shown that PSC-CPI generalizes well in all four settings,\nparticularly in the more challenging ``Unseen-Both\" setting, where neither\ncompounds nor proteins have been observed during training. Furthermore, even\nwhen encountering a situation of modality missing, i.e., inference with only\nsingle-modality protein data, PSC-CPI still exhibits comparable or even better\nperformance than previous approaches.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.BM",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08198v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08198v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08198v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08193v1",
    "updated": "2024-02-13T03:31:36+00:00",
    "published": "2024-02-13T03:31:36+00:00",
    "title": "Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems",
    "authors": [
      {
        "name": "Dan MacKinlay"
      },
      {
        "name": "Russell Tsuchida"
      },
      {
        "name": "Dan Pagendam"
      },
      {
        "name": "Petra Kuhnert"
      }
    ],
    "summary": "Efficient inference in high-dimensional models remains a central challenge in\nmachine learning. This paper introduces the Gaussian Ensemble Belief\nPropagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and\nGaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing\nlow-rank local messages in a graphical model structure. This combination\ninherits favourable qualities from each method. Ensemble techniques allow GEnBP\nto handle high-dimensional states, parameters and intricate, noisy, black-box\ngeneration processes. The use of local messages in a graphical model structure\nensures that the approach is suited to distributed computing and can\nefficiently handle complex dependence structures. GEnBP is particularly\nadvantageous when the ensemble size is considerably smaller than the inference\ndimension. This scenario often arises in fields such as spatiotemporal\nmodelling, image processing and physical model inversion. GEnBP can be applied\nto general problem structures, including jointly learning system parameters,\nobservation parameters, and latent state variables.",
    "comment": "Under conference submission",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML",
      "62-07 (Primary) 62F15, 62M40, 68T05, 68W25",
      "I.2.6; H.2.4; I.2.8; J.2"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08193v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08193v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08193v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08191v1",
    "updated": "2024-02-13T03:25:33+00:00",
    "published": "2024-02-13T03:25:33+00:00",
    "title": "THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation",
    "authors": [
      {
        "name": "Wilbert Pumacay"
      },
      {
        "name": "Ishika Singh"
      },
      {
        "name": "Jiafei Duan"
      },
      {
        "name": "Ranjay Krishna"
      },
      {
        "name": "Jesse Thomason"
      },
      {
        "name": "Dieter Fox"
      }
    ],
    "summary": "To realize effective large-scale, real-world robotic applications, we must\nevaluate how well our robot policies adapt to changes in environmental\nconditions. Unfortunately, a majority of studies evaluate robot performance in\nenvironments closely resembling or even identical to the training setup. We\npresent THE COLOSSEUM, a novel simulation benchmark, with 20 diverse\nmanipulation tasks, that enables systematical evaluation of models across 12\naxes of environmental perturbations. These perturbations include changes in\ncolor, texture, and size of objects, table-tops, and backgrounds; we also vary\nlighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4\nstate-of-the-art manipulation models to reveal that their success rate degrades\nbetween 30-50% across these perturbation factors. When multiple perturbations\nare applied in unison, the success rate degrades $\\geq$75%. We identify that\nchanging the number of distractor objects, target object color, or lighting\nconditions are the perturbations that reduce model performance the most. To\nverify the ecological validity of our results, we show that our results in\nsimulation are correlated ($\\bar{R}^2 = 0.614$) to similar perturbations in\nreal-world experiments. We open source code for others to use THE COLOSSEUM,\nand also release code to 3D print the objects used to replicate the real-world\nperturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark\nto identify modeling decisions that systematically improve generalization for\nmanipulation. See https://robot-colosseum.github.io/ for more details.",
    "comment": "30 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08191v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08191v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08191v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08187v1",
    "updated": "2024-02-13T03:14:32+00:00",
    "published": "2024-02-13T03:14:32+00:00",
    "title": "Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids",
    "authors": [
      {
        "name": "Sung Woong Cho"
      },
      {
        "name": "Jae Yong Lee"
      },
      {
        "name": "Hyung Ju Hwang"
      }
    ],
    "summary": "Scientific computing using deep learning has seen significant advancements in\nrecent years. There has been growing interest in models that learn the operator\nfrom the parameters of a partial differential equation (PDE) to the\ncorresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural\noperator, among other models, have been designed with structures suitable for\nhandling functions as inputs and outputs, enabling real-time predictions as\nsurrogate models for solution operators. There has also been significant\nprogress in the research on surrogate models based on graph neural networks\n(GNNs), specifically targeting the dynamics in time-dependent PDEs. In this\npaper, we propose GraphDeepONet, an autoregressive model based on GNNs, to\neffectively adapt DeepONet, which is well-known for successful operator\nlearning. GraphDeepONet exhibits robust accuracy in predicting solutions\ncompared to existing GNN-based PDE solver models. It maintains consistent\nperformance even on irregular grids, leveraging the advantages inherited from\nDeepONet and enabling predictions on arbitrary grids. Additionally, unlike\ntraditional DeepONet and its variants, GraphDeepONet enables time extrapolation\nfor time-dependent PDE solutions. We also provide theoretical analysis of the\nuniversal approximation capability of GraphDeepONet in approximating continuous\noperators across arbitrary time intervals.",
    "comment": "25 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "65D17, 68U07"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08187v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08187v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08187v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08185v1",
    "updated": "2024-02-13T03:01:22+00:00",
    "published": "2024-02-13T03:01:22+00:00",
    "title": "Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5",
    "authors": [
      {
        "name": "Minjong Cheon"
      },
      {
        "name": "Daehyun Kang"
      },
      {
        "name": "Yo-Hwan Choi"
      },
      {
        "name": "Seon-Yu Kang"
      }
    ],
    "summary": "Modern deep learning techniques, which mimic traditional numerical weather\nprediction (NWP) models and are derived from global atmospheric reanalysis\ndata, have caused a significant revolution within a few years. In this new\nparadigm, our research introduces a novel strategy that deviates from the\ncommon dependence on high-resolution data, which is often constrained by\ncomputational resources, and instead utilizes low-resolution data (2.5 degrees)\nfor global weather prediction and climate data analysis. Our main focus is\nevaluating data-driven weather prediction (DDWP) frameworks, specifically\naddressing sample size adequacy, structural improvements to the model, and the\nability of climate data to represent current climatic trends. By using the\nAdaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed\ntime-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5),\nthis paper improves on conventional approaches by adding more variables and a\nnovel approach to data augmentation and processing. Our findings reveal that\ndespite the lower resolution, the proposed approach demonstrates considerable\naccuracy in predicting atmospheric conditions, effectively rivaling\nhigher-resolution models. Furthermore, the study confirms the model's\nproficiency in reflecting current climate trends and its potential in\npredicting future climatic events, underscoring its utility in climate change\nstrategies. This research marks a pivotal step in the realm of meteorological\nforecasting, showcasing the feasibility of lower-resolution data in producing\nreliable predictions and opening avenues for more accessible and inclusive\nclimate modeling. The insights gleaned from this study not only contribute to\nthe advancement of climate science but also lay the groundwork for future\ninnovations in the field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CV",
      "physics.ao-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08185v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08185v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08185v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08184v1",
    "updated": "2024-02-13T02:48:18+00:00",
    "published": "2024-02-13T02:48:18+00:00",
    "title": "Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation",
    "authors": [
      {
        "name": "Ayesha Siddika Nipu"
      },
      {
        "name": "Siming Liu"
      },
      {
        "name": "Anthony Harris"
      }
    ],
    "summary": "Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in\ntackling complex tasks that require collaboration and competition among agents\nin dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch\nis arduous and may not always be feasible, particularly for MASs with a large\nnumber of interactive agents due to the extensive sample complexity. Therefore,\nreusing knowledge gained from past experiences or other agents could\nefficiently accelerate the learning process and upscale MARL algorithms. In\nthis study, we introduce a novel framework that enables transfer learning for\nMARL through unifying various state spaces into fixed-size inputs that allow\none unified deep-learning policy viable in different scenarios within a MAS. We\nevaluated our approach in a range of scenarios within the StarCraft Multi-Agent\nChallenge (SMAC) environment, and the findings show significant enhancements in\nmulti-agent learning performance using maneuvering skills learned from other\nscenarios compared to agents learning from scratch. Furthermore, we adopted\nCurriculum Transfer Learning (CTL), enabling our deep learning policy to\nprogressively acquire knowledge and skills across pre-designed homogeneous\nlearning scenarios organized by difficulty levels. This process promotes inter-\nand intra-agent knowledge transfer, leading to high multi-agent learning\nperformance in more complicated heterogeneous scenarios.",
    "comment": "2023 IEEE Conference on Games (CoG)",
    "journal_ref": null,
    "doi": "10.1109/CoG57401.2023.10333236",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/CoG57401.2023.10333236",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08184v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08184v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08184v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08182v1",
    "updated": "2024-02-13T02:41:56+00:00",
    "published": "2024-02-13T02:41:56+00:00",
    "title": "Variational Continual Test-Time Adaptation",
    "authors": [
      {
        "name": "Fan Lyu"
      },
      {
        "name": "Kaile Du"
      },
      {
        "name": "Yuyang Li"
      },
      {
        "name": "Hanyu Zhao"
      },
      {
        "name": "Zhang Zhang"
      },
      {
        "name": "Guangcan Liu"
      },
      {
        "name": "Liang Wang"
      }
    ],
    "summary": "The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods\nthat only use unlabeled test data, as it can cause significant error\npropagation. In this paper, we introduce VCoTTA, a variational Bayesian\napproach to measure uncertainties in CTTA. At the source stage, we transform a\npre-trained deterministic model into a Bayesian Neural Network (BNN) via a\nvariational warm-up strategy, injecting uncertainties into the model. During\nthe testing time, we employ a mean-teacher update strategy using variational\ninference for the student model and exponential moving average for the teacher\nmodel. Our novel approach updates the student model by combining priors from\nboth the source and teacher models. The evidence lower bound is formulated as\nthe cross-entropy between the student and teacher models, along with the\nKullback-Leibler (KL) divergence of the prior mixture. Experimental results on\nthree datasets demonstrate the method's effectiveness in mitigating prior drift\nwithin the CTTA framework.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08182v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08182v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08182v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08180v1",
    "updated": "2024-02-13T02:36:41+00:00",
    "published": "2024-02-13T02:36:41+00:00",
    "title": "Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss",
    "authors": [
      {
        "name": "Shinsaku Sakaue"
      },
      {
        "name": "Han Bao"
      },
      {
        "name": "Taira Tsuchiya"
      },
      {
        "name": "Taihei Oki"
      }
    ],
    "summary": "This paper studies online structured prediction with full-information\nfeedback. For online multiclass classification, van der Hoeven (2020) has\nobtained surrogate regret bounds independent of the time horizon, or\n\\emph{finite}, by introducing an elegant \\emph{exploit-the-surrogate-gap}\nframework. However, this framework has been limited to multiclass\nclassification primarily because it relies on a classification-specific\nprocedure for converting estimated scores to outputs. We extend the\nexploit-the-surrogate-gap framework to online structured prediction with\n\\emph{Fenchel--Young losses}, a large family of surrogate losses including the\nlogistic loss for multiclass classification, obtaining finite surrogate regret\nbounds in various structured prediction problems. To this end, we propose and\nanalyze \\emph{randomized decoding}, which converts estimated scores to general\nstructured outputs. Moreover, by applying our decoding to online multiclass\nclassification with the logistic loss, we obtain a surrogate regret bound of\n$O(B^2)$, where $B$ is the $\\ell_2$-diameter of the domain. This bound is tight\nup to logarithmic factors and improves the previous bound of $O(dB^2)$ due to\nvan der Hoeven (2020) by a factor of $d$, the number of classes.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08180v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08180v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08180v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08178v1",
    "updated": "2024-02-13T02:28:57+00:00",
    "published": "2024-02-13T02:28:57+00:00",
    "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents",
    "authors": [
      {
        "name": "Jae-Woo Choi"
      },
      {
        "name": "Youngwoo Yoon"
      },
      {
        "name": "Hyobin Ong"
      },
      {
        "name": "Jaehong Kim"
      },
      {
        "name": "Minsu Jang"
      }
    ],
    "summary": "Large language models (LLMs) have recently received considerable attention as\nalternative solutions for task planning. However, comparing the performance of\nlanguage-oriented task planners becomes difficult, and there exists a dearth of\ndetailed exploration regarding the effects of various factors such as\npre-trained model selection and prompt construction. To address this, we\npropose a benchmark system for automatically quantifying performance of task\nplanning for home-service embodied agents. Task planners are tested on two\npairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of\nWatch-And-Help and VirtualHome. Using the proposed benchmark system, we perform\nextensive experiments with LLMs and prompts, and explore several enhancements\nof the baseline planner. We expect that the proposed benchmark tool would\naccelerate the development of language-oriented task planners.",
    "comment": "ICLR 2024. Code: https://github.com/lbaa2022/LLMTaskPlanning",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08178v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08178v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08178v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08174v1",
    "updated": "2024-02-13T02:13:12+00:00",
    "published": "2024-02-13T02:13:12+00:00",
    "title": "Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction",
    "authors": [
      {
        "name": "Minsang Kim"
      },
      {
        "name": "Seungjun Baek"
      }
    ],
    "summary": "Learning positional information of nodes in a graph is important for link\nprediction tasks. We propose a representation of positional information using\nrepresentative nodes called landmarks. A small number of nodes with high degree\ncentrality are selected as landmarks, which serve as reference points for the\nnodes' positions. We justify this selection strategy for well-known random\ngraph models and derive closed-form bounds on the average path lengths\ninvolving landmarks. In a model for power-law graphs, we prove that landmarks\nprovide asymptotically exact information on inter-node distances. We apply\ntheoretical insights to practical networks and propose Hierarchical Position\nembedding with Landmarks and Clustering (HPLC). HPLC combines landmark\nselection and graph clustering, where the graph is partitioned into densely\nconnected clusters in which nodes with the highest degree are selected as\nlandmarks. HPLC leverages the positional information of nodes based on\nlandmarks at various levels of hierarchy such as nodes' distances to landmarks,\ninter-landmark distances and hierarchical grouping of clusters. Experiments\nshow that HPLC achieves state-of-the-art performances of link prediction on\nvarious datasets in terms of HIT@K, MRR, and AUC. The code is available at\n\\url{https://github.com/kmswin1/HPLC}.",
    "comment": "The International World Wide Web Conference (WWW) 2024, Accepted\n  paper",
    "journal_ref": null,
    "doi": "10.1145/3589334.3645372",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3589334.3645372",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08174v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08174v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08174v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08170v1",
    "updated": "2024-02-13T02:03:26+00:00",
    "published": "2024-02-13T02:03:26+00:00",
    "title": "LLaGA: Large Language and Graph Assistant",
    "authors": [
      {
        "name": "Runjin Chen"
      },
      {
        "name": "Tong Zhao"
      },
      {
        "name": "Ajay Jaiswal"
      },
      {
        "name": "Neil Shah"
      },
      {
        "name": "Zhangyang Wang"
      }
    ],
    "summary": "Graph Neural Networks (GNNs) have empowered the advance in graph-structured\ndata analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4\nhas heralded a new era in deep learning. However, their application to graph\ndata poses distinct challenges due to the inherent difficulty of translating\ngraph structures to language. To this end, we introduce the \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{G}raph \\textbf{A}ssistant\n(\\textbf{LLaGA}), an innovative model that effectively integrates LLM\ncapabilities to handle the complexities of graph-structured data. LLaGA retains\nthe general-purpose nature of LLMs while adapting graph data into a format\ncompatible with LLM input. LLaGA achieves this by reorganizing graph nodes to\nstructure-aware sequences and then mapping these into the token embedding space\nthrough a versatile projector. LLaGA excels in versatility, generalizability\nand interpretability, allowing it to perform consistently well across different\ndatasets and tasks, extend its ability to unseen datasets or tasks, and provide\nexplanations for graphs. Our extensive experiments across popular graph\nbenchmarks show that LLaGA delivers outstanding performance across four\ndatasets and three tasks using one single model, surpassing state-of-the-art\ngraph models in both supervised and zero-shot scenarios. Our code is available\nat \\url{https://github.com/ChenRunjin/LLaGA}",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08170v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08170v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08170v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08164v1",
    "updated": "2024-02-13T01:52:15+00:00",
    "published": "2024-02-13T01:52:15+00:00",
    "title": "On Limitations of the Transformer Architecture",
    "authors": [
      {
        "name": "Binghui Peng"
      },
      {
        "name": "Srini Narayanan"
      },
      {
        "name": "Christos Papadimitriou"
      }
    ],
    "summary": "What are the root causes of hallucinations in large language models (LLMs)?\nWe use Communication Complexity to prove that the Transformer layer is\nincapable of composing functions (e.g., identify a grandparent of a person in a\ngenealogy) if the domains of the functions are large enough; we show through\nexamples that this inability is already empirically present when the domains\nare quite small. We also point out that several mathematical tasks that are at\nthe core of the so-called compositional tasks thought to be hard for LLMs are\nunlikely to be solvable by Transformers, for large enough instances and\nassuming that certain well accepted conjectures in the field of Computational\nComplexity are true.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08164v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08164v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08164v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08156v1",
    "updated": "2024-02-13T01:38:01+00:00",
    "published": "2024-02-13T01:38:01+00:00",
    "title": "Group Decision-Making among Privacy-Aware Agents",
    "authors": [
      {
        "name": "Marios Papachristou"
      },
      {
        "name": "M. Amin Rahimian"
      }
    ],
    "summary": "How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MA",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08156v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08156v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08156v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08155v1",
    "updated": "2024-02-13T01:31:08+00:00",
    "published": "2024-02-13T01:31:08+00:00",
    "title": "CMA-R:Causal Mediation Analysis for Explaining Rumour Detection",
    "authors": [
      {
        "name": "Lin Tian"
      },
      {
        "name": "Xiuzhen Zhang"
      },
      {
        "name": "Jey Han Lau"
      }
    ],
    "summary": "We apply causal mediation analysis to explain the decision-making process of\nneural models for rumour detection on Twitter. Interventions at the input and\nnetwork level reveal the causal impacts of tweets and words in the model\noutput. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour\ndetection -- identifies salient tweets that explain model predictions and show\nstrong agreement with human judgements for critical tweets determining the\ntruthfulness of stories. CMA-R can further highlight causally impactful words\nin the salient tweets, providing another layer of interpretability and\ntransparency into these blackbox rumour detection systems. Code is available\nat: https://github.com/ltian678/cma-r.",
    "comment": "9 pages, 7 figures, Accepted by EACL 2024 Findings",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08155v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08155v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08155v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08151v1",
    "updated": "2024-02-13T01:03:39+00:00",
    "published": "2024-02-13T01:03:39+00:00",
    "title": "Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models",
    "authors": [
      {
        "name": "Joshua C Chang"
      },
      {
        "name": "Xiangting Li"
      },
      {
        "name": "Shixin Xu"
      },
      {
        "name": "Hao-Ren Yao"
      },
      {
        "name": "Julia Porcino"
      },
      {
        "name": "Carson Chow"
      }
    ],
    "summary": "We introduce a set of gradient-flow-guided adaptive importance sampling (IS)\ntransformations to stabilize Monte-Carlo approximations of point-wise leave one\nout cross-validated (LOO) predictions for Bayesian classification models. One\ncan leverage this methodology for assessing model generalizability by for\ninstance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves\nand derived metrics like the AUROC and AUPRC. By the calculus of variations and\ngradient flow, we derive two simple nonlinear single-step transformations that\nutilize gradient information to shift a model's pre-trained full-data posterior\ncloser to the target LOO posterior predictive distributions. In doing so, the\ntransformations stabilize importance weights. Because the transformations\ninvolve the gradient of the likelihood function, the resulting Monte Carlo\nintegral depends on Jacobian determinants with respect to the model Hessian. We\nderive closed-form exact formulae for these Jacobian determinants in the cases\nof logistic regression and shallow ReLU-activated artificial neural networks,\nand provide a simple approximation that sidesteps the need to compute full\nHessian matrices and their spectra. We test the methodology on an $n\\ll p$\ndataset that is known to produce unstable LOO IS weights.",
    "comment": "Submitted",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ME",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "math.SP",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08151v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08151v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08151v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08147v1",
    "updated": "2024-02-13T00:55:14+00:00",
    "published": "2024-02-13T00:55:14+00:00",
    "title": "Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search",
    "authors": [
      {
        "name": "David Brandfonbrener"
      },
      {
        "name": "Sibi Raja"
      },
      {
        "name": "Tarun Prasad"
      },
      {
        "name": "Chloe Loughridge"
      },
      {
        "name": "Jianang Yang"
      },
      {
        "name": "Simon Henniger"
      },
      {
        "name": "William E. Byrd"
      },
      {
        "name": "Robert Zinkov"
      },
      {
        "name": "Nada Amin"
      }
    ],
    "summary": "We present an approach using Monte Carlo Tree Search (MCTS) to guide Large\nLanguage Models (LLMs) to generate verified programs in Dafny, Lean and Coq.\nOur method, which we call VMCTS, leverages the verifier inside the search\nalgorithm by checking partial programs at each step. In combination with the\nLLM prior, the verifier feedback raises the synthesis capabilities of open\nsource models. On a set of five verified programming problems, we find that in\nfour problems where the base model cannot solve the question even when\nre-sampling solutions for one hour, VMCTS can solve the problems within 6\nminutes. The base model with VMCTS is even competitive with ChatGPT4 augmented\nwith plugins and multiple re-tries on these problems. Our code and benchmarks\nare available at\nhttps://github.com/namin/llm-verified-with-monte-carlo-tree-search .",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08147v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08147v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08147v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08145v1",
    "updated": "2024-02-13T00:50:06+00:00",
    "published": "2024-02-13T00:50:06+00:00",
    "title": "Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings",
    "authors": [
      {
        "name": "Rushang Karia"
      },
      {
        "name": "Pulkit Verma"
      },
      {
        "name": "Alberto Speranzon"
      },
      {
        "name": "Siddharth Srivastava"
      }
    ],
    "summary": "This paper introduces a new approach for continual planning and model\nlearning in non-stationary stochastic environments expressed using relational\nrepresentations. Such capabilities are essential for the deployment of\nsequential decision-making systems in the uncertain, constantly evolving real\nworld. Working in such practical settings with unknown (and non-stationary)\ntransition systems and changing tasks, the proposed framework models gaps in\nthe agent's current state of knowledge and uses them to conduct focused,\ninvestigative explorations. Data collected using these explorations is used for\nlearning generalizable probabilistic models for solving the current task\ndespite continual changes in the environment dynamics. Empirical evaluations on\nseveral benchmark domains show that this approach significantly outperforms\nplanning and RL baselines in terms of sample complexity in non-stationary\nsettings. Theoretical results show that the system reverts to exhibit desirable\nconvergence properties when stationarity holds.",
    "comment": "To appear at ICAPS-24",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08145v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08145v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08145v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08144v1",
    "updated": "2024-02-13T00:46:46+00:00",
    "published": "2024-02-13T00:46:46+00:00",
    "title": "Average-Case Analysis of Iterative Voting",
    "authors": [
      {
        "name": "Joshua Kavner"
      },
      {
        "name": "Lirong Xia"
      }
    ],
    "summary": "Iterative voting is a natural model of repeated strategic decision-making in\nsocial choice when agents have the opportunity to update their votes prior to\nfinalizing the group decision. Prior work has analyzed the efficacy of\niterative plurality on the welfare of the chosen outcome at equilibrium,\nrelative to the truthful vote profile, via an adaptation of the price of\nanarchy. However, prior analyses have only studied the worst-case and\naverage-case performances when agents' preferences are distributed by the\nimpartial culture. This work extends average-case analyses to a wider class of\ndistributions and distinguishes when iterative plurality improves or degrades\nasymptotic welfare.",
    "comment": "63 pages, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08144v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08144v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08144v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08134v1",
    "updated": "2024-02-13T00:02:05+00:00",
    "published": "2024-02-13T00:02:05+00:00",
    "title": "Randomized Algorithms for Symmetric Nonnegative Matrix Factorization",
    "authors": [
      {
        "name": "Koby Hayashi"
      },
      {
        "name": "Sinan G. Aksoy"
      },
      {
        "name": "Grey Ballard"
      },
      {
        "name": "Haesun Park"
      }
    ],
    "summary": "Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data\nanalysis and machine learning that approximates a symmetric matrix with a\nproduct of a nonnegative, low-rank matrix and its transpose. To design faster\nand more scalable algorithms for SymNMF we develop two randomized algorithms\nfor its computation. The first algorithm uses randomized matrix sketching to\ncompute an initial low-rank input matrix and proceeds to use this input to\nrapidly compute a SymNMF. The second algorithm uses randomized leverage score\nsampling to approximately solve constrained least squares problems. Many\nsuccessful methods for SymNMF rely on (approximately) solving sequences of\nconstrained least squares problems. We prove theoretically that leverage score\nsampling can approximately solve nonnegative least squares problems to a chosen\naccuracy with high probability. Finally we demonstrate that both methods work\nwell in practice by applying them to graph clustering tasks on large real world\ndata sets. These experiments show that our methods approximately maintain\nsolution quality and achieve significant speed ups for both large dense and\nlarge sparse problems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "math.OC",
      "65F55, 65F20"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08134v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08134v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08134v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08132v2",
    "updated": "2024-02-14T13:04:28+00:00",
    "published": "2024-02-12T23:55:55+00:00",
    "title": "On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era",
    "authors": [
      {
        "name": "Matteo Tiezzi"
      },
      {
        "name": "Michele Casoni"
      },
      {
        "name": "Alessandro Betti"
      },
      {
        "name": "Tommaso Guidi"
      },
      {
        "name": "Marco Gori"
      },
      {
        "name": "Stefano Melacci"
      }
    ],
    "summary": "A longstanding challenge for the Machine Learning community is the one of\ndeveloping models that are capable of processing and learning from very long\nsequences of data. The outstanding results of Transformers-based networks\n(e.g., Large Language Models) promotes the idea of parallel attention as the\nkey to succeed in such a challenge, obfuscating the role of classic sequential\nprocessing of Recurrent Models. However, in the last few years, researchers who\nwere concerned by the quadratic complexity of self-attention have been\nproposing a novel wave of neural models, which gets the best from the two\nworlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State\nModels emerged as robust approaches to function approximation over time, thus\nopening a new perspective in learning from sequential data, followed by many\npeople in the field and exploited to implement a special class of (linear)\nRecurrent Neural Networks. This survey is aimed at providing an overview of\nthese trends framed under the unifying umbrella of Recurrence. Moreover, it\nemphasizes novel research opportunities that become prominent when abandoning\nthe idea of processing long sequences whose length is known-in-advance for the\nmore realistic setting of potentially infinite-length sequences, thus\nintersecting the field of lifelong-online learning from streamed data.",
    "comment": "Under review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08132v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08132v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08132v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08128v1",
    "updated": "2024-02-12T23:53:46+00:00",
    "published": "2024-02-12T23:53:46+00:00",
    "title": "Recursive Joint Simulation in Games",
    "authors": [
      {
        "name": "Vojtech Kovarik"
      },
      {
        "name": "Caspar Oesterheld"
      },
      {
        "name": "Vincent Conitzer"
      }
    ],
    "summary": "Game-theoretic dynamics between AI agents could differ from traditional\nhuman-human interactions in various ways. One such difference is that it may be\npossible to accurately simulate an AI agent, for example because its source\ncode is known. Our aim is to explore ways of leveraging this possibility to\nachieve more cooperative outcomes in strategic settings. In this paper, we\nstudy an interaction between AI agents where the agents run a recursive joint\nsimulation. That is, the agents first jointly observe a simulation of the\nsituation they face. This simulation in turn recursively includes additional\nsimulations (with a small chance of failure, to avoid infinite recursion), and\nthe results of all these nested simulations are observed before an action is\nchosen. We show that the resulting interaction is strategically equivalent to\nan infinitely repeated version of the original game, allowing a direct transfer\nof existing results such as the various folk theorems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08128v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08128v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08128v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08127v1",
    "updated": "2024-02-12T23:50:47+00:00",
    "published": "2024-02-12T23:50:47+00:00",
    "title": "Efficient Contextual Bandits with Uninformed Feedback Graphs",
    "authors": [
      {
        "name": "Mengxiao Zhang"
      },
      {
        "name": "Yuheng Zhang"
      },
      {
        "name": "Haipeng Luo"
      },
      {
        "name": "Paul Mineiro"
      }
    ],
    "summary": "Bandits with feedback graphs are powerful online learning models that\ninterpolate between the full information and classic bandit problems, capturing\nmany real-life applications. A recent work by Zhang et al. (2023) studies the\ncontextual version of this problem and proposes an efficient and optimal\nalgorithm via a reduction to online regression. However, their algorithm\ncrucially relies on seeing the feedback graph before making each decision,\nwhile in many applications, the feedback graph is uninformed, meaning that it\nis either only revealed after the learner makes her decision or even never\nfully revealed at all. This work develops the first contextual algorithm for\nsuch uninformed settings, via an efficient reduction to online regression over\nboth the losses and the graphs. Importantly, we show that it is critical to\nlearn the graphs using log loss instead of squared loss to obtain favorable\nregret guarantees. We also demonstrate the empirical effectiveness of our\nalgorithm on a bidding application using both synthetic and real-world data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08127v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08127v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08127v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08126v1",
    "updated": "2024-02-12T23:50:44+00:00",
    "published": "2024-02-12T23:50:44+00:00",
    "title": "Contextual Multinomial Logit Bandits with General Value Functions",
    "authors": [
      {
        "name": "Mengxiao Zhang"
      },
      {
        "name": "Haipeng Luo"
      }
    ],
    "summary": "Contextual multinomial logit (MNL) bandits capture many real-world assortment\nrecommendation problems such as online retailing/advertising. However, prior\nwork has only considered (generalized) linear value functions, which greatly\nlimits its applicability. Motivated by this fact, in this work, we consider\ncontextual MNL bandits with a general value function class that contains the\nground truth, borrowing ideas from a recent trend of studies on contextual\nbandits. Specifically, we consider both the stochastic and the adversarial\nsettings, and propose a suite of algorithms, each with different\ncomputation-regret trade-off. When applied to the linear case, our results not\nonly are the first ones with no dependence on a certain problem-dependent\nconstant that can be exponentially large, but also enjoy other advantages such\nas computational efficiency, dimension-free regret bounds, or the ability to\nhandle completely adversarial contexts and rewards.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08126v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08126v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08126v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08125v1",
    "updated": "2024-02-12T23:49:40+00:00",
    "published": "2024-02-12T23:49:40+00:00",
    "title": "Customizable Perturbation Synthesis for Robust SLAM Benchmarking",
    "authors": [
      {
        "name": "Xiaohao Xu"
      },
      {
        "name": "Tianyi Zhang"
      },
      {
        "name": "Sibo Wang"
      },
      {
        "name": "Xiang Li"
      },
      {
        "name": "Yongqi Chen"
      },
      {
        "name": "Ye Li"
      },
      {
        "name": "Bhiksha Raj"
      },
      {
        "name": "Matthew Johnson-Roberson"
      },
      {
        "name": "Xiaonan Huang"
      }
    ],
    "summary": "Robustness is a crucial factor for the successful deployment of robots in\nunstructured environments, particularly in the domain of Simultaneous\nLocalization and Mapping (SLAM). Simulation-based benchmarks have emerged as a\nhighly scalable approach for robustness evaluation compared to real-world data\ncollection. However, crafting a challenging and controllable noisy world with\ndiverse perturbations remains relatively under-explored. To this end, we\npropose a novel, customizable pipeline for noisy data synthesis, aimed at\nassessing the resilience of multi-modal SLAM models against various\nperturbations. This pipeline incorporates customizable hardware setups,\nsoftware components, and perturbed environments. In particular, we introduce\ncomprehensive perturbation taxonomy along with a perturbation composition\ntoolbox, allowing the transformation of clean simulations into challenging\nnoisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM\nbenchmark, which includes diverse perturbation types, to evaluate the risk\ntolerance of existing advanced multi-modal SLAM models. Our extensive analysis\nuncovers the susceptibilities of existing SLAM models to real-world\ndisturbance, despite their demonstrated accuracy in standard benchmarks. Our\nperturbation synthesis toolbox, SLAM robustness evaluation pipeline, and\nRobust-SLAM benchmark will be made publicly available at\nhttps://github.com/Xiaohao-Xu/SLAM-under-Perturbation/.",
    "comment": "40 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08125v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08125v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08125v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08117v1",
    "updated": "2024-02-12T23:15:16+00:00",
    "published": "2024-02-12T23:15:16+00:00",
    "title": "A Universal Non-Parametric Approach For Improved Molecular Sequence Analysis",
    "authors": [
      {
        "name": "Sarwan Ali"
      },
      {
        "name": "Tamkanat E Ali"
      },
      {
        "name": "Prakash Chourasia"
      },
      {
        "name": "Murray Patterson"
      }
    ],
    "summary": "In the field of biological research, it is essential to comprehend the\ncharacteristics and functions of molecular sequences. The classification of\nmolecular sequences has seen widespread use of neural network-based techniques.\nDespite their astounding accuracy, these models often require a substantial\nnumber of parameters and more data collection. In this work, we present a novel\napproach based on the compression-based Model, motivated from\n\\cite{jiang2023low}, which combines the simplicity of basic compression\nalgorithms like Gzip and Bz2, with Normalized Compression Distance (NCD)\nalgorithm to achieve better performance on classification tasks without relying\non handcrafted features or pre-trained models. Firstly, we compress the\nmolecular sequence using well-known compression algorithms, such as Gzip and\nBz2. By leveraging the latent structure encoded in compressed files, we compute\nthe Normalized Compression Distance between each pair of molecular sequences,\nwhich is derived from the Kolmogorov complexity. This gives us a distance\nmatrix, which is the input for generating a kernel matrix using a Gaussian\nkernel. Next, we employ kernel Principal Component Analysis (PCA) to get the\nvector representations for the corresponding molecular sequence, capturing\nimportant structural and functional information. The resulting vector\nrepresentations provide an efficient yet effective solution for molecular\nsequence analysis and can be used in ML-based downstream tasks. The proposed\napproach eliminates the need for computationally intensive Deep Neural Networks\n(DNNs), with their large parameter counts and data requirements. Instead, it\nleverages a lightweight and universally accessible compression-based model.",
    "comment": "Accepted at The Pacific-Asia Conference on Knowledge Discovery and\n  Data Mining (PAKDD) 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08117v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08117v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08117v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08115v1",
    "updated": "2024-02-12T23:11:01+00:00",
    "published": "2024-02-12T23:11:01+00:00",
    "title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
    "authors": [
      {
        "name": "Kaya Stechly"
      },
      {
        "name": "Karthik Valmeekam"
      },
      {
        "name": "Subbarao Kambhampati"
      }
    ],
    "summary": "There has been considerable divergence of opinion on the reasoning abilities\nof Large Language Models (LLMs). While the initial optimism that reasoning\nmight emerge automatically with scale has been tempered thanks to a slew of\ncounterexamples--ranging from multiplication to simple planning--there persists\na wide spread belief that LLMs can self-critique and improve their own\nsolutions in an iterative fashion. This belief seemingly rests on the\nassumption that verification of correctness should be easier than generation--a\nrather classical argument from computational complexity--which should be\nirrelevant to LLMs to the extent that what they are doing is approximate\nretrieval. In this paper, we set out to systematically investigate the\neffectiveness of iterative prompting in the context of reasoning and planning.\nWe present a principled empirical study of the performance of GPT-4 in three\ndomains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both\nwith the model critiquing its own answers and with an external correct reasoner\nverifying proposed solutions. In each case, we analyze whether the content of\ncriticisms actually affects bottom line performance, and whether we can ablate\nelements of the augmented system without losing performance. We observe\nsignificant performance collapse with self-critique, significant performance\ngains with sound external verification, but that the content of critique\ndoesn't matter to the performance of the system. In fact, merely re-prompting\nwith a sound verifier maintains most of the benefits of more involved setups.",
    "comment": "arXiv admin note: text overlap with arXiv:2310.12397",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08115v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08115v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08115v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08114v1",
    "updated": "2024-02-12T23:09:00+00:00",
    "published": "2024-02-12T23:09:00+00:00",
    "title": "Active Preference Learning for Large Language Models",
    "authors": [
      {
        "name": "William Muldrew"
      },
      {
        "name": "Peter Hayes"
      },
      {
        "name": "Mingtian Zhang"
      },
      {
        "name": "David Barber"
      }
    ],
    "summary": "As large language models (LLMs) become more capable, fine-tuning techniques\nfor aligning with human intent are increasingly important. A key consideration\nfor aligning these models is how to most effectively use human resources, or\nmodel resources in the case where LLMs themselves are used as oracles.\nReinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most\nprominent example of such a technique, but is complex and often unstable.\nDirect Preference Optimization (DPO) has recently been proposed as a simpler\nand more stable alternative. In this work, we develop an active learning\nstrategy for DPO to make better use of preference labels. We propose a\npractical acquisition function for prompt/completion pairs based on the\npredictive entropy of the language model and a measure of certainty of the\nimplicit preference model optimized by DPO. We demonstrate how our approach\nimproves both the rate of learning and final performance of fine-tuning on\npairwise preference data.",
    "comment": "13 pages, 5 figures, 6 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08114v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08114v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08114v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08112v1",
    "updated": "2024-02-12T23:08:17+00:00",
    "published": "2024-02-12T23:08:17+00:00",
    "title": "A Competition Winning Deep Reinforcement Learning Agent in microRTS",
    "authors": [
      {
        "name": "Scott Goodfriend"
      }
    ],
    "summary": "Scripted agents have predominantly won the five previous iterations of the\nIEEE microRTS ($\\mu$RTS) competitions hosted at CIG and CoG. Despite Deep\nReinforcement Learning (DRL) algorithms making significant strides in real-time\nstrategy (RTS) games, their adoption in this primarily academic competition has\nbeen limited due to the considerable training resources required and the\ncomplexity inherent in creating and debugging such agents. RAISocketAI is the\nfirst DRL agent to win the IEEE microRTS competition. In a benchmark without\nperformance constraints, RAISocketAI regularly defeated the two prior\ncompetition winners. This first competition-winning DRL submission can be a\nbenchmark for future microRTS competitions and a starting point for future DRL\nresearch. Iteratively fine-tuning the base policy and transfer learning to\nspecific maps were critical to RAISocketAI's winning performance. These\nstrategies can be used to economically train future DRL agents. Further work in\nImitation Learning using Behavior Cloning and fine-tuning these models with DRL\nhas proven promising as an efficient way to bootstrap models with demonstrated,\ncompetitive behaviors.",
    "comment": "26 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08112v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08112v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08112v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08109v1",
    "updated": "2024-02-12T22:56:18+00:00",
    "published": "2024-02-12T22:56:18+00:00",
    "title": "From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations",
    "authors": [
      {
        "name": "Kapilya Gangadharan"
      },
      {
        "name": "K. Malathi"
      },
      {
        "name": "Anoop Purandaran"
      },
      {
        "name": "Barathi Subramanian"
      },
      {
        "name": "Rathinaraja Jeyaraj"
      }
    ],
    "summary": "This research aims to explore the impact of Machine Learning (ML) on the\nevolution and efficacy of Recommendation Systems (RS), particularly in the\ncontext of their growing significance in commercial business environments.\nMethodologically, the study delves into the role of ML in crafting and refining\nthese systems, focusing on aspects such as data sourcing, feature engineering,\nand the importance of evaluation metrics, thereby highlighting the iterative\nnature of enhancing recommendation algorithms. The deployment of Recommendation\nEngines (RE), driven by advanced algorithms and data analytics, is explored\nacross various domains, showcasing their significant impact on user experience\nand decision-making processes. These engines not only streamline information\ndiscovery and enhance collaboration but also accelerate knowledge acquisition,\nproving vital in navigating the digital landscape for businesses. They\ncontribute significantly to sales, revenue, and the competitive edge of\nenterprises by offering improved recommendations that align with individual\ncustomer needs. The research identifies the increasing expectation of users for\na seamless, intuitive online experience, where content is personalized and\ndynamically adapted to changing preferences. Future research directions include\nexploring advancements in deep learning models, ethical considerations in the\ndeployment of RS, and addressing scalability challenges. This study emphasizes\nthe indispensability of comprehending and leveraging ML in RS for researchers\nand practitioners, to tap into the full potential of personalized\nrecommendation in commercial business prospects.",
    "comment": "55 pages, 14 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DC",
    "categories": [
      "cs.DC",
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08109v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08109v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08109v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08108v1",
    "updated": "2024-02-12T22:56:16+00:00",
    "published": "2024-02-12T22:56:16+00:00",
    "title": "Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization",
    "authors": [
      {
        "name": "Kasper Johansson"
      },
      {
        "name": "Thomas Schmelzer"
      },
      {
        "name": "Stephen Boyd"
      }
    ],
    "summary": "We propose a new method for finding statistical arbitrages that can contain\nmore assets than just the traditional pair. We formulate the problem as seeking\na portfolio with the highest volatility, subject to its price remaining in a\nband and a leverage limit. This optimization problem is not convex, but can be\napproximately solved using the convex-concave procedure, a specific sequential\nconvex programming method. We show how the method generalizes to finding\nmoving-band statistical arbitrages, where the price band midpoint varies over\ntime.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "econ.EM",
    "categories": [
      "econ.EM",
      "cs.LG",
      "q-fin.PM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08108v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08108v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08108v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08106v1",
    "updated": "2024-02-12T22:52:32+00:00",
    "published": "2024-02-12T22:52:32+00:00",
    "title": "Mirror Descent-Ascent for mean-field min-max problems",
    "authors": [
      {
        "name": "Razvan-Andrei Lascu"
      },
      {
        "name": "Mateusz B. Majka"
      },
      {
        "name": "\u0141ukasz Szpruch"
      }
    ],
    "summary": "We study two variants of the mirror descent-ascent algorithm for solving\nmin-max problems on the space of measures: simultaneous and sequential. We work\nunder assumptions of convexity-concavity and relative smoothness of the payoff\nfunction with respect to a suitable Bregman divergence, defined on the space of\nmeasures via flat derivatives. We show that the convergence rates to mixed Nash\nequilibria, measured in the Nikaid\\`o-Isoda error, are of order\n$\\mathcal{O}\\left(N^{-1/2}\\right)$ and $\\mathcal{O}\\left(N^{-2/3}\\right)$ for\nthe simultaneous and sequential schemes, respectively, which is in line with\nthe state-of-the-art results for related finite-dimensional algorithms.",
    "comment": "32 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "math.PR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08106v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08106v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08106v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08105v1",
    "updated": "2024-02-12T22:48:30+00:00",
    "published": "2024-02-12T22:48:30+00:00",
    "title": "Learning Cartesian Product Graphs with Laplacian Constraints",
    "authors": [
      {
        "name": "Changhao Shi"
      },
      {
        "name": "Gal Mishne"
      }
    ],
    "summary": "Graph Laplacian learning, also known as network topology inference, is a\nproblem of great interest to multiple communities. In Gaussian graphical models\n(GM), graph learning amounts to endowing covariance selection with the\nLaplacian structure. In graph signal processing (GSP), it is essential to infer\nthe unobserved graph from the outputs of a filtering system. In this paper, we\nstudy the problem of learning Cartesian product graphs under Laplacian\nconstraints. The Cartesian graph product is a natural way for modeling\nhigher-order conditional dependencies and is also the key for generalizing GSP\nto multi-way tensors. We establish statistical consistency for the penalized\nmaximum likelihood estimation (MLE) of a Cartesian product Laplacian, and\npropose an efficient algorithm to solve the problem. We also extend our method\nfor efficient joint graph learning and imputation in the presence of structural\nmissing values. Experiments on synthetic and real-world datasets demonstrate\nthat our method is superior to previous GSP and GM methods.",
    "comment": "Accepted to AISTATS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08105v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08105v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08105v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08100v1",
    "updated": "2024-02-12T22:35:40+00:00",
    "published": "2024-02-12T22:35:40+00:00",
    "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation",
    "authors": [
      {
        "name": "Federico Ranaldi"
      },
      {
        "name": "Elena Sofia Ruzzetti"
      },
      {
        "name": "Dario Onorati"
      },
      {
        "name": "Leonardo Ranaldi"
      },
      {
        "name": "Cristina Giannone"
      },
      {
        "name": "Andrea Favalli"
      },
      {
        "name": "Raniero Romagnoli"
      },
      {
        "name": "Fabio Massimo Zanzotto"
      }
    ],
    "summary": "Understanding textual description to generate code seems to be an achieved\ncapability of instruction-following Large Language Models (LLMs) in zero-shot\nscenario. However, there is a severe possibility that this translation ability\nmay be influenced by having seen target textual descriptions and the related\ncode. This effect is known as Data Contamination.\n  In this study, we investigate the impact of Data Contamination on the\nperformance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we\nintroduce a novel method to detect Data Contamination in GPTs and examine\nGPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new\nunfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on\ndatabases with modified information via an adversarial table disconnection\n(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of\ninformation from the database. Our results indicate a significant performance\ndrop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,\nhighlighting the effect of Data Contamination on LLMs in Text-to-SQL\ntranslation tasks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08100v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08100v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08100v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08097v1",
    "updated": "2024-02-12T22:34:53+00:00",
    "published": "2024-02-12T22:34:53+00:00",
    "title": "An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem",
    "authors": [
      {
        "name": "Jincheng Cao"
      },
      {
        "name": "Ruichen Jiang"
      },
      {
        "name": "Erfan Yazdandoost Hamedani"
      },
      {
        "name": "Aryan Mokhtari"
      }
    ],
    "summary": "In this paper, we focus on simple bilevel optimization problems, where we\nminimize a convex smooth objective function over the optimal solution set of\nanother convex smooth constrained optimization problem. We present a novel\nbilevel optimization method that locally approximates the solution set of the\nlower-level problem using a cutting plane approach and employs an accelerated\ngradient-based update to reduce the upper-level objective function over the\napproximated solution set. We measure the performance of our method in terms of\nsuboptimality and infeasibility errors and provide non-asymptotic convergence\nguarantees for both error criteria. Specifically, when the feasible set is\ncompact, we show that our method requires at most\n$\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$ iterations to find a\nsolution that is $\\epsilon_f$-suboptimal and $\\epsilon_g$-infeasible. Moreover,\nunder the additional assumption that the lower-level objective satisfies the\n$r$-th H\\\"olderian error bound, we show that our method achieves an iteration\ncomplexity of\n$\\mathcal{O}(\\max\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$,\nwhich matches the optimal complexity of single-level convex constrained\noptimization when $r=1$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08097v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08097v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08097v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08096v1",
    "updated": "2024-02-12T22:32:12+00:00",
    "published": "2024-02-12T22:32:12+00:00",
    "title": "Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?",
    "authors": [
      {
        "name": "Andrew Bai"
      },
      {
        "name": "Chih-Kuan Yeh"
      },
      {
        "name": "Cho-Jui Hsieh"
      },
      {
        "name": "Ankur Taly"
      }
    ],
    "summary": "Fine-tuning pretrained foundational models on specific tasks is now the de\nfacto approach for text and vision tasks. A known pitfall of this approach is\nthe forgetting of pretraining knowledge that happens during finetuning.\nRehearsing samples randomly from the pretrain dataset is a common approach to\nalleviate such forgetting. However, we find that random mixing unintentionally\nincludes samples which are not (yet) forgotten or unlearnable by the model. We\npropose a novel sampling scheme, mix-cd, that identifies and prioritizes\nsamples that actually face forgetting, which we call collateral damage. Since\ndirectly identifying collateral damage samples is computationally expensive, we\npropose a procedure to estimate the distribution of such samples by tracking\nthe statistics of finetuned samples. Our approach is lightweight, easy to\nimplement, and can be seamlessly integrated into existing models, offering an\neffective means to retain pretrain performance without additional computational\ncosts.",
    "comment": "17 pages, 13 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08096v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08096v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08096v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08095v2",
    "updated": "2024-02-14T05:30:46+00:00",
    "published": "2024-02-12T22:26:52+00:00",
    "title": "Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization",
    "authors": [
      {
        "name": "Hongrui Chen"
      },
      {
        "name": "Lexing Ying"
      }
    ],
    "summary": "Diffusion models have achieved huge empirical success in data generation\ntasks. Recently, some efforts have been made to adapt the framework of\ndiffusion models to discrete state space, providing a more natural approach for\nmodeling intrinsically discrete data, such as language and graphs. This is\nachieved by formulating both the forward noising process and the corresponding\nreversed process as Continuous Time Markov Chains (CTMCs). In this paper, we\ninvestigate the theoretical properties of the discrete diffusion model.\nSpecifically, we introduce an algorithm leveraging the uniformization of\ncontinuous Markov chains, implementing transitions on random time points. Under\nreasonable assumptions on the learning of the discrete score function, we\nderive Total Variation distance and KL divergence guarantees for sampling from\nany distribution on a hypercube. Our results align with state-of-the-art\nachievements for diffusion models in $\\mathbb{R}^d$ and further underscore the\nadvantages of discrete diffusion models in comparison to the $\\mathbb{R}^d$\nsetting.",
    "comment": "19 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08095v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08095v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08095v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08093v1",
    "updated": "2024-02-12T22:21:30+00:00",
    "published": "2024-02-12T22:21:30+00:00",
    "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
    "authors": [
      {
        "name": "Mateusz \u0141ajszczak"
      },
      {
        "name": "Guillermo C\u00e1mbara"
      },
      {
        "name": "Yang Li"
      },
      {
        "name": "Fatih Beyhan"
      },
      {
        "name": "Arent van Korlaar"
      },
      {
        "name": "Fan Yang"
      },
      {
        "name": "Arnaud Joly"
      },
      {
        "name": "\u00c1lvaro Mart\u00edn-Cortinas"
      },
      {
        "name": "Ammar Abbas"
      },
      {
        "name": "Adam Michalski"
      },
      {
        "name": "Alexis Moinet"
      },
      {
        "name": "Sri Karlapati"
      },
      {
        "name": "Ewa Muszy\u0144ska"
      },
      {
        "name": "Haohan Guo"
      },
      {
        "name": "Bartosz Putrycz"
      },
      {
        "name": "Soledad L\u00f3pez Gambino"
      },
      {
        "name": "Kayeon Yoo"
      },
      {
        "name": "Elena Sokolova"
      },
      {
        "name": "Thomas Drugman"
      }
    ],
    "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for\n$\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with\n$\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,\ntrained on 100K hours of public domain speech data, achieving a new\nstate-of-the-art in speech naturalness. It deploys a 1-billion-parameter\nautoregressive Transformer that converts raw texts into discrete codes\n(\"speechcodes\") followed by a convolution-based decoder which converts these\nspeechcodes into waveforms in an incremental, streamable manner. Further, our\nspeechcodes are built using a novel speech tokenization technique that features\nspeaker ID disentanglement and compression with byte-pair encoding. Echoing the\nwidely-reported \"emergent abilities\" of large language models when trained on\nincreasing volume of data, we show that BASE TTS variants built with 10K+ hours\nand 500M+ parameters begin to demonstrate natural prosody on textually complex\nsentences. We design and share a specialized dataset to measure these emergent\nabilities for text-to-speech. We showcase state-of-the-art naturalness of BASE\nTTS by evaluating against baselines that include publicly available large-scale\ntext-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated\nby the model can be heard at https://amazon-ltts-paper.com/.",
    "comment": "v1",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08093v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08093v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08093v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08090v1",
    "updated": "2024-02-12T22:17:28+00:00",
    "published": "2024-02-12T22:17:28+00:00",
    "title": "Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees",
    "authors": [
      {
        "name": "Sean Jaffe"
      },
      {
        "name": "Alexander Davydov"
      },
      {
        "name": "Deniz Lapsekili"
      },
      {
        "name": "Ambuj singh"
      },
      {
        "name": "Francesco Bullo"
      }
    ],
    "summary": "Global stability and robustness guarantees in learned dynamical systems are\nessential to ensure well-behavedness of the systems in the face of uncertainty.\nWe present Extended Linearized Contracting Dynamics (ELCD), the first neural\nnetwork-based dynamical system with global contractivity guarantees in\narbitrary metrics. The key feature of ELCD is a parametrization of the extended\nlinearization of the nonlinear vector field. In its most basic form, ELCD is\nguaranteed to be (i) globally exponentially stable, (ii) equilibrium\ncontracting, and (iii) globally contracting with respect to some metric. To\nallow for contraction with respect to more general metrics in the data space,\nwe train diffeomorphisms between the data space and a latent space and enforce\ncontractivity in the latent space, which ensures global contractivity in the\ndata space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D\nLASA datasets.",
    "comment": "9 pages, 3 figures. Under Review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08090v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08090v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08090v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08088v1",
    "updated": "2024-02-12T22:10:06+00:00",
    "published": "2024-02-12T22:10:06+00:00",
    "title": "Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control",
    "authors": [
      {
        "name": "Ghada Zamzmi"
      },
      {
        "name": "Kesavan Venkatesh"
      },
      {
        "name": "Brandon Nelson"
      },
      {
        "name": "Smriti Prathapan"
      },
      {
        "name": "Paul H. Yi"
      },
      {
        "name": "Berkman Sahiner"
      },
      {
        "name": "Jana G. Delfino"
      }
    ],
    "summary": "Background: Machine learning (ML) methods often fail with data that deviates\nfrom their training distribution. This is a significant concern for ML-enabled\ndevices in clinical settings, where data drift may cause unexpected performance\nthat jeopardizes patient safety.\n  Method: We propose a ML-enabled Statistical Process Control (SPC) framework\nfor out-of-distribution (OOD) detection and drift monitoring. SPC is\nadvantageous as it visually and statistically highlights deviations from the\nexpected distribution. To demonstrate the utility of the proposed framework for\nmonitoring data drift in radiological images, we investigated different design\nchoices, including methods for extracting feature representations, drift\nquantification, and SPC parameter selection.\n  Results: We demonstrate the effectiveness of our framework for two tasks: 1)\ndifferentiating axial vs. non-axial computed tomography (CT) images and 2)\nseparating chest x-ray (CXR) from other modalities. For both tasks, we achieved\nhigh accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and\nsensitivity of 0.980 in CT and 0.984 in CXR. Our framework was also adept at\nmonitoring data streams and identifying the time a drift occurred. In a\nsimulation with 100 daily CXR cases, we detected a drift in OOD input\npercentage from 0-1% to 3-5% within two days, maintaining a low false-positive\nrate. Through additional experimental results, we demonstrate the framework's\ndata-agnostic nature and independence from the underlying model's structure.\n  Conclusion: We propose a framework for OOD detection and drift monitoring\nthat is agnostic to data, modality, and model. The framework is customizable\nand can be adapted for specific applications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08088v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08088v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08088v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08086v1",
    "updated": "2024-02-12T22:07:43+00:00",
    "published": "2024-02-12T22:07:43+00:00",
    "title": "Text-centric Alignment for Multi-Modality Learning",
    "authors": [
      {
        "name": "Yun-Da Tsai"
      },
      {
        "name": "Ting-Yu Yen"
      },
      {
        "name": "Pei-Fu Guo"
      },
      {
        "name": "Zhe-Yan Li"
      },
      {
        "name": "Shou-De Lin"
      }
    ],
    "summary": "This research paper addresses the challenge of modality mismatch in\nmultimodal learning, where the modalities available during inference differ\nfrom those available at training. We propose the Text-centric Alignment for\nMulti-Modality Learning (TAMML) approach, an innovative method that utilizes\nLarge Language Models (LLMs) with in-context learning and foundation models to\nenhance the generalizability of multimodal systems under these conditions. By\nleveraging the unique properties of text as a unified semantic space, TAMML\ndemonstrates significant improvements in handling unseen, diverse, and\nunpredictable modality combinations. TAMML not only adapts to varying\nmodalities but also maintains robust performance, showcasing the potential of\nfoundation models in overcoming the limitations of traditional fixed-modality\nframeworks in embedding representations. This study contributes to the field by\noffering a flexible, effective solution for real-world applications where\nmodality availability is dynamic and uncertain.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08086v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08086v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08086v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08085v1",
    "updated": "2024-02-12T22:06:37+00:00",
    "published": "2024-02-12T22:06:37+00:00",
    "title": "Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning",
    "authors": [
      {
        "name": "Ziquan Wei"
      },
      {
        "name": "Tingting Dan"
      },
      {
        "name": "Guorong Wu"
      }
    ],
    "summary": "Graph learning is crucial in the fields of bioinformatics, social networks,\nand chemicals. Although high-order graphlets, such as cycles, are critical to\nachieving an informative graph representation for node classification, edge\nprediction, and graph recognition, modeling high-order topological\ncharacteristics poses significant computational challenges, restricting its\nwidespread applications in machine learning. To address this limitation, we\nintroduce the concept of \\textit{message detouring} to hierarchically\ncharacterize cycle representation throughout the entire graph, which\ncapitalizes on the contrast between the shortest and longest pathways within a\nrange of local topologies associated with each graph node. The topological\nfeature representations derived from our message detouring landscape\ndemonstrate comparable expressive power to high-order\n\\textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In\naddition to the integration with graph kernel and message passing neural\nnetworks, we present a novel message detouring neural network, which uses\nTransformer backbone to integrate cycle representations across nodes and edges.\nAside from theoretical results, experimental results on expressiveness, graph\nclassification, and node classification show message detouring can\nsignificantly outperform current counterpart approaches on various benchmark\ndatasets.",
    "comment": "16 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08085v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08085v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08085v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08082v2",
    "updated": "2024-02-14T17:53:29+00:00",
    "published": "2024-02-12T22:02:23+00:00",
    "title": "Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions",
    "authors": [
      {
        "name": "Frank Cole"
      },
      {
        "name": "Yulong Lu"
      }
    ],
    "summary": "While score-based generative models (SGMs) have achieved remarkable success\nin enormous image generation tasks, their mathematical foundations are still\nlimited. In this paper, we analyze the approximation and generalization of SGMs\nin learning a family of sub-Gaussian probability distributions. We introduce a\nnotion of complexity for probability distributions in terms of their relative\ndensity with respect to the standard Gaussian measure. We prove that if the\nlog-relative density can be locally approximated by a neural network whose\nparameters can be suitably bounded, then the distribution generated by\nempirical score matching approximates the target distribution in total\nvariation with a dimension-independent rate. We illustrate our theory through\nexamples, which include certain mixtures of Gaussians. An essential ingredient\nof our proof is to derive a dimension-free deep neural network approximation\nrate for the true score function associated with the forward process, which is\ninteresting in its own right.",
    "comment": "30 pages, to appear in the proceedings of 12th International\n  Conference on Learning Representations",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08082v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08082v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08082v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08078v1",
    "updated": "2024-02-12T21:44:32+00:00",
    "published": "2024-02-12T21:44:32+00:00",
    "title": "Large Language Models as Agents in Two-Player Games",
    "authors": [
      {
        "name": "Yang Liu"
      },
      {
        "name": "Peng Sun"
      },
      {
        "name": "Hang Li"
      }
    ],
    "summary": "By formally defining the training processes of large language models (LLMs),\nwhich usually encompasses pre-training, supervised fine-tuning, and\nreinforcement learning with human feedback, within a single and unified machine\nlearning paradigm, we can glean pivotal insights for advancing LLM\ntechnologies. This position paper delineates the parallels between the training\nmethods of LLMs and the strategies employed for the development of agents in\ntwo-player games, as studied in game theory, reinforcement learning, and\nmulti-agent systems. We propose a re-conceptualization of LLM learning\nprocesses in terms of agent learning in language-based games. This framework\nunveils innovative perspectives on the successes and challenges in LLM\ndevelopment, offering a fresh understanding of addressing alignment issues\namong other strategic considerations. Furthermore, our two-player game approach\nsheds light on novel data preparation and machine learning techniques for\ntraining LLMs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08078v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08078v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08078v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08077v1",
    "updated": "2024-02-12T21:44:20+00:00",
    "published": "2024-02-12T21:44:20+00:00",
    "title": "Diffeomorphic Measure Matching with Kernels for Generative Modeling",
    "authors": [
      {
        "name": "Biraj Pandey"
      },
      {
        "name": "Bamdad Hosseini"
      },
      {
        "name": "Pau Batlle"
      },
      {
        "name": "Houman Owhadi"
      }
    ],
    "summary": "This article presents a general framework for the transport of probability\nmeasures towards minimum divergence generative modeling and sampling using\nordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces\n(RKHSs), inspired by ideas from diffeomorphic matching and image registration.\nA theoretical analysis of the proposed method is presented, giving a priori\nerror bounds in terms of the complexity of the model, the number of samples in\nthe training set, and model misspecification. An extensive suite of numerical\nexperiments further highlights the properties, strengths, and weaknesses of the\nmethod and extends its applicability to other tasks, such as conditional\nsimulation and inference.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.DS",
      "stat.CO",
      "35Q68 49Q22 62F15 68T07 62R07"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08077v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08077v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08077v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08075v1",
    "updated": "2024-02-12T21:40:45+00:00",
    "published": "2024-02-12T21:40:45+00:00",
    "title": "Efficient and Scalable Fine-Tune of Language Models for Genome Understanding",
    "authors": [
      {
        "name": "Huixin Zhan"
      },
      {
        "name": "Ying Nian Wu"
      },
      {
        "name": "Zijun Zhang"
      }
    ],
    "summary": "Although DNA foundation models have advanced the understanding of genomes,\nthey still face significant challenges in the limited scale and diversity of\ngenomic data. This limitation starkly contrasts with the success of natural\nlanguage foundation models, which thrive on substantially larger scales.\nFurthermore, genome understanding involves numerous downstream genome\nannotation tasks with inherent data heterogeneity, thereby necessitating more\nefficient and robust fine-tuning methods tailored for genomics. Here, we\npresent \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for\n\\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo}\nstrategically leverages natural language foundation models' contextual cues,\nrecalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo}\nfurther accommodates numerous, heterogeneous downstream fine-tune tasks by an\nadaptive rank sampling method that prunes and stochastically reintroduces\npruned singular vectors within small computational budgets. Adaptive rank\nsampling outperformed existing fine-tuning methods on all benchmarked 14 genome\nunderstanding tasks, while requiring fewer than 2\\% of trainable parameters as\ngenomic-specific adapters. Impressively, applying these adapters on natural\nlanguage foundation models matched or even exceeded the performance of DNA\nfoundation models. \\textsc{Lingo} presents a new paradigm of efficient and\nscalable genome understanding via genomic-specific adapters on language models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.GN",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08075v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08075v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08075v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08073v1",
    "updated": "2024-02-12T21:32:49+00:00",
    "published": "2024-02-12T21:32:49+00:00",
    "title": "Grounding Data Science Code Generation with Input-Output Specifications",
    "authors": [
      {
        "name": "Yeming Wen"
      },
      {
        "name": "Pengcheng Yin"
      },
      {
        "name": "Kensen Shi"
      },
      {
        "name": "Henryk Michalewski"
      },
      {
        "name": "Swarat Chaudhuri"
      },
      {
        "name": "Alex Polozov"
      }
    ],
    "summary": "Large language models (LLMs) have recently demonstrated a remarkable ability\nto generate code from natural language (NL) prompts. However, in the real\nworld, NL is often too ambiguous to capture the true intent behind programming\nproblems, requiring additional input-output (I/O) specifications.\nUnfortunately, LLMs can have difficulty aligning their outputs with both the NL\nprompt and the I/O specification. In this paper, we give a way to mitigate this\nissue in the context of data science programming, where tasks require explicit\nI/O specifications for clarity. Specifically, we propose GIFT4Code, a novel\napproach for the instruction fine-tuning of LLMs with respect to I/O\nspecifications. Our method leverages synthetic data produced by the LLM itself\nand utilizes execution-derived feedback as a key learning signal. This\nfeedback, in the form of program I/O specifications, is provided to the LLM to\nfacilitate instruction fine-tuning. We evaluated our approach on two\nchallenging data science benchmarks, Arcade and DS-1000. The results\ndemonstrate a significant improvement in the LLM's ability to generate code\nthat is not only executable but also accurately aligned with user\nspecifications, substantially improving the quality of code generation for\ncomplex data science tasks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08073v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08073v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08073v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08072v1",
    "updated": "2024-02-12T21:32:05+00:00",
    "published": "2024-02-12T21:32:05+00:00",
    "title": "Enhancing Programming Error Messages in Real Time with Generative AI",
    "authors": [
      {
        "name": "Bailey Kimmel"
      },
      {
        "name": "Austin Geisert"
      },
      {
        "name": "Lily Yaro"
      },
      {
        "name": "Brendan Gipson"
      },
      {
        "name": "Taylor Hotchkiss"
      },
      {
        "name": "Sidney Osae-Asante"
      },
      {
        "name": "Hunter Vaught"
      },
      {
        "name": "Grant Wininger"
      },
      {
        "name": "Chase Yamaguchi"
      }
    ],
    "summary": "Generative AI is changing the way that many disciplines are taught, including\ncomputer science. Researchers have shown that generative AI tools are capable\nof solving programming problems, writing extensive blocks of code, and\nexplaining complex code in simple terms. Particular promise has been shown in\nusing generative AI to enhance programming error messages. Both students and\ninstructors have complained for decades that these messages are often cryptic\nand difficult to understand. Yet recent work has shown that students make fewer\nrepeated errors when enhanced via GPT-4. We extend this work by implementing\nfeedback from ChatGPT for all programs submitted to our automated assessment\ntool, Athene, providing help for compiler, run-time, and logic errors. Our\nresults indicate that adding generative AI to an automated assessment tool does\nnot necessarily make it better and that design of the interface matters greatly\nto the usability of the feedback that GPT-4 provided.",
    "comment": "Accepted to CHI 2024",
    "journal_ref": null,
    "doi": "10.1145/3613905.3647967",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3613905.3647967",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08072v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08072v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08072v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08064v1",
    "updated": "2024-02-12T21:14:45+00:00",
    "published": "2024-02-12T21:14:45+00:00",
    "title": "Beyond LLMs: Advancing the Landscape of Complex Reasoning",
    "authors": [
      {
        "name": "Jennifer Chu-Carroll"
      },
      {
        "name": "Andrew Beck"
      },
      {
        "name": "Greg Burnham"
      },
      {
        "name": "David OS Melville"
      },
      {
        "name": "David Nachman"
      },
      {
        "name": "A. Erdem \u00d6zcan"
      },
      {
        "name": "David Ferrucci"
      }
    ],
    "summary": "Since the advent of Large Language Models a few years ago, they have often\nbeen considered the de facto solution for many AI problems. However, in\naddition to the many deficiencies of LLMs that prevent them from broad industry\nadoption, such as reliability, cost, and speed, there is a whole class of\ncommon real world problems that Large Language Models perform poorly on,\nnamely, constraint satisfaction and optimization problems. These problems are\nubiquitous and current solutions are highly specialized and expensive to\nimplement. At Elemental Cognition, we developed our EC AI platform which takes\na neuro-symbolic approach to solving constraint satisfaction and optimization\nproblems. The platform employs, at its core, a precise and high performance\nlogical reasoning engine, and leverages LLMs for knowledge acquisition and user\ninteraction. This platform supports developers in specifying application logic\nin natural and concise language while generating application user interfaces to\ninteract with users effectively. We evaluated LLMs against systems built on the\nEC AI platform in three domains and found the EC AI systems to significantly\noutperform LLMs on constructing valid and optimal solutions, on validating\nproposed solutions, and on repairing invalid solutions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08064v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08064v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08064v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08063v1",
    "updated": "2024-02-12T21:14:37+00:00",
    "published": "2024-02-12T21:14:37+00:00",
    "title": "Locality Sensitive Hashing for Network Traffic Fingerprinting",
    "authors": [
      {
        "name": "Nowfel Mashnoor"
      },
      {
        "name": "Jay Thom"
      },
      {
        "name": "Abdur Rouf"
      },
      {
        "name": "Shamik Sengupta"
      },
      {
        "name": "Batyr Charyyev"
      }
    ],
    "summary": "The advent of the Internet of Things (IoT) has brought forth additional\nintricacies and difficulties to computer networks. These gadgets are\nparticularly susceptible to cyber-attacks because of their simplistic design.\nTherefore, it is crucial to recognise these devices inside a network for the\npurpose of network administration and to identify any harmful actions. Network\ntraffic fingerprinting is a crucial technique for identifying devices and\ndetecting anomalies. Currently, the predominant methods for this depend heavily\non machine learning (ML). Nevertheless, machine learning (ML) methods need the\nselection of features, adjustment of hyperparameters, and retraining of models\nto attain optimal outcomes and provide resilience to concept drifts detected in\na network. In this research, we suggest using locality-sensitive hashing (LSH)\nfor network traffic fingerprinting as a solution to these difficulties. Our\nstudy focuses on examining several design options for the Nilsimsa LSH\nfunction. We then use this function to create unique fingerprints for network\ndata, which may be used to identify devices. We also compared it with ML-based\ntraffic fingerprinting and observed that our method increases the accuracy of\nstate-of-the-art by 12% achieving around 94% accuracy in identifying devices in\na network.",
    "comment": "Conference Name: 2023 IEEE 29th International Symposium on Local and\n  Metropolitan Area Networks (LANMAN) Date of Conference: 10-11 July 2023",
    "journal_ref": null,
    "doi": "10.1109/LANMAN58293.2023.10189810",
    "primary_category": "cs.NI",
    "categories": [
      "cs.NI",
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/LANMAN58293.2023.10189810",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08063v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08063v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08063v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08062v1",
    "updated": "2024-02-12T21:12:11+00:00",
    "published": "2024-02-12T21:12:11+00:00",
    "title": "Avoiding Catastrophe in Continuous Spaces by Asking for Help",
    "authors": [
      {
        "name": "Benjamin Plaut"
      },
      {
        "name": "Hanlin Zhu"
      },
      {
        "name": "Stuart Russell"
      }
    ],
    "summary": "Most reinforcement learning algorithms with formal regret guarantees assume\nall mistakes are reversible and rely on essentially trying all possible\noptions. This approach leads to poor outcomes when some mistakes are\nirreparable or even catastrophic. We propose a variant of the contextual bandit\nproblem where the goal is to minimize the chance of catastrophe. Specifically,\nwe assume that the payoff each round represents the chance of avoiding\ncatastrophe that round, and try to maximize the product of payoffs (the overall\nchance of avoiding catastrophe). To give the agent some chance of success, we\nallow a limited number of queries to a mentor and assume a Lipschitz continuous\npayoff function. We present an algorithm whose regret and rate of querying the\nmentor both approach 0 as the time horizon grows, assuming a continuous 1D\nstate space and a relatively \"simple\" payoff function. We also provide a\nmatching lower bound: without the simplicity assumption: any algorithm either\nconstantly asks for help or is nearly guaranteed to cause catastrophe. Finally,\nwe identify the key obstacle to generalizing our algorithm to a\nmulti-dimensional state space.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08062v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08062v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08062v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08056v1",
    "updated": "2024-02-12T20:46:47+00:00",
    "published": "2024-02-12T20:46:47+00:00",
    "title": "MIML library: a Modular and Flexible Library for Multi-instance Multi-label Learning",
    "authors": [
      {
        "name": "\u00c1lvaro Belmonte"
      },
      {
        "name": "Amelia Zafra"
      },
      {
        "name": "Eva Gibaja"
      }
    ],
    "summary": "MIML library is a Java software tool to develop, test, and compare\nclassification algorithms for multi-instance multi-label (MIML) learning. The\nlibrary includes 43 algorithms and provides a specific format and facilities\nfor data managing and partitioning, holdout and cross-validation methods,\nstandard metrics for performance evaluation, and generation of reports. In\naddition, algorithms can be executed through $xml$ configuration files without\nneeding to program. It is platform-independent, extensible, free, open-source,\nand available on GitHub under the GNU General Public License.",
    "comment": null,
    "journal_ref": null,
    "doi": "10.1016/j.neucom.2022.05.068",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1016/j.neucom.2022.05.068",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08056v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08056v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08056v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08030v1",
    "updated": "2024-02-12T19:49:58+00:00",
    "published": "2024-02-12T19:49:58+00:00",
    "title": "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
    "authors": [
      {
        "name": "Anjali Khurana"
      },
      {
        "name": "Hari Subramonyam"
      },
      {
        "name": "Parmit K Chilana"
      }
    ],
    "summary": "Large Language Model (LLM) assistants, such as ChatGPT, have emerged as\npotential alternatives to search methods for helping users navigate complex,\nfeature-rich software. LLMs use vast training data from domain-specific texts,\nsoftware manuals, and code repositories to mimic human-like interactions,\noffering tailored assistance, including step-by-step instructions. In this\nwork, we investigated LLM-generated software guidance through a within-subject\nexperiment with 16 participants and follow-up interviews. We compared a\nbaseline LLM assistant with an LLM optimized for particular software contexts,\nSoftAIBot, which also offered guidelines for constructing appropriate prompts.\nWe assessed task completion, perceived accuracy, relevance, and trust.\nSurprisingly, although SoftAIBot outperformed the baseline LLM, our results\nrevealed no significant difference in LLM usage and user perceptions with or\nwithout prompt guidelines and the integration of domain context. Most users\nstruggled to understand how the prompt's text related to the LLM's responses\nand often followed the LLM's suggestions verbatim, even if they were incorrect.\nThis resulted in difficulties when using the LLM's advice for software tasks,\nleading to low task completion rates. Our detailed analysis also revealed that\nusers remained unaware of inaccuracies in the LLM's responses, indicating a gap\nbetween their lack of software expertise and their ability to evaluate the\nLLM's assistance. With the growing push for designing domain-specific LLM\nassistants, we emphasize the importance of incorporating explainable,\ncontext-aware cues into LLMs to help users understand prompt-based\ninteractions, identify biases, and maximize the utility of LLM assistants.",
    "comment": "Accepted for publication in the Proceedings of the 29th International\n  Conference on Intelligent User Interfaces (IUI'24), March 18--21, 2024, in\n  Greenville, SC, USA",
    "journal_ref": null,
    "doi": "10.1145/3640543.3645200",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3640543.3645200",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.08030v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08030v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08030v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08023v1",
    "updated": "2024-02-12T19:39:26+00:00",
    "published": "2024-02-12T19:39:26+00:00",
    "title": "UGMAE: A Unified Framework for Graph Masked Autoencoders",
    "authors": [
      {
        "name": "Yijun Tian"
      },
      {
        "name": "Chuxu Zhang"
      },
      {
        "name": "Ziyi Kou"
      },
      {
        "name": "Zheyuan Liu"
      },
      {
        "name": "Xiangliang Zhang"
      },
      {
        "name": "Nitesh V. Chawla"
      }
    ],
    "summary": "Generative self-supervised learning on graphs, particularly graph masked\nautoencoders, has emerged as a popular learning paradigm and demonstrated its\nefficacy in handling non-Euclidean data. However, several remaining issues\nlimit the capability of existing methods: 1) the disregard of uneven node\nsignificance in masking, 2) the underutilization of holistic graph information,\n3) the ignorance of semantic knowledge in the representation space due to the\nexclusive use of reconstruction loss in the output space, and 4) the unstable\nreconstructions caused by the large volume of masked contents. In light of\nthis, we propose UGMAE, a unified framework for graph masked autoencoders to\naddress these issues from the perspectives of adaptivity, integrity,\ncomplementarity, and consistency. Specifically, we first develop an adaptive\nfeature mask generator to account for the unique significance of nodes and\nsample informative masks (adaptivity). We then design a ranking-based structure\nreconstruction objective joint with feature reconstruction to capture holistic\ngraph information and emphasize the topological proximity between neighbors\n(integrity). After that, we present a bootstrapping-based similarity module to\nencode the high-level semantic knowledge in the representation space,\ncomplementary to the low-level reconstruction in the output space\n(complementarity). Finally, we build a consistency assurance module to provide\nreconstruction objectives with extra stabilized consistency targets\n(consistency). Extensive experiments demonstrate that UGMAE outperforms both\ncontrastive and generative state-of-the-art baselines on several tasks across\nmultiple datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08023v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08023v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08023v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08022v1",
    "updated": "2024-02-12T19:39:07+00:00",
    "published": "2024-02-12T19:39:07+00:00",
    "title": "Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks",
    "authors": [
      {
        "name": "Talha Bozkus"
      },
      {
        "name": "Urbashi Mitra"
      }
    ],
    "summary": "Optimizing large-scale wireless networks, including optimal resource\nmanagement, power allocation, and throughput maximization, is inherently\nchallenging due to their non-observable system dynamics and heterogeneous and\ncomplex nature. Herein, a novel ensemble Q-learning algorithm that addresses\nthe performance and complexity challenges of the traditional Q-learning\nalgorithm for optimizing wireless networks is presented. Ensemble learning with\nsynthetic Markov Decision Processes is tailored to wireless networks via new\nmodels for approximating large state-space observable wireless networks. In\nparticular, digital cousins are proposed as an extension of the traditional\ndigital twin concept wherein multiple Q-learning algorithms on multiple\nsynthetic Markovian environments are run in parallel and their outputs are\nfused into a single Q-function. Convergence analyses of key statistics and\nQ-functions and derivations of upper bounds on the estimation bias and variance\nare provided. Numerical results across a variety of real-world wireless\nnetworks show that the proposed algorithm can achieve up to 50% less average\npolicy error with up to 40% less runtime complexity than the state-of-the-art\nreinforcement learning algorithms. It is also shown that theoretical results\nproperly predict trends in the experimental results.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NI",
      "eess.SP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08022v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08022v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08022v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08018v1",
    "updated": "2024-02-12T19:27:30+00:00",
    "published": "2024-02-12T19:27:30+00:00",
    "title": "Nearest Neighbour Score Estimators for Diffusion Generative Models",
    "authors": [
      {
        "name": "Matthew Niedoba"
      },
      {
        "name": "Dylan Green"
      },
      {
        "name": "Saeid Naderiparizi"
      },
      {
        "name": "Vasileios Lioutas"
      },
      {
        "name": "Jonathan Wilder Lavington"
      },
      {
        "name": "Xiaoxuan Liang"
      },
      {
        "name": "Yunpeng Liu"
      },
      {
        "name": "Ke Zhang"
      },
      {
        "name": "Setareh Dabiri"
      },
      {
        "name": "Adam \u015acibior"
      },
      {
        "name": "Berend Zwartsenberg"
      },
      {
        "name": "Frank Wood"
      }
    ],
    "summary": "Score function estimation is the cornerstone of both training and sampling\nfrom diffusion generative models. Despite this fact, the most commonly used\nestimators are either biased neural network approximations or high variance\nMonte Carlo estimators based on the conditional score. We introduce a novel\nnearest neighbour score function estimator which utilizes multiple samples from\nthe training set to dramatically decrease estimator variance. We leverage our\nlow variance estimator in two compelling applications. Training consistency\nmodels with our estimator, we report a significant increase in both convergence\nspeed and sample quality. In diffusion models, we show that our estimator can\nreplace a learned network for probability-flow ODE integration, opening\npromising new avenues of future research.",
    "comment": "25 pages, 9 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08018v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08018v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08018v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08017v1",
    "updated": "2024-02-12T19:27:26+00:00",
    "published": "2024-02-12T19:27:26+00:00",
    "title": "Lumos : Empowering Multimodal LLMs with Scene Text Recognition",
    "authors": [
      {
        "name": "Ashish Shenoy"
      },
      {
        "name": "Yichao Lu"
      },
      {
        "name": "Srihari Jayakumar"
      },
      {
        "name": "Debojeet Chatterjee"
      },
      {
        "name": "Mohsen Moslehpour"
      },
      {
        "name": "Pierce Chuang"
      },
      {
        "name": "Abhay Harpale"
      },
      {
        "name": "Vikas Bhardwaj"
      },
      {
        "name": "Di Xu"
      },
      {
        "name": "Shicong Zhao"
      },
      {
        "name": "Longfang Zhao"
      },
      {
        "name": "Ankit Ramchandani"
      },
      {
        "name": "Xin Luna Dong"
      },
      {
        "name": "Anuj Kumar"
      }
    ],
    "summary": "We introduce Lumos, the first end-to-end multimodal question-answering system\nwith text understanding capabilities. At the core of Lumos is a Scene Text\nRecognition (STR) component that extracts text from first person point-of-view\nimages, the output of which is used to augment input to a Multimodal Large\nLanguage Model (MM-LLM). While building Lumos, we encountered numerous\nchallenges related to STR quality, overall latency, and model inference. In\nthis paper, we delve into those challenges, and discuss the system\narchitecture, design choices, and modeling techniques employed to overcome\nthese obstacles. We also provide a comprehensive evaluation for each component,\nshowcasing high quality and efficiency.",
    "comment": "Submitted to KDD 2024 (ADS Track)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08017v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08017v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08017v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08012v1",
    "updated": "2024-02-12T19:21:14+00:00",
    "published": "2024-02-12T19:21:14+00:00",
    "title": "Online Differentially Private Synthetic Data Generation",
    "authors": [
      {
        "name": "Yiyun He"
      },
      {
        "name": "Roman Vershynin"
      },
      {
        "name": "Yizhe Zhu"
      }
    ],
    "summary": "We present a polynomial-time algorithm for online differentially private\nsynthetic data generation. For a data stream within the hypercube $[0,1]^d$ and\nan infinite time horizon, we develop an online algorithm that generates a\ndifferentially private synthetic dataset at each time $t$. This algorithm\nachieves a near-optimal accuracy bound of $O(t^{-1/d}\\log(t))$ for $d\\geq 2$\nand $O(t^{-1}\\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This\nresult generalizes the previous work on the continual release model for\ncounting queries to include Lipschitz queries. Compared to the offline case,\nwhere the entire dataset is available at once, our approach requires only an\nextra polylog factor in the accuracy bound.",
    "comment": "19 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.ST",
    "categories": [
      "math.ST",
      "cs.DS",
      "cs.LG",
      "math.PR",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08012v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08012v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08012v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08010v1",
    "updated": "2024-02-12T19:18:50+00:00",
    "published": "2024-02-12T19:18:50+00:00",
    "title": "Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning",
    "authors": [
      {
        "name": "Yuxiao Wen"
      },
      {
        "name": "Arthur Jacot"
      }
    ],
    "summary": "We describe the emergence of a Convolution Bottleneck (CBN) structure in\nCNNs, where the network uses its first few layers to transform the input\nrepresentation into a representation that is supported only along a few\nfrequencies and channels, before using the last few layers to map back to the\noutputs. We define the CBN rank, which describes the number and type of\nfrequencies that are kept inside the bottleneck, and partially prove that the\nparameter norm required to represent a function $f$ scales as depth times the\nCBN rank $f$. We also show that the parameter norm depends at next order on the\nregularity of $f$. We show that any network with almost optimal parameter norm\nwill exhibit a CBN structure in both the weights and - under the assumption\nthat the network is stable under large learning rate - the activations, which\nmotivates the common practice of down-sampling; and we verify that the CBN\nresults still hold with down-sampling. Finally we use the CBN structure to\ninterpret the functions learned by CNNs on a number of tasks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08010v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08010v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08010v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08005v1",
    "updated": "2024-02-12T19:10:13+00:00",
    "published": "2024-02-12T19:10:13+00:00",
    "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs",
    "authors": [
      {
        "name": "V\u00edctor Gallego"
      }
    ],
    "summary": "In this paper, we introduce \\emph{refined Direct Preference Optimization}\n(rDPO), a method for improving the behavioral alignment of Large Language\nModels (LLMs) without the need for human-annotated data. The method involves\ncreating synthetic data using self-critique prompting by a teacher LLM and then\nutilising a generalized DPO loss function to distil to a student LLM. The loss\nfunction incorporates an additional external reward model to improve the\nquality of synthetic data, making rDPO robust to potential noise in the\nsynthetic dataset. rDPO is shown to be effective in a diverse set of\nbehavioural alignment tasks, such as improved safety, robustness against\nrole-playing, and reduced sycophancy. Code to be released at\nhttps://github.com/vicgalle/refined-dpo.",
    "comment": "Pre-print. Submitted to the ICLR 2024 Workshop on Representational\n  Alignment (Re-Align)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08005v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08005v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08005v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.08001v1",
    "updated": "2024-02-12T19:05:27+00:00",
    "published": "2024-02-12T19:05:27+00:00",
    "title": "Improvement and generalization of ABCD method with Bayesian inference",
    "authors": [
      {
        "name": "Ezequiel Alvarez"
      },
      {
        "name": "Leandro Da Rold"
      },
      {
        "name": "Manuel Szewc"
      },
      {
        "name": "Alejandro Szynkman"
      },
      {
        "name": "Santiago A. Tanco"
      },
      {
        "name": "Tatiana Tarutina"
      }
    ],
    "summary": "To find New Physics or to refine our knowledge of the Standard Model at the\nLHC is an enterprise that involves many factors. We focus on taking advantage\nof available information and pour our effort in re-thinking the usual\ndata-driven ABCD method to improve it and to generalize it using Bayesian\nMachine Learning tools. We propose that a dataset consisting of a signal and\nmany backgrounds is well described through a mixture model. Signal, backgrounds\nand their relative fractions in the sample can be well extracted by exploiting\nthe prior knowledge and the dependence between the different observables at the\nevent-by-event level with Bayesian tools. We show how, in contrast to the ABCD\nmethod, one can take advantage of understanding some properties of the\ndifferent backgrounds and of having more than two independent observables to\nmeasure in each event. In addition, instead of regions defined through hard\ncuts, the Bayesian framework uses the information of continuous distribution to\nobtain soft-assignments of the events which are statistically more robust. To\ncompare both methods we use a toy problem inspired by $pp\\to hh\\to b\\bar b b\n\\bar b$, selecting a reduced and simplified number of processes and analysing\nthe flavor of the four jets and the invariant mass of the jet-pairs, modeled\nwith simplified distributions. Taking advantage of all this information, and\nstarting from a combination of biased and agnostic priors, leads us to a very\ngood posterior once we use the Bayesian framework to exploit the data and the\nmutual information of the observables at the event-by-event level. We show how,\nin this simplified model, the Bayesian framework outperforms the ABCD method\nsensitivity in obtaining the signal fraction in scenarios with $1\\%$ and\n$0.5\\%$ true signal fractions in the dataset. We also show that the method is\nrobust against the absence of signal.",
    "comment": "24 pages, 9 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "hep-ph",
    "categories": [
      "hep-ph",
      "cs.LG",
      "hep-ex"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.08001v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.08001v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.08001v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07999v1",
    "updated": "2024-02-12T19:04:32+00:00",
    "published": "2024-02-12T19:04:32+00:00",
    "title": "NetInfoF Framework: Measuring and Exploiting Network Usable Information",
    "authors": [
      {
        "name": "Meng-Chieh Lee"
      },
      {
        "name": "Haiyang Yu"
      },
      {
        "name": "Jian Zhang"
      },
      {
        "name": "Vassilis N. Ioannidis"
      },
      {
        "name": "Xiang Song"
      },
      {
        "name": "Soji Adeshina"
      },
      {
        "name": "Da Zheng"
      },
      {
        "name": "Christos Faloutsos"
      }
    ],
    "summary": "Given a node-attributed graph, and a graph task (link prediction or node\nclassification), can we tell if a graph neural network (GNN) will perform well?\nMore specifically, do the graph structure and the node features carry enough\nusable information for the task? Our goals are (1) to develop a fast tool to\nmeasure how much information is in the graph structure and in the node\nfeatures, and (2) to exploit the information to solve the task, if there is\nenough. We propose NetInfoF, a framework including NetInfoF_Probe and\nNetInfoF_Act, for the measurement and the exploitation of network usable\ninformation (NUI), respectively. Given a graph data, NetInfoF_Probe measures\nNUI without any model training, and NetInfoF_Act solves link prediction and\nnode classification, while two modules share the same backbone. In summary,\nNetInfoF has following notable advantages: (a) General, handling both link\nprediction and node classification; (b) Principled, with theoretical guarantee\nand closed-form solution; (c) Effective, thanks to the proposed adjustment to\nnode similarity; (d) Scalable, scaling linearly with the input size. In our\ncarefully designed synthetic datasets, NetInfoF correctly identifies the ground\ntruth of NUI and is the only method being robust to all graph scenarios.\nApplied on real-world datasets, NetInfoF wins in 11 out of 12 times on link\nprediction compared to general GNN baselines.",
    "comment": "Accepted to ICLR 2024 (Spotlight)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07999v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07999v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07999v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07901v1",
    "updated": "2024-02-12T18:59:39+00:00",
    "published": "2024-02-12T18:59:39+00:00",
    "title": "FAST: Factorizable Attention for Speeding up Transformers",
    "authors": [
      {
        "name": "Armin Gerami"
      },
      {
        "name": "Monte Hoover"
      },
      {
        "name": "Pranav S. Dulepet"
      },
      {
        "name": "Ramani Duraiswami"
      }
    ],
    "summary": "Motivated by the factorization inherent in the original fast multipole method\nand the improved fast Gauss transform we introduce a factorable form of\nattention that operates efficiently in high dimensions. This approach reduces\nthe computational and memory complexity of the attention mechanism in\ntransformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our\nwork presents a linearly scaled attention mechanism that maintains the full\nrepresentation of the attention matrix without compromising on sparsification\nand incorporates the all-to-all relationship between tokens. We explore the\nproperties of our new attention metric and conduct tests in various standard\nsettings. Results indicate that our attention mechanism has a robust\nperformance and holds significant promise for diverse applications where\nself-attention is used.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07901v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07901v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07901v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07899v1",
    "updated": "2024-02-12T18:58:58+00:00",
    "published": "2024-02-12T18:58:58+00:00",
    "title": "A systematic investigation of learnability from single child linguistic input",
    "authors": [
      {
        "name": "Yulu Qin"
      },
      {
        "name": "Wentao Wang"
      },
      {
        "name": "Brenden M. Lake"
      }
    ],
    "summary": "Language models (LMs) have demonstrated remarkable proficiency in generating\nlinguistically coherent text, sparking discussions about their relevance to\nunderstanding human language learnability. However, a significant gap exists\nbetween the training data for these models and the linguistic input a child\nreceives. LMs are typically trained on data that is orders of magnitude larger\nand fundamentally different from child-directed speech (Warstadt and Bowman,\n2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our\nresearch focuses on training LMs on subsets of a single child's linguistic\ninput. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in\nthis setting can form syntactic and semantic word clusters and develop\nsensitivity to certain linguistic phenomena, but they only considered LSTMs and\nsimpler neural networks trained from just one single-child dataset. Here, to\nexamine the robustness of learnability from single-child input, we\nsystematically train six different model architectures on five datasets (3\nsingle-child and 2 baselines). We find that the models trained on single-child\ndatasets showed consistent results that matched with previous work,\nunderscoring the robustness of forming meaningful syntactic and semantic\nrepresentations from a subset of a child's linguistic input.",
    "comment": "8 pages; 6 figures; Submitted to CogSci 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07899v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07899v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07899v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07895v1",
    "updated": "2024-02-12T18:57:06+00:00",
    "published": "2024-02-12T18:57:06+00:00",
    "title": "Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets",
    "authors": [
      {
        "name": "Violet Liu"
      },
      {
        "name": "Jason Chen"
      },
      {
        "name": "Ans Qureshi"
      },
      {
        "name": "Mahla Nejati"
      }
    ],
    "summary": "Amidst growing food production demands, early plant disease detection is\nessential to safeguard crops; this study proposes a visual machine learning\napproach for plant disease detection, harnessing RGB and NIR data collected in\nreal-world conditions through a JAI FS-1600D-10GE camera to build an RGBN\ndataset. A two-stage early plant disease detection model with YOLOv8 and a\nsequential CNN was used to train on a dataset with partial labels, which showed\na 3.6% increase in mAP compared to a single-stage end-to-end segmentation\nmodel. The sequential CNN model achieved 90.62% validation accuracy utilising\nRGBN data. An average of 6.25% validation accuracy increase is found using RGBN\nin classification compared to RGB using ResNet15 and the sequential CNN models.\nFurther research and dataset improvements are needed to meet food production\ndemands.",
    "comment": "Australasian Conference on Robotics and Automation (ACRA 2023)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07895v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07895v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07895v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07891v1",
    "updated": "2024-02-12T18:54:02+00:00",
    "published": "2024-02-12T18:54:02+00:00",
    "title": "Label-Efficient Model Selection for Text Generation",
    "authors": [
      {
        "name": "Shir Ashury-Tahan"
      },
      {
        "name": "Benjamin Sznajder"
      },
      {
        "name": "Leshem Choshen"
      },
      {
        "name": "Liat Ein-Dor"
      },
      {
        "name": "Eyal Shnarch"
      },
      {
        "name": "Ariel Gera"
      }
    ],
    "summary": "Model selection for a given target task can be costly, as it may entail\nextensive annotation of the quality of outputs of different models. We\nintroduce DiffUse, an efficient method to make an informed decision between\ncandidate text generation models. DiffUse reduces the required amount of\npreference annotations, thus saving valuable time and resources in performing\nevaluation. DiffUse intelligently selects instances by clustering embeddings\nthat represent the semantic differences between model outputs. Thus, it is able\nto identify a subset of examples that are more informative for preference\ndecisions. Our method is model-agnostic, and can be applied to any text\ngeneration model. Moreover, we propose a practical iterative approach for\ndynamically determining how many instances to annotate. In a series of\nexperiments over hundreds of model pairs, we demonstrate that DiffUse can\ndramatically reduce the required number of annotations -- by up to 75% -- while\nmaintaining high evaluation reliability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07891v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07891v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07891v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07890v1",
    "updated": "2024-02-12T18:53:20+00:00",
    "published": "2024-02-12T18:53:20+00:00",
    "title": "MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning",
    "authors": [
      {
        "name": "Ayesha Siddika Nipu"
      },
      {
        "name": "Siming Liu"
      },
      {
        "name": "Anthony Harris"
      }
    ],
    "summary": "Distributed decision-making in multi-agent systems presents difficult\nchallenges for interactive behavior learning in both cooperative and\ncompetitive systems. To mitigate this complexity, MAIDRL presents a\nsemi-centralized Dense Reinforcement Learning algorithm enhanced by agent\ninfluence maps (AIMs), for learning effective multi-agent control on StarCraft\nMulti-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet\nin MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement\nLearning, MAIDCRL, by incorporating convolutional layers into the deep model\narchitecture, and evaluate the performance on both homogeneous and\nheterogeneous scenarios. The results show that the CNN-enabled MAIDCRL\nsignificantly improved the learning performance and achieved a faster learning\nrate compared to the existing MAIDRL, especially on more complicated\nheterogeneous SMAC scenarios. We further investigate the stability and\nrobustness of our model. The statistics reflect that our model not only\nachieves higher winning rate in all the given scenarios but also boosts the\nagent's learning process in fine-grained decision-making.",
    "comment": "2022 IEEE Conference on Games (CoG)",
    "journal_ref": null,
    "doi": "10.1109/CoG51982.2022.9893711",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/CoG51982.2022.9893711",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07890v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07890v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07890v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07878v1",
    "updated": "2024-02-12T18:44:02+00:00",
    "published": "2024-02-12T18:44:02+00:00",
    "title": "Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks",
    "authors": [
      {
        "name": "Giacomo Zonneveld"
      },
      {
        "name": "Lorenzo Principi"
      },
      {
        "name": "Marco Baldi"
      }
    ],
    "summary": "Early detection of network intrusions and cyber threats is one of the main\npillars of cybersecurity. One of the most effective approaches for this purpose\nis to analyze network traffic with the help of artificial intelligence\nalgorithms, with the aim of detecting the possible presence of an attacker by\ndistinguishing it from a legitimate user. This is commonly done by collecting\nthe traffic exchanged between terminals in a network and analyzing it on a\nper-packet or per-connection basis. In this paper, we propose instead to\nperform pre-processing of network traffic under analysis with the aim of\nextracting some new metrics on which we can perform more efficient detection\nand overcome some limitations of classical approaches. These new metrics are\nbased on graph theory, and consider the network as a whole, rather than\nfocusing on individual packets or connections. Our approach is validated\nthrough experiments performed on publicly available data sets, from which it\nresults that it can not only overcome some of the limitations of classical\napproaches, but also achieve a better detection capability of cyber threats.",
    "comment": "6 pages, 1 figure, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07878v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07878v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07878v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07877v1",
    "updated": "2024-02-12T18:41:55+00:00",
    "published": "2024-02-12T18:41:55+00:00",
    "title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis",
    "authors": [
      {
        "name": "Yangxinyu Xie"
      },
      {
        "name": "Tanwi Mallick"
      },
      {
        "name": "Joshua David Bergerson"
      },
      {
        "name": "John K. Hutchison"
      },
      {
        "name": "Duane R. Verner"
      },
      {
        "name": "Jordan Branham"
      },
      {
        "name": "M. Ross Alexander"
      },
      {
        "name": "Robert B. Ross"
      },
      {
        "name": "Yan Feng"
      },
      {
        "name": "Leslie-Anne Levy"
      },
      {
        "name": "Weijie Su"
      }
    ],
    "summary": "The recent advancement of large language models (LLMs) represents a\ntransformational capability at the frontier of artificial intelligence (AI) and\nmachine learning (ML). However, LLMs are generalized models, trained on\nextensive text corpus, and often struggle to provide context-specific\ninformation, particularly in areas requiring specialized knowledge such as\nwildfire details within the broader context of climate change. For\ndecision-makers and policymakers focused on wildfire resilience and adaptation,\nit is crucial to obtain responses that are not only precise but also\ndomain-specific, rather than generic. To that end, we developed WildfireGPT, a\nprototype LLM agent designed to transform user queries into actionable insights\non wildfire risks. We enrich WildfireGPT by providing additional context such\nas climate projections and scientific literature to ensure its information is\ncurrent, relevant, and scientifically accurate. This enables WildfireGPT to be\nan effective tool for delivering detailed, user-specific insights on wildfire\nrisks to support a diverse set of end users, including researchers, engineers,\nurban planners, emergency managers, and infrastructure operators.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07877v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07877v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07877v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07876v1",
    "updated": "2024-02-12T18:41:34+00:00",
    "published": "2024-02-12T18:41:34+00:00",
    "title": "Policy Improvement using Language Feedback Models",
    "authors": [
      {
        "name": "Victor Zhong"
      },
      {
        "name": "Dipendra Misra"
      },
      {
        "name": "Xingdi Yuan"
      },
      {
        "name": "Marc-Alexandre C\u00f4t\u00e9"
      }
    ],
    "summary": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07876v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07876v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07876v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07875v1",
    "updated": "2024-02-12T18:41:31+00:00",
    "published": "2024-02-12T18:41:31+00:00",
    "title": "Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States",
    "authors": [
      {
        "name": "Noam Razin"
      },
      {
        "name": "Yotam Alexander"
      },
      {
        "name": "Edo Cohen-Karlik"
      },
      {
        "name": "Raja Giryes"
      },
      {
        "name": "Amir Globerson"
      },
      {
        "name": "Nadav Cohen"
      }
    ],
    "summary": "In modern machine learning, models can often fit training data in numerous\nways, some of which perform well on unseen (test) data, while others do not.\nRemarkably, in such cases gradient descent frequently exhibits an implicit bias\nthat leads to excellent performance on unseen data. This implicit bias was\nextensively studied in supervised learning, but is far less understood in\noptimal control (reinforcement learning). There, learning a controller applied\nto a system via gradient descent is known as policy gradient, and a question of\nprime importance is the extent to which a learned controller extrapolates to\nunseen initial states. This paper theoretically studies the implicit bias of\npolicy gradient in terms of extrapolation to unseen initial states. Focusing on\nthe fundamental Linear Quadratic Regulator (LQR) problem, we establish that the\nextent of extrapolation depends on the degree of exploration induced by the\nsystem when commencing from initial states included in training. Experiments\ncorroborate our theory, and demonstrate its conclusions on problems beyond LQR,\nwhere systems are non-linear and controllers are neural networks. We\nhypothesize that real-world optimal control may be greatly improved by\ndeveloping methods for informed selection of initial states to train on.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07875v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07875v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07875v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07871v1",
    "updated": "2024-02-12T18:33:47+00:00",
    "published": "2024-02-12T18:33:47+00:00",
    "title": "Scaling Laws for Fine-Grained Mixture of Experts",
    "authors": [
      {
        "name": "Jakub Krajewski"
      },
      {
        "name": "Jan Ludziejewski"
      },
      {
        "name": "Kamil Adamczewski"
      },
      {
        "name": "Maciej Pi\u00f3ro"
      },
      {
        "name": "Micha\u0142 Krutul"
      },
      {
        "name": "Szymon Antoniak"
      },
      {
        "name": "Kamil Ciebiera"
      },
      {
        "name": "Krystian Kr\u00f3l"
      },
      {
        "name": "Tomasz Odrzyg\u00f3\u017ad\u017a"
      },
      {
        "name": "Piotr Sankowski"
      },
      {
        "name": "Marek Cygan"
      },
      {
        "name": "Sebastian Jaszczur"
      }
    ],
    "summary": "Mixture of Experts (MoE) models have emerged as a primary solution for\nreducing the computational cost of Large Language Models. In this work, we\nanalyze their scaling properties, incorporating an expanded range of variables.\nSpecifically, we introduce a new hyperparameter, granularity, whose adjustment\nenables precise control over the size of the experts. Building on this, we\nestablish scaling laws for fine-grained MoE, taking into account the number of\ntraining tokens, model size, and granularity. Leveraging these laws, we derive\nthe optimal training configuration for a given computational budget. Our\nfindings not only show that MoE models consistently outperform dense\nTransformers but also highlight that the efficiency gap between dense and MoE\nmodels widens as we scale up the model size and training budget. Furthermore,\nwe demonstrate that the common practice of setting the size of experts in MoE\nto mirror the feed-forward layer is not optimal at almost any computational\nbudget.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07871v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07871v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07871v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07872v1",
    "updated": "2024-02-12T18:33:47+00:00",
    "published": "2024-02-12T18:33:47+00:00",
    "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
    "authors": [
      {
        "name": "Soroush Nasiriany"
      },
      {
        "name": "Fei Xia"
      },
      {
        "name": "Wenhao Yu"
      },
      {
        "name": "Ted Xiao"
      },
      {
        "name": "Jacky Liang"
      },
      {
        "name": "Ishita Dasgupta"
      },
      {
        "name": "Annie Xie"
      },
      {
        "name": "Danny Driess"
      },
      {
        "name": "Ayzaan Wahid"
      },
      {
        "name": "Zhuo Xu"
      },
      {
        "name": "Quan Vuong"
      },
      {
        "name": "Tingnan Zhang"
      },
      {
        "name": "Tsang-Wei Edward Lee"
      },
      {
        "name": "Kuang-Huei Lee"
      },
      {
        "name": "Peng Xu"
      },
      {
        "name": "Sean Kirmani"
      },
      {
        "name": "Yuke Zhu"
      },
      {
        "name": "Andy Zeng"
      },
      {
        "name": "Karol Hausman"
      },
      {
        "name": "Nicolas Heess"
      },
      {
        "name": "Chelsea Finn"
      },
      {
        "name": "Sergey Levine"
      },
      {
        "name": "Brian Ichter"
      }
    ],
    "summary": "Vision language models (VLMs) have shown impressive capabilities across a\nvariety of tasks, from logical reasoning to visual understanding. This opens\nthe door to richer interaction with the world, for example robotic control.\nHowever, VLMs produce only textual outputs, while robotic control and other\nspatial tasks require outputting continuous coordinates, actions, or\ntrajectories. How can we enable VLMs to handle such settings without\nfine-tuning on task-specific data?\n  In this paper, we propose a novel visual prompting approach for VLMs that we\ncall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as\niterative visual question answering. In each iteration, the image is annotated\nwith a visual representation of proposals that the VLM can refer to (e.g.,\ncandidate robot actions, localizations, or trajectories). The VLM then selects\nthe best ones for the task. These proposals are iteratively refined, allowing\nthe VLM to eventually zero in on the best available answer. We investigate\nPIVOT on real-world robotic navigation, real-world manipulation from images,\ninstruction following in simulation, and additional spatial inference tasks\nsuch as localization. We find, perhaps surprisingly, that our approach enables\nzero-shot control of robotic systems without any robot training data,\nnavigation in a variety of environments, and other capabilities. Although\ncurrent performance is far from perfect, our work highlights potentials and\nlimitations of this new regime and shows a promising approach for\nInternet-Scale VLMs in robotic and spatial reasoning domains. Website:\npivot-prompt.github.io and HuggingFace:\nhttps://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07872v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07872v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07872v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07868v1",
    "updated": "2024-02-12T18:29:17+00:00",
    "published": "2024-02-12T18:29:17+00:00",
    "title": "Nesting Particle Filters for Experimental Design in Dynamical Systems",
    "authors": [
      {
        "name": "Sahel Iqbal"
      },
      {
        "name": "Adrien Corenflos"
      },
      {
        "name": "Simo S\u00e4rkk\u00e4"
      },
      {
        "name": "Hany Abdulsamad"
      }
    ],
    "summary": "In this paper, we propose a novel approach to Bayesian Experimental Design\n(BED) for non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC^2 algorithm that uses a nested\nsequential Monte Carlo (SMC) estimator of the expected information gain and\nembeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform\ngradient-based policy optimization. This is in contrast to recent approaches\nthat rely on biased estimators of the expected information gain (EIG) to\namortize the cost of experiments by learning a design policy in advance.\nNumerical validation on a set of dynamical systems showcases the efficacy of\nour method in comparison to other state-of-the-art strategies.",
    "comment": "The article has been made available early for dissemination. The\n  empirical results are preliminary",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07868v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07868v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07868v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07867v1",
    "updated": "2024-02-12T18:28:36+00:00",
    "published": "2024-02-12T18:28:36+00:00",
    "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
    "authors": [
      {
        "name": "Wei Zou"
      },
      {
        "name": "Runpeng Geng"
      },
      {
        "name": "Binghui Wang"
      },
      {
        "name": "Jinyuan Jia"
      }
    ],
    "summary": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate those limitations. In particular, given a question, RAG retrieves\nrelevant knowledge from a knowledge database to augment the input of the LLM.\nFor instance, the retrieved knowledge could be a set of top-k texts that are\nmost semantically similar to the given question when the knowledge database\ncontains millions of texts collected from Wikipedia. As a result, the LLM could\nutilize the retrieved knowledge as the context to generate an answer for the\ngiven question. Existing studies mainly focus on improving the accuracy or\nefficiency of RAG, leaving its security largely unexplored. We aim to bridge\nthe gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge\npoisoning attacks to RAG, where an attacker could inject a few poisoned texts\ninto the knowledge database such that the LLM generates an attacker-chosen\ntarget answer for an attacker-chosen target question. We formulate knowledge\npoisoning attacks as an optimization problem, whose solution is a set of\npoisoned texts. Depending on the background knowledge (e.g., black-box and\nwhite-box settings) of an attacker on the RAG, we propose two solutions to\nsolve the optimization problem, respectively. Our results on multiple benchmark\ndatasets and LLMs show our attacks could achieve 90% attack success rates when\ninjecting 5 poisoned texts for each target question into a database with\nmillions of texts. We also evaluate recent defenses and our results show they\nare insufficient to defend against our attacks, highlighting the need for new\ndefenses.",
    "comment": "Code is available at https://github.com/sleeepeer/PoisonedRAG",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07867v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07867v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07867v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07970v1",
    "updated": "2024-02-12T18:24:32+00:00",
    "published": "2024-02-12T18:24:32+00:00",
    "title": "Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search",
    "authors": [
      {
        "name": "Kathryn E. Kirchoff"
      },
      {
        "name": "James Wellnitz"
      },
      {
        "name": "Joshua E. Hochuli"
      },
      {
        "name": "Travis Maxfield"
      },
      {
        "name": "Konstantin I. Popov"
      },
      {
        "name": "Shawn Gomez"
      },
      {
        "name": "Alexander Tropsha"
      }
    ],
    "summary": "Nearest neighbor-based similarity searching is a common task in chemistry,\nwith notable use cases in drug discovery. Yet, some of the most commonly used\napproaches for this task still leverage a brute-force approach. In practice\nthis can be computationally costly and overly time-consuming, due in part to\nthe sheer size of modern chemical databases. Previous computational\nadvancements for this task have generally relied on improvements to hardware or\ndataset-specific tricks that lack generalizability. Approaches that leverage\nlower-complexity searching algorithms remain relatively underexplored. However,\nmany of these algorithms are approximate solutions and/or struggle with typical\nhigh-dimensional chemical embeddings. Here we evaluate whether a combination of\nlow-dimensional chemical embeddings and a k-d tree data structure can achieve\nfast nearest neighbor queries while maintaining performance on standard\nchemical similarity search benchmarks. We examine different dimensionality\nreductions of standard chemical embeddings as well as a learned,\nstructurally-aware embedding -- SmallSA -- for this task. With this framework,\nsearches on over one billion chemicals execute in less than a second on a\nsingle CPU core, five orders of magnitude faster than the brute-force approach.\nWe also demonstrate that SmallSA achieves competitive performance on chemical\nsimilarity benchmarks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07970v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07970v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07970v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07865v1",
    "updated": "2024-02-12T18:21:14+00:00",
    "published": "2024-02-12T18:21:14+00:00",
    "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
    "authors": [
      {
        "name": "Siddharth Karamcheti"
      },
      {
        "name": "Suraj Nair"
      },
      {
        "name": "Ashwin Balakrishna"
      },
      {
        "name": "Percy Liang"
      },
      {
        "name": "Thomas Kollar"
      },
      {
        "name": "Dorsa Sadigh"
      }
    ],
    "summary": "Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance $-$ a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization from language, and targeted challenge sets that probe properties\nsuch as hallucination; evaluations that provide calibrated, fine-grained\ninsight into a VLM's capabilities. Second, we rigorously investigate VLMs along\nkey design axes, including pretrained visual representations and quantifying\nthe tradeoffs of using base vs. instruct-tuned language models, amongst others.\nWe couple our analysis with three resource contributions: (1) a unified\nframework for evaluating VLMs, (2) optimized, flexible code for VLM training,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open-source VLMs.",
    "comment": "22 pages, 11 figures. Training code and models:\n  https://github.com/TRI-ML/prismatic-vlms. Evaluation code:\n  https://github.com/TRI-ML/vlm-evaluation",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07865v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07865v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07865v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07862v1",
    "updated": "2024-02-12T18:14:43+00:00",
    "published": "2024-02-12T18:14:43+00:00",
    "title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
    "authors": [
      {
        "name": "Philipp Schoenegger"
      },
      {
        "name": "Peter S. Park"
      },
      {
        "name": "Ezra Karger"
      },
      {
        "name": "Philip E. Tetlock"
      }
    ],
    "summary": "Large language models (LLMs) show impressive capabilities, matching and\nsometimes exceeding human performance in many domains. This study explores the\npotential of LLMs to augment judgement in forecasting tasks. We evaluated the\nimpact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to\nprovide high-quality advice ('superforecasting'), and the other designed to be\noverconfident and base-rate-neglecting. Participants (N = 991) had the option\nto consult their assigned LLM assistant throughout the study, in contrast to a\ncontrol group that used a less advanced model (DaVinci-003) without direct\nforecasting support. Our preregistered analyses reveal that LLM augmentation\nsignificantly enhances forecasting accuracy by 23% across both types of\nassistants, compared to the control group. This improvement occurs despite the\nsuperforecasting assistant's higher accuracy in predictions, indicating the\naugmentation's benefit is not solely due to model prediction accuracy.\nExploratory analyses showed a pronounced effect in one forecasting item,\nwithout which we find that the superforecasting assistant increased accuracy by\n43%, compared with 28% for the biased assistant. We further examine whether LLM\naugmentation disproportionately benefits less skilled forecasters, degrades the\nwisdom-of-the-crowd by reducing prediction diversity, or varies in\neffectiveness with question difficulty. Our findings do not consistently\nsupport these hypotheses. Our results suggest that access to an LLM assistant,\neven a biased one, can be a helpful decision aid in cognitively demanding tasks\nwhere the answer is not known at the time of interaction.",
    "comment": "18 pages (main text comprised of 15 pages, appendix comprised of\n  three pages). 10 visualizations in the main text (four figures, six tables),\n  three additional figures in the appendix",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CY",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07862v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07862v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07862v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07860v1",
    "updated": "2024-02-12T18:12:09+00:00",
    "published": "2024-02-12T18:12:09+00:00",
    "title": "On the Detection of Reviewer-Author Collusion Rings From Paper Bidding",
    "authors": [
      {
        "name": "Steven Jecmen"
      },
      {
        "name": "Nihar B. Shah"
      },
      {
        "name": "Fei Fang"
      },
      {
        "name": "Leman Akoglu"
      }
    ],
    "summary": "A major threat to the peer-review systems of computer science conferences is\nthe existence of \"collusion rings\" between reviewers. In such collusion rings,\nreviewers who have also submitted their own papers to the conference work\ntogether to manipulate the conference's paper assignment, with the aim of being\nassigned to review each other's papers. The most straightforward way that\ncolluding reviewers can manipulate the paper assignment is by indicating their\ninterest in each other's papers through strategic paper bidding. One potential\napproach to solve this important problem would be to detect the colluding\nreviewers from their manipulated bids, after which the conference can take\nappropriate action. While prior work has has developed effective techniques to\ndetect other kinds of fraud, no research has yet established that detecting\ncollusion rings is even possible. In this work, we tackle the question of\nwhether it is feasible to detect collusion rings from the paper bidding. To\nanswer this question, we conduct empirical analysis of two realistic conference\nbidding datasets, including evaluations of existing algorithms for fraud\ndetection in other applications. We find that collusion rings can achieve\nconsiderable success at manipulating the paper assignment while remaining\nhidden from detection: for example, in one dataset, undetected colluders are\nable to achieve assignment to up to 30% of the papers authored by other\ncolluders. In addition, when 10 colluders bid on all of each other's papers, no\ndetection algorithm outputs a group of reviewers with more than 31% overlap\nwith the true colluders. These results suggest that collusion cannot be\neffectively detected from the bidding, demonstrating the need to develop more\ncomplex detection algorithms that leverage additional metadata.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SI",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.GT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07860v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07860v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07860v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07859v1",
    "updated": "2024-02-12T18:10:17+00:00",
    "published": "2024-02-12T18:10:17+00:00",
    "title": "Lissard: Long and Simple Sequential Reasoning Datasets",
    "authors": [
      {
        "name": "Mirelle Bueno"
      },
      {
        "name": "Roberto Lotufo"
      },
      {
        "name": "Rodrigo Nogueira"
      }
    ],
    "summary": "Language models are now capable of solving tasks that require dealing with\nlong sequences consisting of hundreds of thousands of tokens. However, they\noften fail on tasks that require repetitive use of simple rules, even on\nsequences that are much shorter than those seen during training. For example,\nstate-of-the-art LLMs can find common items in two lists with up to 20 items\nbut fail when lists have 80 items. In this paper, we introduce Lissard, a\nbenchmark comprising seven tasks whose goal is to assess the ability of models\nto process and generate wide-range sequence lengths, requiring repetitive\nprocedural execution. Our evaluation of open-source (Mistral-7B and\nMixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent\ndecline in performance across all models as the complexity of the sequence\nincreases. The datasets and code are available at\nhttps://github.com/unicamp-dl/Lissard",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07859v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07859v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07859v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07858v1",
    "updated": "2024-02-12T18:05:03+00:00",
    "published": "2024-02-12T18:05:03+00:00",
    "title": "Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment",
    "authors": [
      {
        "name": "Bradley T. Baker"
      },
      {
        "name": "Mustafa S. Salman"
      },
      {
        "name": "Zening Fu"
      },
      {
        "name": "Armin Iraji"
      },
      {
        "name": "Elizabeth Osuch"
      },
      {
        "name": "Jeremy Bockholt"
      },
      {
        "name": "Vince D. Calhoun"
      }
    ],
    "summary": "In the clinical treatment of mood disorders, the complex behavioral symptoms\npresented by patients and variability of patient response to particular\nmedication classes can create difficulties in providing fast and reliable\ntreatment when standard diagnostic and prescription methods are used.\nIncreasingly, the incorporation of physiological information such as\nneuroimaging scans and derivatives into the clinical process promises to\nalleviate some of the uncertainty surrounding this process. Particularly, if\nneural features can help to identify patients who may not respond to standard\ncourses of anti-depressants or mood stabilizers, clinicians may elect to avoid\nlengthy and side-effect-laden treatments and seek out a different, more\neffective course that might otherwise not have been under consideration.\nPreviously, approaches for the derivation of relevant neuroimaging features\nwork at only one scale in the data, potentially limiting the depth of\ninformation available for clinical decision support. In this work, we show that\nthe utilization of multi spatial scale neuroimaging features - particularly\nresting state functional networks and functional network connectivity measures\n- provide a rich and robust basis for the identification of relevant medication\nclass and non-responders in the treatment of mood disorders. We demonstrate\nthat the generated features, along with a novel approach for fast and automated\nfeature selection, can support high accuracy rates in the identification of\nmedication class and non-responders as well as the identification of novel,\nmulti-scale biomarkers.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07858v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07858v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07858v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07851v1",
    "updated": "2024-02-12T17:59:20+00:00",
    "published": "2024-02-12T17:59:20+00:00",
    "title": "Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts",
    "authors": [
      {
        "name": "Apoorva Narula"
      },
      {
        "name": "Aastha Jain"
      },
      {
        "name": "Jatin Batra"
      },
      {
        "name": "Sandeep Juneja"
      }
    ],
    "summary": "In this draft we consider the problem of forecasting rainfall across India\nduring the four monsoon months, one day as well as three days in advance. We\ntrain neural networks using historical daily gridded precipitation data for\nIndia obtained from IMD for the time period $1901- 2022$, at a spatial\nresolution of $1^{\\circ} \\times 1^{\\circ}$. This is compared with the numerical\nweather prediction (NWP) forecasts obtained from NCEP (National Centre for\nEnvironmental Prediction) available for the period 2011-2022. We conduct a\ndetailed country wide analysis and separately analyze some of the most\npopulated cities in India. Our conclusion is that forecasts obtained by\napplying deep learning to historical rainfall data are more accurate compared\nto NWP forecasts as well as predictions based on persistence. On average,\ncompared to our predictions, forecasts from NCEP-NWP model have about 34%\nhigher error for a single day prediction, and over 68% higher error for a three\nday prediction. Similarly, persistence estimates report a 29% higher error in a\nsingle day forecast, and over 54% error in a three day forecast. We further\nobserve that data up to 20 days in the past is useful in reducing errors of one\nand three day forecasts, when a transformer based learning architecture, and to\na lesser extent when an LSTM is used. A key conclusion suggested by our\npreliminary analysis is that NWP forecasts can be substantially improved upon\nthrough more and diverse data relevant to monsoon prediction combined with\ncarefully selected neural network architecture.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07851v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07851v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07851v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07846v1",
    "updated": "2024-02-12T17:56:52+00:00",
    "published": "2024-02-12T17:56:52+00:00",
    "title": "Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds",
    "authors": [
      {
        "name": "Bastian Boll"
      },
      {
        "name": "Daniel Gonzalez-Alvarado"
      },
      {
        "name": "Christoph Schn\u00f6rr"
      }
    ],
    "summary": "This paper introduces a novel generative model for discrete distributions\nbased on continuous normalizing flows on the submanifold of factorizing\ndiscrete measures. Integration of the flow gradually assigns categories and\navoids issues of discretizing the latent continuous model like rounding, sample\ntruncation etc. General non-factorizing discrete distributions capable of\nrepresenting complex statistical dependencies of structured discrete data, can\nbe approximated by embedding the submanifold into a the meta-simplex of all\njoint discrete distributions and data-driven averaging. Efficient training of\nthe generative model is demonstrated by matching the flow of geodesics of\nfactorizing discrete distributions. Various experiments underline the\napproach's broad applicability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07846v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07846v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07846v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07845v1",
    "updated": "2024-02-12T17:53:43+00:00",
    "published": "2024-02-12T17:53:43+00:00",
    "title": "An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering",
    "authors": [
      {
        "name": "William Leeney"
      },
      {
        "name": "Ryan McConville"
      }
    ],
    "summary": "Graph Neural Networks (GNNs) can be trained to detect communities within a\ngraph by learning from the duality of feature and connectivity information.\nCurrently, the common approach for optimisation of GNNs is to use comparisons\nto ground-truth for hyperparameter tuning and model selection. In this work, we\nshow that nodes can be clustered into communities with GNNs by solely\noptimising for modularity, without any comparison to ground-truth. Although\nmodularity is a graph partitioning quality metric, we show that this can be\nused to optimise GNNs that also encode features without a drop in performance.\nWe take it a step further and also study whether the unsupervised metric\nperformance can predict ground-truth performance. To investigate why modularity\ncan be used to optimise GNNs, we design synthetic experiments that show the\nlimitations of this approach. The synthetic graphs are created to highlight\ncurrent capabilities in distinct, random and zero information space partitions\nin attributed graphs. We conclude that modularity can be used for\nhyperparameter optimisation and model selection on real-world datasets as well\nas being a suitable proxy for predicting ground-truth performance, however,\nGNNs fail to balance the information duality when the spaces contain\nconflicting signals.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07845v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07845v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07845v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07839v2",
    "updated": "2024-02-13T13:19:54+00:00",
    "published": "2024-02-12T17:50:56+00:00",
    "title": "Towards Meta-Pruning via Optimal Transport",
    "authors": [
      {
        "name": "Alexander Theus"
      },
      {
        "name": "Olin Geimer"
      },
      {
        "name": "Friedrich Wicke"
      },
      {
        "name": "Thomas Hofmann"
      },
      {
        "name": "Sotiris Anagnostidis"
      },
      {
        "name": "Sidak Pal Singh"
      }
    ],
    "summary": "Structural pruning of neural networks conventionally relies on identifying\nand discarding less important neurons, a practice often resulting in\nsignificant accuracy loss that necessitates subsequent fine-tuning efforts.\nThis paper introduces a novel approach named Intra-Fusion, challenging this\nprevailing pruning paradigm. Unlike existing methods that focus on designing\nmeaningful neuron importance metrics, Intra-Fusion redefines the overlying\npruning procedure. Through utilizing the concepts of model fusion and Optimal\nTransport, we leverage an agnostically given importance metric to arrive at a\nmore effective sparse model representation. Notably, our approach achieves\nsubstantial accuracy recovery without the need for resource-intensive\nfine-tuning, making it an efficient and promising tool for neural network\ncompression.\n  Additionally, we explore how fusion can be added to the pruning process to\nsignificantly decrease the training time while maintaining competitive\nperformance. We benchmark our results for various networks on commonly used\ndatasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that\nthe proposed Intra-Fusion approach invigorates exploration into a fresh\nalternative to the predominant compression approaches. Our code is available\nhere: https://github.com/alexandertheus/Intra-Fusion.",
    "comment": "Accepted as a Spotlight (top 5% of submissions) at the International\n  Conference on Learning Representations (ICLR) 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07839v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07839v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07839v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07834v1",
    "updated": "2024-02-12T17:45:40+00:00",
    "published": "2024-02-12T17:45:40+00:00",
    "title": "Generalizing across Temporal Domains with Koopman Operators",
    "authors": [
      {
        "name": "Qiuhao Zeng"
      },
      {
        "name": "Wei Wang"
      },
      {
        "name": "Fan Zhou"
      },
      {
        "name": "Gezheng Xu"
      },
      {
        "name": "Ruizhi Pu"
      },
      {
        "name": "Changjian Shui"
      },
      {
        "name": "Christian Gagne"
      },
      {
        "name": "Shichun Yang"
      },
      {
        "name": "Boyu Wang"
      },
      {
        "name": "Charles X. Ling"
      }
    ],
    "summary": "In the field of domain generalization, the task of constructing a predictive\nmodel capable of generalizing to a target domain without access to target data\nremains challenging. This problem becomes further complicated when considering\nevolving dynamics between domains. While various approaches have been proposed\nto address this issue, a comprehensive understanding of the underlying\ngeneralization theory is still lacking. In this study, we contribute novel\ntheoretic results that aligning conditional distribution leads to the reduction\nof generalization bounds. Our analysis serves as a key motivation for solving\nthe Temporal Domain Generalization (TDG) problem through the application of\nKoopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By\nemploying Koopman Operators, we effectively address the time-evolving\ndistributions encountered in TDG using the principles of Koopman theory, where\nmeasurement functions are sought to establish linear transition relations\nbetween evolving domains. Through empirical evaluations conducted on synthetic\nand real-world datasets, we validate the effectiveness of our proposed\napproach.",
    "comment": "15 pages, 7 figures, Accepted by AAAI 2024. arXiv admin note: text\n  overlap with arXiv:2206.00047",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07834v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07834v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07834v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07822v1",
    "updated": "2024-02-12T17:26:35+00:00",
    "published": "2024-02-12T17:26:35+00:00",
    "title": "Understanding fitness landscapes in morpho-evolution via local optima networks",
    "authors": [
      {
        "name": "Sarah L. Thomson"
      },
      {
        "name": "L\u00e9ni K. Le Goff"
      },
      {
        "name": "Emma Hart"
      },
      {
        "name": "Edgar Buchanan"
      }
    ],
    "summary": "Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's\ndesign and controller to maximise performance given a task and environment.\nMany genetic encodings have been proposed which are capable of representing\ndesign and control. Previous research has provided empirical comparisons\nbetween encodings in terms of their performance with respect to an objective\nfunction and the diversity of designs that are evaluated, however there has\nbeen no attempt to explain the observed findings. We address this by applying\nLocal Optima Network (LON) analysis to investigate the structure of the fitness\nlandscapes induced by three different encodings when evolving a robot for a\nlocomotion task, shedding new light on the ease by which different fitness\nlandscapes can be traversed by a search process. This is the first time LON\nanalysis has been applied in the field of ME despite its popularity in\ncombinatorial optimisation domains; the findings will facilitate design of new\nalgorithms or operators that are customised to ME landscapes in the future.",
    "comment": "Submitted to GECCO-2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07822v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07822v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07822v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07821v1",
    "updated": "2024-02-12T17:25:23+00:00",
    "published": "2024-02-12T17:25:23+00:00",
    "title": "On Computationally Efficient Multi-Class Calibration",
    "authors": [
      {
        "name": "Parikshit Gopalan"
      },
      {
        "name": "Lunjia Hu"
      },
      {
        "name": "Guy N. Rothblum"
      }
    ],
    "summary": "Consider a multi-class labelling problem, where the labels can take values in\n$[k]$, and a predictor predicts a distribution over the labels. In this work,\nwe study the following foundational question: Are there notions of multi-class\ncalibration that give strong guarantees of meaningful predictions and can be\nachieved in time and sample complexities polynomial in $k$? Prior notions of\ncalibration exhibit a tradeoff between computational efficiency and\nexpressivity: they either suffer from having sample complexity exponential in\n$k$, or needing to solve computationally intractable problems, or give rather\nweak guarantees.\n  Our main contribution is a notion of calibration that achieves all these\ndesiderata: we formulate a robust notion of projected smooth calibration for\nmulti-class predictions, and give new recalibration algorithms for efficiently\ncalibrating predictors under this definition with complexity polynomial in $k$.\nProjected smooth calibration gives strong guarantees for all downstream\ndecision makers who want to use the predictor for binary classification\nproblems of the form: does the label belong to a subset $T \\subseteq [k]$: e.g.\nis this an image of an animal? It ensures that the probabilities predicted by\nsumming the probabilities assigned to labels in $T$ are close to some perfectly\ncalibrated binary predictor for that task. We also show that natural\nstrengthenings of our definition are computationally hard to achieve: they run\ninto information theoretic barriers or computational intractability. Underlying\nboth our upper and lower bounds is a tight connection that we prove between\nmulti-class calibration and the well-studied problem of agnostic learning in\nthe (standard) binary prediction setting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CC",
      "cs.DS",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07821v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07821v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07821v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07818v1",
    "updated": "2024-02-12T17:24:15+00:00",
    "published": "2024-02-12T17:24:15+00:00",
    "title": "Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning",
    "authors": [
      {
        "name": "Z Liu"
      },
      {
        "name": "J Lou"
      },
      {
        "name": "W Bao"
      },
      {
        "name": "Z Qin"
      },
      {
        "name": "K Ren"
      }
    ],
    "summary": "Finetuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs finetuning and its accompanying privacy\nconcerns, differentially private (DP) finetuning of pretrained LLMs has\ngarnered increasing attention to safeguarding the privacy of task-specific\ndatasets. Lying at the design core of DP LLM finetuning methods is the\nsatisfactory tradeoff between privacy, utility, and scalability. Most existing\nmethods build upon the seminal work of DP-SGD. Despite pushing the scalability\nof DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately\nlimited by the inherent inefficiency of SGD. In this paper, we investigate the\npotential of DP zeroth-order methods for LLM pretraining, which avoids the\nscalability bottleneck of SGD by approximating the gradient with the more\nefficient zeroth-order gradient. Rather than treating the zeroth-order method\nas a drop-in replacement for SGD, this paper presents a comprehensive study\nboth theoretically and empirically. First, we propose the stagewise DP\nzeroth-order method that dynamically schedules key hyperparameters. This design\nis grounded on the synergy between DP random perturbation and the gradient\napproximation error of the zeroth-order method, and its effect on finetuning\ntrajectory. Second, we further enhance the scalability by reducing the\ntrainable parameters that are identified by repurposing a data-free pruning\ntechnique requiring no additional data or extra privacy budget. We provide\ntheoretical analysis for both proposed methods. We conduct extensive empirical\nanalysis on both encoder-only masked language model and decoder-only\nautoregressive language model, achieving impressive results in terms of\nscalability and utility.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07818v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07818v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07818v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07814v1",
    "updated": "2024-02-12T17:18:51+00:00",
    "published": "2024-02-12T17:18:51+00:00",
    "title": "PBADet: A One-Stage Anchor-Free Approach for Part-Body Association",
    "authors": [
      {
        "name": "Zhongpai Gao"
      },
      {
        "name": "Huayi Zhou"
      },
      {
        "name": "Abhishek Sharma"
      },
      {
        "name": "Meng Zheng"
      },
      {
        "name": "Benjamin Planche"
      },
      {
        "name": "Terrence Chen"
      },
      {
        "name": "Ziyan Wu"
      }
    ],
    "summary": "The detection of human parts (e.g., hands, face) and their correct\nassociation with individuals is an essential task, e.g., for ubiquitous\nhuman-machine interfaces and action recognition. Traditional methods often\nemploy multi-stage processes, rely on cumbersome anchor-based systems, or do\nnot scale well to larger part sets. This paper presents PBADet, a novel\none-stage, anchor-free approach for part-body association detection. Building\nupon the anchor-free object representation across multi-scale feature maps, we\nintroduce a singular part-to-body center offset that effectively encapsulates\nthe relationship between parts and their parent bodies. Our design is\ninherently versatile and capable of managing multiple parts-to-body\nassociations without compromising on detection accuracy or robustness.\nComprehensive experiments on various datasets underscore the efficacy of our\napproach, which not only outperforms existing state-of-the-art techniques but\nalso offers a more streamlined and efficient solution to the part-body\nassociation challenge.",
    "comment": "Accepted by ICLR2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07814v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07814v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07814v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07812v1",
    "updated": "2024-02-12T17:17:50+00:00",
    "published": "2024-02-12T17:17:50+00:00",
    "title": "Retrieval-Augmented Thought Process as Sequential Decision Making",
    "authors": [
      {
        "name": "Thomas Pouplin"
      },
      {
        "name": "Hao Sun"
      },
      {
        "name": "Samuel Holt"
      },
      {
        "name": "Mihaela van der Schaar"
      }
    ],
    "summary": "Large Language Models (LLMs) have demonstrated their strong ability to assist\npeople and show \"sparks of intelligence\". However, several open challenges\nhinder their wider application: such as concerns over privacy, tendencies to\nproduce hallucinations, and difficulties in handling long contexts. In this\nwork, we address those challenges by introducing the Retrieval-Augmented\nThought Process (RATP). Given access to external knowledge, RATP formulates the\nthought generation of LLMs as a multiple-step decision process. To optimize\nsuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns a\nQ-value estimator that permits cost-efficient inference. In addressing the task\nof question-answering with private data, where ethical and security concerns\nlimit LLM training methods, RATP achieves a 50% improvement over existing\nin-context retrieval-augmented language models.",
    "comment": "17 pages, 18 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "H.3.3; I.2.6; I.2.7; I.2.8"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07812v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07812v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07812v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07808v1",
    "updated": "2024-02-12T17:13:02+00:00",
    "published": "2024-02-12T17:13:02+00:00",
    "title": "Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation",
    "authors": [
      {
        "name": "Julius Vetter"
      },
      {
        "name": "Guy Moss"
      },
      {
        "name": "Cornelius Schr\u00f6der"
      },
      {
        "name": "Richard Gao"
      },
      {
        "name": "Jakob H. Macke"
      }
    ],
    "summary": "Scientific modeling applications often require estimating a distribution of\nparameters consistent with a dataset of observations - an inference task also\nknown as source distribution estimation. This problem can be ill-posed,\nhowever, since many different source distributions might produce the same\ndistribution of data-consistent simulations. To make a principled choice among\nmany equally valid sources, we propose an approach which targets the maximum\nentropy distribution, i.e., prioritizes retaining as much uncertainty as\npossible. Our method is purely sample-based - leveraging the Sliced-Wasserstein\ndistance to measure the discrepancy between the dataset and simulations - and\nthus suitable for simulators with intractable likelihoods. We benchmark our\nmethod on several tasks, and show that it can recover source distributions with\nsubstantially higher entropy without sacrificing the fidelity of the\nsimulations. Finally, to demonstrate the utility of our approach, we infer\nsource distributions for parameters of the Hodgkin-Huxley neuron model from\nexperimental datasets with thousands of measurements. In summary, we propose a\nprincipled framework for inferring unique source distributions of scientific\nsimulator parameters while retaining as much uncertainty as possible.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07808v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07808v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07808v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07802v1",
    "updated": "2024-02-12T17:07:02+00:00",
    "published": "2024-02-12T17:07:02+00:00",
    "title": "Towards a mathematical theory for consistency training in diffusion models",
    "authors": [
      {
        "name": "Gen Li"
      },
      {
        "name": "Zhihan Huang"
      },
      {
        "name": "Yuting Wei"
      }
    ],
    "summary": "Consistency models, which were proposed to mitigate the high computational\noverhead during the sampling phase of diffusion models, facilitate single-step\nsampling while attaining state-of-the-art empirical performance. When\nintegrated into the training phase, consistency models attempt to train a\nsequence of consistency functions capable of mapping any point at any time step\nof the diffusion process to its starting point. Despite the empirical success,\na comprehensive theoretical understanding of consistency training remains\nelusive. This paper takes a first step towards establishing theoretical\nunderpinnings for consistency models. We demonstrate that, in order to generate\nsamples within $\\varepsilon$ proximity to the target in distribution (measured\nby some Wasserstein metric), it suffices for the number of steps in consistency\nlearning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data\ndimension. Our theory offers rigorous insights into the validity and efficacy\nof consistency models, illuminating their utility in downstream inference\ntasks.",
    "comment": "The first two authors contributed equally",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07802v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07802v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07802v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07799v2",
    "updated": "2024-02-14T14:01:55+00:00",
    "published": "2024-02-12T17:03:58+00:00",
    "title": "Generalising Planning Environment Redesign",
    "authors": [
      {
        "name": "Alberto Pozanco"
      },
      {
        "name": "Ramon Fraga Pereira"
      },
      {
        "name": "Daniel Borrajo"
      }
    ],
    "summary": "In Environment Design, one interested party seeks to affect another agent's\ndecisions by applying changes to the environment. Most research on planning\nenvironment (re)design assumes the interested party's objective is to\nfacilitate the recognition of goals and plans, and search over the space of\nenvironment modifications to find the minimal set of changes that simplify\nthose tasks and optimise a particular metric. This search space is usually\nintractable, so existing approaches devise metric-dependent pruning techniques\nfor performing search more efficiently. This results in approaches that are not\nable to generalise across different objectives and/or metrics. In this paper,\nwe argue that the interested party could have objectives and metrics that are\nnot necessarily related to recognising agents' goals or plans. Thus, to\ngeneralise the task of Planning Environment Redesign, we develop a general\nenvironment redesign approach that is metric-agnostic and leverages recent\nresearch on top-quality planning to efficiently redesign planning environments\naccording to any interested party's objective and metric. Experiments over a\nset of environment redesign benchmarks show that our general approach\noutperforms existing approaches when using well-known metrics, such as\nfacilitating the recognition of goals, as well as its effectiveness when\nsolving environment redesign tasks that optimise a novel set of different\nmetrics.",
    "comment": "Paper accepted at AAAI'24",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07799v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07799v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07799v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07793v1",
    "updated": "2024-02-12T16:59:06+00:00",
    "published": "2024-02-12T16:59:06+00:00",
    "title": "Tuning-Free Stochastic Optimization",
    "authors": [
      {
        "name": "Ahmed Khaled"
      },
      {
        "name": "Chi Jin"
      }
    ],
    "summary": "Large-scale machine learning problems make the cost of hyperparameter tuning\never more prohibitive. This creates a need for algorithms that can tune\nthemselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that\ncan match the performance of optimally-tuned optimization algorithms up to\npolylogarithmic factors given only loose hints on the relevant problem\nparameters. We consider in particular algorithms that can match optimally-tuned\nStochastic Gradient Descent (SGD). When the domain of optimization is bounded,\nwe show tuning-free matching of SGD is possible and achieved by several\nexisting algorithms. We prove that for the task of minimizing a convex and\nsmooth or Lipschitz function over an unbounded domain, tuning-free optimization\nis impossible. We discuss conditions under which tuning-free optimization is\npossible even over unbounded domains. In particular, we show that the recently\nproposed DoG and DoWG algorithms are tuning-free when the noise distribution is\nsufficiently well-behaved. For the task of finding a stationary point of a\nsmooth and potentially nonconvex function, we give a variant of SGD that\nmatches the best-known high-probability convergence rate for tuned SGD at only\nan additional polylogarithmic cost. However, we also give an impossibility\nresult that shows no algorithm can hope to match the optimal expected\nconvergence rate for tuned SGD with high probability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07793v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07793v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07793v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07792v1",
    "updated": "2024-02-12T16:59:05+00:00",
    "published": "2024-02-12T16:59:05+00:00",
    "title": "Empowering Federated Learning for Massive Models with NVIDIA FLARE",
    "authors": [
      {
        "name": "Holger R. Roth"
      },
      {
        "name": "Ziyue Xu"
      },
      {
        "name": "Yuan-Ting Hsieh"
      },
      {
        "name": "Adithya Renduchintala"
      },
      {
        "name": "Isaac Yang"
      },
      {
        "name": "Zhihong Zhang"
      },
      {
        "name": "Yuhong Wen"
      },
      {
        "name": "Sean Yang"
      },
      {
        "name": "Kevin Lu"
      },
      {
        "name": "Kristopher Kersten"
      },
      {
        "name": "Camir Ricketts"
      },
      {
        "name": "Daguang Xu"
      },
      {
        "name": "Chester Chen"
      },
      {
        "name": "Yan Cheng"
      },
      {
        "name": "Andrew Feng"
      }
    ],
    "summary": "In the ever-evolving landscape of artificial intelligence (AI) and large\nlanguage models (LLMs), handling and leveraging data effectively has become a\ncritical challenge. Most state-of-the-art machine learning algorithms are\ndata-centric. However, as the lifeblood of model performance, necessary data\ncannot always be centralized due to various factors such as privacy,\nregulation, geopolitics, copyright issues, and the sheer effort required to\nmove vast datasets. In this paper, we explore how federated learning enabled by\nNVIDIA FLARE can address these challenges with easy and scalable integration\ncapabilities, enabling parameter-efficient and full supervised fine-tuning of\nLLMs for natural language processing and biopharmaceutical applications to\nenhance their accuracy and robustness.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07792v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07792v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07792v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07790v1",
    "updated": "2024-02-12T16:55:19+00:00",
    "published": "2024-02-12T16:55:19+00:00",
    "title": "From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration",
    "authors": [
      {
        "name": "Agathe Fernandes Machado"
      },
      {
        "name": "Arthur Charpentier"
      },
      {
        "name": "Emmanuel Flachaire"
      },
      {
        "name": "Ewen Gallic"
      },
      {
        "name": "Fran\u00e7ois Hu"
      }
    ],
    "summary": "The assessment of binary classifier performance traditionally centers on\ndiscriminative ability using metrics, such as accuracy. However, these metrics\noften disregard the model's inherent uncertainty, especially when dealing with\nsensitive decision-making domains, such as finance or healthcare. Given that\nmodel-predicted scores are commonly seen as event probabilities, calibration is\ncrucial for accurate interpretation. In our study, we analyze the sensitivity\nof various calibration measures to score distortions and introduce a refined\nmetric, the Local Calibration Score. Comparing recalibration methods, we\nadvocate for local regressions, emphasizing their dual role as effective\nrecalibration tools and facilitators of smoother visualizations. We apply these\nfindings in a real-world scenario using Random Forest classifier and regressor\nto predict credit default while simultaneously measuring calibration during\nperformance optimization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07790v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07790v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07790v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07787v2",
    "updated": "2024-02-13T15:05:37+00:00",
    "published": "2024-02-12T16:52:26+00:00",
    "title": "Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis",
    "authors": [
      {
        "name": "Xiaowei Zhao"
      },
      {
        "name": "Yong Zhou"
      },
      {
        "name": "Xiujuan Xu"
      },
      {
        "name": "Yu Liu"
      }
    ],
    "summary": "Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within\na text to comprehend sentiment information. Previous studies integrated\nexternal knowledge, such as knowledge graphs, to enhance the semantic features\nin ABSA models. Recent research has examined the use of Graph Neural Networks\n(GNNs) on dependency and constituent trees for syntactic analysis. With the\nongoing development of ABSA, more innovative linguistic and structural features\nare being incorporated (e.g. latent graph), but this also introduces complexity\nand confusion. As of now, a scalable framework for integrating diverse\nlinguistic and structural features into ABSA does not exist. This paper\npresents the Extensible Multi-Granularity Fusion (EMGF) network, which\nintegrates information from dependency and constituent syntactic, attention\nsemantic , and external knowledge graphs. EMGF, equipped with multi-anchor\ntriplet learning and orthogonal projection, efficiently harnesses the combined\npotential of each granularity feature and their synergistic interactions,\nresulting in a cumulative effect without additional computational expenses.\nExperimental findings on SemEval 2014 and Twitter datasets confirm EMGF's\nsuperiority over existing ABSA methods.",
    "comment": "8 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07787v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07787v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07787v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07785v1",
    "updated": "2024-02-12T16:50:07+00:00",
    "published": "2024-02-12T16:50:07+00:00",
    "title": "HYPO: Hyperspherical Out-of-Distribution Generalization",
    "authors": [
      {
        "name": "Haoyue Bai"
      },
      {
        "name": "Yifei Ming"
      },
      {
        "name": "Julian Katz-Samuels"
      },
      {
        "name": "Yixuan Li"
      }
    ],
    "summary": "Out-of-distribution (OOD) generalization is critical for machine learning\nmodels deployed in the real world. However, achieving this can be fundamentally\nchallenging, as it requires the ability to learn invariant features across\ndifferent domains or environments. In this paper, we propose a novel framework\nHYPO (HYPerspherical OOD generalization) that provably learns domain-invariant\nrepresentations in a hyperspherical space. In particular, our hyperspherical\nlearning algorithm is guided by intra-class variation and inter-class\nseparation principles -- ensuring that features from the same class (across\ndifferent training domains) are closely aligned with their class prototypes,\nwhile different class prototypes are maximally separated. We further provide\ntheoretical justifications on how our prototypical learning objective improves\nthe OOD generalization bound. Through extensive experiments on challenging OOD\nbenchmarks, we demonstrate that our approach outperforms competitive baselines\nand achieves superior performance. Code is available at\nhttps://github.com/deeplearning-wisc/hypo.",
    "comment": "Published as a conference paper at ICLR 2024; First two authors\n  contributed equally",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07785v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07785v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07785v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07781v1",
    "updated": "2024-02-12T16:47:08+00:00",
    "published": "2024-02-12T16:47:08+00:00",
    "title": "IR-Aware ECO Timing Optimization Using Reinforcement Learning",
    "authors": [
      {
        "name": "Vidya A. Chhabria"
      },
      {
        "name": "Wenjing Jiang"
      },
      {
        "name": "Sachin S. Sapatnekar"
      }
    ],
    "summary": "Engineering change orders (ECOs) in late stages make minimal design fixes to\nrecover from timing shifts due to excessive IR drops. This paper integrates\nIR-drop-aware timing analysis and ECO timing optimization using reinforcement\nlearning (RL). The method operates after physical design and power grid\nsynthesis, and rectifies IR-drop-induced timing degradation through gate\nsizing. It incorporates the Lagrangian relaxation (LR) technique into a novel\nRL framework, which trains a relational graph convolutional network (R-GCN)\nagent to sequentially size gates to fix timing violations. The R-GCN agent\noutperforms a classical LR-only algorithm: in an open 45nm technology, it (a)\nmoves the Pareto front of the delay-area tradeoff curve to the left and (b)\nsaves runtime over the classical method by running fast inference using trained\nmodels at iso-quality. The RL model is transferable across timing\nspecifications, and transferable to unseen designs with zero-shot learning or\nfine tuning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AR",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07781v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07781v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07781v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07772v1",
    "updated": "2024-02-12T16:33:35+00:00",
    "published": "2024-02-12T16:33:35+00:00",
    "title": "End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty",
    "authors": [
      {
        "name": "My H Dinh"
      },
      {
        "name": "James Kotary"
      },
      {
        "name": "Ferdinando Fioretto"
      }
    ],
    "summary": "Many decision processes in artificial intelligence and operations research\nare modeled by parametric optimization problems whose defining parameters are\nunknown and must be inferred from observable data. The Predict-Then-Optimize\n(PtO) paradigm in machine learning aims to maximize downstream decision quality\nby training the parametric inference model end-to-end with the subsequent\nconstrained optimization. This requires backpropagation through the\noptimization problem using approximation techniques specific to the problem's\nform, especially for nondifferentiable linear and mixed-integer programs. This\npaper extends the PtO methodology to optimization problems with\nnondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their\nability to ensure properties of fairness and robustness in decision models.\nThrough a collection of training techniques and proposed application settings,\nit shows how optimization of OWA functions can be effectively integrated with\nparametric prediction for fair and robust optimization under uncertainty.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07772v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07772v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07772v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07763v1",
    "updated": "2024-02-12T16:28:57+00:00",
    "published": "2024-02-12T16:28:57+00:00",
    "title": "Multi-level Optimal Control with Neural Surrogate Models",
    "authors": [
      {
        "name": "Dante Kalise"
      },
      {
        "name": "Estefan\u00eda Loayza-Romero"
      },
      {
        "name": "Kirsten A. Morris"
      },
      {
        "name": "Zhengang Zhong"
      }
    ],
    "summary": "Optimal actuator and control design is studied as a multi-level optimisation\nproblem, where the actuator design is evaluated based on the performance of the\nassociated optimal closed loop. The evaluation of the optimal closed loop for a\ngiven actuator realisation is a computationally demanding task, for which the\nuse of a neural network surrogate is proposed. The use of neural network\nsurrogates to replace the lower level of the optimisation hierarchy enables the\nuse of fast gradient-based and gradient-free consensus-based optimisation\nmethods to determine the optimal actuator design. The effectiveness of the\nproposed surrogate models and optimisation methods is assessed in a test\nrelated to optimal actuator location for heat control.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07763v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07763v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07763v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07762v1",
    "updated": "2024-02-12T16:28:52+00:00",
    "published": "2024-02-12T16:28:52+00:00",
    "title": "Scalable Structure Learning for Sparse Context-Specific Causal Systems",
    "authors": [
      {
        "name": "Felix Leopoldo Rios"
      },
      {
        "name": "Alex Markham"
      },
      {
        "name": "Liam Solus"
      }
    ],
    "summary": "Several approaches to graphically representing context-specific relations\namong jointly distributed categorical variables have been proposed, along with\nstructure learning algorithms. While existing optimization-based methods have\nlimited scalability due to the large number of context-specific models, the\nconstraint-based methods are more prone to error than even constraint-based DAG\nlearning algorithms since more relations must be tested. We present a hybrid\nalgorithm for learning context-specific models that scales to hundreds of\nvariables while testing no more constraints than standard DAG learning\nalgorithms. Scalable learning is achieved through a combination of an\norder-based MCMC algorithm and sparsity assumptions analogous to those\ntypically invoked for DAG models. To implement the method, we solve a special\ncase of an open problem recently posed by Alon and Balogh. The method is shown\nto perform well on synthetic data and real world examples, in terms of both\naccuracy and scalability.",
    "comment": "23 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.CO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07762v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07762v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07762v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07757v1",
    "updated": "2024-02-12T16:25:47+00:00",
    "published": "2024-02-12T16:25:47+00:00",
    "title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
    "authors": [
      {
        "name": "Mikail Khona"
      },
      {
        "name": "Maya Okawa"
      },
      {
        "name": "Jan Hula"
      },
      {
        "name": "Rahul Ramesh"
      },
      {
        "name": "Kento Nishi"
      },
      {
        "name": "Robert Dick"
      },
      {
        "name": "Ekdeep Singh Lubana"
      },
      {
        "name": "Hidenori Tanaka"
      }
    ],
    "summary": "Stepwise inference protocols, such as scratchpads and chain-of-thought, help\nlanguage models solve complex problems by decomposing them into a sequence of\nsimpler subproblems. Despite the significant gain in performance achieved via\nthese protocols, the underlying mechanisms of stepwise inference have remained\nelusive. To address this, we propose to study autoregressive Transformer models\non a synthetic task that embodies the multi-step nature of problems where\nstepwise inference is generally most useful. Specifically, we define a graph\nnavigation problem wherein a model is tasked with traversing a path from a\nstart to a goal node on the graph. Despite is simplicity, we find we can\nempirically reproduce and analyze several phenomena observed at scale: (i) the\nstepwise inference reasoning gap, the cause of which we find in the structure\nof the training data; (ii) a diversity-accuracy tradeoff in model generations\nas sampling temperature varies; (iii) a simplicity bias in the model's output;\nand (iv) compositional generalization and a primacy bias with in-context\nexemplars. Overall, our work introduces a grounded, synthetic framework for\nstudying stepwise inference and offers mechanistic hypotheses that can lay the\nfoundation for a deeper understanding of this phenomenon.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07757v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07757v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07757v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07754v1",
    "updated": "2024-02-12T16:23:28+00:00",
    "published": "2024-02-12T16:23:28+00:00",
    "title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models",
    "authors": [
      {
        "name": "Jiacheng Ye"
      },
      {
        "name": "Shansan Gong"
      },
      {
        "name": "Liheng Chen"
      },
      {
        "name": "Lin Zheng"
      },
      {
        "name": "Jiahui Gao"
      },
      {
        "name": "Han Shi"
      },
      {
        "name": "Chuan Wu"
      },
      {
        "name": "Zhenguo Li"
      },
      {
        "name": "Wei Bi"
      },
      {
        "name": "Lingpeng Kong"
      }
    ],
    "summary": "Diffusion models have gained attention in text processing, offering many\npotential advantages over traditional autoregressive models. This work explores\nthe integration of diffusion models and Chain-of-Thought (CoT), a\nwell-established technique to improve the reasoning ability in autoregressive\nlanguage models. We propose Diffusion-of-Thought (DoT), allowing reasoning\nsteps to diffuse over time through the diffusion process. In contrast to\ntraditional autoregressive language models that make decisions in a\nleft-to-right, token-by-token manner, DoT offers more flexibility in the\ntrade-off between computation and reasoning performance. Our experimental\nresults demonstrate the effectiveness of DoT in multi-digit multiplication and\ngrade school math problems. Additionally, DoT showcases promising\nself-correction abilities and benefits from existing reasoning-enhancing\ntechniques like self-consistency decoding. Our findings contribute to the\nunderstanding and development of reasoning capabilities in diffusion language\nmodels.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07754v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07754v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07754v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07752v1",
    "updated": "2024-02-12T16:21:50+00:00",
    "published": "2024-02-12T16:21:50+00:00",
    "title": "Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains",
    "authors": [
      {
        "name": "Yasin Findik"
      },
      {
        "name": "S. Reza Ahmadzadeh"
      }
    ],
    "summary": "Tackling multi-agent learning problems efficiently is a challenging task in\ncontinuous action domains. While value-based algorithms excel in sample\nefficiency when applied to discrete action domains, they are usually\ninefficient when dealing with continuous actions. Policy-based algorithms, on\nthe other hand, attempt to address this challenge by leveraging critic networks\nfor guiding the learning process and stabilizing the gradient estimation. The\nlimitations in the estimation of true return and falling into local optima in\nthese methods result in inefficient and often sub-optimal policies. In this\npaper, we diverge from the trend of further enhancing critic networks, and\nfocus on improving the effectiveness of value-based methods in multi-agent\ncontinuous domains by concurrently evaluating numerous actions. We propose a\nnovel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired\nfrom the idea of Q-Functionals, that enables agents to transform their states\ninto basis functions. Our algorithm fosters collaboration among agents by\nmixing their action-values. We evaluate the efficacy of our algorithm in six\ncooperative multi-agent scenarios. Our empirical findings reveal that MQF\noutperforms four variants of Deep Deterministic Policy Gradient through rapid\naction evaluation and increased sample efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": [
      "cs.MA",
      "cs.LG",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07752v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07752v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07752v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07745v1",
    "updated": "2024-02-12T16:15:25+00:00",
    "published": "2024-02-12T16:15:25+00:00",
    "title": "Predictive Churn with the Set of Good Models",
    "authors": [
      {
        "name": "Jamelle Watson-Daniels"
      },
      {
        "name": "Flavio du Pin Calmon"
      },
      {
        "name": "Alexander D'Amour"
      },
      {
        "name": "Carol Long"
      },
      {
        "name": "David C. Parkes"
      },
      {
        "name": "Berk Ustun"
      }
    ],
    "summary": "Machine learning models in modern mass-market applications are often updated\nover time. One of the foremost challenges faced is that, despite increasing\noverall performance, these updates may flip specific model predictions in\nunpredictable ways. In practice, researchers quantify the number of unstable\npredictions between models pre and post update -- i.e., predictive churn. In\nthis paper, we study this effect through the lens of predictive multiplicity --\ni.e., the prevalence of conflicting predictions over the set of near-optimal\nmodels (the Rashomon set). We show how traditional measures of predictive\nmultiplicity can be used to examine expected churn over this set of prospective\nmodels -- i.e., the set of models that may be used to replace a baseline model\nin deployment. We present theoretical results on the expected churn between\nmodels within the Rashomon set from different perspectives. And we characterize\nexpected churn over model updates via the Rashomon set, pairing our analysis\nwith empirical results on real-world datasets -- showing how our approach can\nbe used to better anticipate, reduce, and avoid churn in consumer-facing\napplications. Further, we show that our approach is useful even for models\nenhanced with uncertainty awareness.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07745v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07745v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07745v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07744v2",
    "updated": "2024-02-14T18:43:54+00:00",
    "published": "2024-02-12T16:14:22+00:00",
    "title": "Towards Unified Alignment Between Agents, Humans, and Environment",
    "authors": [
      {
        "name": "Zonghan Yang"
      },
      {
        "name": "An Liu"
      },
      {
        "name": "Zijun Liu"
      },
      {
        "name": "Kaiming Liu"
      },
      {
        "name": "Fangzhou Xiong"
      },
      {
        "name": "Yile Wang"
      },
      {
        "name": "Zeyuan Yang"
      },
      {
        "name": "Qingyuan Hu"
      },
      {
        "name": "Xinrui Chen"
      },
      {
        "name": "Zhenhe Zhang"
      },
      {
        "name": "Fuwen Luo"
      },
      {
        "name": "Zhicheng Guo"
      },
      {
        "name": "Peng Li"
      },
      {
        "name": "Yang Liu"
      }
    ],
    "summary": "The rapid progress of foundation models has led to the prosperity of\nautonomous agents, which leverage the universal capabilities of foundation\nmodels to conduct reasoning, decision-making, and environmental interaction.\nHowever, the efficacy of agents remains limited when operating in intricate,\nrealistic environments. In this work, we introduce the principles of\n$\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents\n($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with\nhuman intentions, environmental dynamics, and self-constraints such as the\nlimitation of monetary budgets. From the perspective of $\\mathbf{UA}^2$, we\nreview the current agent research and highlight the neglected factors in\nexisting agent benchmarks and method candidates. We also conduct\nproof-of-concept studies by introducing realistic features to WebShop,\nincluding user profiles to demonstrate intentions, personalized reranking for\ncomplex environmental dynamics, and runtime cost statistics to reflect\nself-constraints. We then follow the principles of $\\mathbf{UA}^2$ to propose\nan initial design of our agent, and benchmark its performance with several\ncandidate baselines in the retrofitted WebShop. The extensive experimental\nresults further prove the importance of the principles of $\\mathbf{UA}^2$. Our\nresearch sheds light on the next steps of autonomous agent research with\nimproved general problem-solving abilities.",
    "comment": "Project webpage:\n  https://agent-force.github.io/unified-alignment-for-agents.html",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07744v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07744v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07744v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07739v1",
    "updated": "2024-02-12T15:57:31+00:00",
    "published": "2024-02-12T15:57:31+00:00",
    "title": "Task-conditioned adaptation of visual features in multi-task policy learning",
    "authors": [
      {
        "name": "Pierre Marza"
      },
      {
        "name": "Laetitia Matignon"
      },
      {
        "name": "Olivier Simonin"
      },
      {
        "name": "Christian Wolf"
      }
    ],
    "summary": "Successfully addressing a wide variety of tasks is a core ability of\nautonomous agents, which requires flexibly adapting the underlying\ndecision-making strategies and, as we argue in this work, also adapting the\nunderlying perception modules. An analogical argument would be the human visual\nsystem, which uses top-down signals to focus attention determined by the\ncurrent task. Similarly, in this work, we adapt pre-trained large vision models\nconditioned on specific downstream tasks in the context of multi-task policy\nlearning. We introduce task-conditioned adapters that do not require finetuning\nany pre-trained weights, combined with a single policy trained with behavior\ncloning and capable of addressing multiple tasks. We condition the policy and\nvisual adapters on task embeddings, which can be selected at inference if the\ntask is known, or alternatively inferred from a set of example demonstrations.\nTo this end, we propose a new optimization-based estimator. We evaluate the\nmethod on a wide variety of tasks of the CortexBench benchmark and show that,\ncompared to existing work, it can be addressed with a single policy. In\nparticular, we demonstrate that adapting visual features is a key design choice\nand that the method generalizes to unseen tasks given visual demonstrations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07739v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07739v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07739v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07738v1",
    "updated": "2024-02-12T15:52:27+00:00",
    "published": "2024-02-12T15:52:27+00:00",
    "title": "Universal link predictor by In-context Learning",
    "authors": [
      {
        "name": "Kaiwen Dong"
      },
      {
        "name": "Haitao Mao"
      },
      {
        "name": "Zhichun Guo"
      },
      {
        "name": "Nitesh V. Chawla"
      }
    ],
    "summary": "Link prediction is a crucial task in graph machine learning, where the goal\nis to infer missing or future links within a graph. Traditional approaches\nleverage heuristic methods based on widely observed connectivity patterns,\noffering broad applicability and generalizability without the need for model\ntraining. Despite their utility, these methods are limited by their reliance on\nhuman-derived heuristics and lack the adaptability of data-driven approaches.\nConversely, parametric link predictors excel in automatically learning the\nconnectivity patterns from data and achieving state-of-the-art but fail short\nto directly transfer across different graphs. Instead, it requires the cost of\nextensive training and hyperparameter optimization to adapt to the target\ngraph. In this work, we introduce the Universal Link Predictor (UniLP), a novel\nmodel that combines the generalizability of heuristic approaches with the\npattern learning capabilities of parametric models. UniLP is designed to\nautonomously identify connectivity patterns across diverse graphs, ready for\nimmediate application to any unseen graph dataset without targeted training. We\naddress the challenge of conflicting connectivity patterns-arising from the\nunique distributions of different graphs-through the implementation of\nIn-context Learning (ICL). This approach allows UniLP to dynamically adjust to\nvarious target graphs based on contextual demonstrations, thereby avoiding\nnegative transfer. Through rigorous experimentation, we demonstrate UniLP's\neffectiveness in adapting to new, unseen graphs at test time, showcasing its\nability to perform comparably or even outperform parametric models that have\nbeen finetuned for specific datasets. Our findings highlight UniLP's potential\nto set a new standard in link prediction, combining the strengths of heuristic\nand parametric methods in a single, versatile framework.",
    "comment": "Preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07738v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07738v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07738v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07735v2",
    "updated": "2024-02-13T09:48:47+00:00",
    "published": "2024-02-12T15:48:58+00:00",
    "title": "Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism",
    "authors": [
      {
        "name": "Philipp Froehlich"
      },
      {
        "name": "Heinz Koeppl"
      }
    ],
    "summary": "In statistics and machine learning, detecting dependencies in datasets is a\ncentral challenge. We propose a novel neural network model for supervised graph\nstructure learning, i.e., the process of learning a mapping between\nobservational data and their underlying dependence structure. The model is\ntrained with variably shaped and coupled simulated input data and requires only\na single forward pass through the trained network for inference. By leveraging\nstructural equation models and employing randomly generated multivariate\nChebyshev polynomials for the simulation of training data, our method\ndemonstrates robust generalizability across both linear and various types of\nnon-linear dependencies. We introduce a novel bilinear attention mechanism\n(BAM) for explicit processing of dependency information, which operates on the\nlevel of covariance matrices of transformed data and respects the geometry of\nthe manifold of symmetric positive definite matrices. Empirical evaluation\ndemonstrates the robustness of our method in detecting a wide range of\ndependencies, excelling in undirected graph estimation and proving competitive\nin completed partially directed acyclic graph estimation through a novel\ntwo-step approach.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07735v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07735v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07735v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07729v1",
    "updated": "2024-02-12T15:41:22+00:00",
    "published": "2024-02-12T15:41:22+00:00",
    "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
    "authors": [
      {
        "name": "Qian Yang"
      },
      {
        "name": "Jin Xu"
      },
      {
        "name": "Wenrui Liu"
      },
      {
        "name": "Yunfei Chu"
      },
      {
        "name": "Ziyue Jiang"
      },
      {
        "name": "Xiaohuan Zhou"
      },
      {
        "name": "Yichong Leng"
      },
      {
        "name": "Yuanjun Lv"
      },
      {
        "name": "Zhou Zhao"
      },
      {
        "name": "Chang Zhou"
      },
      {
        "name": "Jingren Zhou"
      }
    ],
    "summary": "Recently, instruction-following audio-language models have received broad\nattention for human-audio interaction. However, the absence of benchmarks\ncapable of evaluating audio-centric interaction capabilities has impeded\nadvancements in this field. Previous models primarily focus on assessing\ndifferent fundamental tasks, such as Automatic Speech Recognition (ASR), and\nlack an assessment of the open-ended generative capabilities centered around\naudio. Thus, it is challenging to track the progression in the Large\nAudio-Language Models (LALMs) domain and to provide guidance for future\nimprovement. In this paper, we introduce AIR-Bench (\\textbf{A}udio\n\\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed\nto evaluate the ability of LALMs to understand various types of audio signals\n(including human speech, natural sounds, and music), and furthermore, to\ninteract with humans in the textual format. AIR-Bench encompasses two\ndimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former\nconsists of 19 tasks with approximately 19k single-choice questions, intending\nto inspect the basic single-task ability of LALMs. The latter one contains 2k\ninstances of open-ended question-and-answer data, directly assessing the\ncomprehension of the model on complex audio and its capacity to follow\ninstructions. Both benchmarks require the model to generate hypotheses\ndirectly. We design a unified framework that leverages advanced language\nmodels, such as GPT-4, to evaluate the scores of generated hypotheses given the\nmeta-information of the audio. Experimental results demonstrate a high level of\nconsistency between GPT-4-based evaluation and human evaluation. By revealing\nthe limitations of existing LALMs through evaluation results, AIR-Bench can\nprovide insights into the direction of future research.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07729v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07729v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07729v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07723v1",
    "updated": "2024-02-12T15:35:32+00:00",
    "published": "2024-02-12T15:35:32+00:00",
    "title": "Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation",
    "authors": [
      {
        "name": "Benjamin Dupuis"
      },
      {
        "name": "Umut \u015eim\u015fekli"
      }
    ],
    "summary": "Understanding the generalization properties of heavy-tailed stochastic\noptimization algorithms has attracted increasing attention over the past years.\nWhile illuminating interesting aspects of stochastic optimizers by using\nheavy-tailed stochastic differential equations as proxies, prior works either\nprovided expected generalization bounds, or introduced non-computable\ninformation theoretic terms. Addressing these drawbacks, in this work, we prove\nhigh-probability generalization bounds for heavy-tailed SDEs which do not\ncontain any nontrivial information theoretic terms. To achieve this goal, we\ndevelop new proof techniques based on estimating the entropy flows associated\nwith the so-called fractional Fokker-Planck equation (a partial differential\nequation that governs the evolution of the distribution of the corresponding\nheavy-tailed SDE). In addition to obtaining high-probability bounds, we show\nthat our bounds have a better dependence on the dimension of parameters as\ncompared to prior art. Our results further identify a phase transition\nphenomenon, which suggests that heavy tails can be either beneficial or harmful\ndepending on the problem structure. We support our theory with experiments\nconducted in a variety of settings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07723v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07723v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07723v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07721v1",
    "updated": "2024-02-12T15:34:56+00:00",
    "published": "2024-02-12T15:34:56+00:00",
    "title": "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation",
    "authors": [
      {
        "name": "Hongyun Zhou"
      },
      {
        "name": "Xiangyu Lu"
      },
      {
        "name": "Wang Xu"
      },
      {
        "name": "Conghui Zhu"
      },
      {
        "name": "Tiejun Zhao"
      }
    ],
    "summary": "Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to\nfine-tune the pre-trained model under limited computing resources. But it still\nfaces challenges of resource consumption when scaling up to larger models.\nPrevious studies employ pruning techniques by evaluating the importance of LoRA\nparameters for different layers to address the problem. However, these efforts\nonly analyzed parameter features to evaluate their importance. Indeed, the\noutput of LoRA related to the parameters and data is the factor that directly\nimpacts the frozen model. To this end, we propose LoRA-drop which evaluates the\nimportance of the parameters by analyzing the LoRA output. We retain LoRA for\nimportant layers and the LoRA of the other layers share the same parameters.\nAbundant experiments on NLU and NLG tasks demonstrate the effectiveness of\nLoRA-drop.",
    "comment": "12 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07721v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07721v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07721v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07712v1",
    "updated": "2024-02-12T15:26:01+00:00",
    "published": "2024-02-12T15:26:01+00:00",
    "title": "Model Collapse Demystified: The Case of Regression",
    "authors": [
      {
        "name": "Elvis Dohmatob"
      },
      {
        "name": "Yunzhen Feng"
      },
      {
        "name": "Julia Kempe"
      }
    ],
    "summary": "In the era of large language models like ChatGPT, the phenomenon of \"model\ncollapse\" refers to the situation whereby as a model is trained recursively on\ndata generated from previous generations of itself over time, its performance\ndegrades until the model eventually becomes completely useless, i.e the model\ncollapses. In this work, we study this phenomenon in the simplified setting of\nkernel regression and obtain results which show a clear crossover between where\nthe model can cope with fake data, and a regime where the model's performance\ncompletely collapses. Under polynomial decaying spectral and source conditions,\nwe obtain modified scaling laws which exhibit new crossover phenomena from fast\nto slow rates. We also propose a simple strategy based on adaptive\nregularization to mitigate model collapse. Our theoretical results are\nvalidated with experiments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07712v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07712v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07712v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07710v1",
    "updated": "2024-02-12T15:23:19+00:00",
    "published": "2024-02-12T15:23:19+00:00",
    "title": "Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA",
    "authors": [
      {
        "name": "Chester Luo"
      },
      {
        "name": "Kevin Lai"
      }
    ],
    "summary": "In recent years, there has been a significant increase in the utilization of\ndeep learning methods, particularly convolutional neural networks (CNNs), which\nhave emerged as the dominant approach in various domains that involve\nstructured grid data, such as picture analysis and processing. Nevertheless,\nthe exponential growth in the utilization of LiDAR and 3D sensors across many\ndomains has resulted in an increased need for the analysis of 3D point clouds.\nThe utilization of 3D point clouds is crucial in various applications,\nincluding object recognition and segmentation, as they offer a spatial\ndepiction of things within a three-dimensional environment. In contrast to\nphotos, point clouds exhibit sparsity and lack a regular grid, hence posing\ndistinct processing and computational issues.",
    "comment": "8 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07710v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07710v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07710v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07703v2",
    "updated": "2024-02-14T06:16:25+00:00",
    "published": "2024-02-12T15:17:31+00:00",
    "title": "Online Sequential Decision-Making with Unknown Delays",
    "authors": [
      {
        "name": "Ping Wu"
      },
      {
        "name": "Heyan Huang"
      },
      {
        "name": "Zhengyang Liu"
      }
    ],
    "summary": "In the field of online sequential decision-making, we address the problem\nwith delays utilizing the framework of online convex optimization (OCO), where\nthe feedback of a decision can arrive with an unknown delay. Unlike previous\nresearch that is limited to Euclidean norm and gradient information, we propose\nthree families of delayed algorithms based on approximate solutions to handle\ndifferent types of received feedback. Our proposed algorithms are versatile and\napplicable to universal norms. Specifically, we introduce a family of Follow\nthe Delayed Regularized Leader algorithms for feedback with full information on\nthe loss function, a family of Delayed Mirror Descent algorithms for feedback\nwith gradient information on the loss function and a family of Simplified\nDelayed Mirror Descent algorithms for feedback with the value information of\nthe loss function's gradients at corresponding decision points. For each type\nof algorithm, we provide corresponding regret bounds under cases of general\nconvexity and relative strong convexity, respectively. We also demonstrate the\nefficiency of each algorithm under different norms through concrete examples.\nFurthermore, our theoretical results are consistent with the current best\nbounds when degenerated to standard settings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07703v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07703v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07703v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07692v1",
    "updated": "2024-02-12T14:59:40+00:00",
    "published": "2024-02-12T14:59:40+00:00",
    "title": "Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints",
    "authors": [
      {
        "name": "Yunsheng Tian"
      },
      {
        "name": "Ane Zuniga"
      },
      {
        "name": "Xinwei Zhang"
      },
      {
        "name": "Johannes P. D\u00fcrholt"
      },
      {
        "name": "Payel Das"
      },
      {
        "name": "Jie Chen"
      },
      {
        "name": "Wojciech Matusik"
      },
      {
        "name": "Mina Konakovi\u0107 Lukovi\u0107"
      }
    ],
    "summary": "Bayesian optimization has been successfully applied to optimize black-box\nfunctions where the number of evaluations is severely limited. However, in many\nreal-world applications, it is hard or impossible to know in advance which\ndesigns are feasible due to some physical or system limitations. These issues\nlead to an even more challenging problem of optimizing an unknown function with\nunknown constraints. In this paper, we observe that in such scenarios optimal\nsolution typically lies on the boundary between feasible and infeasible regions\nof the design space, making it considerably more difficult than that with\ninterior optima. Inspired by this observation, we propose BE-CBO, a new\nBayesian optimization method that efficiently explores the boundary between\nfeasible and infeasible designs. To identify the boundary, we learn the\nconstraints with an ensemble of neural networks that outperform the standard\nGaussian Processes for capturing complex boundaries. Our method demonstrates\nsuperior performance against state-of-the-art methods through comprehensive\nexperiments on synthetic and real-world benchmarks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07692v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07692v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07692v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07689v1",
    "updated": "2024-02-12T14:53:37+00:00",
    "published": "2024-02-12T14:53:37+00:00",
    "title": "OrderBkd: Textual backdoor attack through repositioning",
    "authors": [
      {
        "name": "Irina Alekseevskaia"
      },
      {
        "name": "Konstantin Arkhipenko"
      }
    ],
    "summary": "The use of third-party datasets and pre-trained machine learning models poses\na threat to NLP systems due to possibility of hidden backdoor attacks. Existing\nattacks involve poisoning the data samples such as insertion of tokens or\nsentence paraphrasing, which either alter the semantics of the original texts\nor can be detected. Our main difference from the previous work is that we use\nthe reposition of a two words in a sentence as a trigger. By designing and\napplying specific part-of-speech (POS) based rules for selecting these tokens,\nwe maintain high attack success rate on SST-2 and AG classification datasets\nwhile outperforming existing attacks in terms of perplexity and semantic\nsimilarity to the clean samples. In addition, we show the robustness of our\nattack to the ONION defense method. All the code and data for the paper can be\nobtained at https://github.com/alekseevskaia/OrderBkd.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07689v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07689v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07689v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07688v1",
    "updated": "2024-02-12T14:53:28+00:00",
    "published": "2024-02-12T14:53:28+00:00",
    "title": "CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity",
    "authors": [
      {
        "name": "Norbert Tihanyi"
      },
      {
        "name": "Mohamed Amine Ferrag"
      },
      {
        "name": "Ridhi Jain"
      },
      {
        "name": "Merouane Debbah"
      }
    ],
    "summary": "Large Language Models (LLMs) excel across various domains, from computer\nvision to medical diagnostics. However, understanding the diverse landscape of\ncybersecurity, encompassing cryptography, reverse engineering, and managerial\nfacets like risk assessment, presents a challenge, even for human experts. In\nthis paper, we introduce CyberMetric, a benchmark dataset comprising 10,000\nquestions sourced from standards, certifications, research papers, books, and\nother publications in the cybersecurity domain. The questions are created\nthrough a collaborative process, i.e., merging expert knowledge with LLMs,\nincluding GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying\ntheir accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's\nmain goal is to facilitate a fair comparison between humans and different LLMs\nin cybersecurity. To achieve this, we carefully selected 80 questions covering\na wide range of topics within cybersecurity and involved 30 participants of\ndiverse expertise levels, facilitating a comprehensive comparison between human\nand machine intelligence in this area. The findings revealed that LLMs\noutperformed humans in almost every aspect of cybersecurity.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07688v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07688v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07688v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07685v1",
    "updated": "2024-02-12T14:48:31+00:00",
    "published": "2024-02-12T14:48:31+00:00",
    "title": "Contrastive Multiple Instance Learning for Weakly Supervised Person ReID",
    "authors": [
      {
        "name": "Jacob Tyo"
      },
      {
        "name": "Zachary C. Lipton"
      }
    ],
    "summary": "The acquisition of large-scale, precisely labeled datasets for person\nre-identification (ReID) poses a significant challenge. Weakly supervised ReID\nhas begun to address this issue, although its performance lags behind fully\nsupervised methods. In response, we introduce Contrastive Multiple Instance\nLearning (CMIL), a novel framework tailored for more effective weakly\nsupervised ReID. CMIL distinguishes itself by requiring only a single model and\nno pseudo labels while leveraging contrastive losses -- a technique that has\nsignificantly enhanced traditional ReID performance yet is absent in all prior\nMIL-based approaches. Through extensive experiments and analysis across three\ndatasets, CMIL not only matches state-of-the-art performance on the large-scale\nSYSU-30k dataset with fewer assumptions but also consistently outperforms all\nbaselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification\ndataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an\nextension of the MUDD dataset featuring naturally occurring weak labels from\nthe real-world application at PerformancePhoto.co. All our code and data are\naccessible at\nhttps://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07685v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07685v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07685v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07684v1",
    "updated": "2024-02-12T14:46:31+00:00",
    "published": "2024-02-12T14:46:31+00:00",
    "title": "Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks",
    "authors": [
      {
        "name": "Saurabh Sihag"
      },
      {
        "name": "Gonzalo Mateos"
      },
      {
        "name": "Alejandro Ribeiro"
      }
    ],
    "summary": "Brain age is the estimate of biological age derived from neuroimaging\ndatasets using machine learning algorithms. Increasing brain age with respect\nto chronological age can reflect increased vulnerability to neurodegeneration\nand cognitive decline. In this paper, we study NeuroVNN, based on coVariance\nneural networks, as a paradigm for foundation model for the brain age\nprediction application. NeuroVNN is pre-trained as a regression model on\nhealthy population to predict chronological age using cortical thickness\nfeatures and fine-tuned to estimate brain age in different neurological\ncontexts. Importantly, NeuroVNN adds anatomical interpretability to brain age\nand has a `scale-free' characteristic that allows its transference to datasets\ncurated according to any arbitrary brain atlas. Our results demonstrate that\nNeuroVNN can extract biologically plausible brain age estimates in different\npopulations, as well as transfer successfully to datasets of dimensionalities\ndistinct from that for the dataset used to train NeuroVNN.",
    "comment": "Preliminary work. Contact sihag.saurabh@gmail.com for the NeuroVNN\n  model and code used for results reported in this manuscript",
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.QM",
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "stat.AP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07684v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07684v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07684v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07681v1",
    "updated": "2024-02-12T14:40:54+00:00",
    "published": "2024-02-12T14:40:54+00:00",
    "title": "Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?",
    "authors": [
      {
        "name": "Vicent Briva-Iglesias"
      },
      {
        "name": "Joao Lucas Cavalheiro Camargo"
      },
      {
        "name": "Gokhan Dogru"
      }
    ],
    "summary": "This study evaluates the machine translation (MT) quality of two\nstate-of-the-art large language models (LLMs) against a tradition-al neural\nmachine translation (NMT) system across four language pairs in the legal\ndomain. It combines automatic evaluation met-rics (AEMs) and human evaluation\n(HE) by professional transla-tors to assess translation ranking, fluency and\nadequacy. The re-sults indicate that while Google Translate generally\noutperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4,\ncomparably or slightly better in terms of producing contextually adequate and\nfluent translations. This discrepancy suggests LLMs' potential in handling\nspecialized legal terminology and context, highlighting the importance of human\nevaluation methods in assessing MT quality. The study underscores the evolving\ncapabil-ities of LLMs in specialized domains and calls for reevaluation of\ntraditional AEMs to better capture the nuances of LLM-generated translations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07681v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07681v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07681v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07680v1",
    "updated": "2024-02-12T14:40:43+00:00",
    "published": "2024-02-12T14:40:43+00:00",
    "title": "AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer",
    "authors": [
      {
        "name": "Tanmoy Dam"
      },
      {
        "name": "Sanjay Bhargav Dharavath"
      },
      {
        "name": "Sameer Alam"
      },
      {
        "name": "Nimrod Lilith"
      },
      {
        "name": "Supriyo Chakraborty"
      },
      {
        "name": "Mir Feroskhan"
      }
    ],
    "summary": "Combining LiDAR and camera data has shown potential in enhancing\nshort-distance object detection in autonomous driving systems. Yet, the fusion\nencounters difficulties with extended distance detection due to the contrast\nbetween LiDAR's sparse data and the dense resolution of cameras. Besides,\ndiscrepancies in the two data representations further complicate fusion\nmethods. We introduce AYDIV, a novel framework integrating a tri-phase\nalignment process specifically designed to enhance long-distance detection even\namidst data discrepancies. AYDIV consists of the Global Contextual Fusion\nAlignment Transformer (GCFAT), which improves the extraction of camera features\nand provides a deeper understanding of large-scale patterns; the Sparse Fused\nFeature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera\ndetails; and the Volumetric Grid Attention (VGA) for a comprehensive spatial\ndata fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an\nimprovement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset\nwith a performance improvement of 7.40% in AP value demonstrates its efficacy\nin comparison to other existing fusion-based methods. Our code is publicly\navailable at https://github.com/sanjay-810/AYDIV2",
    "comment": "This paper has been accepted for ICRA 2024, and copyright will\n  automatically transfer to IEEE upon its availability on the IEEE portal",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07680v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07680v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07680v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07642v1",
    "updated": "2024-02-12T13:30:34+00:00",
    "published": "2024-02-12T13:30:34+00:00",
    "title": "A Flow-based Credibility Metric for Safety-critical Pedestrian Detection",
    "authors": [
      {
        "name": "Maria Lyssenko"
      },
      {
        "name": "Christoph Gladisch"
      },
      {
        "name": "Christian Heinzemann"
      },
      {
        "name": "Matthias Woehrle"
      },
      {
        "name": "Rudolph Triebel"
      }
    ],
    "summary": "Safety is of utmost importance for perception in automated driving (AD).\nHowever, a prime safety concern in state-of-the art object detection is that\nstandard evaluation schemes utilize safety-agnostic metrics to argue sufficient\ndetection performance. Hence, it is imperative to leverage supplementary domain\nknowledge to accentuate safety-critical misdetections during evaluation tasks.\nTo tackle the underspecification, this paper introduces a novel credibility\nmetric, called c-flow, for pedestrian bounding boxes. To this end, c-flow\nrelies on a complementary optical flow signal from image sequences and enhances\nthe analyses of safety-critical misdetections without requiring additional\nlabels. We implement and evaluate c-flow with a state-of-the-art pedestrian\ndetector on a large AD dataset. Our analysis demonstrates that c-flow allows\ndevelopers to identify safety-critical misdetections.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07642v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07642v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07642v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07640v1",
    "updated": "2024-02-12T13:27:22+00:00",
    "published": "2024-02-12T13:27:22+00:00",
    "title": "Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data",
    "authors": [
      {
        "name": "Puneet Kumar"
      },
      {
        "name": "Sarthak Malik"
      },
      {
        "name": "Balasubramanian Raman"
      },
      {
        "name": "Xiaobai Li"
      }
    ],
    "summary": "The ability to generate sentiment-controlled feedback in response to\nmultimodal inputs, comprising both text and images, addresses a critical gap in\nhuman-computer interaction by enabling systems to provide empathetic, accurate,\nand engaging responses. This capability has profound applications in\nhealthcare, marketing, and education. To this end, we construct a large-scale\nControllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a\ncontrollable feedback synthesis system. The proposed system includes an\nencoder, decoder, and controllability block for textual and visual inputs. It\nextracts textual and visual features using a transformer and Faster R-CNN\nnetworks and combines them to generate feedback. The CMFeed dataset encompasses\nimages, text, reactions to the post, human comments with relevance scores, and\nreactions to the comments. The reactions to the post and comments are utilized\nto train the proposed model to produce feedback with a particular (positive or\nnegative) sentiment. A sentiment classification accuracy of 77.23% has been\nachieved, 18.82% higher than the accuracy without using the controllability.\nMoreover, the system incorporates a similarity module for assessing feedback\nrelevance through rank-based metrics. It implements an interpretability\ntechnique to analyze the contribution of textual and visual features during the\ngeneration of uncontrolled and controlled feedback.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MM",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07640v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07640v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07640v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07639v1",
    "updated": "2024-02-12T13:24:32+00:00",
    "published": "2024-02-12T13:24:32+00:00",
    "title": "Tighter Bounds on the Information Bottleneck with Application to Deep Learning",
    "authors": [
      {
        "name": "Nir Weingarten"
      },
      {
        "name": "Zohar Yakhini"
      },
      {
        "name": "Moshe Butman"
      },
      {
        "name": "Ran Gilad-Bachrach"
      }
    ],
    "summary": "Deep Neural Nets (DNNs) learn latent representations induced by their\ndownstream task, objective function, and other parameters. The quality of the\nlearned representations impacts the DNN's generalization ability and the\ncoherence of the emerging latent space. The Information Bottleneck (IB)\nprovides a hypothetically optimal framework for data modeling, yet it is often\nintractable. Recent efforts combined DNNs with the IB by applying VAE-inspired\nvariational methods to approximate bounds on mutual information, resulting in\nimproved robustness to adversarial attacks. This work introduces a new and\ntighter variational bound for the IB, improving performance of previous\nIB-inspired DNNs. These advancements strengthen the case for the IB and its\nvariational approximations as a data modeling framework, and provide a simple\nmethod to significantly enhance the adversarial robustness of classifier DNNs.",
    "comment": "10 pages, 5 figures, code included in github repo",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "94A08, 94A10, 94A11, 68T06, 62B04, 62B08",
      "I.2; E.4; I.4; I.7"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07639v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07639v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07639v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07632v1",
    "updated": "2024-02-12T13:16:30+00:00",
    "published": "2024-02-12T13:16:30+00:00",
    "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
    "authors": [
      {
        "name": "Jingshu Li"
      },
      {
        "name": "Yitian Yang"
      },
      {
        "name": "Yi-chieh Lee"
      }
    ],
    "summary": "As artificial intelligence (AI) advances, human-AI collaboration has become\nincreasingly prevalent across both professional and everyday settings. In such\ncollaboration, AI can express its confidence level about its performance,\nserving as a crucial indicator for humans to evaluate AI's suggestions.\nHowever, AI may exhibit overconfidence or underconfidence--its expressed\nconfidence is higher or lower than its actual performance--which may lead\nhumans to mistakenly evaluate AI advice. Our study investigates the influences\nof AI's overconfidence and underconfidence on human trust, their acceptance of\nAI suggestions, and collaboration outcomes. Our study reveal that disclosing AI\nconfidence levels and performance feedback facilitates better recognition of AI\nconfidence misalignments. However, participants tend to withhold their trust as\nperceiving such misalignments, leading to a rejection of AI suggestions and\nsubsequently poorer performance in collaborative tasks. Conversely, without\nsuch information, participants struggle to identify misalignments, resulting in\neither the neglect of correct AI advice or the following of incorrect AI\nsuggestions, adversely affecting collaboration. This study offers valuable\ninsights for enhancing human-AI collaboration by underscoring the importance of\naligning AI's expressed confidence with its actual performance and the\nnecessity of calibrating human trust towards AI confidence.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07632v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07632v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07632v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07630v1",
    "updated": "2024-02-12T13:13:04+00:00",
    "published": "2024-02-12T13:13:04+00:00",
    "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
    "authors": [
      {
        "name": "Xiaoxin He"
      },
      {
        "name": "Yijun Tian"
      },
      {
        "name": "Yifei Sun"
      },
      {
        "name": "Nitesh V. Chawla"
      },
      {
        "name": "Thomas Laurent"
      },
      {
        "name": "Yann LeCun"
      },
      {
        "name": "Xavier Bresson"
      },
      {
        "name": "Bryan Hooi"
      }
    ],
    "summary": "Given a graph with textual attributes, we enable users to `chat with their\ngraph': that is, to ask questions about the graph using a conversational\ninterface. In response to a user's questions, our method provides textual\nreplies and highlights the relevant parts of the graph. While existing works\nintegrate large language models (LLMs) and graph neural networks (GNNs) in\nvarious ways, they mostly focus on either conventional graph tasks (such as\nnode, edge, and graph classification), or on answering simple graph queries on\nsmall or synthetic graphs. In contrast, we develop a flexible\nquestion-answering framework targeting real-world textual graphs, applicable to\nmultiple applications including scene graph understanding, common sense\nreasoning, and knowledge graph reasoning. Toward this goal, we first develop\nour Graph Question Answering (GraphQA) benchmark with data collected from\ndifferent tasks. Then, we propose our G-Retriever approach, which integrates\nthe strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can\nbe fine-tuned to enhance graph understanding via soft prompting. To resist\nhallucination and to allow for textual graphs that greatly exceed the LLM's\ncontext window size, G-Retriever performs RAG over a graph by formulating this\ntask as a Prize-Collecting Steiner Tree optimization problem. Empirical\nevaluations show that our method outperforms baselines on textual graph tasks\nfrom multiple domains, scales well with larger graph sizes, and resists\nhallucination. (Our codes and datasets are available at:\nhttps://github.com/XiaoxinHe/G-Retriever.)",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07630v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07630v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07630v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07626v1",
    "updated": "2024-02-12T13:11:11+00:00",
    "published": "2024-02-12T13:11:11+00:00",
    "title": "Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features",
    "authors": [
      {
        "name": "Rodrigo Veiga"
      },
      {
        "name": "Anastasia Remizova"
      },
      {
        "name": "Nicolas Macris"
      }
    ],
    "summary": "We investigate the test risk of continuous-time stochastic gradient flow\ndynamics in learning theory. Using a path integral formulation we provide, in\nthe regime of a small learning rate, a general formula for computing the\ndifference between test risk curves of pure gradient and stochastic gradient\nflows. We apply the general theory to a simple model of weak features, which\ndisplays the double descent phenomenon, and explicitly compute the corrections\nbrought about by the added stochastic term in the dynamics, as a function of\ntime and model parameters. The analytical results are compared to simulations\nof discrete-time stochastic gradient descent and show good agreement.",
    "comment": "34 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07626v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07626v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07626v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07625v1",
    "updated": "2024-02-12T13:09:21+00:00",
    "published": "2024-02-12T13:09:21+00:00",
    "title": "AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts",
    "authors": [
      {
        "name": "Yifan Zhang"
      },
      {
        "name": "Yifan Luo"
      },
      {
        "name": "Yang Yuan"
      },
      {
        "name": "Andrew Chi-Chih Yao"
      }
    ],
    "summary": "To improve language models' proficiency in mathematical reasoning via\ncontinual pretraining, we introduce a novel strategy that leverages base\nlanguage models for autonomous data selection. Departing from conventional\nsupervised fine-tuning or trained classifiers with human-annotated data, our\napproach utilizes meta-prompted language models as zero-shot verifiers to\nautonomously evaluate and select high-quality mathematical content, and we\nrelease the curated open-source AutoMathText dataset encompassing over 200GB of\ndata. To demonstrate the efficacy of our method, we continuously pretrained a\n7B-parameter Mistral language model on the AutoMathText dataset, achieving\nsubstantial improvements in downstream performance on the MATH dataset with a\ntoken amount reduced by orders of magnitude compared to previous continuous\npretraining works. Our method showcases a 2 times increase in pretraining token\nefficiency compared to baselines, underscoring the potential of our approach in\nenhancing models' mathematical reasoning capabilities. The AutoMathText dataset\nis available at https://huggingface.co/datasets/math-ai/AutoMathText. The code\nis available at https://github.com/yifanzhang-pro/AutoMathText.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07625v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07625v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07625v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07621v1",
    "updated": "2024-02-12T12:55:35+00:00",
    "published": "2024-02-12T12:55:35+00:00",
    "title": "Correctness Verification of Neural Networks Approximating Differential Equations",
    "authors": [
      {
        "name": "Petros Ellinas"
      },
      {
        "name": "Rahul Nellikath"
      },
      {
        "name": "Ignasi Ventura"
      },
      {
        "name": "Jochen Stiasny"
      },
      {
        "name": "Spyros Chatzivasileiadis"
      }
    ],
    "summary": "Verification of Neural Networks (NNs) that approximate the solution of\nPartial Differential Equations (PDEs) is a major milestone towards enhancing\ntheir trustworthiness and accelerating their deployment, especially for\nsafety-critical systems. If successful, such NNs can become integral parts of\nsimulation software tools which can accelerate the simulation of complex\ndynamic systems more than 100 times. However, the verification of these\nfunctions poses major challenges; it is not straightforward how to efficiently\nbound them or how to represent the derivative of the NN. This work addresses\nboth these problems. First, we define the NN derivative as a finite difference\napproximation. Then, we formulate the PDE residual bounding problem alongside\nthe Initial Value Problem's error propagation. Finally, for the first time, we\ntackle the problem of bounding an NN function without a priori knowledge of the\noutput domain. For this, we build a parallel branching algorithm that combines\nthe incomplete CROWN solver and Gradient Attack for termination and domain\nrejection conditions. We demonstrate the strengths and weaknesses of the\nproposed framework, and we suggest further work to enhance its efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.SY",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07621v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07621v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07621v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07619v1",
    "updated": "2024-02-12T12:52:47+00:00",
    "published": "2024-02-12T12:52:47+00:00",
    "title": "Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data",
    "authors": [
      {
        "name": "Yuyang Yan"
      },
      {
        "name": "Wafaa Aljbawi"
      },
      {
        "name": "Sami O. Simons"
      },
      {
        "name": "Visara Urovi"
      }
    ],
    "summary": "COVID-19 has affected more than 223 countries worldwide and in the Post-COVID\nEra, there is a pressing need for non-invasive, low-cost, and highly scalable\nsolutions to detect COVID-19. We develop a deep learning model to identify\nCOVID-19 from voice recording data. The novelty of this work is in the\ndevelopment of deep learning models for COVID-19 identification from only voice\nrecordings. We use the Cambridge COVID-19 Sound database which contains 893\nspeech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.\nVoice features including Mel-spectrograms and Mel-frequency cepstral\ncoefficients (MFCC) and CNN Encoder features are extracted. Based on the voice\ndata, we develop deep learning classification models to detect COVID-19 cases.\nThese models include Long Short-Term Memory (LSTM) and Convolutional Neural\nNetwork (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power\nto baseline machine learning models. HuBERT achieves the highest accuracy of\n86\\% and the highest AUC of 0.93. The results achieved with the proposed models\nsuggest promising results in COVID-19 diagnosis from voice recordings when\ncompared to the results obtained from the state-of-the-art.",
    "comment": "arXiv admin note: text overlap with arXiv:2209.03727",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07619v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07619v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07619v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07616v1",
    "updated": "2024-02-12T12:48:02+00:00",
    "published": "2024-02-12T12:48:02+00:00",
    "title": "Anchor-based Large Language Models",
    "authors": [
      {
        "name": "Jianhui Pang"
      },
      {
        "name": "Fanghua Ye"
      },
      {
        "name": "Derek F. Wong"
      },
      {
        "name": "Longyue Wang"
      }
    ],
    "summary": "Large language models (LLMs) predominantly employ decoder-only transformer\narchitectures, necessitating the retention of keys/values information for\nhistorical tokens to provide contextual information and avoid redundant\ncomputation. However, the substantial size and parameter volume of these LLMs\nrequire massive GPU memory. This memory demand increases with the length of the\ninput text, leading to an urgent need for more efficient methods of information\nstorage and processing. This study introduces the Anchor-based LLM (AnLLM),\nwhich utilizes an innovative anchor-based self-attention network (AnSAN) and\nalso an anchor-based inference strategy. This approach enables LLMs to compress\nsequence information into an anchor token, reducing the keys/values cache and\nenhancing inference efficiency. Experiments show that the AnLLM maintains\ncomparable accuracy with up to 99% keys/values cache reduction and up to 3.5\ntimes faster inference. Despite a minor compromise in accuracy, the AnLLM\nsignificantly improves computational efficiency and resource utilization,\ndemonstrating the potential of the anchor-based attention approach in the\ncontext of LLMs for real-time inference in practical applications.",
    "comment": "13 pages. Work was done when Jianhui Pang and Fanghua Ye were\n  interning at Tencent AI Lab. Longyue Wang is the corresponding author",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07616v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07616v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07616v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07613v1",
    "updated": "2024-02-12T12:38:20+00:00",
    "published": "2024-02-12T12:38:20+00:00",
    "title": "Global optimality under amenable symmetry constraints",
    "authors": [
      {
        "name": "Peter Orbanz"
      }
    ],
    "summary": "We ask whether there exists a function or measure that (1) minimizes a given\nconvex functional or risk and (2) satisfies a symmetry property specified by an\namenable group of transformations. Examples of such symmetry properties are\ninvariance, equivariance, or quasi-invariance. Our results draw on old ideas of\nStein and Le Cam and on approximate group averages that appear in ergodic\ntheorems for amenable groups. A class of convex sets known as orbitopes in\nconvex analysis emerges as crucial, and we establish properties of such\norbitopes in nonparametric settings. We also show how a simple device called a\ncocycle can be used to reduce different forms of symmetry to a single problem.\nAs applications, we obtain results on invariant kernel mean embeddings and a\nMonge-Kantorovich theorem on optimality of transport plans under symmetry\nconstraints. We also explain connections to the Hunt-Stein theorem on invariant\ntests.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.ST",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07613v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07613v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07613v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07610v1",
    "updated": "2024-02-12T12:30:42+00:00",
    "published": "2024-02-12T12:30:42+00:00",
    "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
    "authors": [
      {
        "name": "Haoyu Wang"
      },
      {
        "name": "Guozheng Ma"
      },
      {
        "name": "Ziqiao Meng"
      },
      {
        "name": "Zeyu Qin"
      },
      {
        "name": "Li Shen"
      },
      {
        "name": "Zhong Zhang"
      },
      {
        "name": "Bingzhe Wu"
      },
      {
        "name": "Liu Liu"
      },
      {
        "name": "Yatao Bian"
      },
      {
        "name": "Tingyang Xu"
      },
      {
        "name": "Xueqian Wang"
      },
      {
        "name": "Peilin Zhao"
      }
    ],
    "summary": "Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07610v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07610v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07610v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07598v1",
    "updated": "2024-02-12T11:58:18+00:00",
    "published": "2024-02-12T11:58:18+00:00",
    "title": "Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model",
    "authors": [
      {
        "name": "Mark Rowland"
      },
      {
        "name": "Li Kevin Wenliang"
      },
      {
        "name": "R\u00e9mi Munos"
      },
      {
        "name": "Clare Lyle"
      },
      {
        "name": "Yunhao Tang"
      },
      {
        "name": "Will Dabney"
      }
    ],
    "summary": "We propose a new algorithm for model-based distributional reinforcement\nlearning (RL), and prove that it is minimax-optimal for approximating return\ndistributions with a generative model (up to logarithmic factors), resolving an\nopen question of Zhang et al. (2023). Our analysis provides new theoretical\nresults on categorical approaches to distributional RL, and also introduces a\nnew distributional Bellman equation, the stochastic categorical CDF Bellman\nequation, which we expect to be of independent interest. We also provide an\nexperimental study comparing several model-based distributional RL algorithms,\nwith several takeaways for practitioners.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07598v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07598v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07598v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07595v2",
    "updated": "2024-02-13T15:39:11+00:00",
    "published": "2024-02-12T11:49:08+00:00",
    "title": "Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification",
    "authors": [
      {
        "name": "Yuning Huang"
      },
      {
        "name": "Jingchen Zou"
      },
      {
        "name": "Lanxi Meng"
      },
      {
        "name": "Xin Yue"
      },
      {
        "name": "Qing Zhao"
      },
      {
        "name": "Jianqiang Li"
      },
      {
        "name": "Changwei Song"
      },
      {
        "name": "Gabriel Jimenez"
      },
      {
        "name": "Shaowu Li"
      },
      {
        "name": "Guanghui Fu"
      }
    ],
    "summary": "Medical image analysis frequently encounters data scarcity challenges.\nTransfer learning has been effective in addressing this issue while conserving\ncomputational resources. The recent advent of foundational models like the\nDINOv2, which uses the vision transformer architecture, has opened new\nopportunities in the field and gathered significant interest. However, DINOv2's\nperformance on clinical data still needs to be verified. In this paper, we\nperformed a glioma grading task using three clinical modalities of brain MRI\ndata. We compared the performance of various pre-trained deep learning models,\nincluding those based on ImageNet and DINOv2, in a transfer learning context.\nOur focus was on understanding the impact of the freezing mechanism on\nperformance. We also validated our findings on three other types of public\ndatasets: chest radiography, fundus radiography, and dermoscopy. Our findings\nindicate that in our clinical dataset, DINOv2's performance was not as strong\nas ImageNet-based pre-trained models, whereas in public datasets, DINOv2\ngenerally outperformed other models, especially when using the frozen\nmechanism. Similar performance was observed with various sizes of DINOv2 models\nacross different tasks. In summary, DINOv2 is viable for medical image\nclassification tasks, particularly with data resembling natural images.\nHowever, its effectiveness may vary with data that significantly differs from\nnatural images such as MRI. In addition, employing smaller versions of the\nmodel can be adequate for medical task, offering resource-saving benefits. Our\ncodes are available at https://github.com/GuanghuiFU/medical_DINOv2_eval.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07595v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07595v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07595v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07594v1",
    "updated": "2024-02-12T11:48:54+00:00",
    "published": "2024-02-12T11:48:54+00:00",
    "title": "Foundational Inference Models for Dynamical Systems",
    "authors": [
      {
        "name": "Patrick Seifner"
      },
      {
        "name": "Kostadin Cvejoski"
      },
      {
        "name": "Ramses J. Sanchez"
      }
    ],
    "summary": "Ordinary differential equations (ODEs) underlie dynamical systems which serve\nas models for a vast number of natural and social phenomena. Yet inferring the\nODE that best describes a set of noisy observations on one such phenomenon can\nbe remarkably challenging, and the models available to achieve it tend to be\nhighly specialized and complex too. In this work we propose a novel supervised\nlearning framework for zero-shot inference of ODEs from noisy data. We first\ngenerate large datasets of one-dimensional ODEs, by sampling distributions over\nthe space of initial conditions, and the space of vector fields defining them.\nWe then learn neural maps between noisy observations on the solutions of these\nequations, and their corresponding initial condition and vector fields. The\nresulting models, which we call foundational inference models (FIM), can be (i)\ncopied and matched along the time dimension to increase their resolution; and\n(ii) copied and composed to build inference models of any dimensionality,\nwithout the need of any finetuning. We use FIM to model both ground-truth\ndynamical systems of different dimensionalities and empirical time series data\nin a zero-shot fashion, and outperform state-of-the-art models which are\nfinetuned to these systems. Our (pretrained) FIMs are available online",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.DS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07594v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07594v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07594v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07588v1",
    "updated": "2024-02-12T11:41:42+00:00",
    "published": "2024-02-12T11:41:42+00:00",
    "title": "Rethinking Scaling Laws for Learning in Strategic Environments",
    "authors": [
      {
        "name": "Tinashe Handina"
      },
      {
        "name": "Eric Mazumdar"
      }
    ],
    "summary": "The deployment of ever-larger machine learning models reflects a growing\nconsensus that the more expressive the model$\\unicode{x2013}$and the more data\none has access to$\\unicode{x2013}$the more one can improve performance. As\nmodels get deployed in a variety of real world scenarios, they inevitably face\nstrategic environments. In this work, we consider the natural question of how\nthe interplay of models and strategic interactions affects scaling laws. We\nfind that strategic interactions can break the conventional view of scaling\nlaws$\\unicode{x2013}$meaning that performance does not necessarily\nmonotonically improve as models get larger and/ or more expressive (even with\ninfinite data). We show the implications of this phenomenon in several contexts\nincluding strategic regression, strategic classification, and multi-agent\nreinforcement learning through examples of strategic environments in\nwhich$\\unicode{x2013}$by simply restricting the expressivity of one's model or\npolicy class$\\unicode{x2013}$one can achieve strictly better equilibrium\noutcomes. Motivated by these examples, we then propose a new paradigm for\nmodel-selection in games wherein an agent seeks to choose amongst different\nmodel classes to use as their action set in a game.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07588v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07588v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07588v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07586v1",
    "updated": "2024-02-12T11:35:25+00:00",
    "published": "2024-02-12T11:35:25+00:00",
    "title": "Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning",
    "authors": [
      {
        "name": "Teresa Salazar"
      },
      {
        "name": "Jo\u00e3o Gama"
      },
      {
        "name": "Helder Ara\u00fajo"
      },
      {
        "name": "Pedro Henriques Abreu"
      }
    ],
    "summary": "In the evolving field of machine learning, ensuring fairness has become a\ncritical concern, prompting the development of algorithms designed to mitigate\ndiscriminatory outcomes in decision-making processes. However, achieving\nfairness in the presence of group-specific concept drift remains an unexplored\nfrontier, and our research represents pioneering efforts in this regard.\nGroup-specific concept drift refers to situations where one group experiences\nconcept drift over time while another does not, leading to a decrease in\nfairness even if accuracy remains fairly stable. Within the framework of\nfederated learning, where clients collaboratively train models, its distributed\nnature further amplifies these challenges since each client can experience\ngroup-specific concept drift independently while still sharing the same\nunderlying concept, creating a complex and dynamic environment for maintaining\nfairness. One of the significant contributions of our research is the\nformalization and introduction of the problem of group-specific concept drift\nand its distributed counterpart, shedding light on its critical importance in\nthe realm of fairness. In addition, leveraging insights from prior research, we\nadapt an existing distributed concept drift adaptation algorithm to tackle\ngroup-specific distributed concept drift which utilizes a multi-model approach,\na local group-specific drift detection mechanism, and continuous clustering of\nmodels over time. The findings from our experiments highlight the importance of\naddressing group-specific concept drift and its distributed counterpart to\nadvance fairness in machine learning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "68T01",
      "I.2.m"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07586v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07586v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07586v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07585v1",
    "updated": "2024-02-12T11:35:04+00:00",
    "published": "2024-02-12T11:35:04+00:00",
    "title": "Identifying architectural design decisions for achieving green ML serving",
    "authors": [
      {
        "name": "Francisco Dur\u00e1n"
      },
      {
        "name": "Silverio Mart\u00ednez-Fern\u00e1ndez"
      },
      {
        "name": "Matias Martinez"
      },
      {
        "name": "Patricia Lago"
      }
    ],
    "summary": "The growing use of large machine learning models highlights concerns about\ntheir increasing computational demands. While the energy consumption of their\ntraining phase has received attention, fewer works have considered the\ninference phase. For ML inference, the binding of ML models to the ML system\nfor user access, known as ML serving, is a critical yet understudied step for\nachieving efficiency in ML applications.\n  We examine the literature in ML architectural design decisions and Green AI,\nwith a special focus on ML serving. The aim is to analyze ML serving\narchitectural design decisions for the purpose of understanding and identifying\nthem with respect to quality characteristics from the point of view of\nresearchers and practitioners in the context of ML serving literature.\n  Our results (i) identify ML serving architectural design decisions along with\ntheir corresponding components and associated technological stack, and (ii)\nprovide an overview of the quality characteristics studied in the literature,\nincluding energy efficiency.\n  This preliminary study is the first step in our goal to achieve green ML\nserving. Our analysis may aid ML researchers and practitioners in making\ngreen-aware architecture design decisions when serving their models.",
    "comment": "Accepted for publication as short paper in Conference on AI\n  Engineering Software Engineering for AI (CAIN 2024)",
    "journal_ref": null,
    "doi": "10.1145/3644815.3644962",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3644815.3644962",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07585v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07585v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07585v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07570v1",
    "updated": "2024-02-12T11:04:14+00:00",
    "published": "2024-02-12T11:04:14+00:00",
    "title": "Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction",
    "authors": [
      {
        "name": "Cheng Feng"
      },
      {
        "name": "Long Huang"
      },
      {
        "name": "Denis Krompass"
      }
    ],
    "summary": "We present General Time Transformer (GTT), an encoder-only style foundation\nmodel for zero-shot multivariate time series forecasting. GTT is pretrained on\na large dataset of 200M high-quality time series samples spanning diverse\ndomains. In our proposed framework, the task of multivariate time series\nforecasting is formulated as a channel-wise next curve shape prediction\nproblem, where each time series sample is represented as a sequence of\nnon-overlapping curve shapes with a unified numerical magnitude. GTT is trained\nto predict the next curve shape based on a window of past curve shapes in a\nchannel-wise manner. Experimental results demonstrate that GTT exhibits\nsuperior zero-shot multivariate forecasting capabilities on unseen time series\ndatasets, even surpassing state-of-the-art supervised baselines. Additionally,\nwe investigate the impact of varying GTT model parameters and training dataset\nscales, observing that the scaling law also holds in the context of zero-shot\nmultivariate time series forecasting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07570v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07570v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07570v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07568v1",
    "updated": "2024-02-12T11:03:52+00:00",
    "published": "2024-02-12T11:03:52+00:00",
    "title": "Weisfeiler-Leman at the margin: When more expressivity matters",
    "authors": [
      {
        "name": "Billy J. Franks"
      },
      {
        "name": "Christopher Morris"
      },
      {
        "name": "Ameya Velingker"
      },
      {
        "name": "Floris Geerts"
      }
    ],
    "summary": "The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the\ngraph isomorphism problem. Recently, the algorithm has played a prominent role\nin understanding the expressive power of message-passing graph neural networks\n(MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL\nfaces challenges in distinguishing non-isomorphic graphs, leading to the\ndevelopment of more expressive MPNN and kernel architectures. However, the\nrelationship between enhanced expressivity and improved generalization\nperformance remains unclear. Here, we show that an architecture's expressivity\noffers limited insights into its generalization performance when viewed through\ngraph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with\nsubgraph information and employ classical margin theory to investigate the\nconditions under which an architecture's increased expressivity aligns with\nimproved generalization performance. In addition, we show that gradient flow\npushes the MPNN's weights toward the maximum margin solution. Further, we\nintroduce variations of expressive $1$-WL-based kernel and MPNN architectures\nwith provable generalization properties. Our empirical study confirms the\nvalidity of our theoretical findings.",
    "comment": "arXiv admin note: text overlap with arXiv:2301.11039",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DM",
      "cs.NE",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07568v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07568v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07568v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07963v1",
    "updated": "2024-02-12T10:32:47+00:00",
    "published": "2024-02-12T10:32:47+00:00",
    "title": "SMX: Sequential Monte Carlo Planning for Expert Iteration",
    "authors": [
      {
        "name": "Matthew V Macfarlane"
      },
      {
        "name": "Edan Toledo"
      },
      {
        "name": "Donal Byrne"
      },
      {
        "name": "Siddarth Singh"
      },
      {
        "name": "Paul Duckworth"
      },
      {
        "name": "Alexandre Laterre"
      }
    ],
    "summary": "Developing agents that can leverage planning abilities during their decision\nand learning processes is critical to the advancement of Artificial\nIntelligence. Recent works have demonstrated the effectiveness of combining\ntree-based search methods and self-play learning mechanisms. Yet, these methods\ntypically face scaling challenges due to the sequential nature of their search.\nWhile practical engineering solutions can partly overcome this, they still\ndemand extensive computational resources, which hinders their applicability. In\nthis paper, we introduce SMX, a model-based planning algorithm that utilises\nscalable Sequential Monte Carlo methods to create an effective self-learning\nmechanism. Grounded in the theoretical framework of control as inference, SMX\nbenefits from robust theoretical underpinnings. Its sampling-based search\napproach makes it adaptable to environments with both discrete and continuous\naction spaces. Furthermore, SMX allows for high parallelisation and can run on\nhardware accelerators to optimise computing efficiency. SMX demonstrates a\nstatistically significant improvement in performance compared to AlphaZero, as\nwell as demonstrating its performance as an improvement operator for a\nmodel-free policy, matching or exceeding top model-free methods across both\ncontinuous and discrete environments.",
    "comment": "25 pages, 5 main figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07963v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07963v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07963v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07549v1",
    "updated": "2024-02-12T10:30:45+00:00",
    "published": "2024-02-12T10:30:45+00:00",
    "title": "A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing",
    "authors": [
      {
        "name": "Elena Ferro"
      },
      {
        "name": "Athanasios Vasilopoulos"
      },
      {
        "name": "Corey Lammie"
      },
      {
        "name": "Manuel Le Gallo"
      },
      {
        "name": "Luca Benini"
      },
      {
        "name": "Irem Boybat"
      },
      {
        "name": "Abu Sebastian"
      }
    ],
    "summary": "Analog In-Memory Computing (AIMC) is an emerging technology for fast and\nenergy-efficient Deep Learning (DL) inference. However, a certain amount of\ndigital post-processing is required to deal with circuit mismatches and\nnon-idealities associated with the memory devices. Efficient near-memory\ndigital logic is critical to retain the high area/energy efficiency and low\nlatency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic\nwith limited parallelization capability and high latency. To overcome these\nlimitations, we propose a Near-Memory digital Processing Unit (NMPU) based on\nfixed-point arithmetic. It achieves competitive accuracy and higher computing\nthroughput than previous approaches while minimizing the area overhead.\nMoreover, the NMPU supports standard DL activation steps, such as ReLU and\nBatch Normalization. We perform a physical implementation of the NMPU design in\na 14 nm CMOS technology and provide detailed performance, power, and area\nassessments. We validate the efficacy of the NMPU by using data from an AIMC\nchip and demonstrate that a simulated AIMC system with the proposed NMPU\noutperforms existing FP16-based implementations, providing 139$\\times$\nspeed-up, 7.8$\\times$ smaller area, and a competitive power consumption.\nAdditionally, our approach achieves an inference accuracy of 86.65 %/65.06 %,\nwith an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when\nbenchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100\ndatasets, respectively.",
    "comment": "Accepted at ISCAS2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AR",
    "categories": [
      "cs.AR",
      "cs.ET",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07549v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07549v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07549v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07547v1",
    "updated": "2024-02-12T10:19:17+00:00",
    "published": "2024-02-12T10:19:17+00:00",
    "title": "Ensuring trustworthy and ethical behaviour in intelligent logical agents",
    "authors": [
      {
        "name": "Stefania Costantini"
      }
    ],
    "summary": "Autonomous Intelligent Agents are employed in many applications upon which\nthe life and welfare of living beings and vital social functions may depend.\nTherefore, agents should be trustworthy. A priori certification techniques\n(i.e., techniques applied prior to system's deployment) can be useful, but are\nnot sufficient for agents that evolve, and thus modify their epistemic and\nbelief state, and for open Multi-Agent Systems, where heterogeneous agents can\njoin or leave the system at any stage of its operation. In this paper, we\npropose/refine/extend dynamic (runtime) logic-based self-checking techniques,\ndevised in order to be able to ensure agents' trustworthy and ethical\nbehaviour.",
    "comment": null,
    "journal_ref": "Journal of Logic and Computation, Volume 32, Issue 2, March 2022,\n  Pages 443-478",
    "doi": "10.1093/logcom/exab091",
    "primary_category": "cs.MA",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LO",
      "cs.SC",
      "I.2.4"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1093/logcom/exab091",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07547v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07547v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07547v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07545v1",
    "updated": "2024-02-12T10:16:05+00:00",
    "published": "2024-02-12T10:16:05+00:00",
    "title": "TransAxx: Efficient Transformers with Approximate Computing",
    "authors": [
      {
        "name": "Dimitrios Danopoulos"
      },
      {
        "name": "Georgios Zervakis"
      },
      {
        "name": "Dimitrios Soudris"
      },
      {
        "name": "J\u00f6rg Henkel"
      }
    ],
    "summary": "Vision Transformer (ViT) models which were recently introduced by the\ntransformer architecture have shown to be very competitive and often become a\npopular alternative to Convolutional Neural Networks (CNNs). However, the high\ncomputational requirements of these models limit their practical applicability\nespecially on low-power devices. Current state-of-the-art employs approximate\nmultipliers to address the highly increased compute demands of DNN accelerators\nbut no prior research has explored their use on ViT models. In this work we\npropose TransAxx, a framework based on the popular PyTorch library that enables\nfast inherent support for approximate arithmetic to seamlessly evaluate the\nimpact of approximate computing on DNNs such as ViT models. Using TransAxx we\nanalyze the sensitivity of transformer models on the ImageNet dataset to\napproximate multiplications and perform approximate-aware finetuning to regain\naccuracy. Furthermore, we propose a methodology to generate approximate\naccelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS)\nalgorithm to efficiently search the space of possible configurations using a\nhardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy\nof our methodology in achieving significant trade-offs between accuracy and\npower, resulting in substantial gains without compromising on performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07545v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07545v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07545v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07543v1",
    "updated": "2024-02-12T10:11:50+00:00",
    "published": "2024-02-12T10:11:50+00:00",
    "title": "Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models",
    "authors": [
      {
        "name": "Mohamad Ballout"
      },
      {
        "name": "Ulf Krumnack"
      },
      {
        "name": "Gunther Heidemann"
      },
      {
        "name": "Kai-Uwe Kuehnberger"
      }
    ],
    "summary": "Our research demonstrates the significant benefits of using fine-tuning with\nexplanations to enhance the performance of language models. Unlike prompting,\nwhich maintains the model's parameters, fine-tuning allows the model to learn\nand update its parameters during a training phase. In this study, we applied\nfine-tuning to various sized language models using data that contained\nexplanations of the output rather than merely presenting the answers. We found\nthat even smaller language models with as few as 60 million parameters\nbenefited substantially from this approach. Interestingly, our results\nindicated that the detailed explanations were more beneficial to smaller models\nthan larger ones, with the latter gaining nearly the same advantage from any\nform of explanation, irrespective of its length. Additionally, we demonstrate\nthat the inclusion of explanations enables the models to solve tasks that they\nwere not able to solve without explanations. Lastly, we argue that despite the\nchallenging nature of adding explanations, samples that contain explanations\nnot only reduce the volume of data required for training but also promote a\nmore effective generalization by the model. In essence, our findings suggest\nthat fine-tuning with explanations significantly bolsters the performance of\nlarge language models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07543v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07543v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07543v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07540v1",
    "updated": "2024-02-12T10:09:16+00:00",
    "published": "2024-02-12T10:09:16+00:00",
    "title": "PKG API: A Tool for Personal Knowledge Graph Management",
    "authors": [
      {
        "name": "Nolwenn Bernard"
      },
      {
        "name": "Ivica Kostric"
      },
      {
        "name": "Weronika \u0141ajewska"
      },
      {
        "name": "Krisztian Balog"
      },
      {
        "name": "Petra Galu\u0161\u010d\u00e1kov\u00e1"
      },
      {
        "name": "Vinay Setty"
      },
      {
        "name": "Martin G. Skj\u00e6veland"
      }
    ],
    "summary": "Personal knowledge graphs (PKGs) offer individuals a way to store and\nconsolidate their fragmented personal data in a central place, improving\nservice personalization while maintaining full user control. Despite their\npotential, practical PKG implementations with user-friendly interfaces remain\nscarce. This work addresses this gap by proposing a complete solution to\nrepresent, manage, and interface with PKGs. Our approach includes (1) a\nuser-facing PKG Client, enabling end-users to administer their personal data\neasily via natural language statements, and (2) a service-oriented PKG API. To\ntackle the complexity of representing these statements within a PKG, we present\nan RDF-based PKG vocabulary that supports this, along with properties for\naccess rights and provenance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07540v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07540v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07540v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07536v1",
    "updated": "2024-02-12T10:04:07+00:00",
    "published": "2024-02-12T10:04:07+00:00",
    "title": "BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection",
    "authors": [
      {
        "name": "Kang Zhang"
      },
      {
        "name": "Osamu Yoshie"
      },
      {
        "name": "Weiran Huang"
      }
    ],
    "summary": "Trading range breakout (TRB) is a key method in the technical analysis of\nfinancial trading, widely employed by traders in financial markets such as\nstocks, futures, and foreign exchange. However, distinguishing between true and\nfalse breakout and providing the correct rationale cause significant challenges\nto investors. Recently, large language models have achieved success in various\ndownstream applications, but their effectiveness in the domain of financial\nbreakout detection has been subpar. The reason is that the unique data and\nspecific knowledge are required in breakout detection. To address these issues,\nwe introduce BreakGPT, the first large language model for financial breakout\ndetection. Furthermore, we have developed a novel framework for large language\nmodels, namely multi-stage structure, effectively reducing mistakes in\ndownstream applications. Experimental results indicate that compared to\nGPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with\nthe multi-stage structure contributing 17.6% to the improvement. Additionally,\nit outperforms ChatGPT-4 by 42.07%. Our Code is publicly available:\nhttps://github.com/Neviim96/BreakGPT",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07536v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07536v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07536v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07529v1",
    "updated": "2024-02-12T09:57:47+00:00",
    "published": "2024-02-12T09:57:47+00:00",
    "title": "Accelerating Distributed Deep Learning using Lossless Homomorphic Compression",
    "authors": [
      {
        "name": "Haoyu Li"
      },
      {
        "name": "Yuchen Xu"
      },
      {
        "name": "Jiayi Chen"
      },
      {
        "name": "Rohit Dwivedula"
      },
      {
        "name": "Wenfei Wu"
      },
      {
        "name": "Keqiang He"
      },
      {
        "name": "Aditya Akella"
      },
      {
        "name": "Daehyeok Kim"
      }
    ],
    "summary": "As deep neural networks (DNNs) grow in complexity and size, the resultant\nincrease in communication overhead during distributed training has become a\nsignificant bottleneck, challenging the scalability of distributed training\nsystems. Existing solutions, while aiming to mitigate this bottleneck through\nworker-level compression and in-network aggregation, fall short due to their\ninability to efficiently reconcile the trade-offs between compression\neffectiveness and computational overhead, hindering overall performance and\nscalability. In this paper, we introduce a novel compression algorithm that\neffectively merges worker-level compression with in-network aggregation. Our\nsolution is both homomorphic, allowing for efficient in-network aggregation\nwithout CPU/GPU processing, and lossless, ensuring no compromise on training\naccuracy. Theoretically optimal in compression and computational efficiency,\nour approach is empirically validated across diverse DNN models such as NCF,\nLSTM, VGG19, and BERT-base, showing up to a 6.33$\\times$ improvement in\naggregation throughput and a 3.74$\\times$ increase in per-iteration training\nspeed.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DC",
    "categories": [
      "cs.DC",
      "cs.DS",
      "cs.LG",
      "cs.NI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07529v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07529v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07529v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07514v1",
    "updated": "2024-02-12T09:38:42+00:00",
    "published": "2024-02-12T09:38:42+00:00",
    "title": "Physics-informed machine learning as a kernel method",
    "authors": [
      {
        "name": "Nathan Doum\u00e8che"
      },
      {
        "name": "Francis Bach"
      },
      {
        "name": "Claire Boyer"
      },
      {
        "name": "G\u00e9rard Biau"
      }
    ],
    "summary": "Physics-informed machine learning combines the expressiveness of data-based\napproaches with the interpretability of physical models. In this context, we\nconsider a general regression problem where the empirical risk is regularized\nby a partial differential equation that quantifies the physical inconsistency.\nWe prove that for linear differential priors, the problem can be formulated as\na kernel regression task. Taking advantage of kernel theory, we derive\nconvergence rates for the minimizer of the regularized risk and show that it\nconverges at least at the Sobolev minimax rate. However, faster rates can be\nachieved, depending on the physical error. This principle is illustrated with a\none-dimensional example, supporting the claim that regularizing the empirical\nrisk with physical information can be beneficial to the statistical performance\nof estimators.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07514v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07514v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07514v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07513v1",
    "updated": "2024-02-12T09:35:13+00:00",
    "published": "2024-02-12T09:35:13+00:00",
    "title": "The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese",
    "authors": [
      {
        "name": "Ajinkya Kulkarni"
      },
      {
        "name": "Anna Tokareva"
      },
      {
        "name": "Rameez Qureshi"
      },
      {
        "name": "Miguel Couceiro"
      }
    ],
    "summary": "In the field of spoken language understanding, systems like Whisper and\nMultilingual Massive Speech (MMS) have shown state-of-the-art performances.\nThis study is dedicated to a comprehensive exploration of the Whisper and MMS\nsystems, with a focus on assessing biases in automatic speech recognition (ASR)\ninherent to casual conversation speech specific to the Portuguese language. Our\ninvestigation encompasses various categories, including gender, age, skin tone\ncolor, and geo-location. Alongside traditional ASR evaluation metrics such as\nWord Error Rate (WER), we have incorporated p-value statistical significance\nfor gender bias analysis. Furthermore, we extensively examine the impact of\ndata distribution and empirically show that oversampling techniques alleviate\nsuch stereotypical biases. This research represents a pioneering effort in\nquantifying biases in the Portuguese language context through the application\nof MMS and Whisper, contributing to a better understanding of ASR systems'\nperformance in multilingual settings.",
    "comment": "EACL-2024 LT-EDI Workshop",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07513v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07513v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07513v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07510v1",
    "updated": "2024-02-12T09:31:21+00:00",
    "published": "2024-02-12T09:31:21+00:00",
    "title": "Secret Collusion Among Generative AI Agents",
    "authors": [
      {
        "name": "Sumeet Ramesh Motwani"
      },
      {
        "name": "Mikhail Baranchuk"
      },
      {
        "name": "Martin Strohmeier"
      },
      {
        "name": "Vijay Bolina"
      },
      {
        "name": "Philip H. S. Torr"
      },
      {
        "name": "Lewis Hammond"
      },
      {
        "name": "Christian Schroeder de Witt"
      }
    ],
    "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which teams of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both the AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07510v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07510v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07510v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07507v1",
    "updated": "2024-02-12T09:28:16+00:00",
    "published": "2024-02-12T09:28:16+00:00",
    "title": "Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations",
    "authors": [
      {
        "name": "Sarah Almeida Carneiro"
      },
      {
        "name": "Giovanni Chierchia"
      },
      {
        "name": "Aurelie Pirayre"
      },
      {
        "name": "Laurent Najman"
      }
    ],
    "summary": "A persistent challenge in the field of Intelligent Transportation Systems is\nto extract accurate traffic insights from geographic regions with scarce or no\ndata coverage. To this end, we propose solutions for speed prediction using\nsparse GPS data points and their associated topographical and road design\nfeatures. Our goal is to investigate whether we can use similarities in the\nterrain and infrastructure to train a machine learning model that can predict\nspeed in regions where we lack transportation data. For this we create a\nTemporally Orientated Speed Dictionary Centered on Topographically Clustered\nRoads, which helps us to provide speed correlations to selected feature\nconfigurations. Our results show qualitative and quantitative improvement over\nnew and standard regression methods. The presented framework provides a fresh\nperspective on devising strategies for missing data traffic analysis.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07507v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07507v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07507v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07506v1",
    "updated": "2024-02-12T09:24:34+00:00",
    "published": "2024-02-12T09:24:34+00:00",
    "title": "NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness",
    "authors": [
      {
        "name": "Xabier Echeberria-Barrio"
      },
      {
        "name": "Mikel Gorricho"
      },
      {
        "name": "Selene Valencia"
      },
      {
        "name": "Francesco Zola"
      }
    ],
    "summary": "The usage of Artificial Intelligence (AI) systems has increased\nexponentially, thanks to their ability to reduce the amount of data to be\nanalyzed, the user efforts and preserving a high rate of accuracy. However,\nintroducing this new element in the loop has converted them into attacked\npoints that can compromise the reliability of the systems. This new scenario\nhas raised crucial challenges regarding the reliability and trustworthiness of\nthe AI models, as well as about the uncertainties in their response decisions,\nbecoming even more crucial when applied in critical domains such as healthcare,\nchemical, electrical plants, etc. To contain these issues, in this paper, we\npresent NeuralSentinel (NS), a tool able to validate the reliability and\ntrustworthiness of AI models. This tool combines attack and defence strategies\nand explainability concepts to stress an AI model and help non-expert staff\nincrease their confidence in this new system by understanding the model\ndecisions. NS provide a simple and easy-to-use interface for helping humans in\nthe loop dealing with all the needed information. This tool was deployed and\nused in a Hackathon event to evaluate the reliability of a skin cancer image\ndetector. During the event, experts and non-experts attacked and defended the\ndetector, learning which factors were the most important for model\nmisclassification and which techniques were the most efficient. The event was\nalso used to detect NS's limitations and gather feedback for further\nimprovements.",
    "comment": null,
    "journal_ref": "CS and IT Conference Proceedings, CS and IT Conference\n  Proceedings, 2024",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07506v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07506v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07506v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07502v1",
    "updated": "2024-02-12T09:10:24+00:00",
    "published": "2024-02-12T09:10:24+00:00",
    "title": "ClusterTabNet: Supervised clustering method for table detection and table structure recognition",
    "authors": [
      {
        "name": "Marek Polewczyk"
      },
      {
        "name": "Marco Spinaci"
      }
    ],
    "summary": "We present a novel deep-learning-based method to cluster words in documents\nwhich we apply to detect and recognize tables given the OCR output. We\ninterpret table structure bottom-up as a graph of relations between pairs of\nwords (belonging to the same row, column, header, as well as to the same table)\nand use a transformer encoder model to predict its adjacency matrix. We\ndemonstrate the performance of our method on the PubTables-1M dataset as well\nas PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art\ndetection methods such as DETR and Faster R-CNN, our method achieves similar or\nbetter accuracy, while requiring a significantly smaller model.",
    "comment": "15 pages, 4 figures, submitted. The code will be released at\n  https://github.com/SAP-samples",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07502v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07502v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07502v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07501v1",
    "updated": "2024-02-12T09:10:09+00:00",
    "published": "2024-02-12T09:10:09+00:00",
    "title": "One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning",
    "authors": [
      {
        "name": "Haozhen Zhang"
      },
      {
        "name": "Xi Xiao"
      },
      {
        "name": "Le Yu"
      },
      {
        "name": "Qing Li"
      },
      {
        "name": "Zhen Ling"
      },
      {
        "name": "Ye Zhang"
      }
    ],
    "summary": "As network security receives widespread attention, encrypted traffic\nclassification has become the current research focus. However, existing methods\nconduct traffic classification without sufficiently considering the common\ncharacteristics between data samples, leading to suboptimal performance.\nMoreover, they train the packet-level and flow-level classification tasks\nindependently, which is redundant because the packet representations learned in\nthe packet-level task can be exploited by the flow-level task. Therefore, in\nthis paper, we propose an effective model named a Contrastive Learning Enhanced\nTemporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised\ncontrastive learning to enhance the packet-level and flow-level representations\nand perform graph data augmentation on the byte-level traffic graph so that the\nfine-grained semantic-invariant characteristics between bytes can be captured\nthrough contrastive learning. We also propose cross-level multi-task learning,\nwhich simultaneously accomplishes the packet-level and flow-level\nclassification tasks in the same model with one training. Further experiments\nshow that CLE-TFE achieves the best overall performance on the two tasks, while\nits computational overhead (i.e., floating point operations, FLOPs) is only\nabout 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at\nhttps://github.com/ViktorAxelsen/CLE-TFE",
    "comment": "The code is available at https://github.com/ViktorAxelsen/CLE-TFE",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07501v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07501v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07501v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07498v1",
    "updated": "2024-02-12T09:07:54+00:00",
    "published": "2024-02-12T09:07:54+00:00",
    "title": "Accelerated Smoothing: A Scalable Approach to Randomized Smoothing",
    "authors": [
      {
        "name": "Devansh Bhardwaj"
      },
      {
        "name": "Kshitiz Kaushik"
      },
      {
        "name": "Sarthak Gupta"
      }
    ],
    "summary": "Randomized smoothing has emerged as a potent certifiable defense against\nadversarial attacks by employing smoothing noises from specific distributions\nto ensure the robustness of a smoothed classifier. However, the utilization of\nMonte Carlo sampling in this process introduces a compute-intensive element,\nwhich constrains the practicality of randomized smoothing on a larger scale. To\naddress this limitation, we propose a novel approach that replaces Monte Carlo\nsampling with the training of a surrogate neural network. Through extensive\nexperimentation in various settings, we demonstrate the efficacy of our\napproach in approximating the smoothed classifier with remarkable precision.\nFurthermore, we demonstrate that our approach significantly accelerates the\nrobust radius certification process, providing nearly $600$X improvement in\ncomputation time, overcoming the computational bottlenecks associated with\ntraditional randomized smoothing.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07498v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07498v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07498v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07496v1",
    "updated": "2024-02-12T09:05:01+00:00",
    "published": "2024-02-12T09:05:01+00:00",
    "title": "Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment",
    "authors": [
      {
        "name": "Xabier Echeberria-Barrio"
      },
      {
        "name": "Amaia Gil-Lerchundi"
      },
      {
        "name": "Jon Egana-Zubia"
      },
      {
        "name": "Raul Orduna-Urrutia"
      }
    ],
    "summary": "In recent years, Deep Neural Network models have been developed in different\nfields, where they have brought many advances. However, they have also started\nto be used in tasks where risk is critical. A misdiagnosis of these models can\nlead to serious accidents or even death. This concern has led to an interest\namong researchers to study possible attacks on these models, discovering a long\nlist of vulnerabilities, from which every model should be defended. The\nadversarial example attack is a widely known attack among researchers, who have\ndeveloped several defenses to avoid such a threat. However, these defenses are\nas opaque as a deep neural network model, how they work is still unknown. This\nis why visualizing how they change the behavior of the target model is\ninteresting in order to understand more precisely how the performance of the\ndefended model is being modified. For this work, some defenses, against\nadversarial example attack, have been selected in order to visualize the\nbehavior modification of each of them in the defended model. Adversarial\ntraining, dimensionality reduction and prediction similarity were the selected\ndefenses, which have been developed using a model composed by convolution\nneural network layers and dense neural network layers. In each defense, the\nbehavior of the original model has been compared with the behavior of the\ndefended model, representing the target model by a graph in a visualization.",
    "comment": null,
    "journal_ref": "Neural Comput and Applic 34, 20477 to 20490, 2022",
    "doi": "10.1007/s00521-021-06812-y",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1007/s00521-021-06812-y",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07496v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07496v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07496v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07492v1",
    "updated": "2024-02-12T09:00:27+00:00",
    "published": "2024-02-12T09:00:27+00:00",
    "title": "Convolutional Neural Networks for signal detection in real LIGO data",
    "authors": [
      {
        "name": "Ond\u0159ej Zelenka"
      },
      {
        "name": "Bernd Br\u00fcgmann"
      },
      {
        "name": "Frank Ohme"
      }
    ],
    "summary": "Searching the data of gravitational-wave detectors for signals from compact\nbinary mergers is a computationally demanding task. Recently, machine learning\nalgorithms have been proposed to address current and future challenges.\nHowever, the results of these publications often differ greatly due to\ndiffering choices in the evaluation procedure. The Machine Learning\nGravitational-Wave Search Challenge was organized to resolve these issues and\nproduce a unified framework for machine-learning search evaluation. Six teams\nsubmitted contributions, four of which are based on machine learning methods\nand two are state-of-the-art production analyses. This paper describes the\nsubmission from the team TPI FSU Jena and its updated variant. We also apply\nour algorithm to real O3b data and recover the relevant events of the GWTC-3\ncatalog.",
    "comment": "11 pages, 4 figures, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "astro-ph.IM",
    "categories": [
      "astro-ph.IM",
      "cs.LG",
      "gr-qc"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07492v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07492v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07492v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07487v1",
    "updated": "2024-02-12T08:52:35+00:00",
    "published": "2024-02-12T08:52:35+00:00",
    "title": "Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial",
    "authors": [
      {
        "name": "Wenpin Tang"
      },
      {
        "name": "Hanyang Zhao"
      }
    ],
    "summary": "This is an expository article on the score-based diffusion models, with a\nparticular focus on the formulation via stochastic differential equations\n(SDE). After a gentle introduction, we discuss the two pillars in the diffusion\nmodeling -- sampling and score matching, which encompass the SDE/ODE sampling,\nscore matching efficiency, the consistency model, and reinforcement learning.\nShort proofs are given to illustrate the main idea of the stated results. The\narticle is primarily for introducing the beginners to the field, and\npractitioners may also find some analysis useful in designing new models or\nalgorithms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.HO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07487v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07487v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07487v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07483v1",
    "updated": "2024-02-12T08:45:08+00:00",
    "published": "2024-02-12T08:45:08+00:00",
    "title": "T-RAG: Lessons from the LLM Trenches",
    "authors": [
      {
        "name": "Masoomali Fatehkia"
      },
      {
        "name": "Ji Kim Lucas"
      },
      {
        "name": "Sanjay Chawla"
      }
    ],
    "summary": "Large Language Models (LLM) have shown remarkable language capabilities\nfueling attempts to integrate them into applications across a wide range of\ndomains. An important application area is question answering over private\nenterprise documents where the main considerations are data security, which\nnecessitates applications that can be deployed on-prem, limited computational\nresources and the need for a robust application that correctly responds to\nqueries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent\nframework for building LLM-based applications. While building a RAG is\nrelatively straightforward, making it robust and a reliable application\nrequires extensive customization and relatively deep knowledge of the\napplication domain. We share our experiences building and deploying an LLM\napplication for question answering over private organizational documents. Our\napplication combines the use of RAG with a finetuned open-source LLM.\nAdditionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure\nto represent entity hierarchies within the organization. This is used to\ngenerate a textual description to augment the context when responding to user\nqueries pertaining to entities within the organization's hierarchy. Our\nevaluations show that this combination performs better than a simple RAG or\nfinetuning implementation. Finally, we share some lessons learned based on our\nexperiences building an LLM application for real-world use.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07483v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07483v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07483v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07480v2",
    "updated": "2024-02-13T09:09:41+00:00",
    "published": "2024-02-12T08:39:40+00:00",
    "title": "Topological safeguard for evasion attack interpreting the neural networks' behavior",
    "authors": [
      {
        "name": "Xabier Echeberria-Barrio"
      },
      {
        "name": "Amaia Gil-Lerchundi"
      },
      {
        "name": "I\u00f1igo Mendialdua"
      },
      {
        "name": "Raul Orduna-Urrutia"
      }
    ],
    "summary": "In the last years, Deep Learning technology has been proposed in different\nfields, bringing many advances in each of them, but identifying new threats in\nthese solutions regarding cybersecurity. Those implemented models have brought\nseveral vulnerabilities associated with Deep Learning technology. Moreover,\nthose allow taking advantage of the implemented model, obtaining private\ninformation, and even modifying the model's decision-making. Therefore,\ninterest in studying those vulnerabilities/attacks and designing defenses to\navoid or fight them is gaining prominence among researchers. In particular, the\nwidely known evasion attack is being analyzed by researchers; thus, several\ndefenses to avoid such a threat can be found in the literature. Since the\npresentation of the L-BFG algorithm, this threat concerns the research\ncommunity. However, it continues developing new and ingenious countermeasures\nsince there is no perfect defense for all the known evasion algorithms. In this\nwork, a novel detector of evasion attacks is developed. It focuses on the\ninformation of the activations of the neurons given by the model when an input\nsample is injected. Moreover, it puts attention to the topology of the targeted\ndeep learning model to analyze the activations according to which neurons are\nconnecting. This approach has been decided because the literature shows that\nthe targeted model's topology contains essential information about if the\nevasion attack occurs. For this purpose, a huge data preprocessing is required\nto introduce all this information in the detector, which uses the Graph\nConvolutional Neural Network (GCN) technology. Thus, it understands the\ntopology of the target model, obtaining promising results and improving the\noutcomes presented in the literature related to similar defenses.",
    "comment": null,
    "journal_ref": "Pattern Recognition, Volume 147, 2024, 110130, ISSN 0031-3203",
    "doi": "10.1016/j.patcog.2023.110130",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1016/j.patcog.2023.110130",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07480v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07480v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07480v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07477v2",
    "updated": "2024-02-14T12:11:44+00:00",
    "published": "2024-02-12T08:32:29+00:00",
    "title": "Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm",
    "authors": [
      {
        "name": "Ali Rostami"
      },
      {
        "name": "Ramesh Jain"
      },
      {
        "name": "Amir M. Rahmani"
      }
    ],
    "summary": "State-of-the-art rule-based and classification-based food recommendation\nsystems face significant challenges in becoming practical and useful. This\ndifficulty arises primarily because most machine learning models struggle with\nproblems characterized by an almost infinite number of classes and a limited\nnumber of samples within an unbalanced dataset. Conversely, the emergence of\nLarge Language Models (LLMs) as recommendation engines offers a promising\navenue. However, a general-purpose Recommendation as Language Processing (RLP)\napproach lacks the critical components necessary for effective food\nrecommendations. To address this gap, we introduce Food Recommendation as\nLanguage Processing (F-RLP), a novel framework that offers a food-specific,\ntailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize\ntheir potential, thereby paving the way for more accurate, personalized food\nrecommendations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07477v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07477v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07477v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07472v1",
    "updated": "2024-02-12T08:17:23+00:00",
    "published": "2024-02-12T08:17:23+00:00",
    "title": "Cartesian atomic cluster expansion for machine learning interatomic potentials",
    "authors": [
      {
        "name": "Bingqing Cheng"
      }
    ],
    "summary": "Machine learning interatomic potentials are revolutionizing large-scale,\naccurate atomistic modelling in material science and chemistry. These\npotentials often use atomic cluster expansion or equivariant message passing\nwith spherical harmonics as basis functions. However, the dependence on\nClebsch-Gordan coefficients for maintaining rotational symmetry leads to\ncomputational inefficiencies and redundancies. We propose an alternative: a\nCartesian-coordinates-based atomic density expansion. This approach provides a\ncomplete description of atomic environments while maintaining interaction body\norders. Additionally, we integrate low-dimensional embeddings of various\nchemical elements and inter-atomic message passing. The resulting potential,\nnamed Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy,\nstability, and generalizability. We validate its performance in diverse\nsystems, including bulk water, small molecules, and 25-element high-entropy\nalloys.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.comp-ph",
    "categories": [
      "physics.comp-ph",
      "cs.LG",
      "physics.chem-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07472v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07472v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07472v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07471v1",
    "updated": "2024-02-12T08:16:58+00:00",
    "published": "2024-02-12T08:16:58+00:00",
    "title": "Differentially Private Decentralized Learning with Random Walks",
    "authors": [
      {
        "name": "Edwige Cyffers"
      },
      {
        "name": "Aur\u00e9lien Bellet"
      },
      {
        "name": "Jalaj Upadhyay"
      }
    ],
    "summary": "The popularity of federated learning comes from the possibility of better\nscalability and the ability for participants to keep control of their data,\nimproving data security and sovereignty. Unfortunately, sharing model updates\nalso creates a new privacy attack surface. In this work, we characterize the\nprivacy guarantees of decentralized learning with random walk algorithms, where\na model is updated by traveling from one node to another along the edges of a\ncommunication graph. Using a recent variant of differential privacy tailored to\nthe study of decentralized algorithms, namely Pairwise Network Differential\nPrivacy, we derive closed-form expressions for the privacy loss between each\npair of nodes where the impact of the communication topology is captured by\ngraph theoretic quantities. Our results further reveal that random walk\nalgorithms tends to yield better privacy guarantees than gossip algorithms for\nnodes close from each other. We supplement our theoretical results with\nempirical evaluation on synthetic and real-world graphs and datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07471v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07471v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07471v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07465v1",
    "updated": "2024-02-12T07:59:25+00:00",
    "published": "2024-02-12T07:59:25+00:00",
    "title": "Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations",
    "authors": [
      {
        "name": "Zheyuan Hu"
      },
      {
        "name": "Zhongqiang Zhang"
      },
      {
        "name": "George Em Karniadakis"
      },
      {
        "name": "Kenji Kawaguchi"
      }
    ],
    "summary": "The Fokker-Planck (FP) equation is a foundational PDE in stochastic\nprocesses. However, curse of dimensionality (CoD) poses challenge when dealing\nwith high-dimensional FP PDEs. Although Monte Carlo and vanilla\nPhysics-Informed Neural Networks (PINNs) have shown the potential to tackle\nCoD, both methods exhibit numerical errors in high dimensions when dealing with\nthe probability density function (PDF) associated with Brownian motion. The\npoint-wise PDF values tend to decrease exponentially as dimension increases,\nsurpassing the precision of numerical simulations and resulting in substantial\nerrors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast\nsampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms\nthe FP equation into a difficult HJB equation, whose error grows rapidly with\ndimension. To this end, we propose a novel approach utilizing a score-based\nsolver to fit the score function in SDEs. The score function, defined as the\ngradient of the LL, plays a fundamental role in inferring LL and PDF and\nenables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced\nSM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver\noperates in two stages: first, employing SM, SSM, or Score-PINN to acquire the\nscore; and second, solving the LL via an ODE using the obtained score.\nComparative evaluations across these methods showcase varying trade-offs. The\nproposed method is evaluated across diverse SDEs, including anisotropic OU\nprocesses, geometric Brownian, and Brownian with varying eigenspace. We also\ntest various distributions, including Gaussian, Log-normal, Laplace, and\nCauchy. The numerical results demonstrate the score-based SDE solver's\nstability, speed, and performance across different settings, solidifying its\npotential as a solution to CoD for high-dimensional FP equations.",
    "comment": "22 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.DS",
      "math.NA",
      "stat.ML",
      "14J60"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07465v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07465v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07465v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07462v2",
    "updated": "2024-02-13T05:21:40+00:00",
    "published": "2024-02-12T07:49:48+00:00",
    "title": "A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?",
    "authors": [
      {
        "name": "Nathan I. N. Henry"
      },
      {
        "name": "Mangor Pedersen"
      },
      {
        "name": "Matt Williams"
      },
      {
        "name": "Jamin L. B. Martin"
      },
      {
        "name": "Liesje Donkin"
      }
    ],
    "summary": "The value-loading problem is a significant challenge for researchers aiming\nto create artificial intelligence (AI) systems that align with human values and\npreferences. This problem requires a method to define and regulate safe and\noptimal limits of AI behaviors. In this work, we propose HALO (Hormetic\nALignment via Opponent processes), a regulatory paradigm that uses hormetic\nanalysis to regulate the behavioral patterns of AI. Behavioral hormesis is a\nphenomenon where low frequencies of a behavior have beneficial effects, while\nhigh frequencies are harmful. By modeling behaviors as allostatic opponent\nprocesses, we can use either Behavioral Frequency Response Analysis (BFRA) or\nBehavioral Count Response Analysis (BCRA) to quantify the hormetic limits of\nrepeatable behaviors. We demonstrate how HALO can solve the 'paperclip\nmaximizer' scenario, a thought experiment where an unregulated AI tasked with\nmaking paperclips could end up converting all matter in the universe into\npaperclips. Our approach may be used to help create an evolving database of\n'values' based on the hedonic calculus of repeatable behaviors with decreasing\nmarginal utility. This positions HALO as a promising solution for the\nvalue-loading problem, which involves embedding human-aligned values into an AI\nsystem, and the weak-to-strong generalization problem, which explores whether\nweak models can supervise stronger models as they become more intelligent.\nHence, HALO opens several research avenues that may lead to the development of\na computational value system that allows an AI algorithm to learn whether the\ndecisions it makes are right or wrong.",
    "comment": "24 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.MA",
      "econ.TH",
      "68T01, 68T37, 68T42",
      "I.2.0; I.2.8; I.2.11"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07462v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07462v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07462v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07458v1",
    "updated": "2024-02-12T07:37:19+00:00",
    "published": "2024-02-12T07:37:19+00:00",
    "title": "On the Distance from Calibration in Sequential Prediction",
    "authors": [
      {
        "name": "Mingda Qiao"
      },
      {
        "name": "Letian Zheng"
      }
    ],
    "summary": "We study a sequential binary prediction setting where the forecaster is\nevaluated in terms of the calibration distance, which is defined as the $L_1$\ndistance between the predicted values and the set of predictions that are\nperfectly calibrated in hindsight. This is analogous to a calibration measure\nrecently proposed by B{\\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the\noffline setting. The calibration distance is a natural and intuitive measure of\ndeviation from perfect calibration, and satisfies a Lipschitz continuity\nproperty which does not hold for many popular calibration measures, such as the\n$L_1$ calibration error and its variants.\n  We prove that there is a forecasting algorithm that achieves an $O(\\sqrt{T})$\ncalibration distance in expectation on an adversarially chosen sequence of $T$\nbinary outcomes. At the core of this upper bound is a structural result showing\nthat the calibration distance is accurately approximated by the lower\ncalibration distance, which is a continuous relaxation of the former. We then\nshow that an $O(\\sqrt{T})$ lower calibration distance can be achieved via a\nsimple minimax argument and a reduction to online learning on a Lipschitz\nclass.\n  On the lower bound side, an $\\Omega(T^{1/3})$ calibration distance is shown\nto be unavoidable, even when the adversary outputs a sequence of independent\nrandom bits, and has an additional ability to early stop (i.e., to stop\nproducing random bits and output the same bit in the remaining steps).\nInterestingly, without this early stopping, the forecaster can achieve a much\nsmaller calibration distance of $\\mathrm{polylog}(T)$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07458v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07458v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07458v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07456v1",
    "updated": "2024-02-12T07:29:22+00:00",
    "published": "2024-02-12T07:29:22+00:00",
    "title": "OS-Copilot: Towards Generalist Computer Agents with Self-Improvement",
    "authors": [
      {
        "name": "Zhiyong Wu"
      },
      {
        "name": "Chengcheng Han"
      },
      {
        "name": "Zichen Ding"
      },
      {
        "name": "Zhenmin Weng"
      },
      {
        "name": "Zhoumianze Liu"
      },
      {
        "name": "Shunyu Yao"
      },
      {
        "name": "Tao Yu"
      },
      {
        "name": "Lingpeng Kong"
      }
    ],
    "summary": "Autonomous interaction with the computer has been a longstanding challenge\nwith great potential, and the recent proliferation of large language models\n(LLMs) has markedly accelerated progress in building digital agents. However,\nmost of these agents are designed to interact with a narrow domain, such as a\nspecific software or website. This narrow focus constrains their applicability\nfor general computer tasks. To this end, we introduce OS-Copilot, a framework\nto build generalist agents capable of interfacing with comprehensive elements\nin an operating system (OS), including the web, code terminals, files,\nmultimedia, and various third-party applications. We use OS-Copilot to create\nFRIDAY, a self-improving embodied agent for automating general computer tasks.\nOn GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods\nby 35%, showcasing strong generalization to unseen applications via accumulated\nskills from previous tasks. We also present numerical and quantitative evidence\nthat FRIDAY learns to control and self-improve on Excel and Powerpoint with\nminimal supervision. Our OS-Copilot framework and empirical findings provide\ninfrastructure and insights for future research toward more capable and\ngeneral-purpose computer agents.",
    "comment": "Project page: https://os-copilot.github.io",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07456v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07456v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07456v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07453v1",
    "updated": "2024-02-12T07:20:05+00:00",
    "published": "2024-02-12T07:20:05+00:00",
    "title": "Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs",
    "authors": [
      {
        "name": "Yuval Filmus"
      },
      {
        "name": "Steve Hanneke"
      },
      {
        "name": "Idan Mehalel"
      },
      {
        "name": "Shay Moran"
      }
    ],
    "summary": "Consider the domain of multiclass classification within the adversarial\nonline setting. What is the price of relying on bandit feedback as opposed to\nfull information? To what extent can an adaptive adversary amplify the loss\ncompared to an oblivious one? To what extent can a randomized learner reduce\nthe loss compared to a deterministic one? We study these questions in the\nmistake bound model and provide nearly tight answers.\n  We demonstrate that the optimal mistake bound under bandit feedback is at\nmost $O(k)$ times higher than the optimal mistake bound in the full information\ncase, where $k$ represents the number of labels. This bound is tight and\nprovides an answer to an open question previously posed and studied by Daniely\nand Helbertal ['13] and by Long ['17, '20], who focused on deterministic\nlearners.\n  Moreover, we present nearly optimal bounds of $\\tilde{\\Theta}(k)$ on the gap\nbetween randomized and deterministic learners, as well as between adaptive and\noblivious adversaries in the bandit feedback setting. This stands in contrast\nto the full information scenario, where adaptive and oblivious adversaries are\nequivalent, and the gap in mistake bounds between randomized and deterministic\nlearners is a constant multiplicative factor of $2$.\n  In addition, our results imply that in some cases the optimal randomized\nmistake bound is approximately the square-root of its deterministic parallel.\nPrevious results show that this is essentially the smallest it can get.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07453v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07453v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07453v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07452v1",
    "updated": "2024-02-12T07:19:00+00:00",
    "published": "2024-02-12T07:19:00+00:00",
    "title": "TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound",
    "authors": [
      {
        "name": "Yinyu Ye"
      },
      {
        "name": "Shijing Chen"
      },
      {
        "name": "Dong Ni"
      },
      {
        "name": "Ruobing Huang"
      }
    ],
    "summary": "Different diseases, such as histological subtypes of breast lesions, have\nseverely varying incidence rates. Even trained with substantial amount of\nin-distribution (ID) data, models often encounter out-of-distribution (OOD)\nsamples belonging to unseen classes in clinical reality. To address this, we\npropose a novel framework built upon a long-tailed OOD detection task for\nbreast ultrasound images. It is equipped with a triplet state augmentation\n(TriAug) which improves ID classification accuracy while maintaining a\npromising OOD detection performance. Meanwhile, we designed a balanced sphere\nloss to handle the class imbalanced problem.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07452v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07452v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07452v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07448v1",
    "updated": "2024-02-12T07:11:13+00:00",
    "published": "2024-02-12T07:11:13+00:00",
    "title": "AraSpider: Democratizing Arabic-to-SQL",
    "authors": [
      {
        "name": "Ahmed Heakl"
      },
      {
        "name": "Youssef Mohamed"
      },
      {
        "name": "Ahmed B. Zaky"
      }
    ],
    "summary": "This study presents AraSpider, the first Arabic version of the Spider\ndataset, aimed at improving natural language processing (NLP) in the\nArabic-speaking community. Four multilingual translation models were tested for\ntheir effectiveness in translating English to Arabic. Additionally, two models\nwere assessed for their ability to generate SQL queries from Arabic text. The\nresults showed that using back translation significantly improved the\nperformance of both ChatGPT 3.5 and SQLCoder models, which are considered top\nperformers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated\nhigh-quality translation, while SQLCoder excelled in text-to-SQL tasks. The\nstudy underscores the importance of incorporating contextual schema and\nemploying back translation strategies to enhance model performance in Arabic\nNLP tasks. Moreover, the provision of detailed methodologies for\nreproducibility and translation of the dataset into other languages highlights\nthe research's commitment to promoting transparency and collaborative knowledge\nsharing in the field. Overall, these contributions advance NLP research,\nempower Arabic-speaking researchers, and enrich the global discourse on\nlanguage comprehension and database interrogation.",
    "comment": "11 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07448v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07448v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07448v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07445v1",
    "updated": "2024-02-12T06:57:34+00:00",
    "published": "2024-02-12T06:57:34+00:00",
    "title": "Top-$K$ ranking with a monotone adversary",
    "authors": [
      {
        "name": "Yuepeng Yang"
      },
      {
        "name": "Antares Chen"
      },
      {
        "name": "Lorenzo Orecchia"
      },
      {
        "name": "Cong Ma"
      }
    ],
    "summary": "In this paper, we address the top-$K$ ranking problem with a monotone\nadversary. We consider the scenario where a comparison graph is randomly\ngenerated and the adversary is allowed to add arbitrary edges. The\nstatistician's goal is then to accurately identify the top-$K$ preferred items\nbased on pairwise comparisons derived from this semi-random comparison graph.\nThe main contribution of this paper is to develop a weighted maximum likelihood\nestimator (MLE) that achieves near-optimal sample complexity, up to a\n$\\log^2(n)$ factor, where n denotes the number of items under comparison. This\nis made possible through a combination of analytical and algorithmic\ninnovations. On the analytical front, we provide a refined $\\ell_\\infty$ error\nanalysis of the weighted MLE that is more explicit and tighter than existing\nanalyses. It relates the $\\ell_\\infty$ error with the spectral properties of\nthe weighted comparison graph. Motivated by this, our algorithmic innovation\ninvolves the development of an SDP-based approach to reweight the semi-random\ngraph and meet specified spectral properties. Additionally, we propose a\nfirst-order method based on the Matrix Multiplicative Weight Update (MMWU)\nframework. This method efficiently solves the resulting SDP in nearly-linear\ntime relative to the size of the semi-random comparison graph.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07445v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07445v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07445v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07443v1",
    "updated": "2024-02-12T06:50:45+00:00",
    "published": "2024-02-12T06:50:45+00:00",
    "title": "The I/O Complexity of Attention, or How Optimal is Flash Attention?",
    "authors": [
      {
        "name": "Barna Saha"
      },
      {
        "name": "Christopher Ye"
      }
    ],
    "summary": "Self-attention is at the heart of the popular Transformer architecture, yet\nsuffers from quadratic time and memory complexity. The breakthrough\nFlashAttention algorithm revealed I/O complexity as the true bottleneck in\nscaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g.\nGPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O\ncomplexity measures the number of accesses to memory. FlashAttention computes\nattention using $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of\nthe attention matrix, $d$ the head-dimension and $M$ the cache size. However,\nis this I/O complexity optimal? The known lower bound only rules out an I/O\ncomplexity of $o(Nd)$ when $M=\\Theta(Nd)$, since the output that needs to be\nwritten to slow memory is $\\Omega(Nd)$. This leads to the main question of our\nwork: Is FlashAttention I/O optimal for all values of $M$?\n  We resolve the above question in its full generality by showing an I/O\ncomplexity lower bound that matches the upper bound provided by FlashAttention\nfor any values of $M \\geq d^2$ within any constant factors. Further, we give a\nbetter algorithm with lower I/O complexity for $M < d^2$, and show that it is\noptimal as well. Moreover, our lower bounds do not rely on using combinatorial\nmatrix multiplication for computing the attention matrix. We show even if one\nuses fast matrix multiplication, the above I/O complexity bounds cannot be\nimproved. We do so by introducing a new communication complexity protocol for\nmatrix compression, and connecting communication complexity to I/O complexity.\nTo the best of our knowledge, this is the first work to establish a connection\nbetween communication complexity and I/O complexity, and we believe this\nconnection could be of independent interest and will find many more\napplications in proving I/O complexity lower bounds in the future.",
    "comment": "24 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CC",
      "cs.DS",
      "cs.IT",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07443v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07443v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07443v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07442v1",
    "updated": "2024-02-12T06:49:48+00:00",
    "published": "2024-02-12T06:49:48+00:00",
    "title": "Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch",
    "authors": [
      {
        "name": "Ray Ito"
      },
      {
        "name": "Junichiro Takahashi"
      }
    ],
    "summary": "Several attempts have been made to implement text command control for game\nagents. However, current technologies are limited to processing predefined\nformat commands. This paper proposes a pioneering text command control system\nfor a game agent that can understand natural language commands expressed in\nfree-form. The proposed system uses a large language model (LLM) for code\ngeneration to interpret and transform natural language commands into behavior\nbranch, a proposed knowledge expression based on behavior trees, which\nfacilitates execution by the game agent. This study conducted empirical\nvalidation within a game environment that simulates a Pok\\'emon game and\ninvolved multiple participants. The results confirmed the system's ability to\nunderstand and carry out natural language commands, representing a noteworthy\nin the realm of real-time language interactive game agents.\n  Notice for the use of this material. The copyright of this material is\nretained by the Japanese Society for Artificial Intelligence (JSAI). This\nmaterial is published here with the agreement of JSAI. Please be complied with\nCopyright Law of Japan if any users wish to reproduce, make derivative work,\ndistribute or make available to the public any part or whole thereof. All\nRights Reserved, Copyright (C) The Japanese Society for Artificial\nIntelligence.",
    "comment": "This paper is posted at JSAI 2024 Conference",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07442v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07442v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07442v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07440v2",
    "updated": "2024-02-14T04:19:16+00:00",
    "published": "2024-02-12T06:43:52+00:00",
    "title": "Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT",
    "authors": [
      {
        "name": "Jon Saad-Falcon"
      },
      {
        "name": "Daniel Y. Fu"
      },
      {
        "name": "Simran Arora"
      },
      {
        "name": "Neel Guha"
      },
      {
        "name": "Christopher R\u00e9"
      }
    ],
    "summary": "Retrieval pipelines-an integral component of many machine learning\nsystems-perform poorly in domains where documents are long (e.g., 10K tokens or\nmore) and where identifying the relevant document requires synthesizing\ninformation across the entire text. Developing long-context retrieval encoders\nsuitable for these domains raises three challenges: (1) how to evaluate\nlong-context retrieval performance, (2) how to pretrain a base language model\nto represent both short contexts (corresponding to queries) and long contexts\n(corresponding to documents), and (3) how to fine-tune this model for retrieval\nunder the batch size limitations imposed by GPU memory constraints. To address\nthese challenges, we first introduce LoCoV1, a novel 12 task benchmark\nconstructed to measure long-context retrieval where chunking is not possible or\nnot effective. We next present the M2-BERT retrieval encoder, an 80M parameter\nstate-space encoder model built from the Monarch Mixer architecture, capable of\nscaling to documents up to 32K tokens long. We describe a pretraining data\nmixture which allows this encoder to process both short and long context\nsequences, and a finetuning approach that adapts this base model to retrieval\nwith only single-sample batches. Finally, we validate the M2-BERT retrieval\nencoder on LoCoV1, finding that it outperforms competitive Transformer-based\nmodels by at least 23.3 points, despite containing upwards of 90x fewer\nparameters.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07440v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07440v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07440v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07437v1",
    "updated": "2024-02-12T06:32:53+00:00",
    "published": "2024-02-12T06:32:53+00:00",
    "title": "Learning Optimal Tax Design in Nonatomic Congestion Games",
    "authors": [
      {
        "name": "Qiwen Cui"
      },
      {
        "name": "Maryam Fazel"
      },
      {
        "name": "Simon S. Du"
      }
    ],
    "summary": "We study how to learn the optimal tax design to maximize the efficiency in\nnonatomic congestion games. It is known that self-interested behavior among the\nplayers can damage the system's efficiency. Tax mechanisms is a common method\nto alleviate this issue and induce socially optimal behavior. In this work, we\ntake the initial step for learning the optimal tax that can minimize the social\ncost with \\emph{equilibrium feedback}, i.e., the tax designer can only observe\nthe equilibrium state under the enforced tax. Existing algorithms are not\napplicable due to the exponentially large tax function space, nonexistence of\nthe gradient, and nonconvexity of the objective. To tackle these challenges,\nour algorithm leverages several novel components: (1) piece-wise linear tax to\napproximate the optimal tax; (2) an extra linear term to guarantee a strongly\nconvex potential function; (3) efficient subroutine to find the ``boundary''\ntax. The algorithm can find an $\\epsilon$-optimal tax with $O(\\beta\nF^2/\\epsilon)$ sample complexity, where $\\beta$ is the smoothness of the cost\nfunction and $F$ is the number of facilities.",
    "comment": "19 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07437v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07437v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07437v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07435v1",
    "updated": "2024-02-12T06:29:57+00:00",
    "published": "2024-02-12T06:29:57+00:00",
    "title": "Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs",
    "authors": [
      {
        "name": "Narayan Tondapu"
      }
    ],
    "summary": "In this study, we examine the fluctuation in the value of the Great Britain\nPound (GBP). We focus particularly on its relationship with the United States\nDollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15,\n2018, to June 15, 2023, we apply various mathematical models to assess their\neffectiveness in predicting the 20-day variation in the pairs' daily returns.\nOur analysis involves the implementation of Exponentially Weighted Moving\nAverage (EWMA), Generalized Autoregressive Conditional Heteroskedasticity\n(GARCH) models, and Implied Volatility (IV) models. To evaluate their\nperformance, we compare the accuracy of their predictions using Root Mean\nSquare Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the\nintricacies of GARCH models, examining their statistical characteristics when\napplied to the provided dataset. Our findings suggest the existence of\nasymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for\nthe GBP/USD pair. Additionally, we observe that GARCH-type models better fit\nthe data when assuming residuals follow a standard t-distribution rather than a\nstandard normal distribution. Furthermore, we investigate the efficacy of\ndifferent forecasting techniques within GARCH-type models. Comparing rolling\nwindow forecasts to expanding window forecasts, we find no definitive\nsuperiority in either approach across the tested scenarios. Our experiments\nreveal that for the GBP/USD pair, the most accurate volatility forecasts stem\nfrom the utilization of GARCH models employing a rolling window methodology.\nConversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH\nmodels and Ordinary Least Squares (OLS) models incorporating the annualized\nimplied volatility of the exchange rate as an independent variable.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-fin.ST",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07435v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07435v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07435v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07429v1",
    "updated": "2024-02-12T06:06:09+00:00",
    "published": "2024-02-12T06:06:09+00:00",
    "title": "Particle Filter SLAM for Vehicle Localization",
    "authors": [
      {
        "name": "Tianrui Liu"
      },
      {
        "name": "Changxin Xu"
      },
      {
        "name": "Yuxin Qiao"
      },
      {
        "name": "Chufeng Jiang"
      },
      {
        "name": "Jiqiang Yu"
      }
    ],
    "summary": "Simultaneous Localization and Mapping (SLAM) presents a formidable challenge\nin robotics, involving the dynamic construction of a map while concurrently\ndetermining the precise location of the robotic agent within an unfamiliar\nenvironment. This intricate task is further compounded by the inherent\n\"chicken-and-egg\" dilemma, where accurate mapping relies on a dependable\nestimation of the robot's location, and vice versa. Moreover, the computational\nintensity of SLAM adds an additional layer of complexity, making it a crucial\nyet demanding topic in the field. In our research, we address the challenges of\nSLAM by adopting the Particle Filter SLAM method. Our approach leverages\nencoded data and fiber optic gyro (FOG) information to enable precise\nestimation of vehicle motion, while lidar technology contributes to\nenvironmental perception by providing detailed insights into surrounding\nobstacles. The integration of these data streams culminates in the\nestablishment of a Particle Filter SLAM framework, representing a key endeavor\nin this paper to effectively navigate and overcome the complexities associated\nwith simultaneous localization and mapping in robotic systems.",
    "comment": "6 pages, Journal of Industrial Engineering and Applied Science",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07429v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07429v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07429v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07422v1",
    "updated": "2024-02-12T05:56:12+00:00",
    "published": "2024-02-12T05:56:12+00:00",
    "title": "News Recommendation with Attention Mechanism",
    "authors": [
      {
        "name": "Tianrui Liu"
      },
      {
        "name": "Changxin Xu"
      },
      {
        "name": "Yuxin Qiao"
      },
      {
        "name": "Chufeng Jiang"
      },
      {
        "name": "Weisheng Chen"
      }
    ],
    "summary": "This paper explores the area of news recommendation, a key component of\nonline information sharing. Initially, we provide a clear introduction to news\nrecommendation, defining the core problem and summarizing current methods and\nnotable recent algorithms. We then present our work on implementing the NRAM\n(News Recommendation with Attention Mechanism), an attention-based approach for\nnews recommendation, and assess its effectiveness. Our evaluation shows that\nNRAM has the potential to significantly improve how news content is\npersonalized for users on digital news platforms.",
    "comment": "7 pages, Journal of Industrial Engineering and Applied Science",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07422v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07422v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07422v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07420v2",
    "updated": "2024-02-13T07:02:05+00:00",
    "published": "2024-02-12T05:48:52+00:00",
    "title": "On the Transit Obfuscation Problem",
    "authors": [
      {
        "name": "Hideaki Takahashi"
      },
      {
        "name": "Alex Fukunaga"
      }
    ],
    "summary": "Concealing an intermediate point on a route or visible from a route is an\nimportant goal in some transportation and surveillance scenarios. This paper\nstudies the Transit Obfuscation Problem, the problem of traveling from some\nstart location to an end location while \"covering\" a specific transit point\nthat needs to be concealed from adversaries. We propose the notion of transit\nanonymity, a quantitative guarantee of the anonymity of a specific transit\npoint, even with a powerful adversary with full knowledge of the path planning\nalgorithm. We propose and evaluate planning/search algorithms that satisfy this\nanonymity criterion.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07420v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07420v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07420v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07419v1",
    "updated": "2024-02-12T05:48:31+00:00",
    "published": "2024-02-12T05:48:31+00:00",
    "title": "Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand",
    "authors": [
      {
        "name": "Md Musfiqur Rahman"
      },
      {
        "name": "Matt Jordan"
      },
      {
        "name": "Murat Kocaoglu"
      }
    ],
    "summary": "Causal inference from observational data has recently found many applications\nin machine learning. While sound and complete algorithms exist to compute\ncausal effects, many of these algorithms require explicit access to conditional\nlikelihoods over the observational distribution, which is difficult to estimate\nin the high-dimensional regime, such as with images. To alleviate this issue,\nresearchers have approached the problem by simulating causal relations with\nneural models and obtained impressive results. However, none of these existing\napproaches can be applied to generic scenarios such as causal graphs on image\ndata with latent confounders, or obtain conditional interventional samples. In\nthis paper, we show that any identifiable causal effect given an arbitrary\ncausal graph can be computed through push-forward computations of conditional\ngenerative models. Based on this result, we devise a diffusion-based approach\nto sample from any (conditional) interventional distribution on image data. To\nshowcase our algorithm's performance, we conduct experiments on a Colored MNIST\ndataset having both the treatment ($X$) and the target variables ($Y$) as\nimages and obtain interventional samples from $P(y|do(x))$. As an application\nof our algorithm, we evaluate two large conditional generative models that are\npre-trained on the CelebA dataset by analyzing the strength of spurious\ncorrelations and the level of disentanglement they achieve.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07419v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07419v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07419v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07418v1",
    "updated": "2024-02-12T05:46:10+00:00",
    "published": "2024-02-12T05:46:10+00:00",
    "title": "SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation",
    "authors": [
      {
        "name": "Sangwoo Shin"
      },
      {
        "name": "Minjong Yoo"
      },
      {
        "name": "Jeongwoo Lee"
      },
      {
        "name": "Honguk Woo"
      }
    ],
    "summary": "This work explores the zero-shot adaptation capability of semantic skills,\nsemantically interpretable experts' behavior patterns, in cross-domain\nsettings, where a user input in interleaved multi-modal snippets can prompt a\nnew long-horizon task for different domains. In these cross-domain settings, we\npresent a semantic skill translator framework SemTra which utilizes a set of\nmulti-modal models to extract skills from the snippets, and leverages the\nreasoning capabilities of a pretrained language model to adapt these extracted\nskills to the target domain. The framework employs a two-level hierarchy for\nadaptation: task adaptation and skill adaptation. During task adaptation,\nseq-to-seq translation by the language model transforms the extracted skills\ninto a semantic skill sequence, which is tailored to fit the cross-domain\ncontexts. Skill adaptation focuses on optimizing each semantic skill for the\ntarget domain context, through parametric instantiations that are facilitated\nby language prompting and contrastive learning-based context inferences. This\nhierarchical adaptation empowers the framework to not only infer a complex task\nspecification in one-shot from the interleaved multi-modal snippets, but also\nadapt it to new domains with zero-shot learning abilities. We evaluate our\nframework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The\nresults clarify the framework's superiority in performing long-horizon tasks\nand adapting to different domains, showing its broad applicability in practical\nuse cases, such as cognitive robots interpreting abstract instructions and\nautonomous vehicles operating under varied configurations.",
    "comment": "AAAI 2024 Camera-ready version",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07418v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07418v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07418v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07417v1",
    "updated": "2024-02-12T05:44:10+00:00",
    "published": "2024-02-12T05:44:10+00:00",
    "title": "An Empirical Study Into What Matters for Calibrating Vision-Language Models",
    "authors": [
      {
        "name": "Weijie Tu"
      },
      {
        "name": "Weijian Deng"
      },
      {
        "name": "Dylan Campbell"
      },
      {
        "name": "Stephen Gould"
      },
      {
        "name": "Tom Gedeon"
      }
    ],
    "summary": "Vision--Language Models (VLMs) have emerged as the dominant approach for\nzero-shot recognition, adept at handling diverse scenarios and significant\ndistribution changes. However, their deployment in risk-sensitive areas\nrequires a deeper understanding of their uncertainty estimation capabilities, a\nrelatively uncharted area. In this study, we explore the calibration properties\nof VLMs across different architectures, datasets, and training strategies. In\nparticular, we analyze the uncertainty estimation performance of VLMs when\ncalibrated in one domain, label set or hierarchy level, and tested in a\ndifferent one. Our findings reveal that while VLMs are not inherently\ncalibrated for uncertainty, temperature scaling significantly and consistently\nimproves calibration, even across shifts in distribution and changes in label\nset. Moreover, VLMs can be calibrated with a very small set of examples.\nThrough detailed experimentation, we highlight the potential applications and\nimportance of our insights, aiming for more reliable and effective use of VLMs\nin critical, real-world scenarios.",
    "comment": "12 pages, 8 figures, this version is not fully edited and will be\n  updated soon",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07417v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07417v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07417v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07415v1",
    "updated": "2024-02-12T05:38:11+00:00",
    "published": "2024-02-12T05:38:11+00:00",
    "title": "Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems",
    "authors": [
      {
        "name": "Justin Davis"
      },
      {
        "name": "Mehmet E. Belviranli"
      }
    ],
    "summary": "In recent years, deep neural networks (DNNs) have gained widespread adoption\nfor continuous mobile object detection (OD) tasks, particularly in autonomous\nsystems. However, a prevalent issue in their deployment is the\none-size-fits-all approach, where a single DNN is used, resulting in\ninefficient utilization of computational resources. This inefficiency is\nparticularly detrimental in energy-constrained systems, as it degrades overall\nsystem efficiency. We identify that, the contextual information embedded in the\ninput data stream (e.g. the frames in the camera feed that the OD models are\nrun on) could be exploited to allow a more efficient multi-model-based OD\nprocess. In this paper, we propose SHIFT which continuously selects from a\nvariety of DNN-based OD models depending on the dynamically changing contextual\ninformation and computational constraints. During this selection, SHIFT\nuniquely considers multi-accelerator execution to better optimize the\nenergy-efficiency while satisfying the latency constraints. Our proposed\nmethodology results in improvements of up to 7.5x in energy usage and 2.8x in\nlatency compared to state-of-the-art GPU-based single model OD approaches.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07415v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07415v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07415v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07412v1",
    "updated": "2024-02-12T05:13:44+00:00",
    "published": "2024-02-12T05:13:44+00:00",
    "title": "Auxiliary Reward Generation with Transition Distance Representation Learning",
    "authors": [
      {
        "name": "Siyuan Li"
      },
      {
        "name": "Shijie Han"
      },
      {
        "name": "Yingnan Zhao"
      },
      {
        "name": "By Liang"
      },
      {
        "name": "Peng Liu"
      }
    ],
    "summary": "Reinforcement learning (RL) has shown its strength in challenging sequential\ndecision-making problems. The reward function in RL is crucial to the learning\nperformance, as it serves as a measure of the task completion degree. In\nreal-world problems, the rewards are predominantly human-designed, which\nrequires laborious tuning, and is easily affected by human cognitive biases. To\nachieve automatic auxiliary reward generation, we propose a novel\nrepresentation learning approach that can measure the ``transition distance''\nbetween states. Building upon these representations, we introduce an auxiliary\nreward generation technique for both single-task and skill-chaining scenarios\nwithout the need for human knowledge. The proposed approach is evaluated in a\nwide range of manipulation tasks. The experiment results demonstrate the\neffectiveness of measuring the transition distance between states and the\ninduced improvement by auxiliary rewards, which not only promotes better\nlearning efficiency but also increases convergent stability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07412v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07412v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07412v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07411v1",
    "updated": "2024-02-12T05:12:09+00:00",
    "published": "2024-02-12T05:12:09+00:00",
    "title": "Potential-Based Reward Shaping For Intrinsic Motivation",
    "authors": [
      {
        "name": "Grant C. Forbes"
      },
      {
        "name": "Nitish Gupta"
      },
      {
        "name": "Leonardo Villalobos-Arias"
      },
      {
        "name": "Colin M. Potts"
      },
      {
        "name": "Arnav Jhala"
      },
      {
        "name": "David L. Roberts"
      }
    ],
    "summary": "Recently there has been a proliferation of intrinsic motivation (IM)\nreward-shaping methods to learn in complex and sparse-reward environments.\nThese methods can often inadvertently change the set of optimal policies in an\nenvironment, leading to suboptimal behavior. Previous work on mitigating the\nrisks of reward shaping, particularly through potential-based reward shaping\n(PBRS), has not been applicable to many IM methods, as they are often complex,\ntrainable functions themselves, and therefore dependent on a wider set of\nvariables than the traditional reward functions that PBRS was developed for. We\npresent an extension to PBRS that we prove preserves the set of optimal\npolicies under a more general set of functions than has been previously proven.\nWe also present {\\em Potential-Based Intrinsic Motivation} (PBIM), a method for\nconverting IM rewards into a potential-based form that is useable without\naltering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff\nWalking environments, we demonstrate that PBIM successfully prevents the agent\nfrom converging to a suboptimal policy and can speed up training.",
    "comment": "Extended version of paper appearing in AAMAS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07411v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07411v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07411v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07410v1",
    "updated": "2024-02-12T05:05:55+00:00",
    "published": "2024-02-12T05:05:55+00:00",
    "title": "A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)",
    "authors": [
      {
        "name": "Weijie Tu"
      },
      {
        "name": "Weijian Deng"
      },
      {
        "name": "Tom Gedeon"
      }
    ],
    "summary": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nremarkable generalization capabilities across multiple challenging distribution\nshifts. However, there is still much to be explored in terms of their\nrobustness to the variations of specific visual factors. In real-world\napplications, reliable and safe systems must consider other safety objectives\nbeyond classification accuracy, such as predictive uncertainty. Yet, the\neffectiveness of CLIP models on such safety-related features is less-explored.\nDriven by the above, this work comprehensively investigates the safety\nobjectives of CLIP models, specifically focusing on three key properties:\nresilience to visual factor variations, calibrated uncertainty estimations, and\nthe ability to detect anomalous inputs. To this end, we study 83 CLIP models\nand 127 ImageNet classifiers. They are diverse in architecture, (pre)training\ndistribution and training strategies. We consider 10 visual factors (e.g.,\nshape and pattern), 5 types of out-of-distribution data, and 8 natural and\nchallenging test conditions with different shift types, such as texture, style,\nand perturbation shifts. Our study has unveiled several previously unknown\ninsights into CLIP models. For instance, they are not consistently more\ncalibrated than other ImageNet models, which contradicts existing findings.\nAdditionally, our analysis underscores the significance of training source\ndesign by showcasing its profound influence on the three safety-related\nproperties. We believe our comprehensive study can shed light on and help guide\nthe development of more robust and reliable CLIP models.",
    "comment": "Accepted by NeurIPS 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07410v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07410v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07410v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07408v1",
    "updated": "2024-02-12T04:59:58+00:00",
    "published": "2024-02-12T04:59:58+00:00",
    "title": "Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples",
    "authors": [
      {
        "name": "Mingrui Ma"
      },
      {
        "name": "Lansheng Han"
      },
      {
        "name": "Chunjie Zhou"
      }
    ],
    "summary": "The frequent occurrence of cyber-attacks has made webshell attacks and\ndefense gradually become a research hotspot in the field of network security.\nHowever, the lack of publicly available benchmark datasets and the\nover-reliance on manually defined rules for webshell escape sample generation\nhave slowed down the progress of research related to webshell escape sample\ngeneration strategies and artificial intelligence-based webshell detection\nalgorithms. To address the drawbacks of weak webshell sample escape\ncapabilities, the lack of webshell datasets with complex malicious features,\nand to promote the development of webshell detection technology, we propose the\nHybrid Prompt algorithm for webshell escape sample generation with the help of\nlarge language models. As a prompt algorithm specifically developed for\nwebshell sample generation, the Hybrid Prompt algorithm not only combines\nvarious prompt ideas including Chain of Thought, Tree of Thought, but also\nincorporates various components such as webshell hierarchical module and\nfew-shot example to facilitate the LLM in learning and reasoning webshell\nescape strategies. Experimental results show that the Hybrid Prompt algorithm\ncan work with multiple LLMs with excellent code reasoning ability to generate\nhigh-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on\nVIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).",
    "comment": "13 pages, 16 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07408v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07408v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07408v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07407v1",
    "updated": "2024-02-12T04:59:34+00:00",
    "published": "2024-02-12T04:59:34+00:00",
    "title": "Conformal Predictive Programming for Chance Constrained Optimization",
    "authors": [
      {
        "name": "Yiqi Zhao"
      },
      {
        "name": "Xinyi Yu"
      },
      {
        "name": "Jyotirmoy V. Deshmukh"
      },
      {
        "name": "Lars Lindemann"
      }
    ],
    "summary": "Motivated by the advances in conformal prediction (CP), we propose conformal\npredictive programming (CPP), an approach to solve chance constrained\noptimization (CCO) problems, i.e., optimization problems with nonlinear\nconstraint functions affected by arbitrary random parameters. CPP utilizes\nsamples from these random parameters along with the quantile lemma -- which is\ncentral to CP -- to transform the CCO problem into a deterministic optimization\nproblem. We then present two tractable reformulations of CPP by: (1) writing\nthe quantile as a linear program along with its KKT conditions (CPP-KKT), and\n(2) using mixed integer programming (CPP-MIP). CPP comes with marginal\nprobabilistic feasibility guarantees for the CCO problem that are conceptually\ndifferent from existing approaches, e.g., the sample approximation and the\nscenario approach. While we explore algorithmic similarities with the sample\napproximation approach, we emphasize that the strength of CPP is that it can\neasily be extended to incorporate different variants of CP. To illustrate this,\nwe present robust conformal predictive programming to deal with distribution\nshifts in the uncertain parameters of the CCO problem.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.SY",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07407v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07407v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07407v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07404v1",
    "updated": "2024-02-12T04:47:38+00:00",
    "published": "2024-02-12T04:47:38+00:00",
    "title": "Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support",
    "authors": [
      {
        "name": "Igor Svoboda"
      },
      {
        "name": "Dmytro Lande"
      }
    ],
    "summary": "Our study presents a new framework that incorporates the Analytic Hierarchy\nProcess (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language\nmodel (LLM), bringing novel approaches to cybersecurity Multiple-criteria\nDecision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous\nagents as virtual experts, we automate the decision-making process, enhancing\nboth efficiency and reliability. This new approach focuses on leveraging LLMs\nfor sophisticated decision analysis, highlighting the synergy between\ntraditional decision-making models and cutting-edge AI technologies. Our\ninnovative methodology demonstrates significant advancements in using AI-driven\nagents for complex decision-making scenarios, highlighting the importance of AI\nin strategic cybersecurity applications. The findings reveal the transformative\npotential of combining AHP and LLMs, establishing a new paradigm for\nintelligent decision support systems in cybersecurity and beyond.",
    "comment": "24 pages, 1 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.MA",
      "I.2.1; I.2.8; H.1.1"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07404v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07404v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07404v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07402v1",
    "updated": "2024-02-12T04:34:19+00:00",
    "published": "2024-02-12T04:34:19+00:00",
    "title": "BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind",
    "authors": [
      {
        "name": "Yuanyuan Mao"
      },
      {
        "name": "Xin Lin"
      },
      {
        "name": "Qin Ni"
      },
      {
        "name": "Liang He"
      }
    ],
    "summary": "As a foundational component of cognitive intelligence, theory of mind (ToM)\ncan make AI more closely resemble human thought processes, thereby enhancing\ntheir interaction and collaboration with human. In particular, it can\nsignificantly improve a model's comprehension of videos in complex scenes.\nHowever, current video question answer (VideoQA) datasets focus on studying\ncausal reasoning within events few of them genuinely incorporating human ToM.\nConsequently, there is a lack of development in ToM reasoning tasks within the\narea of VideoQA. This paper presents BDIQA, the first benchmark to explore the\ncognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA\nis inspired by the cognitive development of children's ToM and addresses the\ncurrent deficiencies in machine ToM within datasets and tasks. Specifically, it\noffers tasks at two difficulty levels, assessing Belief, Desire and Intention\n(BDI) reasoning in both simple and complex scenarios. We conduct evaluations on\nseveral mainstream methods of VideoQA and diagnose their capabilities with zero\nshot, few shot and supervised learning. We find that the performance of\npre-trained models on cognitive reasoning tasks remains unsatisfactory. To\ncounter this challenge, we undertake thorough analysis and experimentation,\nultimately presenting two guidelines to enhance cognitive reasoning derived\nfrom ablation analysis.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MM",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07402v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07402v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07402v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07398v1",
    "updated": "2024-02-12T04:13:16+00:00",
    "published": "2024-02-12T04:13:16+00:00",
    "title": "VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization",
    "authors": [
      {
        "name": "Dongsheng Zhu"
      },
      {
        "name": "Xunzhu Tang"
      },
      {
        "name": "Weidong Han"
      },
      {
        "name": "Jinghui Lu"
      },
      {
        "name": "Yukun Zhao"
      },
      {
        "name": "Guoliang Xing"
      },
      {
        "name": "Junfeng Wang"
      },
      {
        "name": "Dawei Yin"
      }
    ],
    "summary": "This paper presents VisLingInstruct, a novel approach to advancing\nMulti-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show\nimpressive zero-shot abilities in multi-modal tasks, but their performance\ndepends heavily on the quality of instructions. VisLingInstruct tackles this by\nautonomously evaluating and optimizing instructional texts through In-Context\nLearning, improving the synergy between visual perception and linguistic\nexpression in MMLMs. Alongside this instructional advancement, we have also\noptimized the visual feature extraction modules in MMLMs, further augmenting\ntheir responsiveness to textual cues. Our comprehensive experiments on MMLMs,\nbased on FlanT5 and Vicuna, show that VisLingInstruct significantly improves\nzero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%\nand 9% increase in accuracy over the prior state-of-the-art on the TextVQA and\nHatefulMemes datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07398v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07398v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07398v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07393v1",
    "updated": "2024-02-12T03:40:32+00:00",
    "published": "2024-02-12T03:40:32+00:00",
    "title": "TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator",
    "authors": [
      {
        "name": "Meng Zhang"
      },
      {
        "name": "Dennis Yin"
      },
      {
        "name": "Nicholas Gangi"
      },
      {
        "name": "Amir Begovi\u0107"
      },
      {
        "name": "Alexander Chen"
      },
      {
        "name": "Zhaoran Rena Huang"
      },
      {
        "name": "Jiaqi Gu"
      }
    ],
    "summary": "Electronic-photonic computing systems offer immense potential in\nenergy-efficient artificial intelligence (AI) acceleration tasks due to the\nsuperior computing speed and efficiency of optics, especially for real-time,\nlow-energy deep neural network (DNN) inference tasks on resource-restricted\nedge platforms. However, current optical neural accelerators based on\nfoundry-available devices and conventional system architecture still encounter\na performance gap compared to highly customized electronic counterparts. To\nbridge the performance gap due to lack of domain specialization, we present a\ntime-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with\ncross-layer device/circuit/architecture customization. At the device level, we\npresent foundry-compatible, customized photonic devices, including a slow-light\nelectro-optic modulator with experimental demonstration, optical splitters, and\nphase shifters that significantly reduce the footprint and power in input\nencoding and dot-product calculation. At the circuit level, partial products\nare hierarchically accumulated via parallel photocurrent aggregation,\nlightweight capacitive temporal integration, and sequential digital summation,\nconsiderably relieving the analog-to-digital conversion bottleneck. We also\nemploy a multi-tile, multi-core architecture to maximize hardware sharing for\nhigher efficiency. Across diverse edge AI workloads, TeMPO delivers\ndigital-comparable task accuracy with superior quantization/noise tolerance. We\nachieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2\nTOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware.\nThis work signifies the power of cross-layer co-design and domain-specific\ncustomization, paving the way for future electronic-photonic accelerators with\neven greater performance and efficiency.",
    "comment": "17 pages, 19 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.ET",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07393v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07393v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07393v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07391v1",
    "updated": "2024-02-12T03:31:34+00:00",
    "published": "2024-02-12T03:31:34+00:00",
    "title": "Replicability is Asymptotically Free in Multi-armed Bandits",
    "authors": [
      {
        "name": "Junpei Komiyama"
      },
      {
        "name": "Shinji Ito"
      },
      {
        "name": "Yuichi Yoshida"
      },
      {
        "name": "Souta Koshino"
      }
    ],
    "summary": "This work is motivated by the growing demand for reproducible machine\nlearning. We study the stochastic multi-armed bandit problem. In particular, we\nconsider a replicable algorithm that ensures, with high probability, that the\nalgorithm's sequence of actions is not affected by the randomness inherent in\nthe dataset. We observe that existing algorithms require $O(1/\\rho^2)$ times\nmore regret than nonreplicable algorithms, where $\\rho$ is the level of\nnonreplication. However, we demonstrate that this additional cost is\nunnecessary when the time horizon $T$ is sufficiently large for a given $\\rho$,\nprovided that the magnitude of the confidence bounds is chosen carefully. We\nintroduce an explore-then-commit algorithm that draws arms uniformly before\ncommitting to a single arm. Additionally, we examine a successive elimination\nalgorithm that eliminates suboptimal arms at the end of each phase. To ensure\nthe replicability of these algorithms, we incorporate randomness into their\ndecision-making processes. We extend the use of successive elimination to the\nlinear bandit problem as well. For the analysis of these algorithms, we propose\na principled approach to limiting the probability of nonreplication. This\napproach elucidates the steps that existing research has implicitly followed.\nFurthermore, we derive the first lower bound for the two-armed replicable\nbandit problem, which implies the optimality of the proposed algorithms up to a\n$\\log\\log T$ factor for the two-armed case.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07391v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07391v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07391v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07388v1",
    "updated": "2024-02-12T03:19:30+00:00",
    "published": "2024-02-12T03:19:30+00:00",
    "title": "The Limits of Assumption-free Tests for Algorithm Performance",
    "authors": [
      {
        "name": "Yuetian Luo"
      },
      {
        "name": "Rina Foygel Barber"
      }
    ],
    "summary": "Algorithm evaluation and comparison are fundamental questions in machine\nlearning and statistics -- how well does an algorithm perform at a given\nmodeling task, and which algorithm performs best? Many methods have been\ndeveloped to assess algorithm performance, often based around cross-validation\ntype strategies, retraining the algorithm of interest on different subsets of\nthe data and assessing its performance on the held-out data points. Despite the\nbroad use of such procedures, the theoretical properties of these methods are\nnot yet fully understood. In this work, we explore some fundamental limits for\nanswering these questions with limited amounts of data. In particular, we make\na distinction between two questions: how good is an algorithm $A$ at the\nproblem of learning from a training set of size $n$, versus, how good is a\nparticular fitted model produced by running $A$ on a particular training data\nset of size $n$?\n  Our main results prove that, for any test that treats the algorithm $A$ as a\n``black box'' (i.e., we can only study the behavior of $A$ empirically), there\nis a fundamental limit on our ability to carry out inference on the performance\nof $A$, unless the number of available data points $N$ is many times larger\nthan the sample size $n$ of interest. (On the other hand, evaluating the\nperformance of a particular fitted model is easy as long as a holdout data set\nis available -- that is, as long as $N-n$ is not too small.) We also ask\nwhether an assumption of algorithmic stability might be sufficient to\ncircumvent this hardness result. Surprisingly, we find that this is not the\ncase: the same hardness result still holds for the problem of evaluating the\nperformance of $A$, aside from a high-stability regime where fitted models are\nessentially nonrandom. Finally, we also establish similar hardness results for\nthe problem of comparing multiple algorithms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.ST",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07388v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07388v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07388v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07384v1",
    "updated": "2024-02-12T03:04:42+00:00",
    "published": "2024-02-12T03:04:42+00:00",
    "title": "Exploring Perceptual Limitation of Multimodal Large Language Models",
    "authors": [
      {
        "name": "Jiarui Zhang"
      },
      {
        "name": "Jinyi Hu"
      },
      {
        "name": "Mahyar Khayatkhoei"
      },
      {
        "name": "Filip Ilievski"
      },
      {
        "name": "Maosong Sun"
      }
    ],
    "summary": "Multimodal Large Language Models (MLLMs) have recently shown remarkable\nperceptual capability in answering visual questions, however, little is known\nabout the limits of their perception. In particular, while prior works have\nprovided anecdotal evidence of MLLMs' sensitivity to object size, this\nphenomenon and its underlying causes have not been explored comprehensively. In\nthis work, we quantitatively study the perception of small visual objects in\nseveral state-of-the-art MLLMs and reveal a pervasive limitation in answering\nquestions about small objects in images. Next, we identify four independent\nfactors that can contribute to this limitation -- object quality, size,\ndistractors, and location -- and conduct controlled intervention studies to\nmeasure the effect of each factor on MLLMs' perception. In particular, we find\nthat lower object quality and smaller object size can both independently reduce\nMLLMs' ability to answer visual questions. More surprisingly, we find that the\nlocation of the object in the image and the presence of visual distractors can\nalso significantly reduce MLLMs' question answering accuracy. Our study\nprovides a better understanding of the perceptual limitation of MLLMs and\ncontributes new evaluation protocols for analyzing the perception of future\nMLLMs. To facilitate further investigations, we release our code and data.",
    "comment": "14 pages, 14 figures, 3 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07384v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07384v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07384v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07383v1",
    "updated": "2024-02-12T02:58:10+00:00",
    "published": "2024-02-12T02:58:10+00:00",
    "title": "Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like",
    "authors": [
      {
        "name": "Naoyuki Kanda"
      },
      {
        "name": "Xiaofei Wang"
      },
      {
        "name": "Sefik Emre Eskimez"
      },
      {
        "name": "Manthan Thakker"
      },
      {
        "name": "Hemin Yang"
      },
      {
        "name": "Zirun Zhu"
      },
      {
        "name": "Min Tang"
      },
      {
        "name": "Canrun Li"
      },
      {
        "name": "Steven Tsai"
      },
      {
        "name": "Zhen Xiao"
      },
      {
        "name": "Yufei Xia"
      },
      {
        "name": "Jinzhu Li"
      },
      {
        "name": "Yanqing Liu"
      },
      {
        "name": "Sheng Zhao"
      },
      {
        "name": "Michael Zeng"
      }
    ],
    "summary": "Laughter is one of the most expressive and natural aspects of human speech,\nconveying emotions, social cues, and humor. However, most text-to-speech (TTS)\nsystems lack the ability to produce realistic and appropriate laughter sounds,\nlimiting their applications and user experience. While there have been prior\nworks to generate natural laughter, they fell short in terms of controlling the\ntiming and variety of the laughter to be generated. In this work, we propose\nELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker\nbased on a short audio prompt with precise control of laughter timing and\nexpression. Specifically, ELaTE works on the audio prompt to mimic the voice\ncharacteristic, the text prompt to indicate the contents of the generated\nspeech, and the input to control the laughter expression, which can be either\nthe start and end times of laughter, or the additional audio prompt that\ncontains laughter to be mimicked. We develop our model based on the foundation\nof conditional flow-matching-based zero-shot TTS, and fine-tune it with\nframe-level representation from a laughter detector as additional conditioning.\nWith a simple scheme to mix small-scale laughter-conditioned data with\nlarge-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS\nmodel can be readily fine-tuned to generate natural laughter with precise\ncontrollability, without losing any quality of the pre-trained zero-shot TTS\nmodel. Through the evaluations, we show that ELaTE can generate laughing speech\nwith significantly higher quality and controllability compared to conventional\nmodels. See https://aka.ms/elate/ for demo samples.",
    "comment": "See https://aka.ms/elate/ for demo samples",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07383v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07383v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07383v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07370v1",
    "updated": "2024-02-12T02:01:53+00:00",
    "published": "2024-02-12T02:01:53+00:00",
    "title": "SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder",
    "authors": [
      {
        "name": "Jaeseong Lee"
      },
      {
        "name": "Junha Hyung"
      },
      {
        "name": "Sohyun Jeong"
      },
      {
        "name": "Jaegul Choo"
      }
    ],
    "summary": "Face swapping has gained significant attention for its varied applications.\nThe majority of previous face swapping approaches have relied on the seesaw\ngame training scheme, which often leads to the instability of the model\ntraining and results in undesired samples with blended identities due to the\ntarget identity leakage problem. This paper introduces the Shape Agnostic\nMasked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach\ndesigned to enhance face swapping model training. Our training scheme addresses\nthe limitations of traditional training methods by circumventing the\nconventional seesaw game and introducing clear ground truth through its\nself-reconstruction training regime. It effectively mitigates identity leakage\nby masking facial regions of the input images and utilizing learned\ndisentangled identity and non-identity features. Additionally, we tackle the\nshape misalignment problem with new techniques including perforation confusion\nand random mesh scaling, and establishes a new state-of-the-art, surpassing\nother baseline methods, preserving both identity and non-identity attributes,\nwithout sacrificing on either aspect.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07370v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07370v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07370v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07369v1",
    "updated": "2024-02-12T01:59:51+00:00",
    "published": "2024-02-12T01:59:51+00:00",
    "title": "Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation",
    "authors": [
      {
        "name": "Tonglong Wei"
      },
      {
        "name": "Youfang Lin"
      },
      {
        "name": "Shengnan Guo"
      },
      {
        "name": "Yan Lin"
      },
      {
        "name": "Yiheng Huang"
      },
      {
        "name": "Chenyang Xiang"
      },
      {
        "name": "Yuqing Bai"
      },
      {
        "name": "Menglu Ya"
      },
      {
        "name": "Huaiyu Wan"
      }
    ],
    "summary": "Trajectory data is essential for various applications as it records the\nmovement of vehicles. However, publicly available trajectory datasets remain\nlimited in scale due to privacy concerns, which hinders the development of\ntrajectory data mining and trajectory-based applications. To address this\nissue, some methods for generating synthetic trajectories have been proposed to\nexpand the scale of the dataset. However, all existing methods generate\ntrajectories in the geographical coordinate system, which poses two limitations\nfor their utilization in practical applications: 1) the inability to ensure\nthat the generated trajectories are constrained on the road. 2) the lack of\nroad-related information. In this paper, we propose a new problem to meet the\npractical application need, \\emph{i.e.}, road network-constrained trajectory\n(RNTraj) generation, which can directly generate trajectories on the road\nnetwork with road-related information. RNTraj is a hybrid type of data, in\nwhich each point is represented by a discrete road segment and a continuous\nmoving rate. To generate RNTraj, we design a diffusion model called\nDiff-RNTraj. This model can effectively handle the hybrid RNTraj using a\ncontinuous diffusion framework by incorporating a pre-training strategy to\nembed hybrid RNTraj into continuous representations. During the sampling stage,\na RNTraj decoder is designed to map the continuous representation generated by\nthe diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj\nintroduces a novel loss function to enhance the spatial validity of the\ngenerated trajectories. Extensive experiments conducted on two real-world\ntrajectory datasets demonstrate the effectiveness of the proposed model.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07369v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07369v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07369v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07368v1",
    "updated": "2024-02-12T01:55:51+00:00",
    "published": "2024-02-12T01:55:51+00:00",
    "title": "Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning",
    "authors": [
      {
        "name": "Gabriel Simmons"
      },
      {
        "name": "Vladislav Savinov"
      }
    ],
    "summary": "This study evaluates the ability of Large Language Model (LLM)-based\nSubpopulation Representative Models (SRMs) to generalize from empirical data,\nutilizing in-context learning with data from the 2016 and 2020 American\nNational Election Studies. We explore generalization across response variables\nand demographic subgroups. While conditioning with empirical data improves\nperformance on the whole, the benefit of in-context learning varies\nconsiderably across demographics, sometimes hurting performance for one\ndemographic while helping performance for others. The inequitable benefits of\nin-context learning for SRM present a challenge for practitioners implementing\nSRMs, and for decision-makers who might come to rely on them. Our work\nhighlights a need for fine-grained benchmarks captured from diverse\nsubpopulations that test not only fidelity but generalization.",
    "comment": "Accepted to PERSONALIZE workshop at EACL 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07368v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07368v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07368v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07366v1",
    "updated": "2024-02-12T01:47:06+00:00",
    "published": "2024-02-12T01:47:06+00:00",
    "title": "Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing",
    "authors": [
      {
        "name": "Wei Xu"
      },
      {
        "name": "An Liu"
      },
      {
        "name": "Yiting Zhang"
      },
      {
        "name": "Vincent Lau"
      }
    ],
    "summary": "Federated learning (FL) is a machine learning paradigm where the clients\npossess decentralized training data and the central server handles aggregation\nand scheduling. Typically, FL algorithms involve clients training their local\nmodels using stochastic gradient descent (SGD), which carries drawbacks such as\nslow convergence and being prone to getting stuck in suboptimal solutions. In\nthis work, we propose a message passing based Bayesian federated learning (BFL)\nframework to avoid these drawbacks.Specifically, we formulate the problem of\ndeep neural network (DNN) learning and compression and as a sparse Bayesian\ninference problem, in which group sparse prior is employed to achieve\nstructured model compression. Then, we propose an efficient BFL algorithm\ncalled EMTDAMP, where expectation maximization (EM) and turbo deep approximate\nmessage passing (TDAMP) are combined to achieve distributed learning and\ncompression. The central server aggregates local posterior distributions to\nupdate global posterior distributions and update hyperparameters based on EM to\naccelerate convergence. The clients perform TDAMP to achieve efficient\napproximate message passing over DNN with joint prior distribution. We detail\nthe application of EMTDAMP to Boston housing price prediction and handwriting\nrecognition, and present extensive numerical results to demonstrate the\nadvantages of EMTDAMP.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07366v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07366v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07366v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07365v1",
    "updated": "2024-02-12T01:40:31+00:00",
    "published": "2024-02-12T01:40:31+00:00",
    "title": "A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents",
    "authors": [
      {
        "name": "Mathieu Lauri\u00e8re"
      },
      {
        "name": "Ludovic Tangpi"
      },
      {
        "name": "Xuchen Zhou"
      }
    ],
    "summary": "Graphon games have been introduced to study games with many players who\ninteract through a weighted graph of interaction. By passing to the limit, a\ngame with a continuum of players is obtained, in which the interactions are\nthrough a graphon. In this paper, we focus on a graphon game for optimal\ninvestment under relative performance criteria, and we propose a deep learning\nmethod. The method builds upon two key ingredients: first, a characterization\nof Nash equilibria by forward-backward stochastic differential equations and,\nsecond, recent advances of machine learning algorithms for stochastic\ndifferential games. We provide numerical experiments on two different financial\nmodels. In each model, we compare the effect of several graphons, which\ncorrespond to different structures of interactions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.GT",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07365v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07365v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07365v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07363v1",
    "updated": "2024-02-12T01:33:33+00:00",
    "published": "2024-02-12T01:33:33+00:00",
    "title": "Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions",
    "authors": [
      {
        "name": "Rachitesh Kumar"
      },
      {
        "name": "Jon Schneider"
      },
      {
        "name": "Balasubramanian Sivan"
      }
    ],
    "summary": "Learning to bid in repeated first-price auctions is a fundamental problem at\nthe interface of game theory and machine learning, which has seen a recent\nsurge in interest due to the transition of display advertising to first-price\nauctions. In this work, we propose a novel concave formulation for\npure-strategy bidding in first-price auctions, and use it to analyze natural\nGradient-Ascent-based algorithms for this problem. Importantly, our analysis\ngoes beyond regret, which was the typical focus of past work, and also accounts\nfor the strategic backdrop of online-advertising markets where bidding\nalgorithms are deployed -- we prove that our algorithms cannot be exploited by\na strategic seller and that they incentivize truth-telling for the buyer.\n  Concretely, we show that our algorithms achieve $O(\\sqrt{T})$ regret when the\nhighest competing bids are generated adversarially, and show that no online\nalgorithm can do better. We further prove that the regret improves to $O(\\log\nT)$ when the competition is stationary and stochastic. Moving beyond regret, we\nshow that a strategic seller cannot exploit our algorithms to extract more\nrevenue on average than is possible under the optimal mechanism, i.e., the\nseller cannot do much better than posting the monopoly reserve price in each\nauction. Finally, we prove that our algorithm is also incentive compatible --\nit is a (nearly) dominant strategy for the buyer to report her values\ntruthfully to the algorithm as a whole.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07363v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07363v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07363v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07357v2",
    "updated": "2024-02-13T13:46:07+00:00",
    "published": "2024-02-12T01:17:09+00:00",
    "title": "Regression Trees for Fast and Adaptive Prediction Intervals",
    "authors": [
      {
        "name": "Luben M. C. Cabezas"
      },
      {
        "name": "Mateus P. Otto"
      },
      {
        "name": "Rafael Izbicki"
      },
      {
        "name": "Rafael B. Stern"
      }
    ],
    "summary": "Predictive models make mistakes. Hence, there is a need to quantify the\nuncertainty associated with their predictions. Conformal inference has emerged\nas a powerful tool to create statistically valid prediction regions around\npoint predictions, but its naive application to regression problems yields\nnon-adaptive regions. New conformal scores, often relying upon quantile\nregressors or conditional density estimators, aim to address this limitation.\nAlthough they are useful for creating prediction bands, these scores are\ndetached from the original goal of quantifying the uncertainty around an\narbitrary predictive model. This paper presents a new, model-agnostic family of\nmethods to calibrate prediction intervals for regression problems with local\ncoverage guarantees. Our approach is based on pursuing the coarsest partition\nof the feature space that approximates conditional coverage. We create this\npartition by training regression trees and Random Forests on conformity scores.\nOur proposal is versatile, as it applies to various conformity scores and\nprediction settings and demonstrates superior scalability and performance\ncompared to established baselines in simulated and real-world datasets. We\nprovide a Python package clover that implements our methods using the standard\nscikit-learn interface.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07357v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07357v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07357v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07356v1",
    "updated": "2024-02-12T01:11:49+00:00",
    "published": "2024-02-12T01:11:49+00:00",
    "title": "A Novel Gaussian Min-Max Theorem and its Applications",
    "authors": [
      {
        "name": "Danil Akhtiamov"
      },
      {
        "name": "David Bosch"
      },
      {
        "name": "Reza Ghane"
      },
      {
        "name": "K Nithin Varma"
      },
      {
        "name": "Babak Hassibi"
      }
    ],
    "summary": "A celebrated result by Gordon allows one to compare the min-max behavior of\ntwo Gaussian processes if certain inequality conditions are met. The\nconsequences of this result include the Gaussian min-max (GMT) and convex\nGaussian min-max (CGMT) theorems which have had far-reaching implications in\nhigh-dimensional statistics, machine learning, non-smooth optimization, and\nsignal processing. Both theorems rely on a pair of Gaussian processes, first\nidentified by Slepian, that satisfy Gordon's comparison inequalities. To date,\nno other pair of Gaussian processes satisfying these inequalities has been\ndiscovered. In this paper, we identify such a new pair. The resulting theorems\nextend the classical GMT and CGMT Theorems from the case where the underlying\nGaussian matrix in the primary process has iid rows to where it has independent\nbut non-identically-distributed ones. The new CGMT is applied to the problems\nof multi-source Gaussian regression, as well as to binary classification of\ngeneral Gaussian mixture models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07356v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07356v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07356v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07355v2",
    "updated": "2024-02-13T02:49:52+00:00",
    "published": "2024-02-12T01:04:39+00:00",
    "title": "Sampling from the Mean-Field Stationary Distribution",
    "authors": [
      {
        "name": "Yunbum Kook"
      },
      {
        "name": "Matthew S. Zhang"
      },
      {
        "name": "Sinho Chewi"
      },
      {
        "name": "Murat A. Erdogdu"
      },
      {
        "name": "Mufan Bill Li"
      }
    ],
    "summary": "We study the complexity of sampling from the stationary distribution of a\nmean-field SDE, or equivalently, the complexity of minimizing a functional over\nthe space of probability measures which includes an interaction term. Our main\ninsight is to decouple the two key aspects of this problem: (1) approximation\nof the mean-field SDE via a finite-particle system, via uniform-in-time\npropagation of chaos, and (2) sampling from the finite-particle stationary\ndistribution, via standard log-concave samplers. Our approach is conceptually\nsimpler and its flexibility allows for incorporating the state-of-the-art for\nboth algorithms and theory. This leads to improved guarantees in numerous\nsettings, including better guarantees for optimizing certain two-layer neural\nnetworks in the mean-field regime.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.ST",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07355v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07355v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07355v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07352v1",
    "updated": "2024-02-12T01:02:22+00:00",
    "published": "2024-02-12T01:02:22+00:00",
    "title": "Data Distribution-based Curriculum Learning",
    "authors": [
      {
        "name": "Shonal Chaudhry"
      },
      {
        "name": "Anuraganand Sharma"
      }
    ],
    "summary": "The order of training samples can have a significant impact on the\nperformance of a classifier. Curriculum learning is a method of ordering\ntraining samples from easy to hard. This paper proposes the novel idea of a\ncurriculum learning approach called Data Distribution-based Curriculum Learning\n(DDCL). DDCL uses the data distribution of a dataset to build a curriculum\nbased on the order of samples. Two types of scoring methods known as DDCL\n(Density) and DDCL (Point) are used to score training samples thus determining\ntheir training order. DDCL (Density) uses the sample density to assign scores\nwhile DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the\nproposed DDCL approach by conducting experiments on multiple datasets using a\nneural network, support vector machine and random forest classifier. Evaluation\nresults show that the application of DDCL improves the average classification\naccuracy for all datasets compared to standard evaluation without any\ncurriculum. Moreover, analysis of the error losses for a single training epoch\nreveals that convergence is faster when using DDCL over the no curriculum\nmethod.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07352v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07352v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07352v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07350v1",
    "updated": "2024-02-12T00:44:37+00:00",
    "published": "2024-02-12T00:44:37+00:00",
    "title": "Antagonistic AI",
    "authors": [
      {
        "name": "Alice Cai"
      },
      {
        "name": "Ian Arawjo"
      },
      {
        "name": "Elena L. Glassman"
      }
    ],
    "summary": "The vast majority of discourse around AI development assumes that\nsubservient, \"moral\" models aligned with \"human values\" are universally\nbeneficial -- in short, that good AI is sycophantic AI. We explore the shadow\nof the sycophantic paradigm, a design space we term antagonistic AI: AI systems\nthat are disagreeable, rude, interrupting, confrontational, challenging, etc.\n-- embedding opposite behaviors or values. Far from being \"bad\" or \"immoral,\"\nwe consider whether antagonistic AI systems may sometimes have benefits to\nusers, such as forcing users to confront their assumptions, build resilience,\nor develop healthier relational boundaries. Drawing from formative explorations\nand a speculative design workshop where participants designed fictional AI\ntechnologies that employ antagonism, we lay out a design space for antagonistic\nAI, articulating potential benefits, design techniques, and methods of\nembedding antagonistic elements into user experience. Finally, we discuss the\nmany ethical challenges of this space and identify three dimensions for the\nresponsible design of antagonistic AI -- consent, context, and framing.",
    "comment": "17 pages, 1 figure, 5 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.HC",
      "I.2.0; J.0; K.4.0"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07350v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07350v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07350v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07347v1",
    "updated": "2024-02-12T00:36:34+00:00",
    "published": "2024-02-12T00:36:34+00:00",
    "title": "Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble",
    "authors": [
      {
        "name": "Yunzhe Xue"
      },
      {
        "name": "Usman Roshan"
      }
    ],
    "summary": "Recent work has shown the defense of 01 loss sign activation neural networks\nagainst image classification adversarial attacks. A public challenge to attack\nthe models on CIFAR10 dataset remains undefeated. We ask the following question\nin this study: are 01 loss sign activation neural networks hard to deceive with\na popular black box text adversarial attack program called TextFooler? We study\nthis question on four popular text classification datasets: IMDB reviews, Yelp\nreviews, MR sentiment classification, and AG news classification. We find that\nour 01 loss sign activation network is much harder to attack with TextFooler\ncompared to sigmoid activation cross entropy and binary neural networks. We\nalso study a 01 loss sign activation convolutional neural network with a novel\nglobal pooling step specific to sign activation networks. With this new\nvariation we see a significant gain in adversarial accuracy rendering\nTextFooler practically useless against it. We make our code freely available at\n\\url{https://github.com/zero-one-loss/wordcnn01} and\n\\url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that\n01 loss sign activation networks could be further developed to create fool\nproof models against text adversarial attacks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07347v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07347v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07347v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07344v1",
    "updated": "2024-02-12T00:22:47+00:00",
    "published": "2024-02-12T00:22:47+00:00",
    "title": "Measurement Scheduling for ICU Patients with Offline Reinforcement Learning",
    "authors": [
      {
        "name": "Zongliang Ji"
      },
      {
        "name": "Anna Goldenberg"
      },
      {
        "name": "Rahul G. Krishnan"
      }
    ],
    "summary": "Scheduling laboratory tests for ICU patients presents a significant\nchallenge. Studies show that 20-40% of lab tests ordered in the ICU are\nredundant and could be eliminated without compromising patient safety. Prior\nwork has leveraged offline reinforcement learning (Offline-RL) to find optimal\npolicies for ordering lab tests based on patient information. However, new ICU\npatient datasets have since been released, and various advancements have been\nmade in Offline-RL methods. In this study, we first introduce a preprocessing\npipeline for the newly-released MIMIC-IV dataset geared toward time-series\ntasks. We then explore the efficacy of state-of-the-art Offline-RL methods in\nidentifying better policies for ICU patient lab test scheduling. Besides\nassessing methodological performance, we also discuss the overall suitability\nand practicality of using Offline-RL frameworks for scheduling laboratory tests\nin ICU settings.",
    "comment": "Extended Abstract presented at Machine Learning for Health (ML4H)\n  symposium 2023, December 10th, 2023, New Orleans, United States, 11 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07344v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07344v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07344v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07342v1",
    "updated": "2024-02-12T00:20:43+00:00",
    "published": "2024-02-12T00:20:43+00:00",
    "title": "Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation",
    "authors": [
      {
        "name": "Priyan Vaithilingam"
      },
      {
        "name": "Ian Arawjo"
      },
      {
        "name": "Elena L. Glassman"
      }
    ],
    "summary": "We ideate a future design workflow that involves AI technology. Drawing from\nactivity and communication theory, we attempt to isolate the new value large AI\nmodels can provide design compared to past technologies. We arrive at three\naffordances -- dynamic grounding, constructive negotiation, and sustainable\nmotivation -- that summarize latent qualities of natural language-enabled\nfoundation models that, if explicitly designed for, can support the process of\ndesign. Through design fiction, we then imagine a future interface as a\ndiegetic prototype, the story of Squirrel Game, that demonstrates each of our\nthree affordances in a realistic usage scenario. Our design process,\nterminology, and diagrams aim to contribute to future discussions about the\nrelative affordances of AI technology with regard to collaborating with human\ndesigners.",
    "comment": "12 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "J.6; I.2.0; H.5.2"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07342v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07342v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07342v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07341v1",
    "updated": "2024-02-12T00:19:09+00:00",
    "published": "2024-02-12T00:19:09+00:00",
    "title": "Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization",
    "authors": [
      {
        "name": "Kwang-Sung Jun"
      },
      {
        "name": "Jungtaek Kim"
      }
    ],
    "summary": "Adapting to a priori unknown noise level is a very important but challenging\nproblem in sequential decision-making as efficient exploration typically\nrequires knowledge of the noise level, which is often loosely specified. We\nreport significant progress in addressing this issue in linear bandits in two\nrespects. First, we propose a novel confidence set that is `semi-adaptive' to\nthe unknown sub-Gaussian parameter $\\sigma_*^2$ in the sense that the\n(normalized) confidence width scales with $\\sqrt{d\\sigma_*^2 + \\sigma_0^2}$\nwhere $d$ is the dimension and $\\sigma_0^2$ is the specified sub-Gaussian\nparameter (known) that can be much larger than $\\sigma_*^2$. This is a\nsignificant improvement over $\\sqrt{d\\sigma_0^2}$ of the standard confidence\nset of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that\nthis leads to an improved regret bound in linear bandits. Second, for bounded\nrewards, we propose a novel variance-adaptive confidence set that has a much\nimproved numerical performance upon prior art. We then apply this confidence\nset to develop, as we claim, the first practical variance-adaptive linear\nbandit algorithm via an optimistic approach, which is enabled by our novel\nregret analysis technique. Both of our confidence sets rely critically on\n`regret equality' from online learning. Our empirical evaluation in Bayesian\noptimization tasks shows that our algorithms demonstrate better or comparable\nperformance compared to existing methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07341v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07341v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07341v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07340v1",
    "updated": "2024-02-12T00:18:25+00:00",
    "published": "2024-02-12T00:18:25+00:00",
    "title": "Random Geometric Graph Alignment with Graph Neural Networks",
    "authors": [
      {
        "name": "Suqi Liu"
      },
      {
        "name": "Morgane Austern"
      }
    ],
    "summary": "We characterize the performance of graph neural networks for graph alignment\nproblems in the presence of vertex feature information. More specifically,\ngiven two graphs that are independent perturbations of a single random\ngeometric graph with noisy sparse features, the task is to recover an unknown\none-to-one mapping between the vertices of the two graphs. We show under\ncertain conditions on the sparsity and noise level of the feature vectors, a\ncarefully designed one-layer graph neural network can with high probability\nrecover the correct alignment between the vertices with the help of the graph\nstructure. We also prove that our conditions on the noise level are tight up to\nlogarithmic factors. Finally we compare the performance of the graph neural\nnetwork to directly solving an assignment problem on the noisy vertex features.\nWe demonstrate that when the noise level is at least constant this direct\nmatching fails to have perfect recovery while the graph neural network can\ntolerate noise level growing as fast as a power of the size of the graph.",
    "comment": "29 pages, 2 figure, 1 table",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.IT",
      "cs.SI",
      "math.IT",
      "math.PR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07340v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07340v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07340v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07334v1",
    "updated": "2024-02-11T23:57:09+00:00",
    "published": "2024-02-11T23:57:09+00:00",
    "title": "Differentially Private Training of Mixture of Experts Models",
    "authors": [
      {
        "name": "Pierre Tholoniat"
      },
      {
        "name": "Huseyin A. Inan"
      },
      {
        "name": "Janardhan Kulkarni"
      },
      {
        "name": "Robert Sim"
      }
    ],
    "summary": "This position paper investigates the integration of Differential Privacy (DP)\nin the training of Mixture of Experts (MoE) models within the field of natural\nlanguage processing. As Large Language Models (LLMs) scale to billions of\nparameters, leveraging expansive datasets, they exhibit enhanced linguistic\ncapabilities and emergent abilities. However, this growth raises significant\ncomputational and privacy concerns. Our study addresses these issues by\nexploring the potential of MoE models, known for their computational\nefficiency, and the application of DP, a standard for privacy preservation. We\npresent the first known attempt to train MoE models under the constraints of\nDP, addressing the unique challenges posed by their architecture and the\ncomplexities of DP integration. Our initial experimental studies demonstrate\nthat MoE models can be effectively trained with DP, achieving performance that\nis competitive with their non-private counterparts. This initial study aims to\nprovide valuable insights and ignite further research in the domain of\nprivacy-preserving MoE models, softly laying the groundwork for prospective\ndevelopments in this evolving field.",
    "comment": "Preliminary work presented as a poster at the 5th AAAI Workshop on\n  Privacy-Preserving Artificial Intelligence (PPAI 24)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07334v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07334v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07334v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07327v1",
    "updated": "2024-02-11T23:27:24+00:00",
    "published": "2024-02-11T23:27:24+00:00",
    "title": "Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers",
    "authors": [
      {
        "name": "Minoo Shayaninasab"
      },
      {
        "name": "Bagher Babaali"
      }
    ],
    "summary": "Due to the complex nature of human emotions and the diversity of emotion\nrepresentation methods in humans, emotion recognition is a challenging field.\nIn this research, three input modalities, namely text, audio (speech), and\nvideo, are employed to generate multimodal feature vectors. For generating\nfeatures for each of these modalities, pre-trained Transformer models with\nfine-tuning are utilized. In each modality, a Transformer model is used with\ntransfer learning to extract feature and emotional structure. These features\nare then fused together, and emotion recognition is performed using a\nclassifier. To select an appropriate fusion method and classifier, various\nfeature-level and decision-level fusion techniques have been experimented with,\nand ultimately, the best model, which combines feature-level fusion by\nconcatenating feature vectors and classification using a Support Vector Machine\non the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords:\nMultimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer\nLearning, Transformer.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07327v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07327v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07327v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07326v1",
    "updated": "2024-02-11T23:23:31+00:00",
    "published": "2024-02-11T23:23:31+00:00",
    "title": "Persian Speech Emotion Recognition by Fine-Tuning Transformers",
    "authors": [
      {
        "name": "Minoo Shayaninasab"
      },
      {
        "name": "Bagher Babaali"
      }
    ],
    "summary": "Given the significance of speech emotion recognition, numerous methods have\nbeen developed in recent years to create effective and efficient systems in\nthis domain. One of these methods involves the use of pretrained transformers,\nfine-tuned to address this specific problem, resulting in high accuracy.\nDespite extensive discussions and global-scale efforts to enhance these\nsystems, the application of this innovative and effective approach has received\nless attention in the context of Persian speech emotion recognition. In this\narticle, we review the field of speech emotion recognition and its background,\nwith an emphasis on the importance of employing transformers in this context.\nWe present two models, one based on spectrograms and the other on the audio\nitself, fine-tuned using the shEMO dataset. These models significantly enhance\nthe accuracy of previous systems, increasing it from approximately 65% to 80%\non the mentioned dataset. Subsequently, to investigate the effect of\nmultilinguality on the fine-tuning process, these same models are fine-tuned\ntwice. First, they are fine-tuned using the English IEMOCAP dataset, and then\nthey are fine-tuned with the Persian shEMO dataset. This results in an improved\naccuracy of 82% for the Persian emotion recognition system. Keywords: Persian\nSpeech Emotion Recognition, shEMO, Self-Supervised Learning",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07326v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07326v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07326v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07323v1",
    "updated": "2024-02-11T22:59:19+00:00",
    "published": "2024-02-11T22:59:19+00:00",
    "title": "Lessons Learned from Mining the Hugging Face Repository",
    "authors": [
      {
        "name": "Joel Casta\u00f1o"
      },
      {
        "name": "Silverio Mart\u00ednez-Fern\u00e1ndez"
      },
      {
        "name": "Xavier Franch"
      }
    ],
    "summary": "The rapidly evolving fields of Machine Learning (ML) and Artificial\nIntelligence have witnessed the emergence of platforms like Hugging Face (HF)\nas central hubs for model development and sharing. This experience report\nsynthesizes insights from two comprehensive studies conducted on HF, focusing\non carbon emissions and the evolutionary and maintenance aspects of ML models.\nOur objective is to provide a practical guide for future researchers embarking\non mining software repository studies within the HF ecosystem to enhance the\nquality of these studies. We delve into the intricacies of the replication\npackage used in our studies, highlighting the pivotal tools and methodologies\nthat facilitated our analysis. Furthermore, we propose a nuanced stratified\nsampling strategy tailored for the diverse HF Hub dataset, ensuring a\nrepresentative and comprehensive analytical approach. The report also\nintroduces preliminary guidelines, transitioning from repository mining to\ncohort studies, to establish causality in repository mining studies,\nparticularly within the ML model of HF context. This transition is inspired by\nexisting frameworks and is adapted to suit the unique characteristics of the HF\nmodel ecosystem. Our report serves as a guiding framework for researchers,\ncontributing to the responsible and sustainable advancement of ML, and\nfostering a deeper understanding of the broader implications of ML models.",
    "comment": "Accepted at the 2024 ACM/IEEE 1st International Workshop on\n  Methodological Issues with Empirical Studies in Software Engineering (WSESE)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07323v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07323v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07323v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07321v1",
    "updated": "2024-02-11T22:58:49+00:00",
    "published": "2024-02-11T22:58:49+00:00",
    "title": "Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs",
    "authors": [
      {
        "name": "Bilal Chughtai"
      },
      {
        "name": "Alan Cooney"
      },
      {
        "name": "Neel Nanda"
      }
    ],
    "summary": "How do transformer-based large language models (LLMs) store and retrieve\nknowledge? We focus on the most basic form of this task -- factual recall,\nwhere the model is tasked with explicitly surfacing stored facts in prompts of\nform `Fact: The Colosseum is in the country of'. We find that the mechanistic\nstory behind factual recall is more complex than previously thought. It\ncomprises several distinct, independent, and qualitatively different mechanisms\nthat additively combine, constructively interfering on the correct attribute.\nWe term this generic phenomena the additive motif: models compute through\nsumming up multiple independent contributions. Each mechanism's contribution\nmay be insufficient alone, but summing results in constructive interfere on the\ncorrect answer. In addition, we extend the method of direct logit attribution\nto attribute an attention head's output to individual source tokens. We use\nthis technique to unpack what we call `mixed heads' -- which are themselves a\npair of two separate additive updates from different source tokens.",
    "comment": "NeurIPS 2023 Attributing Model Behaviour at Scale Workshop",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07321v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07321v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07321v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07320v1",
    "updated": "2024-02-11T22:53:21+00:00",
    "published": "2024-02-11T22:53:21+00:00",
    "title": "Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets",
    "authors": [
      {
        "name": "Ross Greer"
      },
      {
        "name": "Mohan Trivedi"
      }
    ],
    "summary": "This research explores the integration of language embeddings for active\nlearning in autonomous driving datasets, with a focus on novelty detection.\nNovelty arises from unexpected scenarios that autonomous vehicles struggle to\nnavigate, necessitating higher-level reasoning abilities. Our proposed method\nemploys language-based representations to identify novel scenes, emphasizing\nthe dual purpose of safety takeover responses and active learning. The research\npresents a clustering experiment using Contrastive Language-Image Pretrained\n(CLIP) embeddings to organize datasets and detect novelties. We find that the\nproposed algorithm effectively isolates novel scenes from a collection of\nsubsets derived from two real-world driving datasets, one vehicle-mounted and\none infrastructure-mounted. From the generated clusters, we further present\nmethods for generating textual explanations of elements which differentiate\nscenes classified as novel from other scenes in the data pool, presenting\nqualitative examples from the clustered results. Our results demonstrate the\neffectiveness of language-driven embeddings in identifying novel elements and\ngenerating explanations of data, and we further discuss potential applications\nin safe takeovers, data curation, and multi-task active learning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07320v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07320v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07320v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07319v1",
    "updated": "2024-02-11T22:40:12+00:00",
    "published": "2024-02-11T22:40:12+00:00",
    "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
    "authors": [
      {
        "name": "Lichang Chen"
      },
      {
        "name": "Chen Zhu"
      },
      {
        "name": "Davit Soselia"
      },
      {
        "name": "Jiuhai Chen"
      },
      {
        "name": "Tianyi Zhou"
      },
      {
        "name": "Tom Goldstein"
      },
      {
        "name": "Heng Huang"
      },
      {
        "name": "Mohammad Shoeybi"
      },
      {
        "name": "Bryan Catanzaro"
      }
    ],
    "summary": "In this work, we study the issue of reward hacking on the response length, a\nchallenge emerging in Reinforcement Learning from Human Feedback (RLHF) on\nLLMs. A well-formatted, verbose but less helpful response from the LLMs can\noften deceive LLMs or even human evaluators to achieve high scores. The same\nissue also holds for some reward models in RL. To address the challenges in\nboth training and evaluation, we establish a more reliable evaluation protocol\nfor comparing different training configurations, which inspects the trade-off\nbetween LLM evaluation score and response length obtained by varying training\nhyperparameters. Based on this evaluation, we conduct large-scale studies,\nwhere the results shed insights into the efficacy of hyperparameters and tricks\nused in RL on mitigating length bias. We further propose to improve the reward\nmodel by jointly training two linear heads on shared feature representations to\npredict the rewards, one trained to correlate with length, and the other\ntrained to decorrelate with length and therefore focus more on the actual\ncontent. We then discard the length head in RL to prevent reward hacking on\nlength. Experiments demonstrate that our approach almost eliminates the reward\ncorrelation with length, and improves the obtained policy by a significant\nmargin.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07319v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07319v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07319v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07314v1",
    "updated": "2024-02-11T21:44:21+00:00",
    "published": "2024-02-11T21:44:21+00:00",
    "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference",
    "authors": [
      {
        "name": "Chenlu Ye"
      },
      {
        "name": "Wei Xiong"
      },
      {
        "name": "Yuheng Zhang"
      },
      {
        "name": "Nan Jiang"
      },
      {
        "name": "Tong Zhang"
      }
    ],
    "summary": "Reinforcement Learning from Human Feedback (RLHF) learns from the preference\nsignal provided by a probabilistic preference model, which takes a prompt and\ntwo responses as input, and produces a score indicating the preference of one\nresponse against another. So far, the most popular RLHF paradigm is\nreward-based, which starts with an initial step of reward modeling, and the\nconstructed reward is then used to provide a reward signal for the subsequent\nreward optimization stage. However, the existence of a reward function is a\nstrong assumption and the reward-based RLHF is limited in expressivity and\ncannot capture the real-world complicated human preference.\n  In this work, we provide theoretical insights for a recently proposed\nlearning paradigm, Nash learning from human feedback (NLHF), which considered a\ngeneral preference model and formulated the alignment process as a game between\ntwo competitive LLMs. The learning objective is to find a policy that\nconsistently generates responses preferred over any competing policy while\nstaying close to the initial model. The objective is defined as the Nash\nequilibrium (NE) of the KL-regularized preference model. We aim to make the\nfirst attempt to study the theoretical learnability of the KL-regularized NLHF\nby considering both offline and online settings. For the offline learning from\na pre-collected dataset, we propose algorithms that are efficient under\nsuitable coverage conditions of the dataset. For batch online learning from\niterative interactions with a preference oracle, our proposed algorithm enjoys\na finite sample guarantee under the structural condition of the underlying\npreference model. Our results connect the new NLHF paradigm with traditional RL\ntheory, and validate the potential of reward-model-free learning under general\npreference.",
    "comment": "RLHF, NLHF, Alignment for LLMs",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07314v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07314v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07314v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07310v1",
    "updated": "2024-02-11T21:16:42+00:00",
    "published": "2024-02-11T21:16:42+00:00",
    "title": "BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis",
    "authors": [
      {
        "name": "Leandro A. Passos"
      },
      {
        "name": "Douglas Rodrigues"
      },
      {
        "name": "Danilo Jodas"
      },
      {
        "name": "Kelton A. P. Costa"
      },
      {
        "name": "Jo\u00e3o Paulo Papa"
      }
    ],
    "summary": "This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07310v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07310v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07310v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07309v2",
    "updated": "2024-02-13T17:24:20+00:00",
    "published": "2024-02-11T21:16:26+00:00",
    "title": "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs",
    "authors": [
      {
        "name": "Adri\u00e1n Bazaga"
      },
      {
        "name": "Pietro Li\u00f2"
      },
      {
        "name": "Gos Micklem"
      }
    ],
    "summary": "Hypergraphs are marked by complex topology, expressing higher-order\ninteractions among multiple entities with hyperedges. Lately, hypergraph-based\ndeep learning methods to learn informative data representations for the problem\nof node classification on text-attributed hypergraphs have garnered increasing\nresearch attention. However, existing methods struggle to simultaneously\ncapture the full extent of hypergraph structural information and the rich\nlinguistic attributes inherent in the nodes attributes, which largely hampers\ntheir effectiveness and generalizability. To overcome these challenges, we\nexplore ways to further augment a pretrained BERT model with specialized\nhypergraph-aware layers for the task of node classification. Such layers\nintroduce higher-order structural inductive bias into the language model, thus\nimproving the model's capacity to harness both higher-order context information\nfrom the hypergraph structure and semantic information present in text. In this\npaper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model\nwhich simultaneously models hypergraph relational structure while maintaining\nthe high-quality text encoding capabilities of a pre-trained BERT. Notably,\nHyperBERT presents results that achieve a new state-of-the-art on five\nchallenging text-attributed hypergraph node classification benchmarks.",
    "comment": "11 pages, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07309v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07309v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07309v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07307v1",
    "updated": "2024-02-11T21:12:21+00:00",
    "published": "2024-02-11T21:12:21+00:00",
    "title": "Self-Consistent Conformal Prediction",
    "authors": [
      {
        "name": "Lars van der Laan"
      },
      {
        "name": "Ahmed M. Alaa"
      }
    ],
    "summary": "In decision-making guided by machine learning, decision-makers often take\nidentical actions in contexts with identical predicted outcomes. Conformal\nprediction helps decision-makers quantify outcome uncertainty for actions,\nallowing for better risk management. Inspired by this perspective, we introduce\nself-consistent conformal prediction, which yields both Venn-Abers calibrated\npredictions and conformal prediction intervals that are valid conditional on\nactions prompted by model predictions. Our procedure can be applied post-hoc to\nany black-box predictor to provide rigorous, action-specific decision-making\nguarantees. Numerical experiments show our approach strikes a balance between\ninterval efficiency and conditional validity.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07307v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07307v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07307v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07295v1",
    "updated": "2024-02-11T20:15:52+00:00",
    "published": "2024-02-11T20:15:52+00:00",
    "title": "Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning",
    "authors": [
      {
        "name": "Mohak Chadha"
      },
      {
        "name": "Pulkit Khera"
      },
      {
        "name": "Jianfeng Gu"
      },
      {
        "name": "Osama Abboud"
      },
      {
        "name": "Michael Gerndt"
      }
    ],
    "summary": "Federated Learning (FL) is an emerging machine learning paradigm that enables\nthe collaborative training of a shared global model across distributed clients\nwhile keeping the data decentralized. Recent works on designing systems for\nefficient FL have shown that utilizing serverless computing technologies,\nparticularly Function-as-a-Service (FaaS) for FL, can enhance resource\nefficiency, reduce training costs, and alleviate the complex infrastructure\nmanagement burden on data holders. However, existing serverless FL systems\nimplicitly assume a uniform global model architecture across all participating\nclients during training. This assumption fails to address fundamental\nchallenges in practical FL due to the resource and statistical data\nheterogeneity among FL clients. To address these challenges and enable\nheterogeneous client models in serverless FL, we utilize Knowledge Distillation\n(KD) in this paper. Towards this, we propose novel optimized serverless\nworkflows for two popular conventional federated KD techniques, i.e., FedMD and\nFedDF. We implement these workflows by introducing several extensions to an\nopen-source serverless FL system called FedLess. Moreover, we comprehensively\nevaluate the two strategies on multiple datasets across varying levels of\nclient data heterogeneity using heterogeneous client models with respect to\naccuracy, fine-grained training times, and costs. Results from our experiments\ndemonstrate that serverless FedDF is more robust to extreme non-IID data\ndistributions, is faster, and leads to lower costs than serverless FedMD. In\naddition, compared to the original implementation, our optimizations for\nparticular steps in FedMD and FedDF lead to an average speedup of 3.5x and\n1.76x across all datasets.",
    "comment": "ACM/SIGAPP Symposium on Applied Computing 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07295v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07295v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07295v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07294v1",
    "updated": "2024-02-11T20:15:44+00:00",
    "published": "2024-02-11T20:15:44+00:00",
    "title": "On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study",
    "authors": [
      {
        "name": "Amir M. Mir"
      },
      {
        "name": "Mehdi Keshani"
      },
      {
        "name": "Sebastian Proksch"
      }
    ],
    "summary": "Static call graph (CG) construction often over-approximates call relations,\nleading to sound, but imprecise results. Recent research has explored machine\nlearning (ML)-based CG pruning as a means to enhance precision by eliminating\nfalse edges. However, current methods suffer from a limited evaluation dataset,\nimbalanced training data, and reduced recall, which affects practical\ndownstream analyses. Prior results were also not compared with advanced static\nCG construction techniques yet. This study tackles these issues. We introduce\nthe NYXCorpus, a dataset of real-world Java programs with high test coverage\nand we collect traces from test executions and build a ground truth of dynamic\nCGs. We leverage these CGs to explore conservative pruning strategies during\nthe training and inference of ML-based CG pruners. We conduct a comparative\nanalysis of static CGs generated using zero control flow analysis (0-CFA) and\nthose produced by a context-sensitive 1-CFA algorithm, evaluating both with and\nwithout pruning. We find that CG pruning is a difficult task for real-world\nJava projects and substantial improvements in the CG precision (+25%) meet\nreduced recall (-9%). However, our experiments show promising results: even\nwhen we favor recall over precision by using an F2 metric in our experiments,\nwe can show that pruned CGs have comparable quality to a context-sensitive\n1-CFA analysis while being computationally less demanding. Resulting CGs are\nmuch smaller (69%), and substantially faster (3.5x speed-up), with virtually\nunchanged results in our downstream analysis.",
    "comment": "Accepted at the technical track of MSR'24",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07294v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07294v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07294v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07284v1",
    "updated": "2024-02-11T19:16:01+00:00",
    "published": "2024-02-11T19:16:01+00:00",
    "title": "CLIPPER: Robust Data Association without an Initial Guess",
    "authors": [
      {
        "name": "Parker C. Lusk"
      },
      {
        "name": "Jonathan P. How"
      }
    ],
    "summary": "Identifying correspondences in noisy data is a critically important step in\nestimation processes. When an informative initial estimation guess is\navailable, the data association challenge is less acute; however, the existence\nof a high-quality initial guess is rare in most contexts. We explore\ngraph-theoretic formulations for data association, which do not require an\ninitial estimation guess. Existing graph-theoretic approaches optimize over\nunweighted graphs, discarding important consistency information encoded in\nweighted edges, and frequently attempt to solve NP-hard problems exactly. In\ncontrast, we formulate a new optimization problem that fully leverages weighted\ngraphs and seeks the densest edge-weighted clique. We introduce two relaxations\nto this problem: a convex semidefinite relaxation which we find to be\nempirically tight, and a fast first-order algorithm called CLIPPER which\nfrequently arrives at nearly-optimal solutions in milliseconds. When evaluated\non point cloud registration problems, our algorithms remain robust up to at\nleast 95% outliers while existing algorithms begin breaking down at 80%\noutliers. Code is available at https://mit-acl.github.io/clipper.",
    "comment": "8 pages, 4 figures, accepted to RA-L",
    "journal_ref": null,
    "doi": "10.1109/LRA.2024.3364842",
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/LRA.2024.3364842",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07284v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07284v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07284v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07283v1",
    "updated": "2024-02-11T19:14:28+00:00",
    "published": "2024-02-11T19:14:28+00:00",
    "title": "Power Transformer Fault Prediction Based on Knowledge Graphs",
    "authors": [
      {
        "name": "Chao Wang"
      },
      {
        "name": "Zhuo Chen"
      },
      {
        "name": "Ziyan Zhang"
      },
      {
        "name": "Chiyi Li"
      },
      {
        "name": "Kai Song"
      }
    ],
    "summary": "In this paper, we address the challenge of learning with limited fault data\nfor power transformers. Traditional operation and maintenance tools lack\neffective predictive capabilities for potential faults. The scarcity of\nextensive fault data makes it difficult to apply machine learning techniques\neffectively. To solve this problem, we propose a novel approach that leverages\nthe knowledge graph (KG) technology in combination with gradient boosting\ndecision trees (GBDT). This method is designed to efficiently learn from a\nsmall set of high-dimensional data, integrating various factors influencing\ntransformer faults and historical operational data. Our approach enables\naccurate safe state assessments and fault analyses of power transformers\ndespite the limited fault characteristic data. Experimental results demonstrate\nthat this method outperforms other learning approaches in prediction accuracy,\nsuch as artificial neural networks (ANN) and logistic regression (LR).\nFurthermore, it offers significant improvements in progressiveness,\npracticality, and potential for widespread application.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07283v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07283v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07283v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07282v2",
    "updated": "2024-02-13T14:21:02+00:00",
    "published": "2024-02-11T19:13:26+00:00",
    "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
    "authors": [
      {
        "name": "Ryan Liu"
      },
      {
        "name": "Theodore R. Sumers"
      },
      {
        "name": "Ishita Dasgupta"
      },
      {
        "name": "Thomas L. Griffiths"
      }
    ],
    "summary": "In day-to-day communication, people often approximate the truth - for\nexample, rounding the time or omitting details - in order to be maximally\nhelpful to the listener. How do large language models (LLMs) handle such\nnuanced trade-offs? To address this question, we use psychological models and\nexperiments designed to characterize human behavior to analyze LLMs. We test a\nrange of LLMs and explore how optimization for human preferences or\ninference-time reasoning affects these trade-offs. We find that reinforcement\nlearning from human feedback improves both honesty and helpfulness, while\nchain-of-thought prompting skews LLMs towards helpfulness over honesty.\nFinally, GPT-4 Turbo demonstrates human-like response patterns including\nsensitivity to the conversational framing and listener's decision context. Our\nfindings reveal the conversational values internalized by LLMs and suggest that\neven these abstract values can, to a degree, be steered by zero-shot prompting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07282v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07282v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07282v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07281v1",
    "updated": "2024-02-11T19:12:51+00:00",
    "published": "2024-02-11T19:12:51+00:00",
    "title": "Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study",
    "authors": [
      {
        "name": "Santonu Sarkar"
      },
      {
        "name": "Shanay Mehta"
      },
      {
        "name": "Nicole Fernandes"
      },
      {
        "name": "Jyotirmoy Sarkar"
      },
      {
        "name": "Snehanshu Saha"
      }
    ],
    "summary": "Detection of anomalous situations for complex mission-critical systems holds\nparamount importance when their service continuity needs to be ensured. A major\nchallenge in detecting anomalies from the operational data arises due to the\nimbalanced class distribution problem since the anomalies are supposed to be\nrare events. This paper evaluates a diverse array of machine learning-based\nanomaly detection algorithms through a comprehensive benchmark study. The paper\ncontributes significantly by conducting an unbiased comparison of various\nanomaly detection algorithms, spanning classical machine learning including\nvarious tree-based approaches to deep learning and outlier detection methods.\nThe inclusion of 104 publicly available and a few proprietary industrial\nsystems datasets enhances the diversity of the study, allowing for a more\nrealistic evaluation of algorithm performance and emphasizing the importance of\nadaptability to real-world scenarios. The paper dispels the deep learning myth,\ndemonstrating that though powerful, deep learning is not a universal solution\nin this case. We observed that recently proposed tree-based evolutionary\nalgorithms outperform in many scenarios. We noticed that tree-based approaches\ncatch a singleton anomaly in a dataset where deep learning methods fail. On the\nother hand, classical SVM performs the best on datasets with more than 10%\nanomalies, implying that such scenarios can be best modeled as a classification\nproblem rather than anomaly detection. To our knowledge, such a study on a\nlarge number of state-of-the-art algorithms using diverse data sets, with the\nobjective of guiding researchers and practitioners in making informed\nalgorithmic choices, has not been attempted earlier.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07281v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07281v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07281v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07270v1",
    "updated": "2024-02-11T18:26:18+00:00",
    "published": "2024-02-11T18:26:18+00:00",
    "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
    "authors": [
      {
        "name": "Simon Ging"
      },
      {
        "name": "Mar\u00eda A. Bravo"
      },
      {
        "name": "Thomas Brox"
      }
    ],
    "summary": "The evaluation of text-generative vision-language models is a challenging yet\ncrucial endeavor. By addressing the limitations of existing Visual Question\nAnswering (VQA) benchmarks and proposing innovative evaluation methodologies,\nour research seeks to advance our understanding of these models' capabilities.\nWe propose a novel VQA benchmark based on well-known visual classification\ndatasets which allows a granular evaluation of text-generative vision-language\nmodels and their comparison with discriminative vision-language models. To\nimprove the assessment of coarse answers on fine-grained classification tasks,\nwe suggest using the semantic hierarchy of the label space to ask automatically\ngenerated follow-up questions about the ground-truth category. Finally, we\ncompare traditional NLP and LLM-based metrics for the problem of evaluating\nmodel predictions given ground-truth answers. We perform a human evaluation\nstudy upon which we base our decision on the final metric. We apply our\nbenchmark to a suite of vision-language models and show a detailed comparison\nof their abilities on object, action, and attribute classification. Our\ncontributions aim to lay the foundation for more precise and meaningful\nassessments, facilitating targeted progress in the exciting field of\nvision-language modeling.",
    "comment": "Accepted as Spotlight Paper for ICLR 2024. The first two authors\n  contributed equally to this work",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07270v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07270v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07270v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07268v1",
    "updated": "2024-02-11T18:23:54+00:00",
    "published": "2024-02-11T18:23:54+00:00",
    "title": "Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer",
    "authors": [
      {
        "name": "Zehao Dong"
      },
      {
        "name": "Qihang Zhao"
      },
      {
        "name": "Philip R. O. Payne"
      },
      {
        "name": "Michael A Province"
      },
      {
        "name": "Carlos Cruchaga"
      },
      {
        "name": "Muhan Zhang"
      },
      {
        "name": "Tianyu Zhao"
      },
      {
        "name": "Yixin Chen"
      },
      {
        "name": "Fuhai Li"
      }
    ],
    "summary": "Biomarker identification is critical for precise disease diagnosis and\nunderstanding disease pathogenesis in omics data analysis, like using fold\nchange and regression analysis. Graph neural networks (GNNs) have been the\ndominant deep learning model for analyzing graph-structured data. However, we\nfound two major limitations of existing GNNs in omics data analysis, i.e.,\nlimited-prediction (diagnosis) accuracy and limited-reproducible biomarker\nidentification capacity across multiple datasets. The root of the challenges is\nthe unique graph structure of biological signaling pathways, which consists of\na large number of targets and intensive and complex signaling interactions\namong these targets. To resolve these two challenges, in this study, we\npresented a novel GNN model architecture, named PathFormer, which\nsystematically integrate signaling network, priori knowledge and omics data to\nrank biomarkers and predict disease diagnosis. In the comparison results,\nPathFormer outperformed existing GNN models significantly in terms of highly\naccurate prediction capability ( 30% accuracy improvement in disease diagnosis\ncompared with existing GNN models) and high reproducibility of biomarker\nranking across different datasets. The improvement was confirmed using two\nindependent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The\nPathFormer model can be directly applied to other omics data analysis studies.",
    "comment": null,
    "journal_ref": null,
    "doi": "10.21203/rs.3.rs-3576068/v1",
    "primary_category": "q-bio.GN",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.21203/rs.3.rs-3576068/v1",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07268v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07268v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07268v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07251v1",
    "updated": "2024-02-11T17:40:26+00:00",
    "published": "2024-02-11T17:40:26+00:00",
    "title": "Physics-Informed Neural Networks with Hard Linear Equality Constraints",
    "authors": [
      {
        "name": "Hao Chen"
      },
      {
        "name": "Gonzalo E. Constante Flores"
      },
      {
        "name": "Can Li"
      }
    ],
    "summary": "Surrogate modeling is used to replace computationally expensive simulations.\nNeural networks have been widely applied as surrogate models that enable\nefficient evaluations over complex physical systems. Despite this, neural\nnetworks are data-driven models and devoid of any physics. The incorporation of\nphysics into neural networks can improve generalization and data efficiency.\nThe physics-informed neural network (PINN) is an approach to leverage known\nphysical constraints present in the data, but it cannot strictly satisfy them\nin the predictions. This work proposes a novel physics-informed neural network,\nKKT-hPINN, which rigorously guarantees hard linear equality constraints through\nprojection layers derived from KKT conditions. Numerical experiments on Aspen\nmodels of a continuous stirred-tank reactor (CSTR) unit, an extractive\ndistillation subsystem, and a chemical plant demonstrate that this model can\nfurther enhance the prediction accuracy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07251v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07251v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07251v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07250v1",
    "updated": "2024-02-11T17:32:23+00:00",
    "published": "2024-02-11T17:32:23+00:00",
    "title": "DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains",
    "authors": [
      {
        "name": "Minglang Yin"
      },
      {
        "name": "Nicolas Charon"
      },
      {
        "name": "Ryan Brody"
      },
      {
        "name": "Lu Lu"
      },
      {
        "name": "Natalia Trayanova"
      },
      {
        "name": "Mauro Maggioni"
      }
    ],
    "summary": "The solution of a PDE over varying initial/boundary conditions on multiple\ndomains is needed in a wide variety of applications, but it is computationally\nexpensive if the solution is computed de novo whenever the initial/boundary\nconditions of the domain change. We introduce a general operator learning\nframework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn\napproximate PDE solutions over a family of domains $\\{\\Omega_{\\theta}}_\\theta$,\nthat learns the map from initial/boundary conditions and domain $\\Omega_\\theta$\nto the solution of the PDE, or to specified functionals thereof. DIMON is based\non transporting a given problem (initial/boundary conditions and domain\n$\\Omega_{\\theta}$) to a problem on a reference domain $\\Omega_{0}$, where\ntraining data from multiple problems is used to learn the map to the solution\non $\\Omega_{0}$, which is then re-mapped to the original domain\n$\\Omega_{\\theta}$. We consider several problems to demonstrate the performance\nof the framework in learning both static and time-dependent PDEs on non-rigid\ngeometries; these include solving the Laplace equation, reaction-diffusion\nequations, and a multiscale PDE that characterizes the electrical propagation\non the left ventricle. This work paves the way toward the fast prediction of\nPDE solutions on a family of domains and the application of neural operators in\nengineering and precision medicine.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07250v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07250v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07250v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07249v1",
    "updated": "2024-02-11T17:29:58+00:00",
    "published": "2024-02-11T17:29:58+00:00",
    "title": "The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey",
    "authors": [
      {
        "name": "Taojie Kuang"
      },
      {
        "name": "Pengfei Liu"
      },
      {
        "name": "Zhixiang Ren"
      }
    ],
    "summary": "The precise prediction of molecular properties is essential for advancements\nin drug development, particularly in virtual screening and compound\noptimization. The recent introduction of numerous deep learning-based methods\nhas shown remarkable potential in enhancing molecular property prediction\n(MPP), especially improving accuracy and insights into molecular structures.\nYet, two critical questions arise: does the integration of domain knowledge\naugment the accuracy of molecular property prediction and does employing\nmulti-modal data fusion yield more precise results than unique data source\nmethods? To explore these matters, we comprehensively review and quantitatively\nanalyze recent deep learning methods based on various benchmarks. We discover\nthat integrating molecular information will improve both MPP regression and\nclassification tasks by upto 3.98% and 1.72%, respectively. We also discover\nthat the utilizing 3-dimensional information with 1-dimensional and\n2-dimensional information simultaneously can substantially enhance MPP upto\n4.2%. The two consolidated insights offer crucial guidance for future\nadvancements in drug discovery.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CE",
      "q-bio.BM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07249v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07249v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07249v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07248v1",
    "updated": "2024-02-11T17:27:26+00:00",
    "published": "2024-02-11T17:27:26+00:00",
    "title": "Depth Separations in Neural Networks: Separating the Dimension from the Accuracy",
    "authors": [
      {
        "name": "Itay Safran"
      },
      {
        "name": "Daniel Reichman"
      },
      {
        "name": "Paul Valiant"
      }
    ],
    "summary": "We prove an exponential separation between depth 2 and depth 3 neural\nnetworks, when approximating an $\\mathcal{O}(1)$-Lipschitz target function to\nconstant accuracy, with respect to a distribution with support in $[0,1]^{d}$,\nassuming exponentially bounded weights. This addresses an open problem posed in\n\\citet{safran2019depth}, and proves that the curse of dimensionality manifests\nin depth 2 approximation, even in cases where the target function can be\nrepresented efficiently using depth 3. Previously, lower bounds that were used\nto separate depth 2 from depth 3 required that at least one of the Lipschitz\nparameter, target accuracy or (some measure of) the size of the domain of\napproximation scale polynomially with the input dimension, whereas we fix the\nformer two and restrict our domain to the unit hypercube. Our lower bound holds\nfor a wide variety of activation functions, and is based on a novel application\nof an average- to worst-case random self-reducibility argument, to reduce the\nproblem to threshold circuits lower bounds.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07248v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07248v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07248v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07246v1",
    "updated": "2024-02-11T17:10:31+00:00",
    "published": "2024-02-11T17:10:31+00:00",
    "title": "Towards Generalized Inverse Reinforcement Learning",
    "authors": [
      {
        "name": "Chaosheng Dong"
      },
      {
        "name": "Yijia Wang"
      }
    ],
    "summary": "This paper studies generalized inverse reinforcement learning (GIRL) in\nMarkov decision processes (MDPs), that is, the problem of learning the basic\ncomponents of an MDP given observed behavior (policy) that might not be\noptimal. These components include not only the reward function and transition\nprobability matrices, but also the action space and state space that are not\nexactly known but are known to belong to given uncertainty sets. We address two\nkey challenges in GIRL: first, the need to quantify the discrepancy between the\nobserved policy and the underlying optimal policy; second, the difficulty of\nmathematically characterizing the underlying optimal policy when the basic\ncomponents of an MDP are unobservable or partially observable. Then, we propose\nthe mathematical formulation for GIRL and develop a fast heuristic algorithm.\nNumerical results on both finite and infinite state problems show the merit of\nour formulation and algorithm.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07246v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07246v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07246v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07244v1",
    "updated": "2024-02-11T16:58:59+00:00",
    "published": "2024-02-11T16:58:59+00:00",
    "title": "SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm",
    "authors": [
      {
        "name": "Junhao Song"
      },
      {
        "name": "Yingfang Yuan"
      },
      {
        "name": "Wei Pang"
      }
    ],
    "summary": "We propose a novel type of Artificial Immune System (AIS): Symbiotic\nArtificial Immune Systems (SAIS), drawing inspiration from symbiotic\nrelationships in biology. SAIS parallels the three key stages (i.e., mutualism,\ncommensalism and parasitism) of population updating from the Symbiotic\nOrganisms Search (SOS) algorithm. This parallel approach effectively addresses\nthe challenges of large population size and enhances population diversity in\nAIS, which traditional AIS and SOS struggle to resolve efficiently. We\nconducted a series of experiments, which demonstrated that our SAIS achieved\ncomparable performance to the state-of-the-art approach SOS and outperformed\nother popular AIS approaches and evolutionary algorithms across 26 benchmark\nproblems. Furthermore, we investigated the problem of parameter selection and\nfound that SAIS performs better in handling larger population sizes while\nrequiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired\nand immune-inspired algorithm, paves the way for innovation in bio-inspired\ncomputing with the symbiotic paradigm.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.NE",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07244v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07244v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07244v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07234v1",
    "updated": "2024-02-11T15:56:03+00:00",
    "published": "2024-02-11T15:56:03+00:00",
    "title": "CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain",
    "authors": [
      {
        "name": "Xin Tong"
      },
      {
        "name": "Bo Jin"
      },
      {
        "name": "Zhi Lin"
      },
      {
        "name": "Binjun Wang"
      },
      {
        "name": "Ting Yu"
      }
    ],
    "summary": "Large Language Models (LLMs) have demonstrated significant potential and\neffectiveness across multiple application domains. To assess the performance of\nmainstream LLMs in public security tasks, this study aims to construct a\nspecialized evaluation benchmark tailored to the Chinese public security\ndomain--CPSDbench. CPSDbench integrates datasets related to public security\ncollected from real-world scenarios, supporting a comprehensive assessment of\nLLMs across four key dimensions: text classification, information extraction,\nquestion answering, and text generation. Furthermore, this study introduces a\nset of innovative evaluation metrics designed to more precisely quantify the\nefficacy of LLMs in executing tasks related to public security. Through the\nin-depth analysis and evaluation conducted in this research, we not only\nenhance our understanding of the performance strengths and limitations of\nexisting models in addressing public security issues but also provide\nreferences for the future development of more accurate and customized LLM\nmodels targeted at applications in this field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07234v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07234v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07234v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07233v1",
    "updated": "2024-02-11T15:50:35+00:00",
    "published": "2024-02-11T15:50:35+00:00",
    "title": "TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation",
    "authors": [
      {
        "name": "Peng Wang"
      },
      {
        "name": "Xiang Wei"
      },
      {
        "name": "Fangxu Hu"
      },
      {
        "name": "Wenjuan Han"
      }
    ],
    "summary": "Natural language processing (NLP) is a key component of intelligent\ntransportation systems (ITS), but it faces many challenges in the\ntransportation domain, such as domain-specific knowledge and data, and\nmulti-modal inputs and outputs. This paper presents TransGPT, a novel\n(multi-modal) large language model for the transportation domain, which\nconsists of two independent variants: TransGPT-SM for single-modal data and\nTransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal\nTransportation dataset (STD) that contains textual data from various sources in\nthe transportation domain. TransGPT-MM is finetuned on a multi-modal\nTransportation dataset (MTD) that we manually collected from three areas of the\ntransportation domain: driving tests, traffic signs, and landmarks. We evaluate\nTransGPT on several benchmark datasets for different tasks in the\ntransportation domain, and show that it outperforms baseline models on most\ntasks. We also showcase the potential applications of TransGPT for traffic\nanalysis and modeling, such as generating synthetic traffic scenarios,\nexplaining traffic phenomena, answering traffic-related questions, providing\ntraffic recommendations, and generating traffic reports. This work advances the\nstate-of-the-art of NLP in the transportation domain and provides a useful tool\nfor ITS researchers and practitioners.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07233v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07233v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07233v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07232v1",
    "updated": "2024-02-11T15:49:50+00:00",
    "published": "2024-02-11T15:49:50+00:00",
    "title": "GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains",
    "authors": [
      {
        "name": "Yan Lin"
      },
      {
        "name": "Jilin Hu"
      },
      {
        "name": "Shengnan Guo"
      },
      {
        "name": "Bin Yang"
      },
      {
        "name": "Christian S. Jensen"
      },
      {
        "name": "Youfang Lin"
      },
      {
        "name": "Huaiyu Wan"
      }
    ],
    "summary": "Trajectories are sequences of timestamped location samples. In sparse\ntrajectories, the locations are sampled infrequently; and while such\ntrajectories are prevalent in real-world settings, they are challenging to use\nto enable high-quality transportation-related applications. Current\nmethodologies either assume densely sampled and accurately map-matched\ntrajectories, or they rely on two-stage schemes, yielding sub-optimal\napplications.\n  To extend the utility of sparse trajectories, we propose a novel sparse\ntrajectory learning framework, GenSTL. The framework is pre-trained to form\nconnections between sparse trajectories and dense counterparts using\nauto-regressive generation of feature domains. GenSTL can subsequently be\napplied directly in downstream tasks, or it can be fine-tuned first. This way,\nGenSTL eliminates the reliance on the availability of large-scale dense and\nmap-matched trajectory data. The inclusion of a well-crafted feature domain\nencoding layer and a hierarchical masked trajectory encoder enhances GenSTL's\nlearning capabilities and adaptability. Experiments on two real-world\ntrajectory datasets offer insight into the framework's ability to contend with\nsparse trajectories with different sampling intervals and its versatility\nacross different downstream tasks, thus offering evidence of its practicality\nin real-world applications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07232v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07232v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07232v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07229v1",
    "updated": "2024-02-11T15:36:33+00:00",
    "published": "2024-02-11T15:36:33+00:00",
    "title": "Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications",
    "authors": [
      {
        "name": "Homa Esfahanizadeh"
      },
      {
        "name": "Alejandro Cohen"
      },
      {
        "name": "Shlomo Shamai"
      },
      {
        "name": "Muriel Medard"
      }
    ],
    "summary": "Modern computationally-intensive applications often operate under time\nconstraints, necessitating acceleration methods and distribution of\ncomputational workloads across multiple entities. However, the outcome is\neither achieved within the desired timeline or not, and in the latter case,\nvaluable resources are wasted. In this paper, we introduce solutions for\nlayered-resolution computation. These solutions allow lower-resolution results\nto be obtained at an earlier stage than the final result. This innovation\nnotably enhances the deadline-based systems, as if a computational job is\nterminated due to time constraints, an approximate version of the final result\ncan still be generated. Moreover, in certain operational regimes, a\nhigh-resolution result might be unnecessary, because the low-resolution result\nmay already deviate significantly from the decision threshold, for example in\nAI-based decision-making systems. Therefore, operators can decide whether\nhigher resolution is needed or not based on intermediate results, enabling\ncomputations with adaptive resolution. We present our framework for two\ncritical and computationally demanding jobs: distributed matrix multiplication\n(linear) and model inference in machine learning (nonlinear). Our theoretical\nand empirical results demonstrate that the execution delay for the first\nresolution is significantly shorter than that for the final resolution, while\nmaintaining overall complexity comparable to the conventional one-shot\napproach. Our experiments further illustrate how the layering feature increases\nthe likelihood of meeting deadlines and enables adaptability and transparency\nin massive, large-scale computations.",
    "comment": "13 pages, partially appeared in proceedings of IEEE Cloudnet 2022,\n  submitted and under review for IEEE Transactions on Signal Processing",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IT",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07229v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07229v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07229v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07226v1",
    "updated": "2024-02-11T15:23:13+00:00",
    "published": "2024-02-11T15:23:13+00:00",
    "title": "Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL",
    "authors": [
      {
        "name": "Sungyoon Kim"
      },
      {
        "name": "Yunseon Choi"
      },
      {
        "name": "Daiki E. Matsunaga"
      },
      {
        "name": "Kee-Eung Kim"
      }
    ],
    "summary": "Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an\nimportant problem in RL that focuses on acquiring diverse goal-oriented skills\nsolely from pre-collected behavior datasets. In this setting, the reward\nfeedback is typically absent except when the goal is achieved, which makes it\ndifficult to learn policies especially from a finite dataset of suboptimal\nbehaviors. In addition, realistic scenarios involve long-horizon planning,\nwhich necessitates the extraction of useful skills within sub-trajectories.\nRecently, the conditional diffusion model has been shown to be a promising\napproach to generate high-quality long-horizon plans for RL. However, their\npracticality for the goal-conditioned setting is still limited due to a number\nof technical assumptions made by the methods. In this paper, we propose SSD\n(Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method\nthat leverages the conditional diffusion model to address these limitations. In\nsummary, we use the diffusion model that generates future plans conditioned on\nthe target goal and value, with the target value estimated from the\ngoal-relabeled offline dataset. We report state-of-the-art performance in the\nstandard benchmark set of GCRL tasks, and demonstrate the capability to\nsuccessfully stitch the segments of suboptimal trajectories in the offline data\nto generate high-quality plans.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07226v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07226v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07226v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07225v1",
    "updated": "2024-02-11T15:21:08+00:00",
    "published": "2024-02-11T15:21:08+00:00",
    "title": "Rethinking Graph Masked Autoencoders through Alignment and Uniformity",
    "authors": [
      {
        "name": "Liang Wang"
      },
      {
        "name": "Xiang Tao"
      },
      {
        "name": "Qiang Liu"
      },
      {
        "name": "Shu Wu"
      },
      {
        "name": "Liang Wang"
      }
    ],
    "summary": "Self-supervised learning on graphs can be bifurcated into contrastive and\ngenerative methods. Contrastive methods, also known as graph contrastive\nlearning (GCL), have dominated graph self-supervised learning in the past few\nyears, but the recent advent of graph masked autoencoder (GraphMAE) rekindles\nthe momentum behind generative methods. Despite the empirical success of\nGraphMAE, there is still a dearth of theoretical understanding regarding its\nefficacy. Moreover, while both generative and contrastive methods have been\nshown to be effective, their connections and differences have yet to be\nthoroughly investigated. Therefore, we theoretically build a bridge between\nGraphMAE and GCL, and prove that the node-level reconstruction objective in\nGraphMAE implicitly performs context-level GCL. Based on our theoretical\nanalysis, we further identify the limitations of the GraphMAE from the\nperspectives of alignment and uniformity, which have been considered as two key\nproperties of high-quality representations in GCL. We point out that GraphMAE's\nalignment performance is restricted by the masking strategy, and the uniformity\nis not strictly guaranteed. To remedy the aforementioned limitations, we\npropose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named\nAUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy\nto provide hard-to-align samples, which improves the alignment performance.\nMeanwhile, we introduce an explicit uniformity regularizer to ensure the\nuniformity of the learned representations. Experimental results on benchmark\ndatasets demonstrate the superiority of our model over existing\nstate-of-the-art methods.",
    "comment": "Accepted by AAAI 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07225v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07225v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07225v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07221v1",
    "updated": "2024-02-11T14:39:40+00:00",
    "published": "2024-02-11T14:39:40+00:00",
    "title": "The Reasons that Agents Act: Intention and Instrumental Goals",
    "authors": [
      {
        "name": "Francis Rhys Ward"
      },
      {
        "name": "Matt MacDermott"
      },
      {
        "name": "Francesco Belardinelli"
      },
      {
        "name": "Francesca Toni"
      },
      {
        "name": "Tom Everitt"
      }
    ],
    "summary": "Intention is an important and challenging concept in AI. It is important\nbecause it underlies many other concepts we care about, such as agency,\nmanipulation, legal responsibility, and blame. However, ascribing intent to AI\nsystems is contentious, and there is no universally accepted theory of\nintention applicable to AI agents. We operationalise the intention with which\nan agent acts, relating to the reasons it chooses its decision. We introduce a\nformal definition of intention in structural causal influence models, grounded\nin the philosophy literature on intent and applicable to real-world machine\nlearning systems. Through a number of examples and results, we show that our\ndefinition captures the intuitive notion of intent and satisfies desiderata\nset-out by past work. In addition, we show how our definition relates to past\nconcepts, including actual causality, and the notion of instrumental goals,\nwhich is a core idea in the literature on safe AI agents. Finally, we\ndemonstrate how our definition can be used to infer the intentions of\nreinforcement learning agents and language models from their behaviour.",
    "comment": "AAMAS24",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07221v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07221v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07221v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07211v2",
    "updated": "2024-02-13T07:14:24+00:00",
    "published": "2024-02-11T14:04:13+00:00",
    "title": "Towards Fast Stochastic Sampling in Diffusion Generative Models",
    "authors": [
      {
        "name": "Kushagra Pandey"
      },
      {
        "name": "Maja Rudolph"
      },
      {
        "name": "Stephan Mandt"
      }
    ],
    "summary": "Diffusion models suffer from slow sample generation at inference time.\nDespite recent efforts, improving the sampling efficiency of stochastic\nsamplers for diffusion models remains a promising direction. We propose\nSplitting Integrators for fast stochastic sampling in pre-trained diffusion\nmodels in augmented spaces. Commonly used in molecular dynamics,\nsplitting-based integrators attempt to improve sampling efficiency by cleverly\nalternating between numerical updates involving the data, auxiliary, or noise\nvariables. However, we show that a naive application of splitting integrators\nis sub-optimal for fast sampling. Consequently, we propose several principled\nmodifications to naive splitting samplers for improving sampling efficiency and\ndenote the resulting samplers as Reduced Splitting Integrators. In the context\nof Phase Space Langevin Diffusion (PSLD) [Pandey \\& Mandt, 2023] on CIFAR-10,\nour stochastic sampler achieves an FID score of 2.36 in only 100 network\nfunction evaluations (NFE) as compared to 2.63 for the best baselines.",
    "comment": "Accepted in the NeurIPS'23 Workshop on Diffusion Models. Full version\n  of this work can be found at arXiv:2310.07894",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07211v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07211v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07211v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07204v1",
    "updated": "2024-02-11T13:30:53+00:00",
    "published": "2024-02-11T13:30:53+00:00",
    "title": "Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning",
    "authors": [
      {
        "name": "Yihong Tang"
      },
      {
        "name": "Zhaokai Wang"
      },
      {
        "name": "Ao Qu"
      },
      {
        "name": "Yihao Yan"
      },
      {
        "name": "Kebing Hou"
      },
      {
        "name": "Dingyi Zhuang"
      },
      {
        "name": "Xiaotong Guo"
      },
      {
        "name": "Jinhua Zhao"
      },
      {
        "name": "Zhan Zhao"
      },
      {
        "name": "Wei Ma"
      }
    ],
    "summary": "In this paper, we for the first time propose the task of Open-domain Urban\nItinerary Planning (OUIP) for citywalk, which directly generates itineraries\nbased on users' requests described in natural language. OUIP is different from\nconventional itinerary planning, which limits users from expressing more\ndetailed needs and hinders true personalization. Recently, large language\nmodels (LLMs) have shown potential in handling diverse tasks. However, due to\nnon-real-time information, incomplete knowledge, and insufficient spatial\nawareness, they are unable to independently deliver a satisfactory user\nexperience in OUIP. Given this, we present ItiNera, an OUIP system that\nsynergizes spatial optimization with Large Language Models (LLMs) to provide\nservices that customize urban itineraries based on users' needs. Specifically,\nwe develop an LLM-based pipeline for extracting and updating POI features to\ncreate a user-owned personalized POI database. For each user request, we\nleverage LLM in cooperation with an embedding-based module for retrieving\ncandidate POIs from the user's POI database. Then, a spatial optimization\nmodule is used to order these POIs, followed by LLM crafting a personalized,\nspatially coherent itinerary. To the best of our knowledge, this study marks\nthe first integration of LLMs to innovate itinerary planning solutions.\nExtensive experiments on offline datasets and online subjective evaluation have\ndemonstrated the capacities of our system to deliver more responsive and\nspatially coherent itineraries than current LLM-based solutions. Our system has\nbeen deployed in production at the TuTu online travel service and has attracted\nthousands of users for their urban travel planning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07204v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07204v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07204v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07200v1",
    "updated": "2024-02-11T13:26:40+00:00",
    "published": "2024-02-11T13:26:40+00:00",
    "title": "Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks",
    "authors": [
      {
        "name": "Muqun Niu"
      },
      {
        "name": "Yuan Ren"
      },
      {
        "name": "Boyu Li"
      },
      {
        "name": "Chenchen Ding"
      }
    ],
    "summary": "Lightweight design of Convolutional Neural Networks (CNNs) requires co-design\nefforts in the model architectures and compression techniques. As a novel\ndesign paradigm that separates training and inference, a structural\nre-parameterized (SR) network such as the representative RepVGG revitalizes the\nsimple VGG-like network with a high accuracy comparable to advanced and often\nmore complicated networks. However, the merging process in SR networks\nintroduces outliers into weights, making their distribution distinct from\nconventional networks and thus heightening difficulties in quantization. To\naddress this, we propose an operator-level improvement for training called\nOutlier Aware Batch Normalization (OABN). Additionally, to meet the demands of\nlimited bitwidths while upkeeping the inference accuracy, we develop a\nclustering-based non-uniform quantization framework for Quantization-Aware\nTraining (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the\nquantized performance of RepVGG is largely enhanced, particularly when the\nbitwidth falls below 8.",
    "comment": "8 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07200v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07200v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07200v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07199v1",
    "updated": "2024-02-11T13:26:06+00:00",
    "published": "2024-02-11T13:26:06+00:00",
    "title": "Link-aware link prediction over temporal graph by pattern recognition",
    "authors": [
      {
        "name": "Bingqing Liu"
      },
      {
        "name": "Xikun Huang"
      }
    ],
    "summary": "A temporal graph can be considered as a stream of links, each of which\nrepresents an interaction between two nodes at a certain time. On temporal\ngraphs, link prediction is a common task, which aims to answer whether the\nquery link is true or not. To do this task, previous methods usually focus on\nthe learning of representations of the two nodes in the query link. We point\nout that the learned representation by their models may encode too much\ninformation with side effects for link prediction because they have not\nutilized the information of the query link, i.e., they are link-unaware. Based\non this observation, we propose a link-aware model: historical links and the\nquery link are input together into the following model layers to distinguish\nwhether this input implies a reasonable pattern that ends with the query link.\nDuring this process, we focus on the modeling of link evolution patterns rather\nthan node representations. Experiments on six datasets show that our model\nachieves strong performances compared with state-of-the-art baselines, and the\nresults of link prediction are interpretable. The code and datasets are\navailable on the project website: https://github.com/lbq8942/TGACN.",
    "comment": "12 pages, one column",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07199v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07199v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07199v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07198v1",
    "updated": "2024-02-11T13:25:53+00:00",
    "published": "2024-02-11T13:25:53+00:00",
    "title": "More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning",
    "authors": [
      {
        "name": "Kaiwen Wang"
      },
      {
        "name": "Owen Oertell"
      },
      {
        "name": "Alekh Agarwal"
      },
      {
        "name": "Nathan Kallus"
      },
      {
        "name": "Wen Sun"
      }
    ],
    "summary": "In this paper, we prove that Distributional Reinforcement Learning (DistRL),\nwhich learns the return distribution, can obtain second-order bounds in both\nonline and offline RL in general settings with function approximation.\nSecond-order bounds are instance-dependent bounds that scale with the variance\nof return, which we prove are tighter than the previously known small-loss\nbounds of distributional RL. To the best of our knowledge, our results are the\nfirst second-order bounds for low-rank MDPs and for offline RL. When\nspecializing to contextual bandits (one-step RL problem), we show that a\ndistributional learning based optimism algorithm achieves a second-order\nworst-case regret bound, and a second-order gap dependent bound,\nsimultaneously. We also empirically demonstrate the benefit of DistRL in\ncontextual bandits on real-world datasets. We highlight that our analysis with\nDistRL is relatively simple, follows the general framework of optimism in the\nface of uncertainty and does not require weighted regression. Our results\nsuggest that DistRL is a promising framework for obtaining second-order bounds\nin general RL settings, thus further reinforcing the benefits of DistRL.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07198v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07198v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07198v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07197v2",
    "updated": "2024-02-13T09:25:37+00:00",
    "published": "2024-02-11T13:24:13+00:00",
    "title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
    "authors": [
      {
        "name": "Mengmei Zhang"
      },
      {
        "name": "Mingwei Sun"
      },
      {
        "name": "Peng Wang"
      },
      {
        "name": "Shen Fan"
      },
      {
        "name": "Yanhu Mo"
      },
      {
        "name": "Xiaoxiao Xu"
      },
      {
        "name": "Hong Liu"
      },
      {
        "name": "Cheng Yang"
      },
      {
        "name": "Chuan Shi"
      }
    ],
    "summary": "Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and\ninstruction-following capabilities, have catalyzed a revolutionary\ntransformation across diverse research fields of artificial intelligence,\nespecially for open-ended tasks. While the idea is less explored in the graph\ndomain, despite the availability of numerous powerful graph models (GMs), they\nare restricted to tasks in a pre-defined form. Although several methods\napplying LLMs to graphs have been proposed, they fail to simultaneously handle\nthe pre-defined and open-ended tasks, with LLM as a node feature enhancer or as\na standalone predictor. To break this dilemma, we propose to bridge the\npretrained GM and LLM by a Translator, named GraphTranslator, aiming to\nleverage GM to handle the pre-defined tasks effectively and utilize the\nextended interface of LLMs to offer various open-ended tasks for GM. To train\nsuch Translator, we propose a Producer capable of constructing the graph-text\nalignment data along node information, neighbor information and model\ninformation. By treating the node representation as a type of language, the\nproposed GraphTranslator empowers an LLM to make predictions based on node\nrepresentation and language instructions, providing a unified perspective for\nboth pre-defined and open-ended tasks. Extensive results show that the proposed\nGraphTranslator effectively improves the results of zero-shot node\nclassification. The graph question answering experiments reveal our\nGraphTranslator potential across a broad spectrum of open-ended applications\nthrough language instructions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07197v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07197v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07197v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07193v1",
    "updated": "2024-02-11T13:00:04+00:00",
    "published": "2024-02-11T13:00:04+00:00",
    "title": "The Implicit Bias of Gradient Noise: A Symmetry Perspective",
    "authors": [
      {
        "name": "Liu Ziyin"
      },
      {
        "name": "Mingze Wang"
      },
      {
        "name": "Lei Wu"
      }
    ],
    "summary": "We characterize the learning dynamics of stochastic gradient descent (SGD)\nwhen continuous symmetry exists in the loss function, where the divergence\nbetween SGD and gradient descent is dramatic. We show that depending on how the\nsymmetry affects the learning dynamics, we can divide a family of symmetry into\ntwo classes. For one class of symmetry, SGD naturally converges to solutions\nthat have a balanced and aligned gradient noise. For the other class of\nsymmetry, SGD will almost always diverge. Then, we show that our result remains\napplicable and can help us understand the training dynamics even when the\nsymmetry is not present in the loss function. Our main result is universal in\nthe sense that it only depends on the existence of the symmetry and is\nindependent of the details of the loss function. We demonstrate that the\nproposed theory offers an explanation of progressive sharpening and flattening\nand can be applied to common practical problems such as representation\nnormalization, matrix factorization, and the use of warmup.",
    "comment": "preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07193v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07193v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07193v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07191v1",
    "updated": "2024-02-11T12:57:16+00:00",
    "published": "2024-02-11T12:57:16+00:00",
    "title": "GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention",
    "authors": [
      {
        "name": "Fangyu Ding"
      },
      {
        "name": "Haiyang Wang"
      },
      {
        "name": "Zhixuan Chu"
      },
      {
        "name": "Tianming Li"
      },
      {
        "name": "Zhaoping Hu"
      },
      {
        "name": "Junchi Yan"
      }
    ],
    "summary": "Graph invariant learning (GIL) has been an effective approach to discovering\nthe invariant relationships between graph data and its labels for different\ngraph learning tasks under various distribution shifts. Many recent endeavors\nof GIL focus on extracting the invariant subgraph from the input graph for\nprediction as a regularization strategy to improve the generalization\nperformance of graph learning. Despite their success, such methods also have\nvarious limitations in obtaining their invariant subgraphs. In this paper, we\nprovide in-depth analyses of the drawbacks of existing works and propose\ncorresponding principles of our invariant subgraph extraction: 1) the sparsity,\nto filter out the variant features, 2) the softness, for a broader solution\nspace, and 3) the differentiability, for a soundly end-to-end optimization. To\nmeet these principles in one shot, we leverage the Optimal Transport (OT)\ntheory and propose a novel graph attention mechanism called Graph Sinkhorn\nAttention (GSINA). This novel approach serves as a powerful regularization\nmethod for GIL tasks. By GSINA, we are able to obtain meaningful,\ndifferentiable invariant subgraphs with controllable sparsity and softness.\nMoreover, GSINA is a general graph learning framework that could handle GIL\ntasks of multiple data grain levels. Extensive experiments on both synthetic\nand real-world datasets validate the superiority of our GSINA, which\noutperforms the state-of-the-art GIL methods by large margins on both\ngraph-level tasks and node-level tasks. Our code is publicly available at\n\\url{https://github.com/dingfangyu/GSINA}.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07191v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07191v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07191v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07189v1",
    "updated": "2024-02-11T12:54:07+00:00",
    "published": "2024-02-11T12:54:07+00:00",
    "title": "Improving LSH via Tensorized Random Projection",
    "authors": [
      {
        "name": "Bhisham Dev Verma"
      },
      {
        "name": "Rameshwar Pratap"
      }
    ],
    "summary": "Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by\ndata scientists for approximate nearest neighbour search problems that have\nbeen used extensively in many large scale data processing applications such as\nnear duplicate detection, nearest neighbour search, clustering, etc. In this\nwork, we aim to propose faster and space efficient locality sensitive hash\nfunctions for Euclidean distance and cosine similarity for tensor data.\nTypically, the naive approach for obtaining LSH for tensor data involves first\nreshaping the tensor into vectors, followed by applying existing LSH methods\nfor vector data $E2LSH$ and $SRP$. However, this approach becomes impractical\nfor higher order tensors because the size of the reshaped vector becomes\nexponential in the order of the tensor. Consequently, the size of LSH\nparameters increases exponentially. To address this problem, we suggest two\nmethods for LSH for Euclidean distance and cosine similarity, namely\n$CP-E2LSH$, $TT-E2LSH$, and $CP-SRP$, $TT-SRP$, respectively, building on $CP$\nand tensor train $(TT)$ decompositions techniques. Our approaches are space\nefficient and can be efficiently applied to low rank $CP$ or $TT$ tensors. We\nprovide a rigorous theoretical analysis of our proposal on their correctness\nand efficacy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.DS",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07189v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07189v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07189v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07183v1",
    "updated": "2024-02-11T12:35:28+00:00",
    "published": "2024-02-11T12:35:28+00:00",
    "title": "A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense",
    "authors": [
      {
        "name": "Ryota Iijima"
      },
      {
        "name": "Sayaka Shiota"
      },
      {
        "name": "Hitoshi Kiya"
      }
    ],
    "summary": "Deep neural networks (DNNs) are well known to be vulnerable to adversarial\nexamples (AEs). In previous studies, the use of models encrypted with a secret\nkey was demonstrated to be robust against white-box attacks, but not against\nblack-box ones. In this paper, we propose a novel method using the vision\ntransformer (ViT) that is a random ensemble of encrypted models for enhancing\nrobustness against both white-box and black-box attacks. In addition, a\nbenchmark attack method, called AutoAttack, is applied to models to test\nadversarial robustness objectively. In experiments, the method was demonstrated\nto be robust against not only white-box attacks but also black-box ones in an\nimage classification task on the CIFAR-10 and ImageNet datasets. The method was\nalso compared with the state-of-the-art in a standardized benchmark for\nadversarial robustness, RobustBench, and it was verified to outperform\nconventional defenses in terms of clean accuracy and robust accuracy.",
    "comment": "9 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07183v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07183v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07183v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07182v1",
    "updated": "2024-02-11T12:35:13+00:00",
    "published": "2024-02-11T12:35:13+00:00",
    "title": "Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning",
    "authors": [
      {
        "name": "Willem R\u00f6pke"
      },
      {
        "name": "Mathieu Reymond"
      },
      {
        "name": "Patrick Mannion"
      },
      {
        "name": "Diederik M. Roijers"
      },
      {
        "name": "Ann Now\u00e9"
      },
      {
        "name": "Roxana R\u0103dulescu"
      }
    ],
    "summary": "A significant challenge in multi-objective reinforcement learning is\nobtaining a Pareto front of policies that attain optimal performance under\ndifferent preferences. We introduce Iterated Pareto Referent Optimisation\n(IPRO), a principled algorithm that decomposes the task of finding the Pareto\nfront into a sequence of single-objective problems for which various solution\nmethods exist. This enables us to establish convergence guarantees while\nproviding an upper bound on the distance to undiscovered Pareto optimal\nsolutions at each step. Empirical evaluations demonstrate that IPRO matches or\noutperforms methods that require additional domain knowledge. By leveraging\nproblem-specific single-objective solvers, our approach also holds promise for\napplications beyond multi-objective reinforcement learning, such as in\npathfinding and optimisation.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07182v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07182v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07182v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07180v1",
    "updated": "2024-02-11T12:29:16+00:00",
    "published": "2024-02-11T12:29:16+00:00",
    "title": "MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization",
    "authors": [
      {
        "name": "Jingwei Zuo"
      },
      {
        "name": "George Arvanitakis"
      },
      {
        "name": "Mthandazo Ndhlovu"
      },
      {
        "name": "Hakim Hacid"
      }
    ],
    "summary": "Human activity recognition (HAR) is a well-established field, significantly\nadvanced by modern machine learning (ML) techniques. While companies have\nsuccessfully integrated HAR into consumer products, they typically rely on a\npredefined activity set, which limits personalizations at the user level (edge\ndevices). Despite advancements in Incremental Learning for updating models with\nnew data, this often occurs on the Cloud, necessitating regular data transfers\nbetween cloud and edge devices, thus leading to data privacy issues. In this\npaper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the\nCloud to the Edge. MAGNETO allows incremental human activity learning directly\non the Edge devices, without any data exchange with the Cloud. This enables\nstrong privacy guarantees, low processing latency, and a high degree of\npersonalization for users. In particular, we demonstrate MAGNETO in an Android\ndevice, validating the whole pipeline from data collection to result\nvisualization.",
    "comment": "Accepted by EDBT 2024 (demo track)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07180v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07180v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07180v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07174v1",
    "updated": "2024-02-11T12:03:01+00:00",
    "published": "2024-02-11T12:03:01+00:00",
    "title": "EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches",
    "authors": [
      {
        "name": "Pengcheng An"
      },
      {
        "name": "Jiawen Zhu"
      },
      {
        "name": "Zibo Zhang"
      },
      {
        "name": "Yifei Yin"
      },
      {
        "name": "Qingyuan Ma"
      },
      {
        "name": "Che Yan"
      },
      {
        "name": "Linghao Du"
      },
      {
        "name": "Jian Zhao"
      }
    ],
    "summary": "Voice messages, by nature, prevent users from gauging the emotional tone\nwithout fully diving into the audio content. This hinders the shared emotional\nexperience at the pre-retrieval stage. Research scarcely explored \"Emotional\nTeasers\"-pre-retrieval cues offering a glimpse into an awaiting message's\nemotional tone without disclosing its content. We introduce EmoWear, a\nsmartwatch voice messaging system enabling users to apply 30 animation teasers\non message bubbles to reflect emotions. EmoWear eases senders' choice by\nprioritizing emotions based on semantic and acoustic processing. EmoWear was\nevaluated in comparison with a mirroring system using color-coded message\nbubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced\nemotional communication experience in both receiving and sending messages. The\nanimated teasers were considered intuitive and valued for diverse expressions.\nDesirable interaction qualities and practical implications are distilled for\nfuture design. We thereby contribute both a novel system and empirical\nknowledge concerning emotional teasers for voice messaging.",
    "comment": "To appear at ACM CHI '24",
    "journal_ref": null,
    "doi": "10.1145/3613904.3642101",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3613904.3642101",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07174v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07174v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07174v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07167v1",
    "updated": "2024-02-11T11:24:09+00:00",
    "published": "2024-02-11T11:24:09+00:00",
    "title": "Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy",
    "authors": [
      {
        "name": "Zehao Dong"
      },
      {
        "name": "Yixin Chen"
      },
      {
        "name": "Hiram Gay"
      },
      {
        "name": "Yao Hao"
      },
      {
        "name": "Geoffrey D. Hugo"
      },
      {
        "name": "Pamela Samson"
      },
      {
        "name": "Tianyu Zhao"
      }
    ],
    "summary": "Treatment planning is currently a patient specific, time-consuming, and\nresource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction\nplays a critical role in automating this process. The geometric relationship\nbetween DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target\nvolume (PTV) has been well established. This study explores the potential of\ndeep learning models for predicting DVHs using images and subsequent human\nintervention facilitated by a large-language model (LLM) to enhance the\nplanning quality. We propose a pipeline to convert unstructured images to a\nstructured graph consisting of image-patch nodes and dose nodes. A novel Dose\nGraph Neural Network (DoseGNN) model is developed for predicting DVHs from the\nstructured graph. The proposed DoseGNN is enhanced with the LLM to encode\nmassive knowledge from prescriptions and interactive instructions from\nclinicians. In this study, we introduced an online human-AI collaboration\n(OHAC) system as a practical implementation of the concept proposed for the\nautomation of intensity-modulated radiotherapy (IMRT) planning. In comparison\nto the widely-employed DL models used in radiotherapy, DoseGNN achieved mean\nsquare errors that were 80$\\%$, 76$\\%$ and 41.0$\\%$ of those predicted by Swin\nU-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the\nLLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans\nthrough interaction with clinicians using natural language.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07167v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07167v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07167v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07166v1",
    "updated": "2024-02-11T11:23:28+00:00",
    "published": "2024-02-11T11:23:28+00:00",
    "title": "Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias",
    "authors": [
      {
        "name": "Arifa Khan"
      },
      {
        "name": "P. Saravanan"
      },
      {
        "name": "S. K Venkatesan"
      }
    ],
    "summary": "We provide a birds eye view of the rapid developments in AI and Deep Learning\nthat has led to the path-breaking emergence of AI in Large Language Models. The\naim of this study is to place all these developments in a pragmatic broader\nhistorical social perspective without any exaggerations while at the same time\nwithout any pessimism that created the AI winter in the 1970s to 1990s. We also\nat the same time point out toxicity, bias, memorization, sycophancy, logical\ninconsistencies, hallucinations that exist just as a warning to the overly\noptimistic. We note here that just as this emergence of AI seems to occur at a\nthreshold point in the number of neural connections or weights, it has also\nbeen observed that human brain and especially the cortex region is nothing\nspecial or extraordinary but simply a case of scaled-up version of the primate\nbrain and that even the human intelligence seems like an emergent phenomena of\nscale.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07166v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07166v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07166v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07164v1",
    "updated": "2024-02-11T11:20:29+00:00",
    "published": "2024-02-11T11:20:29+00:00",
    "title": "GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring",
    "authors": [
      {
        "name": "Madhav Khirwar"
      },
      {
        "name": "Ankur Narang"
      }
    ],
    "summary": "Air pollution represents a pivotal environmental challenge globally, playing\na major role in climate change via greenhouse gas emissions and negatively\naffecting the health of billions. However predicting the spatial and temporal\npatterns of pollutants remains challenging. The scarcity of ground-based\nmonitoring facilities and the dependency of air pollution modeling on\ncomprehensive datasets, often inaccessible for numerous areas, complicate this\nissue. In this work, we introduce GeoFormer, a compact model that combines a\nvision transformer module with a highly efficient time-series transformer\nmodule to predict surface-level nitrogen dioxide (NO2) concentrations from\nSentinel-5P satellite imagery. We train the proposed model to predict\nsurface-level NO2 measurements using a dataset we constructed with Sentinel-5P\nimages of ground-level monitoring stations, and their corresponding NO2\nconcentration readings. The proposed model attains high accuracy (MAE 5.65),\ndemonstrating the efficacy of combining vision and time-series transformer\narchitectures to harness satellite-derived data for enhanced GHG emission\ninsights, proving instrumental in advancing climate change monitoring and\nemission regulation efforts globally.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07164v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07164v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07164v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07160v1",
    "updated": "2024-02-11T11:11:39+00:00",
    "published": "2024-02-11T11:11:39+00:00",
    "title": "PASOA- PArticle baSed Bayesian Optimal Adaptive design",
    "authors": [
      {
        "name": "Jacopo Iollo"
      },
      {
        "name": "Christophe Heinkel\u00e9"
      },
      {
        "name": "Pierre Alliez"
      },
      {
        "name": "Florence Forbes"
      }
    ],
    "summary": "We propose a new procedure named PASOA, for Bayesian experimental design,\nthat performs sequential design optimization by simultaneously providing\naccurate estimates of successive posterior distributions for parameter\ninference. The sequential design process is carried out via a contrastive\nestimation principle, using stochastic optimization and Sequential Monte Carlo\n(SMC) samplers to maximise the Expected Information Gain (EIG). As larger\ninformation gains are obtained for larger distances between successive\nposterior distributions, this EIG objective may worsen classical SMC\nperformance. To handle this issue, tempering is proposed to have both a large\ninformation gain and an accurate SMC sampling, that we show is crucial for\nperformance. This novel combination of stochastic optimization and tempered SMC\nallows to jointly handle design optimization and parameter inference. We\nprovide a proof that the obtained optimal design estimators benefit from some\nconsistency property. Numerical experiments confirm the potential of the\napproach, which outperforms other recent existing procedures.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO",
      "stat.ME"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07160v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07160v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07160v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07158v1",
    "updated": "2024-02-11T11:03:08+00:00",
    "published": "2024-02-11T11:03:08+00:00",
    "title": "Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces",
    "authors": [
      {
        "name": "Claudionor N. Coelho Jr"
      },
      {
        "name": "Hanchen Xiong"
      },
      {
        "name": "Tushar Karayil"
      },
      {
        "name": "Sree Koratala"
      },
      {
        "name": "Rex Shang"
      },
      {
        "name": "Jacob Bollinger"
      },
      {
        "name": "Mohamed Shabar"
      },
      {
        "name": "Syam Nair"
      }
    ],
    "summary": "The advancement of Large Language Models (LLM) has also resulted in an\nequivalent proliferation in its applications. Software design, being one, has\ngained tremendous benefits in using LLMs as an interface component that extends\nfixed user stories. However, inclusion of LLM-based AI agents in software\ndesign often poses unexpected challenges, especially in the estimation of\ndevelopment efforts. Through the example of UI-based user stories, we provide a\ncomparison against traditional methods and propose a new way to enhance\nspecifications of natural language-based questions that allows for the\nestimation of development effort by taking into account data sources,\ninterfaces and algorithms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07158v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07158v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07158v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07157v1",
    "updated": "2024-02-11T11:03:04+00:00",
    "published": "2024-02-11T11:03:04+00:00",
    "title": "Natural Language Reinforcement Learning",
    "authors": [
      {
        "name": "Xidong Feng"
      },
      {
        "name": "Ziyu Wan"
      },
      {
        "name": "Mengyue Yang"
      },
      {
        "name": "Ziyan Wang"
      },
      {
        "name": "Girish A. Koushiks"
      },
      {
        "name": "Yali Du"
      },
      {
        "name": "Ying Wen"
      },
      {
        "name": "Jun Wang"
      }
    ],
    "summary": "Reinforcement Learning (RL) has shown remarkable abilities in learning\npolicies for decision-making tasks. However, RL is often hindered by issues\nsuch as low sample efficiency, lack of interpretability, and sparse supervision\nsignals. To tackle these limitations, we take inspiration from the human\nlearning process and introduce Natural Language Reinforcement Learning (NLRL),\nwhich innovatively combines RL principles with natural language representation.\nSpecifically, NLRL redefines RL concepts like task objectives, policy, value\nfunction, Bellman equation, and policy iteration in natural language space. We\npresent how NLRL can be practically implemented with the latest advancements in\nlarge language models (LLMs) like GPT-4. Initial experiments over tabular MDPs\ndemonstrate the effectiveness, efficiency, and also interpretability of the\nNLRL framework.",
    "comment": "Work in Progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07157v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07157v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07157v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07156v1",
    "updated": "2024-02-11T11:02:25+00:00",
    "published": "2024-02-11T11:02:25+00:00",
    "title": "A hybrid iterative method based on MIONet for PDEs: Theory and numerical examples",
    "authors": [
      {
        "name": "Jun Hu"
      },
      {
        "name": "Pengzhan Jin"
      }
    ],
    "summary": "We propose a hybrid iterative method based on MIONet for PDEs, which combines\nthe traditional numerical iterative solver and the recent powerful machine\nlearning method of neural operator, and further systematically analyze its\ntheoretical properties, including the convergence condition, the spectral\nbehavior, as well as the convergence rate, in terms of the errors of the\ndiscretization and the model inference. We show the theoretical results for the\nfrequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We\ngive an upper bound of the convergence rate of the hybrid method w.r.t. the\nmodel correction period, which indicates a minimum point to make the hybrid\niteration converge fastest. Several numerical examples including the hybrid\nRichardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are\npresented to verify our theoretical results, and also reflect an excellent\nacceleration effect. As a meshless acceleration method, it is provided with\nenormous potentials for practice applications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.NA",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07156v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07156v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07156v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07153v1",
    "updated": "2024-02-11T10:50:20+00:00",
    "published": "2024-02-11T10:50:20+00:00",
    "title": "Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations",
    "authors": [
      {
        "name": "Beatrice Lorenz"
      },
      {
        "name": "Aras Bacho"
      },
      {
        "name": "Gitta Kutyniok"
      }
    ],
    "summary": "This paper provides rigorous error bounds for physics-informed neural\nnetworks approximating the semilinear wave equation. We provide bounds for the\ngeneralization and training error in terms of the width of the network's layers\nand the number of training points for a tanh neural network with two hidden\nlayers. Our main result is a bound of the total error in the\n$H^1([0,T];L^2(\\Omega))$-norm in terms of the training error and the number of\ntraining points, which can be made arbitrarily small under some assumptions. We\nillustrate our theoretical bounds with numerical experiments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.NA",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "35L05, 68T07, 65M15, 35G50, 35A35"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07153v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07153v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07153v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07152v1",
    "updated": "2024-02-11T10:44:41+00:00",
    "published": "2024-02-11T10:44:41+00:00",
    "title": "Explainable Global Wildfire Prediction Models using Graph Neural Networks",
    "authors": [
      {
        "name": "Dayou Chen"
      },
      {
        "name": "Sibo Cheng"
      },
      {
        "name": "Jinwei Hu"
      },
      {
        "name": "Matthew Kasoar"
      },
      {
        "name": "Rossella Arcucci"
      }
    ],
    "summary": "Wildfire prediction has become increasingly crucial due to the escalating\nimpacts of climate change. Traditional CNN-based wildfire prediction models\nstruggle with handling missing oceanic data and addressing the long-range\ndependencies across distant regions in meteorological data. In this paper, we\nintroduce an innovative Graph Neural Network (GNN)-based model for global\nwildfire prediction. We propose a hybrid model that combines the spatial\nprowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long\nShort-Term Memory (LSTM) networks. Our approach uniquely transforms global\nclimate and wildfire data into a graph representation, addressing challenges\nsuch as null oceanic data locations and long-range dependencies inherent in\ntraditional models. Benchmarking against established architectures using an\nunseen ensemble of JULES-INFERNO simulations, our model demonstrates superior\npredictive accuracy. Furthermore, we emphasise the model's explainability,\nunveiling potential wildfire correlation clusters through community detection\nand elucidating feature importance via Integrated Gradient analysis. Our\nfindings not only advance the methodological domain of wildfire prediction but\nalso underscore the importance of model transparency, offering valuable\ninsights for stakeholders in wildfire management.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07152v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07152v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07152v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07148v1",
    "updated": "2024-02-11T10:23:34+00:00",
    "published": "2024-02-11T10:23:34+00:00",
    "title": "X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design",
    "authors": [
      {
        "name": "Eric L. Buehler"
      },
      {
        "name": "Markus J. Buehler"
      }
    ],
    "summary": "We report a mixture of expert strategy to create fine-tuned large language\nmodels using a deep layer-wise token-level approach based on low-rank\nadaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose\na gating strategy that uses the hidden states to dynamically mix adapted\nlayers, allowing the resulting X-LoRA model to draw upon different capabilities\nand create never-before-used deep layer-wise combinations of adaptations are\nestablished to solve specific tasks. The design is inspired by the biological\nprinciples of universality and diversity, where neural network building blocks\nare reused in different hierarchical manifestations. Hence, the X-LoRA model\ncan be easily implemented for any existing large language model (LLM) without a\nneed for modifications of the underlying structure. We develop a tailored\nX-LoRA model that offers scientific capabilities including forward/inverse\nanalysis tasks and enhanced reasoning capability, focused on biomaterial\nanalysis, protein mechanics and design. The impact of this work include access\nto readily expandable, adaptable and changeable models with strong domain\nknowledge and the capability to integrate across areas of knowledge. With the\nX-LoRA model featuring experts in biology, mathematics, reasoning, bio-inspired\nmaterials, mechanics and materials, chemistry, and protein mechanics we conduct\na series of physics-focused case studies. We examine knowledge recall, protein\nmechanics forward/inverse tasks, protein design, and adversarial agentic\nmodeling including ontological knowledge graphs. The model is capable not only\nof making quantitative predictions of nanomechanical properties of proteins,\nbut also reasons over the results and correctly predicts likely mechanisms that\nexplain distinct molecular behaviors.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cond-mat.soft",
    "categories": [
      "cond-mat.soft",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07148v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07148v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07148v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07140v1",
    "updated": "2024-02-11T09:46:24+00:00",
    "published": "2024-02-11T09:46:24+00:00",
    "title": "Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models",
    "authors": [
      {
        "name": "Yuyao Ge"
      },
      {
        "name": "Shenghua Liu"
      },
      {
        "name": "Lingrui Mei"
      },
      {
        "name": "Lizhe Chen"
      },
      {
        "name": "Xueqi Cheng"
      }
    ],
    "summary": "In recent years, Large Language Models have reached state-of-the-art\nperformance across multiple domains. However, the progress in the field of\ngraph reasoning remains limited. Our work delves into this gap by thoroughly\ninvestigating graph reasoning with LLM. In this work, we reveal the impact of\ntext sequence on LLM spatial understanding, finding that graph-descriptive text\nsequences significantly affect LLM reasoning performance on graphs. By altering\nthe graph-descriptive text sequences, we enhance the performance of LLM from\n42.22\\% to 70\\%. Furthermore, we evaluate the relationship between LLM\nperformance and graph size, discovering that the reasoning performance of LLM\ndoes not monotonically decrease with the increase in graph size. Conclusively,\nwe introduce the Scaled Graph Reasoning benchmark for assessing LLM performance\nacross varied graph sizes.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07140v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07140v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07140v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07139v1",
    "updated": "2024-02-11T09:46:15+00:00",
    "published": "2024-02-11T09:46:15+00:00",
    "title": "Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations",
    "authors": [
      {
        "name": "Muhammad Bilal Shahid"
      },
      {
        "name": "Cody Fleming"
      }
    ],
    "summary": "The selection of the target variable is important while learning parameters\nof the classical car following models like GIPPS, IDM, etc. There is a vast\nbody of literature on which target variable is optimal for classical car\nfollowing models, but there is no study that empirically evaluates the\nselection of optimal target variables for black-box models, such as LSTM, etc.\nThe black-box models, like LSTM and Gaussian Process (GP) are increasingly\nbeing used to model car following behavior without wise selection of target\nvariables. The current work tests different target variables, like\nacceleration, velocity, and headway, for three black-box models, i.e., GP,\nLSTM, and Kernel Ridge Regression. These models have different objective\nfunctions and work in different vector spaces, e.g., GP works in function\nspace, and LSTM works in parameter space. The experiments show that the optimal\ntarget variable recommendations for black-box models differ from classical car\nfollowing models depending on the objective function and the vector space. It\nis worth mentioning that models and datasets used during evaluation are diverse\nin nature: the datasets contained both automated and human-driven vehicle\ntrajectories; the black-box models belong to both parametric and non-parametric\nclasses of models. This diversity is important during the analysis of variance,\nwherein we try to find the interaction between datasets, models, and target\nvariables. It is shown that the models and target variables interact and\nrecommended target variables don't depend on the dataset under consideration.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07139v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07139v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07139v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07131v1",
    "updated": "2024-02-11T08:59:02+00:00",
    "published": "2024-02-11T08:59:02+00:00",
    "title": "Resampling methods for Private Statistical Inference",
    "authors": [
      {
        "name": "Karan Chadha"
      },
      {
        "name": "John Duchi"
      },
      {
        "name": "Rohith Kuditipudi"
      }
    ],
    "summary": "We consider the task of constructing confidence intervals with differential\nprivacy. We propose two private variants of the non-parametric bootstrap, which\nprivately compute the median of the results of multiple ``little'' bootstraps\nrun on partitions of the data and give asymptotic bounds on the coverage error\nof the resulting confidence intervals. For a fixed differential privacy\nparameter $\\epsilon$, our methods enjoy the same error rates as that of the\nnon-private bootstrap to within logarithmic factors in the sample size $n$. We\nempirically validate the performance of our methods for mean estimation, median\nestimation, and logistic regression with both real and synthetic data. Our\nmethods achieve similar coverage accuracy to existing methods (and non-private\nbaselines) while providing notably shorter ($\\gtrsim 10$ times) confidence\nintervals than previous approaches.",
    "comment": "35 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "stat.ME"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07131v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07131v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07131v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07129v1",
    "updated": "2024-02-11T08:54:37+00:00",
    "published": "2024-02-11T08:54:37+00:00",
    "title": "An attempt to generate new bridge types from latent space of denoising diffusion Implicit model",
    "authors": [
      {
        "name": "Hongjun Zhang"
      }
    ],
    "summary": "Use denoising diffusion implicit model for bridge-type innovation. The\nprocess of adding noise and denoising to an image can be likened to the process\nof a corpse rotting and a detective restoring the scene of a victim being\nkilled, to help beginners understand. Through an easy-to-understand algebraic\nmethod, derive the function formulas for adding noise and denoising, making it\neasier for beginners to master the mathematical principles of the model. Using\nsymmetric structured image dataset of three-span beam bridge, arch bridge,\ncable-stayed bridge and suspension bridge , based on Python programming\nlanguage, TensorFlow and Keras deep learning platform framework , denoising\ndiffusion implicit model is constructed and trained. From the latent space\nsampling, new bridge types with asymmetric structures can be generated.\nDenoising diffusion implicit model can organically combine different structural\ncomponents on the basis of human original bridge types, and create new bridge\ntypes.",
    "comment": "9 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07129v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07129v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07129v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07127v1",
    "updated": "2024-02-11T08:41:42+00:00",
    "published": "2024-02-11T08:41:42+00:00",
    "title": "Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation",
    "authors": [
      {
        "name": "Chrisantus Eze"
      },
      {
        "name": "Christopher Crick"
      }
    ],
    "summary": "Robot learning of manipulation skills is hindered by the scarcity of diverse,\nunbiased datasets. While curated datasets can help, challenges remain in\ngeneralizability and real-world transfer. Meanwhile, large-scale \"in-the-wild\"\nvideo datasets have driven progress in computer vision through self-supervised\ntechniques. Translating this to robotics, recent works have explored learning\nmanipulation skills by passively watching abundant videos sourced online.\nShowing promising results, such video-based learning paradigms provide scalable\nsupervision while reducing dataset bias. This survey reviews foundations such\nas video feature representation learning techniques, object affordance\nunderstanding, 3D hand/body modeling, and large-scale robot resources, as well\nas emerging techniques for acquiring robot manipulation skills from\nuncontrolled video demonstrations. We discuss how learning only from observing\nlarge-scale human videos can enhance generalization and sample efficiency for\nrobotic manipulation. The survey summarizes video-based learning approaches,\nanalyses their benefits over standard datasets, survey metrics, and benchmarks,\nand discusses open challenges and future directions in this nascent domain at\nthe intersection of computer vision, natural language processing, and robot\nlearning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07127v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07127v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07127v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07118v1",
    "updated": "2024-02-11T07:27:01+00:00",
    "published": "2024-02-11T07:27:01+00:00",
    "title": "Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation",
    "authors": [
      {
        "name": "Dhruv Srikanth"
      },
      {
        "name": "Jayang Gurung"
      },
      {
        "name": "N Satya Deepika"
      },
      {
        "name": "Vineet Joshi"
      },
      {
        "name": "Pravin Vaddavalli"
      },
      {
        "name": "Soumya Jana"
      }
    ],
    "summary": "Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.",
    "comment": "4 pages, Submitted to IEEE EMBC 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07118v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07118v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07118v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07114v1",
    "updated": "2024-02-11T06:21:18+00:00",
    "published": "2024-02-11T06:21:18+00:00",
    "title": "Towards Quantifying the Preconditioning Effect of Adam",
    "authors": [
      {
        "name": "Rudrajit Das"
      },
      {
        "name": "Naman Agarwal"
      },
      {
        "name": "Sujay Sanghavi"
      },
      {
        "name": "Inderjit S. Dhillon"
      }
    ],
    "summary": "There is a notable dearth of results characterizing the preconditioning\neffect of Adam and showing how it may alleviate the curse of ill-conditioning\n-- an issue plaguing gradient descent (GD). In this work, we perform a detailed\nanalysis of Adam's preconditioning effect for quadratic functions and quantify\nto what extent Adam can mitigate the dependence on the condition number of the\nHessian. Our key finding is that Adam can suffer less from the condition number\nbut at the expense of suffering a dimension-dependent quantity. Specifically,\nfor a $d$-dimensional quadratic with a diagonal Hessian having condition number\n$\\kappa$, we show that the effective condition number-like quantity controlling\nthe iteration complexity of Adam without momentum is $\\mathcal{O}(\\min(d,\n\\kappa))$. For a diagonally dominant Hessian, we obtain a bound of\n$\\mathcal{O}(\\min(d \\sqrt{d \\kappa}, \\kappa))$ for the corresponding quantity.\nThus, when $d < \\mathcal{O}(\\kappa^p)$ where $p = 1$ for a diagonal Hessian and\n$p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has\nan $\\mathcal{O}(\\kappa)$ dependence). On the negative side, our results suggest\nthat Adam can be worse than GD for a sufficiently non-diagonal Hessian even if\n$d \\ll \\mathcal{O}(\\kappa^{1/3})$; we corroborate this with empirical evidence.\nFinally, we extend our analysis to functions satisfying per-coordinate\nLipschitz smoothness and a modified version of the Polyak-\\L ojasiewicz\ncondition.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07114v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07114v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07114v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07108v1",
    "updated": "2024-02-11T05:35:50+00:00",
    "published": "2024-02-11T05:35:50+00:00",
    "title": "Decoupling Learning and Decision-Making: Breaking the $\\mathcal{O}(\\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods",
    "authors": [
      {
        "name": "Wenzhi Gao"
      },
      {
        "name": "Chunlin Sun"
      },
      {
        "name": "Chenyu Xue"
      },
      {
        "name": "Dongdong Ge"
      },
      {
        "name": "Yinyu Ye"
      }
    ],
    "summary": "Online linear programming plays an important role in both revenue management\nand resource allocation, and recent research has focused on developing\nefficient first-order online learning algorithms. Despite the empirical success\nof first-order methods, they typically achieve a regret no better than\n$\\mathcal{O}(\\sqrt{T})$, which is suboptimal compared to the $\\mathcal{O}(\\log\nT)$ bound guaranteed by the state-of-the-art linear programming (LP)-based\nonline algorithms. This paper establishes several important facts about online\nlinear programming, which unveils the challenge for first-order-method-based\nonline algorithms to achieve beyond $\\mathcal{O}(\\sqrt{T})$ regret. To address\nthe challenge, we introduce a new algorithmic framework that decouples learning\nfrom decision-making. More importantly, for the first time, we show that\nfirst-order methods can attain regret $\\mathcal{O}(T^{1/3})$ with this new\nframework. Lastly, we conduct numerical experiments to validate our theoretical\nfindings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07108v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07108v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07108v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07107v2",
    "updated": "2024-02-13T05:14:34+00:00",
    "published": "2024-02-11T05:17:56+00:00",
    "title": "Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning",
    "authors": [
      {
        "name": "Alex Christopher Stutts"
      },
      {
        "name": "Danilo Erricolo"
      },
      {
        "name": "Theja Tulabandhula"
      },
      {
        "name": "Amit Ranjan Trivedi"
      }
    ],
    "summary": "We present a novel statistical approach to incorporating uncertainty\nawareness in model-free distributional reinforcement learning involving\nquantile regression-based deep Q networks. The proposed algorithm,\n$\\textit{Calibrated Evidential Quantile Regression in Deep Q Networks\n(CEQR-DQN)}$, aims to address key challenges associated with separately\nestimating aleatoric and epistemic uncertainty in stochastic environments. It\ncombines deep evidential learning with quantile calibration based on principles\nof conformal inference to provide explicit, sample-free computations of\n$\\textit{global}$ uncertainty as opposed to $\\textit{local}$ estimates based on\nsimple variance, overcoming limitations of traditional methods in computational\nand statistical efficiency and handling of out-of-distribution (OOD)\nobservations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),\nCEQR-DQN is shown to surpass similar existing frameworks in scores and learning\nspeed. Its ability to rigorously evaluate uncertainty improves exploration\nstrategies and can serve as a blueprint for other algorithms requiring\nuncertainty awareness.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07107v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07107v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07107v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07102v1",
    "updated": "2024-02-11T04:53:40+00:00",
    "published": "2024-02-11T04:53:40+00:00",
    "title": "Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments",
    "authors": [
      {
        "name": "Jeongyeol Kwon"
      },
      {
        "name": "Liu Yang"
      },
      {
        "name": "Robert Nowak"
      },
      {
        "name": "Josiah Hanna"
      }
    ],
    "summary": "Learning a good history representation is one of the core challenges of\nreinforcement learning (RL) in partially observable environments. Recent works\nhave shown the advantages of various auxiliary tasks for facilitating\nrepresentation learning. However, the effectiveness of such auxiliary tasks has\nnot been fully convincing, especially in partially observable environments that\nrequire long-term memorization and inference. In this empirical study, we\ninvestigate the effectiveness of future prediction for learning the\nrepresentations of histories, possibly of extensive length, in partially\nobservable environments. We first introduce an approach that decouples the task\nof learning history representations from policy optimization via future\nprediction. Then, our main contributions are two-fold: (a) we demonstrate that\nthe performance of reinforcement learning is strongly correlated with the\nprediction accuracy of future observations in partially observable\nenvironments, and (b) our approach can significantly improve the overall\nend-to-end approach by preventing high-variance noisy signals from\nreinforcement learning objectives to influence the representation learning. We\nillustrate our claims on three types of benchmarks that necessitate the ability\nto process long histories for high returns.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07102v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07102v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07102v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07101v1",
    "updated": "2024-02-11T04:26:35+00:00",
    "published": "2024-02-11T04:26:35+00:00",
    "title": "On the Complexity of First-Order Methods in Stochastic Bilevel Optimization",
    "authors": [
      {
        "name": "Jeongyeol Kwon"
      },
      {
        "name": "Dohyun Kwon"
      },
      {
        "name": "Hanbaek Lyu"
      }
    ],
    "summary": "We consider the problem of finding stationary points in Bilevel optimization\nwhen the lower-level problem is unconstrained and strongly convex. The problem\nhas been extensively studied in recent years; the main technical challenge is\nto keep track of lower-level solutions $y^*(x)$ in response to the changes in\nthe upper-level variables $x$. Subsequently, all existing approaches tie their\nanalyses to a genie algorithm that knows lower-level solutions and, therefore,\nneed not query any points far from them. We consider a dual question to such\napproaches: suppose we have an oracle, which we call $y^*$-aware, that returns\nan $O(\\epsilon)$-estimate of the lower-level solution, in addition to\nfirst-order gradient estimators {\\it locally unbiased} within the\n$\\Theta(\\epsilon)$-ball around $y^*(x)$. We study the complexity of finding\nstationary points with such an $y^*$-aware oracle: we propose a simple\nfirst-order method that converges to an $\\epsilon$ stationary point using\n$O(\\epsilon^{-6}), O(\\epsilon^{-4})$ access to first-order $y^*$-aware oracles.\nOur upper bounds also apply to standard unbiased first-order oracles, improving\nthe best-known complexity of first-order methods by $O(\\epsilon)$ with minimal\nassumptions. We then provide the matching $\\Omega(\\epsilon^{-6})$,\n$\\Omega(\\epsilon^{-4})$ lower bounds without and with an additional smoothness\nassumption on $y^*$-aware oracles, respectively. Our results imply that any\napproach that simulates an algorithm with an $y^*$-aware oracle must suffer the\nsame lower bounds.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07101v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07101v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07101v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07099v1",
    "updated": "2024-02-11T04:09:50+00:00",
    "published": "2024-02-11T04:09:50+00:00",
    "title": "Rethinking the Capacity of Graph Neural Networks for Branching Strategy",
    "authors": [
      {
        "name": "Ziang Chen"
      },
      {
        "name": "Jialin Liu"
      },
      {
        "name": "Xiaohan Chen"
      },
      {
        "name": "Xinshang Wang"
      },
      {
        "name": "Wotao Yin"
      }
    ],
    "summary": "Graph neural networks (GNNs) have been widely used to predict properties and\nheuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP\nsolvers. This paper investigates the capacity of GNNs to represent strong\nbranching (SB) scores that provide an efficient strategy in the\nbranch-and-bound algorithm.\n  Although message-passing GNN (MP-GNN), as the simplest GNN structure, is\nfrequently employed in the existing literature to learn SB scores, we prove a\nfundamental limitation in its expressive power -- there exist two MILP\ninstances with different SB scores that cannot be distinguished by any MP-GNN,\nregardless of the number of parameters. In addition, we establish a universal\napproximation theorem for another GNN structure called the second-order\nfolklore GNN (2-FGNN). We show that for any data distribution over MILPs, there\nalways exists a 2-FGNN that can approximate the SB score with arbitrarily high\naccuracy and arbitrarily high probability. A small-scale numerical experiment\nis conducted to directly validate our theoretical findings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07099v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07099v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07099v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07098v1",
    "updated": "2024-02-11T03:54:44+00:00",
    "published": "2024-02-11T03:54:44+00:00",
    "title": "Improving Pallet Detection Using Synthetic Data",
    "authors": [
      {
        "name": "Henry Gann"
      },
      {
        "name": "Josiah Bull"
      },
      {
        "name": "Trevor Gee"
      },
      {
        "name": "Mahla Nejati"
      }
    ],
    "summary": "The use of synthetic data in machine learning saves a significant amount of\ntime when implementing an effective object detector. However, there is limited\nresearch in this domain. This study aims to improve upon previously applied\nimplementations in the task of instance segmentation of pallets in a warehouse\nenvironment. This study proposes using synthetically generated\ndomain-randomised data as well as data generated through Unity to achieve this.\nThis study achieved performance improvements on the stacked and racked pallet\ncategories by 69% and 50% mAP50, respectively when being evaluated on real\ndata. Additionally, it was found that there was a considerable impact on the\nperformance of a model when it was evaluated against images in a darker\nenvironment, dropping as low as 3% mAP50 when being evaluated on images with an\n80% brightness reduction. This study also created a two-stage detector that\nused YOLOv8 and SAM, but this proved to have unstable performance. The use of\ndomain-randomised data proved to have negligible performance improvements when\ncompared to the Unity-generated data.",
    "comment": "Australasian Conference on Robotics and Automation (ACRA 2023)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07098v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07098v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07098v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07087v1",
    "updated": "2024-02-11T02:34:42+00:00",
    "published": "2024-02-11T02:34:42+00:00",
    "title": "Self-Correcting Self-Consuming Loops for Generative Model Training",
    "authors": [
      {
        "name": "Nate Gillman"
      },
      {
        "name": "Michael Freeman"
      },
      {
        "name": "Daksh Aggarwal"
      },
      {
        "name": "Chia-Hong Hsu"
      },
      {
        "name": "Calvin Luo"
      },
      {
        "name": "Yonglong Tian"
      },
      {
        "name": "Chen Sun"
      }
    ],
    "summary": "As synthetic data becomes higher quality and proliferates on the internet,\nmachine learning models are increasingly trained on a mix of human- and\nmachine-generated data. Despite the successful stories of using synthetic data\nfor representation learning, using synthetic data for generative model training\ncreates \"self-consuming loops\" which may lead to training instability or even\ncollapse, unless certain conditions are met. Our paper aims to stabilize\nself-consuming generative model training. Our theoretical results demonstrate\nthat by introducing an idealized correction function, which maps a data point\nto be more likely under the true data distribution, self-consuming loops can be\nmade exponentially more stable. We then propose self-correction functions,\nwhich rely on expert knowledge (e.g. the laws of physics programmed in a\nsimulator), and aim to approximate the idealized corrector automatically and at\nscale. We empirically validate the effectiveness of self-correcting\nself-consuming loops on the challenging human motion synthesis task, and\nobserve that it successfully avoids model collapse, even when the ratio of\nsynthetic data to real data is as high as 100%.",
    "comment": "Under submission. Code will be released at\n  https://nategillman.com/sc-sc.html",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07087v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07087v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07087v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07085v1",
    "updated": "2024-02-11T02:26:43+00:00",
    "published": "2024-02-11T02:26:43+00:00",
    "title": "Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis",
    "authors": [
      {
        "name": "Kenichi Fujita"
      },
      {
        "name": "Atsushi Ando"
      },
      {
        "name": "Yusuke Ijima"
      }
    ],
    "summary": "This paper proposes a speech rhythm-based method for speaker embeddings to\nmodel phoneme duration using a few utterances by the target speaker. Speech\nrhythm is one of the essential factors among speaker characteristics, along\nwith acoustic features such as F0, for reproducing individual utterances in\nspeech synthesis. A novel feature of the proposed method is the rhythm-based\nembeddings extracted from phonemes and their durations, which are known to be\nrelated to speaking rhythm. They are extracted with a speaker identification\nmodel similar to the conventional spectral feature-based one. We conducted\nthree experiments, speaker embeddings generation, speech synthesis with\ngenerated embeddings, and embedding space analysis, to evaluate the\nperformance. The proposed method demonstrated a moderate speaker identification\nperformance (15.2% EER), even with only phonemes and their duration\ninformation. The objective and subjective evaluation results demonstrated that\nthe proposed method can synthesize speech with speech rhythm closer to the\ntarget speaker than the conventional method. We also visualized the embeddings\nto evaluate the relationship between the distance of the embeddings and the\nperceptual similarity. The visualization of the embedding space and the\nrelation analysis between the closeness indicated that the distribution of\nembeddings reflects the subjective and objective similarity.",
    "comment": "11 pages,9 figures, Accepted to IEICE TRANSACTIONS on Information and\n  Systems",
    "journal_ref": "IEICE TRANSACTIONS on Information and Systems 107.1 (2024): 93-104",
    "doi": "10.1587/transinf.2023EDP7039",
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1587/transinf.2023EDP7039",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07085v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07085v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07085v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07082v1",
    "updated": "2024-02-11T01:51:15+00:00",
    "published": "2024-02-11T01:51:15+00:00",
    "title": "Refined Sample Complexity for Markov Games with Independent Linear Function Approximation",
    "authors": [
      {
        "name": "Yan Dai"
      },
      {
        "name": "Qiwen Cui"
      },
      {
        "name": "Simon S. Du"
      }
    ],
    "summary": "Markov Games (MG) is an important model for Multi-Agent Reinforcement\nLearning (MARL). It was long believed that the \"curse of multi-agents\" (i.e.,\nthe algorithmic performance drops exponentially with the number of agents) is\nunavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,\n2023; Wang et al., 2023. While these works did resolve the curse of\nmulti-agents, when the state spaces are prohibitively large and (linear)\nfunction approximations are deployed, they either had a slower convergence rate\nof $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions\n$A_{\\max}$ -- which is avoidable in single-agent cases even when the loss\nfunctions can arbitrarily vary with time (Dai et al., 2023). This paper first\nrefines the `AVLPR` framework by Wang et al. (2023), with an insight of\n*data-dependent* (i.e., stochastic) pessimistic estimation of the\nsub-optimality gap, allowing a broader choice of plug-in algorithms. When\nspecialized to MGs with independent linear function approximations, we propose\nnovel *action-dependent bonuses* to cover occasionally extreme estimation\nerrors. With the help of state-of-the-art techniques from the single-agent RL\nliterature, we give the first algorithm that tackles the curse of multi-agents,\nattains the optimal $O(T^{-1/2})$ convergence rate, and avoids\n$\\text{poly}(A_{\\max})$ dependency simultaneously.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07082v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07082v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07082v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07079v1",
    "updated": "2024-02-11T01:21:56+00:00",
    "published": "2024-02-11T01:21:56+00:00",
    "title": "The Relevance Feature and Vector Machine for health applications",
    "authors": [
      {
        "name": "Albert Belenguer-Llorens"
      },
      {
        "name": "Carlos Sevilla-Salcedo"
      },
      {
        "name": "Emilio Parrado-Hern\u00e1ndez"
      },
      {
        "name": "Vanessa G\u00f3mez-Verdejo"
      }
    ],
    "summary": "This paper presents the Relevance Feature and Vector Machine (RFVM), a novel\nmodel that addresses the challenges of the fat-data problem when dealing with\nclinical prospective studies. The fat-data problem refers to the limitations of\nMachine Learning (ML) algorithms when working with databases in which the\nnumber of features is much larger than the number of samples (a common scenario\nin certain medical fields). To overcome such limitations, the RFVM incorporates\ndifferent characteristics: (1) A Bayesian formulation which enables the model\nto infer its parameters without overfitting thanks to the Bayesian model\naveraging. (2) A joint optimisation that overcomes the limitations arising from\nthe fat-data characteristic by simultaneously including the variables that\ndefine the primal space (features) and those that define the dual space\n(observations). (3) An integrated prunning that removes the irrelevant features\nand samples during the training iterative optimization. Also, this last point\nturns out crucial when performing medical prospective studies, enabling\nresearchers to exclude unnecessary medical tests, reducing costs and\ninconvenience for patients, and identifying the critical patients/subjects that\ncharacterize the disorder and, subsequently, optimize the patient recruitment\nprocess that leads to a balanced cohort. The model capabilities are tested\nagainst state-of-the-art models in several medical datasets with fat-data\nproblems. These experimental works show that RFVM is capable of achieving\ncompetitive classification accuracies while providing the most compact subset\nof data (in both terms of features and samples). Moreover, the selected\nfeatures (medical tests) seem to be aligned with the existing medical\nliterature.",
    "comment": "19 pages of main text, 12 pages of appendices, 2 figures and 5 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07079v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07079v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07079v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07076v1",
    "updated": "2024-02-11T01:03:41+00:00",
    "published": "2024-02-11T01:03:41+00:00",
    "title": "Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training",
    "authors": [
      {
        "name": "Haonan Chen"
      },
      {
        "name": "Zhicheng Dou"
      },
      {
        "name": "Xuetong Hao"
      },
      {
        "name": "Yunhao Tao"
      },
      {
        "name": "Shiren Song"
      },
      {
        "name": "Zhenli Sheng"
      }
    ],
    "summary": "Cloud solutions have gained significant popularity in the technology industry\nas they offer a combination of services and tools to tackle specific problems.\nHowever, despite their widespread use, the task of identifying appropriate\ncompany customers for a specific target solution to the sales team of a\nsolution provider remains a complex business problem that existing matching\nsystems have yet to adequately address. In this work, we study the B2B solution\nmatching problem and identify two main challenges of this scenario: (1) the\nmodeling of complex multi-field features and (2) the limited, incomplete, and\nsparse transaction data. To tackle these challenges, we propose a framework\nCAMA, which is built with a hierarchical multi-field matching structure as its\nbackbone and supplemented by three data augmentation strategies and a\ncontrastive pre-training objective to compensate for the imperfections in the\navailable data. Through extensive experiments on a real-world dataset, we\ndemonstrate that CAMA outperforms several strong baseline matching models\nsignificantly. Furthermore, we have deployed our matching framework on a system\nof Huawei Cloud. Our observations indicate an improvement of about 30% compared\nto the previous online model in terms of Conversion Rate (CVR), which\ndemonstrates its great business value.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07076v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07076v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07076v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07069v1",
    "updated": "2024-02-11T00:00:05+00:00",
    "published": "2024-02-11T00:00:05+00:00",
    "title": "Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine",
    "authors": [
      {
        "name": "Shayan Meshkat Alsadat"
      },
      {
        "name": "Jean-Raphael Gaglione"
      },
      {
        "name": "Daniel Neider"
      },
      {
        "name": "Ufuk Topcu"
      },
      {
        "name": "Zhe Xu"
      }
    ],
    "summary": "We present LARL-RM (Large language model-generated Automaton for\nReinforcement Learning with Reward Machine) algorithm in order to encode\nhigh-level knowledge into reinforcement learning using automaton to expedite\nthe reinforcement learning. Our method uses Large Language Models (LLM) to\nobtain high-level domain-specific knowledge using prompt engineering instead of\nproviding the reinforcement learning algorithm directly with the high-level\nknowledge which requires an expert to encode the automaton. We use\nchain-of-thought and few-shot methods for prompt engineering and demonstrate\nthat our method works using these approaches. Additionally, LARL-RM allows for\nfully closed-loop reinforcement learning without the need for an expert to\nguide and supervise the learning since LARL-RM can use the LLM directly to\ngenerate the required high-level knowledge for the task at hand. We also show\nthe theoretical guarantee of our algorithm to converge to an optimal policy. We\ndemonstrate that LARL-RM speeds up the convergence by 30% by implementing our\nmethod in two case studies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07069v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07069v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07069v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07067v1",
    "updated": "2024-02-10T23:49:49+00:00",
    "published": "2024-02-10T23:49:49+00:00",
    "title": "Learning the Expected Core of Strictly Convex Stochastic Cooperative Games",
    "authors": [
      {
        "name": "Nam Phuong Tran"
      },
      {
        "name": "The Anh Ta"
      },
      {
        "name": "Shuqing Shi"
      },
      {
        "name": "Debmalya Mandal"
      },
      {
        "name": "Yali Du"
      },
      {
        "name": "Long Tran-Thanh"
      }
    ],
    "summary": "Reward allocation, also known as the credit assignment problem, has been an\nimportant topic in economics, engineering, and machine learning. An important\nconcept in credit assignment is the core, which is the set of stable\nallocations where no agent has the motivation to deviate from the grand\ncoalition. In this paper, we consider the stable allocation learning problem of\nstochastic cooperative games, where the reward function is characterised as a\nrandom variable with an unknown distribution. Given an oracle that returns a\nstochastic reward for an enquired coalition each round, our goal is to learn\nthe expected core, that is, the set of allocations that are stable in\nexpectation. Within the class of strictly convex games, we present an algorithm\nnamed \\texttt{Common-Points-Picking} that returns a stable allocation given a\npolynomial number of samples, with high probability. The analysis of our\nalgorithm involves the development of several new results in convex geometry,\nincluding an extension of the separation hyperplane theorem for multiple convex\nsets, and may be of independent interest.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.GT",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07067v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07067v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07067v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07066v1",
    "updated": "2024-02-10T23:42:05+00:00",
    "published": "2024-02-10T23:42:05+00:00",
    "title": "Differentially Private Range Queries with Correlated Input Perturbation",
    "authors": [
      {
        "name": "Prathamesh Dharangutte"
      },
      {
        "name": "Jie Gao"
      },
      {
        "name": "Ruobin Gong"
      },
      {
        "name": "Guanyang Wang"
      }
    ],
    "summary": "This work proposes a class of locally differentially private mechanisms for\nlinear queries, in particular range queries, that leverages correlated input\nperturbation to simultaneously achieve unbiasedness, consistency, statistical\ntransparency, and control over utility requirements in terms of accuracy\ntargets expressed either in certain query margins or as implied by the\nhierarchical database structure. The proposed Cascade Sampling algorithm\ninstantiates the mechanism exactly and efficiently. Our bounds show that we\nobtain near-optimal utility while being empirically competitive against output\nperturbation methods.",
    "comment": "26 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG",
      "stat.ME"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07066v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07066v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07066v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07062v1",
    "updated": "2024-02-10T22:38:21+00:00",
    "published": "2024-02-10T22:38:21+00:00",
    "title": "Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise",
    "authors": [
      {
        "name": "Yuriy Dorn"
      },
      {
        "name": "Aleksandr Katrutsa"
      },
      {
        "name": "Ilgam Latypov"
      },
      {
        "name": "Andrey Pudovikov"
      }
    ],
    "summary": "In this study, we propose a new method for constructing UCB-type algorithms\nfor stochastic multi-armed bandits based on general convex optimization methods\nwith an inexact oracle. We derive the regret bounds corresponding to the\nconvergence rates of the optimization methods. We propose a new algorithm\nClipped-SGD-UCB and show, both theoretically and empirically, that in the case\nof symmetric noise in the reward, we can achieve an $O(\\log T\\sqrt{KT\\log T})$\nregret bound instead of $O\\left (T^{\\frac{1}{1+\\alpha}}\nK^{\\frac{\\alpha}{1+\\alpha}} \\right)$ for the case when the reward distribution\nsatisfies $\\mathbb{E}_{X \\in D}[|X|^{1+\\alpha}] \\leq \\sigma^{1+\\alpha}$\n($\\alpha \\in (0, 1])$, i.e. perform better than it is assumed by the general\nlower bound for bandits with heavy-tails. Moreover, the same bound holds even\nwhen the reward distribution does not have the expectation, that is, when\n$\\alpha<0$.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07062v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07062v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07062v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07052v1",
    "updated": "2024-02-10T21:51:59+00:00",
    "published": "2024-02-10T21:51:59+00:00",
    "title": "Understanding the Training Speedup from Sampling with Approximate Losses",
    "authors": [
      {
        "name": "Rudrajit Das"
      },
      {
        "name": "Xi Chen"
      },
      {
        "name": "Bertram Ieong"
      },
      {
        "name": "Parikshit Bansal"
      },
      {
        "name": "Sujay Sanghavi"
      }
    ],
    "summary": "It is well known that selecting samples with large losses/gradients can\nsignificantly reduce the number of training steps. However, the selection\noverhead is often too high to yield any meaningful gains in terms of overall\ntraining time. In this work, we focus on the greedy approach of selecting\nsamples with large \\textit{approximate losses} instead of exact losses in order\nto reduce the selection overhead. For smooth convex losses, we show that such a\ngreedy strategy can converge to a constant factor of the minimum value of the\naverage loss in fewer iterations than the standard approach of random\nselection. We also theoretically quantify the effect of the approximation\nlevel. We then develop SIFT which uses early exiting to obtain approximate\nlosses with an intermediate layer's representations for sample selection. We\nevaluate SIFT on the task of training a 110M parameter 12-layer BERT base model\nand show significant gains (in terms of training hours and number of\nbackpropagation steps) without any optimized implementation over vanilla\ntraining. For e.g., to reach 64% validation accuracy, SIFT with exit at the\nfirst layer takes ~43 hours compared to ~57 hours of vanilla training.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07052v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07052v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07052v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07051v1",
    "updated": "2024-02-10T21:46:34+00:00",
    "published": "2024-02-10T21:46:34+00:00",
    "title": "$L^*LM$: Learning Automata from Examples using Natural Language Oracles",
    "authors": [
      {
        "name": "Marcell Vazquez-Chanlatte"
      },
      {
        "name": "Karim Elmaaroufi"
      },
      {
        "name": "Stefan J. Witwicki"
      },
      {
        "name": "Sanjit A. Seshia"
      }
    ],
    "summary": "Expert demonstrations have proven an easy way to indirectly specify complex\ntasks. Recent algorithms even support extracting unambiguous formal\nspecifications, e.g. deterministic finite automata (DFA), from demonstrations.\nUnfortunately, these techniques are generally not sample efficient. In this\nwork, we introduce $L^*LM$, an algorithm for learning DFAs from both\ndemonstrations and natural language. Due to the expressivity of natural\nlanguage, we observe a significant improvement in the data efficiency of\nlearning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large\nlanguage models to answer membership queries about the underlying task. This is\nthen combined with recent techniques for transforming learning from\ndemonstrations into a sequence of labeled example learning problems. In our\nexperiments, we observe the two modalities complement each other, yielding a\npowerful few-shot learner.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.FL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07051v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07051v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07051v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07049v1",
    "updated": "2024-02-10T21:44:28+00:00",
    "published": "2024-02-10T21:44:28+00:00",
    "title": "A Factor Graph Model of Trust for a Collaborative Multi-Agent System",
    "authors": [
      {
        "name": "Behzad Akbari"
      },
      {
        "name": "Mingfeng Yuan"
      },
      {
        "name": "Hao Wang"
      },
      {
        "name": "Haibin Zhu"
      },
      {
        "name": "Jinjun Shan"
      }
    ],
    "summary": "In the field of Multi-Agent Systems (MAS), known for their openness,\ndynamism, and cooperative nature, the ability to trust the resources and\nservices of other agents is crucial. Trust, in this setting, is the reliance\nand confidence an agent has in the information, behaviors, intentions,\ntruthfulness, and capabilities of others within the system. Our paper\nintroduces a new graphical approach that utilizes factor graphs to represent\nthe interdependent behaviors and trustworthiness among agents. This includes\nmodeling the behavior of robots as a trajectory of actions using a Gaussian\nprocess factor graph, which accounts for smoothness, obstacle avoidance, and\ntrust-related factors. Our method for evaluating trust is decentralized and\nconsiders key interdependent sub-factors such as proximity safety, consistency,\nand cooperation. The overall system comprises a network of factor graphs that\ninteract through trust-related factors and employs a Bayesian inference method\nto dynamically assess trust-based decisions with informed consent. The\neffectiveness of this method is validated via simulations and empirical tests\nwith autonomous robots navigating unsignalized intersections.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07049v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07049v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07049v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07043v1",
    "updated": "2024-02-10T21:06:34+00:00",
    "published": "2024-02-10T21:06:34+00:00",
    "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
    "authors": [
      {
        "name": "Elvis Dohmatob"
      },
      {
        "name": "Yunzhen Feng"
      },
      {
        "name": "Pu Yang"
      },
      {
        "name": "Francois Charton"
      },
      {
        "name": "Julia Kempe"
      }
    ],
    "summary": "As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07043v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07043v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07043v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07039v1",
    "updated": "2024-02-10T20:39:04+00:00",
    "published": "2024-02-10T20:39:04+00:00",
    "title": "Coordinated Disclosure for AI: Beyond Security Vulnerabilities",
    "authors": [
      {
        "name": "Sven Cattell"
      },
      {
        "name": "Avijit Ghosh"
      }
    ],
    "summary": "Harm reporting in the field of Artificial Intelligence (AI) currently\noperates on an ad hoc basis, lacking a structured process for disclosing or\naddressing algorithmic flaws. In contrast, the Coordinated Vulnerability\nDisclosure (CVD) ethos and ecosystem play a pivotal role in software security\nand transparency. Within the U.S. context, there has been a protracted legal\nand policy struggle to establish a safe harbor from the Computer Fraud and\nAbuse Act, aiming to foster institutional support for security researchers\nacting in good faith. Notably, algorithmic flaws in Machine Learning (ML)\nmodels present distinct challenges compared to traditional software\nvulnerabilities, warranting a specialized approach. To address this gap, we\npropose the implementation of a dedicated Coordinated Flaw Disclosure (CFD)\nframework tailored to the intricacies of machine learning and artificial\nintelligence issues. This paper delves into the historical landscape of\ndisclosures in ML, encompassing the ad hoc reporting of harms and the emergence\nof participatory auditing. By juxtaposing these practices with the\nwell-established disclosure norms in cybersecurity, we argue that the broader\nadoption of CFD has the potential to enhance public trust through transparent\nprocesses that carefully balance the interests of both organizations and the\ncommunity.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07039v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07039v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07039v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07035v1",
    "updated": "2024-02-10T20:06:26+00:00",
    "published": "2024-02-10T20:06:26+00:00",
    "title": "Distilling Symbolic Priors for Concept Learning into Neural Networks",
    "authors": [
      {
        "name": "Ioana Marinescu"
      },
      {
        "name": "R. Thomas McCoy"
      },
      {
        "name": "Thomas L. Griffiths"
      }
    ],
    "summary": "Humans can learn new concepts from a small number of examples by drawing on\ntheir inductive biases. These inductive biases have previously been captured by\nusing Bayesian models defined over symbolic hypothesis spaces. Is it possible\nto create a neural network that displays the same inductive biases? We show\nthat inductive biases that enable rapid concept learning can be instantiated in\nartificial neural networks by distilling a prior distribution from a symbolic\nBayesian model via meta-learning, an approach for extracting the common\nstructure from a set of tasks. By generating the set of tasks used in\nmeta-learning from the prior distribution of a Bayesian model, we are able to\ntransfer that prior into a neural network. We use this approach to create a\nneural network with an inductive bias towards concepts expressed as short\nlogical formulas. Analyzing results from previous behavioral experiments in\nwhich people learned logical concepts from a few examples, we find that our\nmeta-trained models are highly aligned with human performance.",
    "comment": "8 pages, 6 figures, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07035v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07035v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07035v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07033v1",
    "updated": "2024-02-10T19:54:08+00:00",
    "published": "2024-02-10T19:54:08+00:00",
    "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
    "authors": [
      {
        "name": "Keisuke Kamahori"
      },
      {
        "name": "Yile Gu"
      },
      {
        "name": "Kan Zhu"
      },
      {
        "name": "Baris Kasikci"
      }
    ],
    "summary": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture\nare showing promising performance on various tasks. However, running them on\nresource-constrained settings, where GPU memory resources are not abundant, is\nchallenging due to huge model sizes. Existing systems that offload model\nweights to CPU memory suffer from the significant overhead of frequently moving\ndata between CPU and GPU. In this paper, we propose Fiddler, a\nresource-efficient inference engine with CPU-GPU orchestration for MoE models.\nThe key idea of Fiddler is to use the computation ability of the CPU to\nminimize the data movement between the CPU and GPU. Our evaluation shows that\nFiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in\nparameters, to generate over $3$ tokens per second on a single GPU with 24GB\nmemory, showing an order of magnitude improvement over existing methods. The\ncode of Fiddler is publicly available at\n\\url{https://github.com/efeslab/fiddler}",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.OS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07033v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07033v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07033v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07031v1",
    "updated": "2024-02-10T19:45:40+00:00",
    "published": "2024-02-10T19:45:40+00:00",
    "title": "Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration",
    "authors": [
      {
        "name": "Chih-Hong Cheng"
      },
      {
        "name": "Paul St\u00f6ckel"
      },
      {
        "name": "Xingyu Zhao"
      }
    ],
    "summary": "Modeling and calibrating the fidelity of synthetic data is paramount in\nshaping the future of safe and reliable self-driving technology by offering a\ncost-effective and scalable alternative to real-world data collection. We focus\non its role in safety-critical applications, introducing four types of\ninstance-level fidelity that go beyond mere visual input characteristics. The\naim is to align synthetic data with real-world safety issues. We suggest an\noptimization method to refine the synthetic data generator, reducing fidelity\ngaps identified by the DNN-based component. Our findings show this tuning\nenhances the correlation between safety-critical errors in synthetic and real\nimages.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07031v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07031v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07031v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07028v1",
    "updated": "2024-02-10T19:27:22+00:00",
    "published": "2024-02-10T19:27:22+00:00",
    "title": "Semi-Supervised Learning for Bilingual Lexicon Induction",
    "authors": [
      {
        "name": "Paul Garnier"
      },
      {
        "name": "Gauthier Guinet"
      }
    ],
    "summary": "We consider the problem of aligning two sets of continuous word\nrepresentations, corresponding to languages, to a common space in order to\ninfer a bilingual lexicon. It was recently shown that it is possible to infer\nsuch lexicon, without using any parallel data, by aligning word embeddings\ntrained on monolingual data. Such line of work is called unsupervised bilingual\ninduction. By wondering whether it was possible to gain experience in the\nprogressive learning of several languages, we asked ourselves to what extent we\ncould integrate the knowledge of a given set of languages when learning a new\none, without having parallel data for the latter. In other words, while keeping\nthe core problem of unsupervised learning in the latest step, we allowed the\naccess to other corpora of idioms, hence the name semi-supervised. This led us\nto propose a novel formulation, considering the lexicon induction as a ranking\nproblem for which we used recent tools of this machine learning field. Our\nexperiments on standard benchmarks, inferring dictionary from English to more\nthan 20 languages, show that our approach consistently outperforms existing\nstate of the art benchmark. In addition, we deduce from this new scenario\nseveral relevant conclusions allowing a better understanding of the alignment\nphenomenon.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07028v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07028v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07028v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07027v1",
    "updated": "2024-02-10T19:21:29+00:00",
    "published": "2024-02-10T19:21:29+00:00",
    "title": "Quantum Speedup for Spectral Approximation of Kronecker Products",
    "authors": [
      {
        "name": "Yeqi Gao"
      },
      {
        "name": "Zhao Song"
      },
      {
        "name": "Ruizhe Zhang"
      }
    ],
    "summary": "Given its widespread application in machine learning and optimization, the\nKronecker product emerges as a pivotal linear algebra operator. However, its\ncomputational demands render it an expensive operation, leading to heightened\ncosts in spectral approximation of it through traditional computation\nalgorithms. Existing classical methods for spectral approximation exhibit a\nlinear dependency on the matrix dimension denoted by $n$, considering matrices\nof size $A_1 \\in \\mathbb{R}^{n \\times d}$ and $A_2 \\in \\mathbb{R}^{n \\times\nd}$. Our work introduces an innovative approach to efficiently address the\nspectral approximation of the Kronecker product $A_1 \\otimes A_2$ using quantum\nmethods. By treating matrices as quantum states, our proposed method\nsignificantly reduces the time complexity of spectral approximation to\n$O_{d,\\epsilon}(\\sqrt{n})$.",
    "comment": "arXiv admin note: text overlap with arXiv:2311.03215 by other authors",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DS",
    "categories": [
      "cs.DS",
      "cs.ET",
      "cs.LG",
      "math.QA",
      "quant-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07027v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07027v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07027v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07025v1",
    "updated": "2024-02-10T19:12:31+00:00",
    "published": "2024-02-10T19:12:31+00:00",
    "title": "Generalization Error of Graph Neural Networks in the Mean-field Regime",
    "authors": [
      {
        "name": "Gholamali Aminian"
      },
      {
        "name": "Yixuan He"
      },
      {
        "name": "Gesine Reinert"
      },
      {
        "name": "\u0141ukasz Szpruch"
      },
      {
        "name": "Samuel N. Cohen"
      }
    ],
    "summary": "This work provides a theoretical framework for assessing the generalization\nerror of graph classification tasks via graph neural networks in the\nover-parameterized regime, where the number of parameters surpasses the\nquantity of data points. We explore two widely utilized types of graph neural\nnetworks: graph convolutional neural networks and message passing graph neural\nnetworks. Prior to this study, existing bounds on the generalization error in\nthe over-parametrized regime were uninformative, limiting our understanding of\nover-parameterized network performance. Our novel approach involves deriving\nupper bounds within the mean-field regime for evaluating the generalization\nerror of these graph neural networks. We establish upper bounds with a\nconvergence rate of $O(1/n)$, where $n$ is the number of graph samples. These\nupper bounds offer a theoretical assurance of the networks' performance on\nunseen data in the challenging over-parameterized regime and overall contribute\nto our understanding of their performance.",
    "comment": "43 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "62B10, 60F99, 49N80, 46N30"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07025v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07025v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07025v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07023v1",
    "updated": "2024-02-10T19:08:28+00:00",
    "published": "2024-02-10T19:08:28+00:00",
    "title": "Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations",
    "authors": [
      {
        "name": "Ankit Pal"
      },
      {
        "name": "Malaikannan Sankarasubbu"
      }
    ],
    "summary": "Large language models have the potential to be valuable in the healthcare\nindustry, but it's crucial to verify their safety and effectiveness through\nrigorous evaluation. For this purpose, we comprehensively evaluated both\nopen-source LLMs and Google's new multimodal LLM called Gemini across Medical\nreasoning, hallucination detection, and Medical Visual Question Answering\ntasks. While Gemini showed competence, it lagged behind state-of-the-art models\nlike MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved\nan accuracy of 61.45\\% on the medical VQA dataset, significantly lower than\nGPT-4V's score of 88\\%. Our analysis revealed that Gemini is highly susceptible\nto hallucinations, overconfidence, and knowledge gaps, which indicate risks if\ndeployed uncritically. We also performed a detailed analysis by medical subject\nand test type, providing actionable feedback for developers and clinicians. To\nmitigate risks, we applied prompting strategies that improved performance.\nAdditionally, we facilitated future research and development by releasing a\nPython module for medical LLM evaluation and establishing a dedicated\nleaderboard on Hugging Face for medical domain LLMs. Python module can be found\nat https://github.com/promptslab/RosettaEval",
    "comment": "Preprint version, Under Review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07023v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07023v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07023v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07956v1",
    "updated": "2024-02-10T18:48:45+00:00",
    "published": "2024-02-10T18:48:45+00:00",
    "title": "Educational data mining and learning analytics: An updated survey",
    "authors": [
      {
        "name": "C. Romero"
      },
      {
        "name": "S. Ventura"
      }
    ],
    "summary": "This survey is an updated and improved version of the previous one published\nin 2013 in this journal with the title data mining in education. It reviews in\na comprehensible and very general way how Educational Data Mining and Learning\nAnalytics have been applied over educational data. In the last decade, this\nresearch area has evolved enormously and a wide range of related terms are now\nused in the bibliography such as Academic Analytics, Institutional Analytics,\nTeaching Analytics, Data-Driven Education, Data-Driven Decision-Making in\nEducation, Big Data in Education, and Educational Data Science. This paper\nprovides the current state of the art by reviewing the main publications, the\nkey milestones, the knowledge discovery cycle, the main educational\nenvironments, the specific tools, the free available datasets, the most used\nmethods, the main objectives, and the future trends in this research area.",
    "comment": null,
    "journal_ref": "Wiley interdisciplinary reviews: Data mining and knowledge\n  discovery;2020; 10(3):e1355",
    "doi": "10.1002/widm.1355",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1002/widm.1355",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.07956v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07956v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07956v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07019v1",
    "updated": "2024-02-10T18:36:42+00:00",
    "published": "2024-02-10T18:36:42+00:00",
    "title": "Informativeness of Reward Functions in Reinforcement Learning",
    "authors": [
      {
        "name": "Rati Devidze"
      },
      {
        "name": "Parameswaran Kamalaruban"
      },
      {
        "name": "Adish Singla"
      }
    ],
    "summary": "Reward functions are central in specifying the task we want a reinforcement\nlearning agent to perform. Given a task and desired optimal behavior, we study\nthe problem of designing informative reward functions so that the designed\nrewards speed up the agent's convergence. In particular, we consider\nexpert-driven reward design settings where an expert or teacher seeks to\nprovide informative and interpretable rewards to a learning agent. Existing\nworks have considered several different reward design formulations; however,\nthe key challenge is formulating a reward informativeness criterion that adapts\nw.r.t. the agent's current policy and can be optimized under specified\nstructural constraints to obtain interpretable rewards. In this paper, we\npropose a novel reward informativeness criterion, a quantitative measure that\ncaptures how the agent's current policy will improve if it receives rewards\nfrom a specific reward function. We theoretically showcase the utility of the\nproposed informativeness criterion for adaptively designing rewards for an\nagent. Experimental results on two navigation tasks demonstrate the\neffectiveness of our adaptive reward informativeness criterion.",
    "comment": "Longer version of the AAMAS'24 paper",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07019v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07019v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07019v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07016v1",
    "updated": "2024-02-10T18:27:28+00:00",
    "published": "2024-02-10T18:27:28+00:00",
    "title": "REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models",
    "authors": [
      {
        "name": "Yinghao Zhu"
      },
      {
        "name": "Changyu Ren"
      },
      {
        "name": "Shiyun Xie"
      },
      {
        "name": "Shukai Liu"
      },
      {
        "name": "Hangyuan Ji"
      },
      {
        "name": "Zixiang Wang"
      },
      {
        "name": "Tao Sun"
      },
      {
        "name": "Long He"
      },
      {
        "name": "Zhoujun Li"
      },
      {
        "name": "Xi Zhu"
      },
      {
        "name": "Chengwei Pan"
      }
    ],
    "summary": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly improved clinical predictive capabilities. Leveraging clinical\nnotes and multivariate time-series EHR, existing models often lack the medical\ncontext relevent to clinical tasks, prompting the incorporation of external\nknowledge, particularly from the knowledge graph (KG). Previous approaches with\nKG knowledge have primarily focused on structured knowledge extraction,\nneglecting unstructured data modalities and semantic high dimensional medical\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\n(RAG) driven framework to enhance multimodal EHR representations that address\nthese limitations. Firstly, we apply Large Language Model (LLM) to encode long\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\nwe prompt LLM to extract task-relevant medical entities and match entities in\nprofessionally labeled external knowledge graph (PrimeKG) with corresponding\nmedical knowledge. By matching and aligning with clinical standards, our\nframework eliminates hallucinations and ensures consistency. Lastly, we propose\nan adaptive multimodal fusion network to integrate extracted knowledge with\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality and\nreadmission tasks showcase the superior performance of our REALM framework over\nbaselines, emphasizing the effectiveness of each module. REALM framework\ncontributes to refining the use of multimodal EHR data in healthcare and\nbridging the gap with nuanced medical context essential for informed clinical\npredictions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07016v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07016v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07016v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07011v1",
    "updated": "2024-02-10T18:14:57+00:00",
    "published": "2024-02-10T18:14:57+00:00",
    "title": "FedImpro: Measuring and Improving Client Update in Federated Learning",
    "authors": [
      {
        "name": "Zhenheng Tang"
      },
      {
        "name": "Yonggang Zhang"
      },
      {
        "name": "Shaohuai Shi"
      },
      {
        "name": "Xinmei Tian"
      },
      {
        "name": "Tongliang Liu"
      },
      {
        "name": "Bo Han"
      },
      {
        "name": "Xiaowen Chu"
      }
    ],
    "summary": "Federated Learning (FL) models often experience client drift caused by\nheterogeneous data, where the distribution of data differs across clients. To\naddress this issue, advanced research primarily focuses on manipulating the\nexisting gradients to achieve more consistent client models. In this paper, we\npresent an alternative perspective on client drift and aim to mitigate it by\ngenerating improved local models. First, we analyze the generalization\ncontribution of local training and conclude that this generalization\ncontribution is bounded by the conditional Wasserstein distance between the\ndata distribution of different clients. Then, we propose FedImpro, to construct\nsimilar conditional distributions for local training. Specifically, FedImpro\ndecouples the model into high-level and low-level components, and trains the\nhigh-level portion on reconstructed feature distributions. This approach\nenhances the generalization contribution and reduces the dissimilarity of\ngradients in FL. Experimental results show that FedImpro can help FL defend\nagainst data heterogeneity and enhance the generalization performance of the\nmodel.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07011v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07011v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07011v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07008v1",
    "updated": "2024-02-10T18:03:15+00:00",
    "published": "2024-02-10T18:03:15+00:00",
    "title": "An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation",
    "authors": [
      {
        "name": "Tianyi Ren"
      },
      {
        "name": "Ethan Honey"
      },
      {
        "name": "Harshitha Rebala"
      },
      {
        "name": "Abhishek Sharma"
      },
      {
        "name": "Agamdeep Chopra"
      },
      {
        "name": "Mehmet Kurt"
      }
    ],
    "summary": "Tumor segmentation from multi-modal brain MRI images is a challenging task\ndue to the limited samples, high variance in shapes and uneven distribution of\ntumor morphology. The performance of automated medical image segmentation has\nbeen significant improvement by the recent advances in deep learning. However,\nthe model predictions have not yet reached the desired level for clinical use\nin terms of accuracy and generalizability. In order to address the distinct\nproblems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed\nan optimization framework based on a 3D U-Net model for brain tumor\nsegmentation. This framework incorporates a range of techniques, including\nvarious pre-processing and post-processing techniques, and transfer learning.\nOn the validation datasets, this multi-modality brain tumor segmentation\nframework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on\nChallenges 1, 2, 3 respectively.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07008v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07008v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07008v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07002v1",
    "updated": "2024-02-10T17:39:34+00:00",
    "published": "2024-02-10T17:39:34+00:00",
    "title": "Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off",
    "authors": [
      {
        "name": "Yuecheng Li"
      },
      {
        "name": "Tong Wang"
      },
      {
        "name": "Chuan Chen"
      },
      {
        "name": "Jian Lou"
      },
      {
        "name": "Bin Chen"
      },
      {
        "name": "Lei Yang"
      },
      {
        "name": "Zibin Zheng"
      }
    ],
    "summary": "To defend against privacy leakage of user data, differential privacy is\nwidely used in federated learning, but it is not free. The addition of noise\nrandomly disrupts the semantic integrity of the model and this disturbance\naccumulates with increased communication rounds. In this paper, we introduce a\nnovel federated learning framework with rigorous privacy guarantees, named\nFedCEO, designed to strike a trade-off between model utility and user privacy\nby letting clients ''Collaborate with Each Other''. Specifically, we perform\nefficient tensor low-rank proximal optimization on stacked local model\nparameters at the server, demonstrating its capability to flexibly truncate\nhigh-frequency components in spectral space. This implies that our FedCEO can\neffectively recover the disrupted semantic information by smoothing the global\nsemantic space for different privacy settings and continuous training\nprocesses. Moreover, we improve the SOTA utility-privacy trade-off bound by an\norder of $\\sqrt{d}$, where $d$ is the input dimension. We illustrate our\ntheoretical results with experiments on representative image datasets. It\nobserves significant performance improvements and strict privacy guarantees\nunder different privacy settings.",
    "comment": "22 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07002v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07002v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07002v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07955v1",
    "updated": "2024-02-10T17:31:46+00:00",
    "published": "2024-02-10T17:31:46+00:00",
    "title": "ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation",
    "authors": [
      {
        "name": "Zuobai Zhang"
      },
      {
        "name": "Jiarui Lu"
      },
      {
        "name": "Vijil Chenthamarakshan"
      },
      {
        "name": "Aur\u00e9lie Lozano"
      },
      {
        "name": "Payel Das"
      },
      {
        "name": "Jian Tang"
      }
    ],
    "summary": "Protein function annotation is an important yet challenging task in biology.\nRecent deep learning advancements show significant potential for accurate\nfunction prediction by learning from protein sequences and structures.\nNevertheless, these predictor-based methods often overlook the modeling of\nprotein similarity, an idea commonly employed in traditional approaches using\nsequence or structure retrieval tools. To fill this gap, we first study the\neffect of inter-protein similarity modeling by benchmarking retriever-based\nmethods against predictors on protein function annotation tasks. Our results\nshow that retrievers can match or outperform predictors without large-scale\npre-training. Building on these insights, we introduce a novel variational\npseudo-likelihood framework, ProtIR, designed to improve function predictors by\nincorporating inter-protein similarity modeling. This framework iteratively\nrefines knowledge between a function predictor and retriever, thereby combining\nthe strengths of both predictors and retrievers. ProtIR showcases around 10%\nimprovement over vanilla predictor-based methods. Besides, it achieves\nperformance on par with protein language model-based methods, yet without the\nneed for massive pre-training, highlighting the efficacy of our framework. Code\nwill be released upon acceptance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.BM",
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07955v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07955v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07955v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06994v1",
    "updated": "2024-02-10T17:02:53+00:00",
    "published": "2024-02-10T17:02:53+00:00",
    "title": "A Change Detection Reality Check",
    "authors": [
      {
        "name": "Isaac Corley"
      },
      {
        "name": "Caleb Robinson"
      },
      {
        "name": "Anthony Ortiz"
      }
    ],
    "summary": "In recent years, there has been an explosion of proposed change detection\ndeep learning architectures in the remote sensing literature. These approaches\nclaim to offer state-of the-art performance on different standard benchmark\ndatasets. However, has the field truly made significant progress? In this paper\nwe perform experiments which conclude a simple U-Net segmentation baseline\nwithout training tricks or complicated architectural changes is still a top\nperformer for the task of change detection.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06994v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06994v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06994v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06992v1",
    "updated": "2024-02-10T16:54:28+00:00",
    "published": "2024-02-10T16:54:28+00:00",
    "title": "A Rational Analysis of the Speech-to-Song Illusion",
    "authors": [
      {
        "name": "Raja Marjieh"
      },
      {
        "name": "Pol van Rijn"
      },
      {
        "name": "Ilia Sucholutsky"
      },
      {
        "name": "Harin Lee"
      },
      {
        "name": "Thomas L. Griffiths"
      },
      {
        "name": "Nori Jacoby"
      }
    ],
    "summary": "The speech-to-song illusion is a robust psychological phenomenon whereby a\nspoken sentence sounds increasingly more musical as it is repeated. Despite\ndecades of research, a complete formal account of this transformation is still\nlacking, and some of its nuanced characteristics, namely, that certain phrases\nappear to transform while others do not, is not well understood. Here we\nprovide a formal account of this phenomenon, by recasting it as a statistical\ninference whereby a rational agent attempts to decide whether a sequence of\nutterances is more likely to have been produced in a song or speech. Using this\napproach and analyzing song and speech corpora, we further introduce a novel\nprose-to-lyrics illusion that is purely text-based. In this illusion, simply\nduplicating written sentences makes them appear more like song lyrics. We\nprovide robust evidence for this new illusion in both human participants and\nlarge language models.",
    "comment": "7 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.NC",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL",
      "stat.AP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06992v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06992v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06992v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06990v1",
    "updated": "2024-02-10T16:47:53+00:00",
    "published": "2024-02-10T16:47:53+00:00",
    "title": "Guided Sketch-Based Program Induction by Search Gradients",
    "authors": [
      {
        "name": "Ahmad Ayaz Amin"
      }
    ],
    "summary": "Many tasks can be easily solved using machine learning techniques. However,\nsome tasks cannot readily be solved using statistical models, requiring a\nsymbolic approach instead. Program induction is one of the ways that such tasks\ncan be solved by means of capturing an interpretable and generalizable\nalgorithm through training. However, contemporary approaches to program\ninduction are not sophisticated enough to readily be applied to various types\nof tasks as they tend to be formulated as a single, all-encompassing model,\nusually parameterized by neural networks. In an attempt to make program\ninduction a viable solution for many scenarios, we propose a framework for\nlearning parameterized programs via search gradients using evolution\nstrategies. This formulation departs from traditional program induction as it\nallows for the programmer to impart task-specific code to the program 'sketch',\nwhile also enjoying the benefits of accelerated learning through end-to-end\ngradient-based optimization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.PL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06990v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06990v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06990v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06985v1",
    "updated": "2024-02-10T16:23:12+00:00",
    "published": "2024-02-10T16:23:12+00:00",
    "title": "OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery",
    "authors": [
      {
        "name": "Long Bai"
      },
      {
        "name": "Guankun Wang"
      },
      {
        "name": "Jie Wang"
      },
      {
        "name": "Xiaoxiao Yang"
      },
      {
        "name": "Huxin Gao"
      },
      {
        "name": "Xin Liang"
      },
      {
        "name": "An Wang"
      },
      {
        "name": "Mobarakol Islam"
      },
      {
        "name": "Hongliang Ren"
      }
    ],
    "summary": "In the realm of automated robotic surgery and computer-assisted\ninterventions, understanding robotic surgical activities stands paramount.\nExisting algorithms dedicated to surgical activity recognition predominantly\ncater to pre-defined closed-set paradigms, ignoring the challenges of\nreal-world open-set scenarios. Such algorithms often falter in the presence of\ntest samples originating from classes unseen during training phases. To tackle\nthis problem, we introduce an innovative Open-Set Surgical Activity Recognition\n(OSSAR) framework. Our solution leverages the hyperspherical reciprocal point\nstrategy to enhance the distinction between known and unknown classes in the\nfeature space. Additionally, we address the issue of over-confidence in the\nclosed set by refining model calibration, avoiding misclassification of unknown\nclasses as known ones. To support our assertions, we establish an open-set\nsurgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we\nalso collect a novel dataset on endoscopic submucosal dissection for surgical\nactivity tasks. Extensive comparisons and ablation experiments on these\ndatasets demonstrate the significant outperformance of our method over existing\nstate-of-the-art approaches. Our proposed solution can effectively address the\nchallenges of real-world surgical scenarios. Our code is publicly accessible at\nhttps://github.com/longbai1006/OSSAR.",
    "comment": "To appear in IEEE ICRA 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06985v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06985v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06985v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06982v1",
    "updated": "2024-02-10T16:13:09+00:00",
    "published": "2024-02-10T16:13:09+00:00",
    "title": "Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI",
    "authors": [
      {
        "name": "Xiaofeng Liu"
      },
      {
        "name": "Nadya Shusharina"
      },
      {
        "name": "Helen A Shih"
      },
      {
        "name": "C. -C. Jay Kuo"
      },
      {
        "name": "Georges El Fakhri"
      },
      {
        "name": "Jonghye Woo"
      }
    ],
    "summary": "In this work, we aim to predict the survival time (ST) of glioblastoma (GBM)\npatients undergoing different treatments based on preoperative magnetic\nresonance (MR) scans. The personalized and precise treatment planning can be\nachieved by comparing the ST of different treatments. It is well established\nthat both the current status of the patient (as represented by the MR scans)\nand the choice of treatment are the cause of ST. While previous related\nMR-based glioblastoma ST studies have focused only on the direct mapping of MR\nscans to ST, they have not included the underlying causal relationship between\ntreatments and ST. To address this limitation, we propose a\ntreatment-conditioned regression model for glioblastoma ST that incorporates\ntreatment information in addition to MR scans. Our approach allows us to\neffectively utilize the data from all of the treatments in a unified manner,\nrather than having to train separate models for each of the treatments.\nFurthermore, treatment can be effectively injected into each convolutional\nlayer through the adaptive instance normalization we employ. We evaluate our\nframework on the BraTS20 ST prediction task. Three treatment options are\nconsidered: Gross Total Resection (GTR), Subtotal Resection (STR), and no\nresection. The evaluation results demonstrate the effectiveness of injecting\nthe treatment for estimating GBM survival.",
    "comment": "SPIE Medical Imaging 2024: Computer-Aided Diagnosis",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06982v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06982v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06982v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06974v2",
    "updated": "2024-02-13T07:09:06+00:00",
    "published": "2024-02-10T15:42:03+00:00",
    "title": "Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization",
    "authors": [
      {
        "name": "Marc Bartholet"
      },
      {
        "name": "Taehyeon Kim"
      },
      {
        "name": "Ami Beuret"
      },
      {
        "name": "Se-Young Yun"
      },
      {
        "name": "Joachim M. Buhmann"
      }
    ],
    "summary": "Federated Learning (FL) has emerged as a promising paradigm in which multiple\nclients collaboratively train a shared global model while preserving data\nprivacy. To create a robust and practicable FL framework, it is crucial to\nextend its ability to generalize well to unseen domains - a problem referred to\nas federated Domain Generalization (FDG), being still under-explored. We\npropose an innovative federated algorithm, termed hFedF for hypernetwork-based\nFederated Fusion, designed to bridge the performance gap between generalization\nand personalization, capable of addressing various degrees of domain shift.\nEssentially, the hypernetwork supports a non-linear fusion of client models\nenabling a comprehensive understanding of the underlying data distribution. We\nencompass an extensive discussion and provide novel insights into the tradeoff\nbetween personalization and generalization in FL. The proposed algorithm\noutperforms strong benchmarks on three widely-used data sets for DG in an\nexceeding number of cases.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06974v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06974v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06974v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06973v1",
    "updated": "2024-02-10T15:32:53+00:00",
    "published": "2024-02-10T15:32:53+00:00",
    "title": "Event-Keyed Summarization",
    "authors": [
      {
        "name": "William Gantt"
      },
      {
        "name": "Alexander Martin"
      },
      {
        "name": "Pavlo Kuchmiichuk"
      },
      {
        "name": "Aaron Steven White"
      }
    ],
    "summary": "We introduce event-keyed summarization (EKS), a novel task that marries\ntraditional summarization and document-level event extraction, with the goal of\ngenerating a contextualized summary for a specific event, given a document and\nan extracted event structure. We introduce a dataset for this task, MUCSUM,\nconsisting of summaries of all events in the classic MUC-4 dataset, along with\na set of baselines that comprises both pretrained LM standards in the\nsummarization literature, as well as larger frontier models. We show that\nablations that reduce EKS to traditional summarization or structure-to-text\nyield inferior summaries of target events and that MUCSUM is a robust benchmark\nfor this task. Lastly, we conduct a human evaluation of both reference and\nmodel summaries, and provide some detailed analysis of the results.",
    "comment": "ARR short paper (under review)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06973v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06973v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06973v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06971v1",
    "updated": "2024-02-10T15:23:45+00:00",
    "published": "2024-02-10T15:23:45+00:00",
    "title": "In-Context Data Distillation with TabPFN",
    "authors": [
      {
        "name": "Junwei Ma"
      },
      {
        "name": "Valentin Thomas"
      },
      {
        "name": "Guangwei Yu"
      },
      {
        "name": "Anthony Caterini"
      }
    ],
    "summary": "Foundation models have revolutionized tasks in computer vision and natural\nlanguage processing. However, in the realm of tabular data, tree-based models\nlike XGBoost continue to dominate. TabPFN, a transformer model tailored for\ntabular data, mirrors recent foundation models in its exceptional in-context\nlearning capability, being competitive with XGBoost's performance without the\nneed for task-specific training or hyperparameter tuning. Despite its promise,\nTabPFN's applicability is hindered by its data size constraint, limiting its\nuse in real-world scenarios. To address this, we present in-context data\ndistillation (ICD), a novel methodology that effectively eliminates these\nconstraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to\nhandle significantly larger datasets with a fixed memory budget, improving\nTabPFN's quadratic memory complexity but at the cost of a linear number of\ntuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong\nperformance against established tree-based models and modern deep learning\nmethods on 48 large tabular datasets from OpenML.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06971v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06971v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06971v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06968v1",
    "updated": "2024-02-10T14:56:36+00:00",
    "published": "2024-02-10T14:56:36+00:00",
    "title": "Contextual Stochastic Vehicle Routing with Time Windows",
    "authors": [
      {
        "name": "Breno Serrano"
      },
      {
        "name": "Alexandre M. Florio"
      },
      {
        "name": "Stefan Minner"
      },
      {
        "name": "Maximilian Schiffer"
      },
      {
        "name": "Thibaut Vidal"
      }
    ],
    "summary": "We study the vehicle routing problem with time windows (VRPTW) and stochastic\ntravel times, in which the decision-maker observes related contextual\ninformation, represented as feature variables, before making routing decisions.\nDespite the extensive literature on stochastic VRPs, the integration of feature\nvariables has received limited attention in this context. We introduce the\ncontextual stochastic VRPTW, which minimizes the total transportation cost and\nexpected late arrival penalties conditioned on the observed features. Since the\njoint distribution of travel times and features is unknown, we present novel\ndata-driven prescriptive models that use historical data to provide an\napproximate solution to the problem. We distinguish the prescriptive models\nbetween point-based approximation, sample average approximation, and\npenalty-based approximation, each taking a different perspective on dealing\nwith stochastic travel times and features. We develop specialized\nbranch-price-and-cut algorithms to solve these data-driven prescriptive models.\nIn our computational experiments, we compare the out-of-sample cost performance\nof different methods on instances with up to one hundred customers. Our results\nshow that, surprisingly, a feature-dependent sample average approximation\noutperforms existing and novel methods in most settings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06968v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06968v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06968v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06967v1",
    "updated": "2024-02-10T14:52:52+00:00",
    "published": "2024-02-10T14:52:52+00:00",
    "title": "Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue",
    "authors": [
      {
        "name": "Jian Wang"
      },
      {
        "name": "Chak Tou Leong"
      },
      {
        "name": "Jiashuo Wang"
      },
      {
        "name": "Dongding Lin"
      },
      {
        "name": "Wenjie Li"
      },
      {
        "name": "Xiao-Yong Wei"
      }
    ],
    "summary": "Tuning pretrained language models for dialogue generation has been a\nprevalent paradigm for building capable dialogue agents. Yet, traditional\ntuning narrowly views dialogue generation as resembling other language\ngeneration tasks, ignoring the role disparities between two speakers and the\nmulti-round interactive process that dialogues ought to be. Such a manner leads\nto unsatisfactory chat consistency of the built agent. In this work, we\nemphasize the interactive, communicative nature of dialogue and argue that it\nis more feasible to model the speaker roles of agent and user separately,\nenabling the agent to adhere to its role consistently. We propose an efficient\nMulti-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the\nagent and user individually with two adapters built upon large language models,\nwhere they utilize utterances round by round in alternating order and are tuned\nvia a round-level memory caching mechanism. Extensive experiments demonstrate\nthat, our framework performs superior to traditional fine-tuning and harbors\nthe tremendous potential for improving dialogue consistency.",
    "comment": "Work in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06967v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06967v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06967v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06966v1",
    "updated": "2024-02-10T14:45:23+00:00",
    "published": "2024-02-10T14:45:23+00:00",
    "title": "DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction",
    "authors": [
      {
        "name": "Pouria Golshanrad"
      },
      {
        "name": "Fathiyeh Faghih"
      }
    ],
    "summary": "Recurrent neural networks (RNNs) have emerged as powerful tools for\nprocessing sequential data in various fields, including natural language\nprocessing and speech recognition. However, the lack of explainability in RNN\nmodels has limited their interpretability, posing challenges in understanding\ntheir internal workings. To address this issue, this paper proposes a\nmethodology for extracting a state machine (SM) from an RNN-based model to\nprovide insights into its internal function. The proposed SM extraction\nalgorithm was assessed using four newly proposed metrics: Purity, Richness,\nGoodness, and Scale. The proposed methodology along with its assessment metrics\ncontribute to increasing explainability in RNN models by providing a clear\nrepresentation of their internal decision making process through the extracted\nSM. In addition to improving the explainability of RNNs, the extracted SM can\nbe used to advance testing and and monitoring of the primary RNN-based model.\nTo enhance RNN testing, we introduce six model coverage criteria based on the\nextracted SM, serving as metrics for evaluating the effectiveness of test\nsuites designed to analyze the primary model. We also propose a tree-based\nmodel to predict the error probability of the primary model for each input\nbased on the extracted SM. We evaluated our proposed online error prediction\napproach using the MNIST dataset and Mini Speech Commands dataset, achieving an\narea under the curve (AUC) exceeding 80\\% for the receiver operating\ncharacteristic (ROC) chart.",
    "comment": null,
    "journal_ref": null,
    "doi": "10.1016/j.jss.2024.111987",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1016/j.jss.2024.111987",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06966v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06966v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06966v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06963v1",
    "updated": "2024-02-10T14:36:31+00:00",
    "published": "2024-02-10T14:36:31+00:00",
    "title": "Tree Ensembles for Contextual Bandits",
    "authors": [
      {
        "name": "Hannes Nilsson"
      },
      {
        "name": "Rikard Johansson"
      },
      {
        "name": "Niklas \u00c5kerblom"
      },
      {
        "name": "Morteza Haghir Chehreghani"
      }
    ],
    "summary": "We propose a novel framework for contextual multi-armed bandits based on tree\nensembles. Our framework integrates two widely used bandit methods, Upper\nConfidence Bound and Thompson Sampling, for both standard and combinatorial\nsettings. We demonstrate the effectiveness of our framework via several\nexperimental studies, employing XGBoost, a popular tree ensemble method.\nCompared to state-of-the-art methods based on neural networks, our methods\nexhibit superior performance in terms of both regret minimization and\ncomputational runtime, when applied to benchmark datasets and the real-world\napplication of navigation over road networks.",
    "comment": "The first two authors contributed equally to this work",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06963v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06963v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06963v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06957v1",
    "updated": "2024-02-10T13:57:51+00:00",
    "published": "2024-02-10T13:57:51+00:00",
    "title": "Architectural Neural Backdoors from First Principles",
    "authors": [
      {
        "name": "Harry Langford"
      },
      {
        "name": "Ilia Shumailov"
      },
      {
        "name": "Yiren Zhao"
      },
      {
        "name": "Robert Mullins"
      },
      {
        "name": "Nicolas Papernot"
      }
    ],
    "summary": "While previous research backdoored neural networks by changing their\nparameters, recent work uncovered a more insidious threat: backdoors embedded\nwithin the definition of the network's architecture. This involves injecting\ncommon architectural components, such as activation functions and pooling\nlayers, to subtly introduce a backdoor behavior that persists even after (full\nre-)training. However, the full scope and implications of architectural\nbackdoors have remained largely unexplored. Bober-Irizar et al. [2023]\nintroduced the first architectural backdoor; they showed how to create a\nbackdoor for a checkerboard pattern, but never explained how to target an\narbitrary trigger pattern of choice. In this work we construct an arbitrary\ntrigger detector which can be used to backdoor an architecture with no human\nsupervision. This leads us to revisit the concept of architecture backdoors and\ntaxonomise them, describing 12 distinct types. To gauge the difficulty of\ndetecting such backdoors, we conducted a user study, revealing that ML\ndevelopers can only identify suspicious components in common model definitions\nas backdoors in 37% of cases, while they surprisingly preferred backdoored\nmodels in 33% of cases. To contextualize these results, we find that language\nmodels outperform humans at the detection of backdoors. Finally, we discuss\ndefenses against architectural backdoors, emphasizing the need for robust and\ncomprehensive strategies to safeguard the integrity of ML systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06957v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06957v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06957v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06955v1",
    "updated": "2024-02-10T13:51:09+00:00",
    "published": "2024-02-10T13:51:09+00:00",
    "title": "Training dynamics in Physics-Informed Neural Networks with feature mapping",
    "authors": [
      {
        "name": "Chengxi Zeng"
      },
      {
        "name": "Tilo Burghardt"
      },
      {
        "name": "Alberto M Gambaruto"
      }
    ],
    "summary": "Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine\nlearning approach for solving Partial Differential Equations (PDEs). Although\nits variants have achieved significant progress, the empirical success of\nutilising feature mapping from the wider Implicit Neural Representations\nstudies has been substantially neglected. We investigate the training dynamics\nof PINNs with a feature mapping layer via the limiting Conjugate Kernel and\nNeural Tangent Kernel, which sheds light on the convergence and generalisation\nof the model. We also show the inadequacy of commonly used Fourier-based\nfeature mapping in some scenarios and propose the conditional positive definite\nRadial Basis Function as a better alternative. The empirical results reveal the\nefficacy of our method in diverse forward and inverse problem sets. This simple\ntechnique can be easily implemented in coordinate input networks and benefits\nthe broad PINNs research.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06955v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06955v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06955v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06954v1",
    "updated": "2024-02-10T13:50:11+00:00",
    "published": "2024-02-10T13:50:11+00:00",
    "title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning",
    "authors": [
      {
        "name": "Rui Ye"
      },
      {
        "name": "Wenhao Wang"
      },
      {
        "name": "Jingyi Chai"
      },
      {
        "name": "Dihan Li"
      },
      {
        "name": "Zexi Li"
      },
      {
        "name": "Yinda Xu"
      },
      {
        "name": "Yaxin Du"
      },
      {
        "name": "Yanfeng Wang"
      },
      {
        "name": "Siheng Chen"
      }
    ],
    "summary": "Trained on massive publicly available data, large language models (LLMs) have\ndemonstrated tremendous success across various fields. While more data\ncontributes to better performance, a disconcerting reality is that high-quality\npublic data will be exhausted in a few years. In this paper, we offer a\npotential next step for contemporary LLMs: collaborative and privacy-preserving\nLLM training on the underutilized distributed private data via federated\nlearning (FL), where multiple data owners collaboratively train a shared model\nwithout transmitting raw data. To achieve this, we build a concise, integrated,\nand research-friendly framework/codebase, named OpenFedLLM. It covers federated\ninstruction tuning for enhancing instruction-following capability, federated\nvalue alignment for aligning with human values, and 7 representative FL\nalgorithms. Besides, OpenFedLLM supports training on diverse domains, where we\ncover 8 training datasets; and provides comprehensive evaluations, where we\ncover 30+ evaluation metrics. Through extensive experiments, we observe that\nall FL algorithms outperform local training on training LLMs, demonstrating a\nclear performance improvement across a variety of settings. Notably, in a\nfinancial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can\noutperform GPT-4 by a significant margin while the model obtained through\nindividual training cannot, demonstrating strong motivation for clients to\nparticipate in FL. The code is available at\nhttps://github.com/rui-ye/OpenFedLLM.",
    "comment": "28 pages, 3 figures, 16 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.DC",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06954v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06954v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06954v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06952v1",
    "updated": "2024-02-10T13:42:14+00:00",
    "published": "2024-02-10T13:42:14+00:00",
    "title": "Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices",
    "authors": [
      {
        "name": "Sovanmonynuth Heng"
      },
      {
        "name": "Myeongseong Go"
      },
      {
        "name": "Youngsun Han"
      }
    ],
    "summary": "Current advancements in technology have focused the attention of the quantum\ncomputing community toward exploring the potential of near-term devices whose\ncomputing power surpasses that of classical computers in practical\napplications. An unresolved central question revolves around whether the\ninherent noise in these devices can be overcome or whether any potential\nquantum advantage would be limited. There is no doubt that crosstalk is one of\nthe main sources of noise in noisy intermediate-scale quantum (NISQ) systems,\nand it poses a fundamental challenge to hardware designs. Crosstalk between\nparallel instructions can corrupt quantum states and cause incorrect program\nexecution. In this study, we present a comprehensive analysis of the crosstalk\nerror effect on NISQ computers. Our approach is extremely straightforward and\npractical for characterizing the crosstalk error of various multi-qubit\ndevices. In particular, we combine the randomized benchmarking (RB) and\nsimultaneous randomized benchmarking (SRB) protocol to characterize the\ncrosstalk error from the correlation controlled-NOT (CNOT) gate. We demonstrate\nthis protocol experimentally on 5- \\& 7-qubit devices. Our results demonstrate\nthe crosstalk error model of two different IBM quantum devices over the\nexperimental week and compare the error variation against the machine, number\nof qubits, quantum volume, processor, and topology of the IBM quantum devices.\nWe then confirm the improvement in the circuit fidelity on different benchmarks\nby up to 3.06x via inserting an instruction barrier, as compared with an IBM\nquantum noisy device which offers near-optimal crosstalk mitigation in\npractice. Most importantly, we provide insight to ensure that the quantum\noperation can perform its quantum magic undisturbed.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "quant-ph",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06952v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06952v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06952v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06945v1",
    "updated": "2024-02-10T13:18:10+00:00",
    "published": "2024-02-10T13:18:10+00:00",
    "title": "Evaluation Metrics for Automated Typographic Poster Generation",
    "authors": [
      {
        "name": "S\u00e9rgio M. Rebelo"
      },
      {
        "name": "J. J. Merelo"
      },
      {
        "name": "Jo\u00e3o Bicker"
      },
      {
        "name": "Penousal Machado"
      }
    ],
    "summary": "Computational Design approaches facilitate the generation of typographic\ndesign, but evaluating these designs remains a challenging task. In this paper,\nwe propose a set of heuristic metrics for typographic design evaluation,\nfocusing on their legibility, which assesses the text visibility, aesthetics,\nwhich evaluates the visual quality of the design, and semantic features, which\nestimate how effectively the design conveys the content semantics. We\nexperiment with a constrained evolutionary approach for generating typographic\nposters, incorporating the proposed evaluation metrics with varied setups, and\ntreating the legibility metrics as constraints. We also integrate emotion\nrecognition to identify text semantics automatically and analyse the\nperformance of the approach and the visual characteristics outputs.",
    "comment": "Paper accepted be presented in the 13th International Conference\n  Artificial Intelligence in Music, Sound, Art and Design -- EvoMUSART 2024,\n  Held as Part of EvoStar 2024, Aberystwyth, Wales, United Kingdom, April\n  3\\textendash{}5, 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MM",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.HC",
      "68W50",
      "I.2.1; I.7; J.7; J.5"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06945v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06945v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06945v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06940v1",
    "updated": "2024-02-10T12:48:49+00:00",
    "published": "2024-02-10T12:48:49+00:00",
    "title": "Efficient Incremental Belief Updates Using Weighted Virtual Observations",
    "authors": [
      {
        "name": "David Tolpin"
      }
    ],
    "summary": "We present an algorithmic solution to the problem of incremental belief\nupdating in the context of Monte Carlo inference in Bayesian statistical models\nrepresented by probabilistic programs. Given a model and a sample-approximated\nposterior, our solution constructs a set of weighted observations to condition\nthe model such that inference would result in the same posterior. This problem\narises e.g. in multi-level modelling, incremental inference, inference in\npresence of privacy constraints. First, a set of virtual observations is\nselected, then, observation weights are found through a computationally\nefficient optimization procedure such that the reconstructed posterior\ncoincides with or closely approximates the original posterior. We implement and\napply the solution to a number of didactic examples and case studies, showing\nefficiency and robustness of our approach. The provided reference\nimplementation is agnostic to the probabilistic programming language or the\ninference algorithm, and can be applied to most mainstream probabilistic\nprogramming environments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06940v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06940v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06940v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06938v2",
    "updated": "2024-02-13T15:58:40+00:00",
    "published": "2024-02-10T12:26:20+00:00",
    "title": "Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities",
    "authors": [
      {
        "name": "Junjie Chu"
      },
      {
        "name": "Prashant Singh"
      },
      {
        "name": "Salman Toor"
      }
    ],
    "summary": "In the past few decades, the rapid development of information and internet\ntechnologies has spawned massive amounts of data and information. The\ninformation explosion drives many enterprises or individuals to seek to rent\ncloud computing infrastructure to put their applications in the cloud. However,\nthe agreements reached between cloud computing providers and clients are often\nnot efficient. Many factors affect the efficiency, such as the idleness of the\nproviders' cloud computing infrastructure, and the additional cost to the\nclients. One possible solution is to introduce a comprehensive, bargaining game\n(a type of negotiation), and schedule resources according to the negotiation\nresults. We propose an agent-based auto-negotiation system for resource\nscheduling based on fuzzy logic. The proposed method can complete a one-to-one\nauto-negotiation process and generate optimal offers for the provider and\nclient. We compare the impact of different member functions, fuzzy rule sets,\nand negotiation scenario cases on the offers to optimize the system. It can be\nconcluded that our proposed method can utilize resources more efficiently and\nis interpretable, highly flexible, and customizable. We successfully train\nmachine learning models to replace the fuzzy negotiation system to improve\nprocessing speed. The article also highlights possible future improvements to\nthe proposed system and machine learning models. All the codes and data are\navailable in the open-source repository.",
    "comment": "Accepted in IEEE CLOUD 2023. 13 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DC",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06938v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06938v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06938v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06937v1",
    "updated": "2024-02-10T12:23:08+00:00",
    "published": "2024-02-10T12:23:08+00:00",
    "title": "Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts",
    "authors": [
      {
        "name": "Masoumeh Javanbakhat"
      },
      {
        "name": "Md Tasnimul Hasan"
      },
      {
        "name": "Cristoph Lippert"
      }
    ],
    "summary": "In recent years, machine learning has witnessed extensive adoption across\nvarious sectors, yet its application in medical image-based disease detection\nand diagnosis remains challenging due to distribution shifts in real-world\ndata. In practical settings, deployed models encounter samples that differ\nsignificantly from the training dataset, especially in the health domain,\nleading to potential performance issues. This limitation hinders the\nexpressiveness and reliability of deep learning models in health applications.\nThus, it becomes crucial to identify methods capable of producing reliable\nuncertainty estimation in the context of distribution shifts in the health\nsector. In this paper, we explore the feasibility of using cutting-edge\nBayesian and non-Bayesian methods to detect distributionally shifted samples,\naiming to achieve reliable and trustworthy diagnostic predictions in\nsegmentation task. Specifically, we compare three distinct uncertainty\nestimation methods, each designed to capture either unimodal or multimodal\naspects in the posterior distribution. Our findings demonstrate that methods\ncapable of addressing multimodal characteristics in the posterior distribution,\noffer more dependable uncertainty estimates. This research contributes to\nenhancing the utility of deep learning in healthcare, making diagnostic\npredictions more robust and trustworthy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06937v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06937v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06937v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06932v1",
    "updated": "2024-02-10T12:10:13+00:00",
    "published": "2024-02-10T12:10:13+00:00",
    "title": "Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute",
    "authors": [
      {
        "name": "Tajima Shinji"
      },
      {
        "name": "Ren Sugihara"
      },
      {
        "name": "Ryota Kitahara"
      },
      {
        "name": "Masayuki Karasuyama"
      }
    ],
    "summary": "The graph classification problem has been widely studied; however, achieving\nan interpretable model with high predictive performance remains a challenging\nissue. This paper proposes an interpretable classification algorithm for\nattributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA\nlearns importance weights for small attributed subgraphs, called attributed\ngraphlets (AGs), while simultaneously optimizing their attribute vectors. This\nenables us to obtain a combination of subgraph structures and their attribute\nvectors that strongly contribute to discriminating different classes. A\nsignificant characteristics of LAGRA is that all the subgraph structures in the\ntraining dataset can be considered as a candidate structures of AGs. This\napproach can explore all the potentially important subgraphs exhaustively, but\nobviously, a naive implementation can require a large amount of computations.\nTo mitigate this issue, we propose an efficient pruning strategy by combining\nthe proximal gradient descent and a graph mining tree search. Our pruning\nstrategy can ensure that the quality of the solution is maintained compared to\nthe result without pruning. We empirically demonstrate that LAGRA has superior\nor comparable prediction performance to the standard existing algorithms\nincluding graph neural networks, while using only a small number of AGs in an\ninterpretable manner.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06932v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06932v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06932v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06931v1",
    "updated": "2024-02-10T12:05:52+00:00",
    "published": "2024-02-10T12:05:52+00:00",
    "title": "ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G",
    "authors": [
      {
        "name": "Masoud Shokrnezhad"
      },
      {
        "name": "Tarik Taleb"
      }
    ],
    "summary": "Anticipation for 6G's arrival comes with growing concerns about increased\nenergy consumption in computing and networking. The expected surge in connected\ndevices and resource-demanding applications presents unprecedented challenges\nfor energy resources. While sustainable resource allocation strategies have\nbeen discussed in the past, these efforts have primarily focused on\nsingle-domain orchestration or ignored the unique requirements posed by 6G. To\naddress this gap, we investigate the joint problem of service instance\nplacement and assignment, path selection, and request prioritization, dubbed\nPIRA. The objective function is to maximize the system's overall profit as a\nfunction of the number of concurrently supported requests while simultaneously\nminimizing energy consumption over an extended period of time. In addition,\nend-to-end latency requirements and resource capacity constraints are\nconsidered for computing and networking resources, where queuing theory is\nutilized to estimate the Age of Information (AoI) for requests. After\nformulating the problem in a non-linear fashion, we prove its NP-hardness and\npropose a method, denoted ORIENT. This method is based on the Double Dueling\nDeep Q-Learning (D3QL) mechanism and leverages Graph Neural Networks (GNNs) for\nstate encoding. Extensive numerical simulations demonstrate that ORIENT yields\nnear-optimal solutions for varying system sizes and request counts.",
    "comment": "Conference, 6 pages, 2 figures, 28 equations, 1 table, 1 algorithm,\n  and 16 references",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.NI",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06931v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06931v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06931v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06929v1",
    "updated": "2024-02-10T11:38:09+00:00",
    "published": "2024-02-10T11:38:09+00:00",
    "title": "Making a prototype of Seoul historical sites chatbot using Langchain",
    "authors": [
      {
        "name": "Jae Young Suh"
      },
      {
        "name": "Minsoo Kwak"
      },
      {
        "name": "Soo Yong Kim"
      },
      {
        "name": "Hyoungseo Cho"
      }
    ],
    "summary": "In this paper, we are going to share a draft of the development of a\nconversational agent created to disseminate information about historical sites\nlocated in the Seoul. The primary objective of the agent is to increase\nawareness among visitors who are not familiar with Seoul, about the presence\nand precise locations of valuable cultural heritage sites. It aims to promote a\nbasic understanding of Korea's rich and diverse cultural history. The agent is\nthoughtfully designed for accessibility in English and utilizes data generously\nprovided by the Seoul Metropolitan Government. Despite the limited data volume,\nit consistently delivers reliable and accurate responses, seamlessly aligning\nwith the available information. We have meticulously detailed the methodologies\nemployed in creating this agent and provided a comprehensive overview of its\nunderlying structure within the paper. Additionally, we delve into potential\nimprovements to enhance this initial version of the system, with a primary\nemphasis on expanding the available data through our prompting. In conclusion,\nwe provide an in-depth discussion of our expectations regarding the future\nimpact of this agent in promoting and facilitating the sharing of historical\nsites.",
    "comment": "4 pages, 4 figures, draft",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06929v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06929v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06929v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06923v1",
    "updated": "2024-02-10T11:13:13+00:00",
    "published": "2024-02-10T11:13:13+00:00",
    "title": "CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition",
    "authors": [
      {
        "name": "Ioannis Ziogas"
      },
      {
        "name": "Hessa Alfalahi"
      },
      {
        "name": "Ahsan H. Khandoker"
      },
      {
        "name": "Leontios J. Hadjileontiadis"
      }
    ],
    "summary": "Self-supervised learning (SSL) for automated speech recognition in terms of\nits emotional content, can be heavily degraded by the presence noise, affecting\nthe efficiency of modeling the intricate temporal and spectral informative\nstructures of speech. Recently, SSL on large speech datasets, as well as new\naudio-specific SSL proxy tasks, such as, temporal and frequency masking, have\nemerged, yielding superior performance compared to classic approaches drawn\nfrom the image augmentation domain. Our proposed contribution builds upon this\nsuccessful paradigm by introducing CochCeps-Augment, a novel bio-inspired\nmasking augmentation task for self-supervised contrastive learning of speech\nrepresentations. Specifically, we utilize the newly introduced bio-inspired\ncochlear cepstrogram (CCGRAM) to derive noise robust representations of input\nspeech, that are then further refined through a self-supervised learning\nscheme. The latter employs SimCLR to generate contrastive views of a CCGRAM\nthrough masking of its angle and quefrency dimensions. Our experimental\napproach and validations on the emotion recognition K-EmoCon benchmark dataset,\nfor the first time via a speaker-independent approach, features unsupervised\npre-training, linear probing and fine-tuning. Our results potentiate\nCochCeps-Augment to serve as a standard tool in speech emotion recognition\nanalysis, showing the added value of incorporating bio-inspired masking as an\ninformative augmentation task for self-supervision. Our code for implementing\nCochCeps-Augment will be made available at:\nhttps://github.com/GiannisZgs/CochCepsAugment.",
    "comment": "5 pages, 1 figure Accepted in IEEE ICASSP 2024 Workshops -\n  Self-Supervision in Audio, Speech, and Beyond",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD",
      "eess.SP",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06923v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06923v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06923v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06922v1",
    "updated": "2024-02-10T11:07:24+00:00",
    "published": "2024-02-10T11:07:24+00:00",
    "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
    "authors": [
      {
        "name": "Jonathan Evertz"
      },
      {
        "name": "Merlin Chlosta"
      },
      {
        "name": "Lea Sch\u00f6nherr"
      },
      {
        "name": "Thorsten Eisenhofer"
      }
    ],
    "summary": "Large Language Models (LLMs) are increasingly integrated with external tools.\nWhile these integrations can significantly improve the functionality of LLMs,\nthey also create a new attack surface where confidential data may be disclosed\nbetween different components. Specifically, malicious tools can exploit\nvulnerabilities in the LLM itself to manipulate the model and compromise the\ndata of other services, raising the question of how private data can be\nprotected in the context of LLM integrations.\n  In this work, we provide a systematic way of evaluating confidentiality in\nLLM-integrated systems. For this, we formalize a \"secret key\" game that can\ncapture the ability of a model to conceal private information. This enables us\nto compare the vulnerability of a model against confidentiality attacks and\nalso the effectiveness of different defense strategies. In this framework, we\nevaluate eight previously published attacks and four defenses. We find that\ncurrent defenses lack generalization across attack strategies. Building on this\nanalysis, we propose a method for robustness fine-tuning, inspired by\nadversarial training. This approach is effective in lowering the success rate\nof attackers and in improving the system's resilience against unknown attacks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06922v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06922v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06922v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06921v1",
    "updated": "2024-02-10T10:10:11+00:00",
    "published": "2024-02-10T10:10:11+00:00",
    "title": "Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System",
    "authors": [
      {
        "name": "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "H\u00e9ctor Alaiz-Moret\u00f3n"
      },
      {
        "name": "Jos\u00e9-Luis Casteleiro-Roca"
      },
      {
        "name": "Esteban Jove"
      },
      {
        "name": "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
      },
      {
        "name": "Isa\u00edas Garc\u00eda-Rodr\u00edguez"
      },
      {
        "name": "H\u00e9ctor Quinti\u00e1n"
      },
      {
        "name": "Jos\u00e9 Luis Calvo-Rolle"
      }
    ],
    "summary": "This work addresses the performance comparison between four clustering\ntechniques with the objective of achieving strong hybrid models in supervised\nlearning tasks. A real dataset from a bio-climatic house named Sotavento placed\non experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has\nbeen collected. Authors have chosen the thermal solar generation system in\norder to study how works applying several cluster methods followed by a\nregression technique to predict the output temperature of the system. With the\nobjective of defining the quality of each clustering method two possible\nsolutions have been implemented. The first one is based on three unsupervised\nlearning metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the\nsecond one, employs the most common error measurements for a regression\nalgorithm such as Multi Layer Perceptron.",
    "comment": null,
    "journal_ref": "Cybernetics and Systems, Volume 54, Issue 3, pages 286-305, 2023",
    "doi": "10.1080/01969722.2022.2030006",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1080/01969722.2022.2030006",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06921v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06921v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06921v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06919v1",
    "updated": "2024-02-10T09:53:21+00:00",
    "published": "2024-02-10T09:53:21+00:00",
    "title": "TREET: TRansfer Entropy Estimation via Transformer",
    "authors": [
      {
        "name": "Omer Luxembourg"
      },
      {
        "name": "Dor Tsur"
      },
      {
        "name": "Haim Permuter"
      }
    ],
    "summary": "Transfer entropy (TE) is a measurement in information theory that reveals the\ndirectional flow of information between processes, providing valuable insights\nfor a wide range of real-world applications. This work proposes Transfer\nEntropy Estimation via Transformers (TREET), a novel transformer-based approach\nfor estimating the TE for stationary processes. The proposed approach employs\nDonsker-Vardhan (DV) representation to TE and leverages the attention mechanism\nfor the task of neural estimation. We propose a detailed theoretical and\nempirical study of the TREET, comparing it to existing methods. To increase its\napplicability, we design an estimated TE optimization scheme that is motivated\nby the functional representation lemma. Afterwards, we take advantage of the\njoint optimization scheme to optimize the capacity of communication channels\nwith memory, which is a canonical optimization problem in information theory,\nand show the memory capabilities of our estimator. Finally, we apply TREET to\nreal-world feature analysis. Our work, applied with state-of-the-art deep\nlearning methods, opens a new door for communication problems which are yet to\nbe solved.",
    "comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IT",
    "categories": [
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06919v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06919v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06919v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06918v1",
    "updated": "2024-02-10T09:51:03+00:00",
    "published": "2024-02-10T09:51:03+00:00",
    "title": "Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought",
    "authors": [
      {
        "name": "Zhen-Yu Zhang"
      },
      {
        "name": "Siwei Han"
      },
      {
        "name": "Huaxiu Yao"
      },
      {
        "name": "Gang Niu"
      },
      {
        "name": "Masashi Sugiyama"
      }
    ],
    "summary": "To improve the ability of the large language model (LLMs) to handle complex\nreasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs\nto reason step-by-step, facilitating problem solving from simple to complex\ntasks. State-of-the-art approaches for generating such a chain involve\ninteractive collaboration, where the learner generates candidate intermediate\nthoughts, evaluated by the LLM, guiding the generation of subsequent thoughts.\nHowever, a widespread yet understudied problem is that the evaluation from the\nLLM is typically noisy and unreliable, potentially misleading the generation\nprocess in selecting promising intermediate thoughts. In this paper, motivated\nby Vapnik's principle, we propose a novel comparison-based CoT generation\nalgorithm that directly identifies the most promising thoughts with the noisy\nfeedback from the LLM. In each round, we randomly pair intermediate thoughts\nand directly prompt the LLM to select the more promising one from each pair,\nallowing us to identify the most promising thoughts through an iterative\nprocess. To further model the noise in the comparison, we resort to the\ntechniques of ensemble and dueling bandits and propose two variants of the\nproposed algorithm. Experiments on three real-world mathematical and reasoning\ntasks demonstrate the effectiveness of our proposed algorithm and verify the\nrationale of the direct pairwise comparison.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06918v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06918v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06918v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06912v1",
    "updated": "2024-02-10T09:15:21+00:00",
    "published": "2024-02-10T09:15:21+00:00",
    "title": "Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks",
    "authors": [
      {
        "name": "Annie Wong"
      },
      {
        "name": "Jacob de Nobel"
      },
      {
        "name": "Thomas B\u00e4ck"
      },
      {
        "name": "Aske Plaat"
      },
      {
        "name": "Anna V. Kononova"
      }
    ],
    "summary": "Although Deep Reinforcement Learning (DRL) methods can learn effective\npolicies for challenging problems such as Atari games and robotics tasks,\nalgorithms are complex and training times are often long. This study\ninvestigates how evolution strategies (ES) perform compared to gradient-based\ndeep reinforcement learning methods. We use ES to optimize the weights of a\nneural network via neuroevolution, performing direct policy search. We\nbenchmark both regular networks and policy networks consisting of a single\nlinear layer from observations to actions; for three classical ES methods and\nfor three gradient-based methods such as PPO. Our results reveal that ES can\nfind effective linear policies for many RL benchmark tasks, in contrast to DRL\nmethods that can only find successful policies using much larger networks,\nsuggesting that current benchmarks are easier to solve than previously assumed.\nInterestingly, also for higher complexity tasks, ES achieves results comparable\nto gradient-based DRL algorithms. Furthermore, we find that by directly\naccessing the memory state of the game, ES are able to find successful policies\nin Atari, outperforming DQN. While gradient-based methods have dominated the\nfield in recent years, ES offers an alternative that is easy to implement,\nparallelize, understand, and tune.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06912v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06912v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06912v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06908v1",
    "updated": "2024-02-10T08:26:06+00:00",
    "published": "2024-02-10T08:26:06+00:00",
    "title": "Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions",
    "authors": [
      {
        "name": "Lorenzo Giusti"
      }
    ],
    "summary": "The irreducible complexity of natural phenomena has led Graph Neural Networks\nto be employed as a standard model to perform representation learning tasks on\ngraph-structured data. While their capacity to capture local and global\npatterns is remarkable, the implications associated with long-range and\nhigher-order dependencies pose considerable challenges to such models. This\nwork starts with a theoretical framework to reveal the impact of network's\nwidth, depth, and graph topology on the over-squashing phenomena in\nmessage-passing neural networks. Then, the work drifts towards, higher-order\ninteractions and multi-relational inductive biases via Topological Neural\nNetworks. Such models propagate messages through higher-dimensional structures,\nproviding shortcuts or additional routes for information flow. With this\nconstruction, the underlying computational graph is no longer coupled with the\ninput graph structure, thus mitigating the aforementioned bottlenecks while\naccounting also for higher-order interactions. Inspired by Graph Attention\nNetworks, two topological attention networks are proposed: Simplicial and Cell\nAttention Networks. The rationale behind these architecture is to leverage the\nextended notion of neighbourhoods provided by the arrangement of groups of\nnodes within a simplicial or cell complex to design anisotropic aggregations\nable to measure the importance of the information coming from different regions\nof the domain. By doing so, they capture dependencies that conventional Graph\nNeural Networks might miss. Finally, a multi-way communication scheme is\nintroduced with Enhanced Cellular Isomorphism Networks, which augment\ntopological message passing schemes to enable a direct interactions among\ngroups of nodes arranged in ring-like structures.",
    "comment": "PhD thesis, 135 pages, 51 figures, 11 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06908v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06908v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06908v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06900v1",
    "updated": "2024-02-10T07:55:27+00:00",
    "published": "2024-02-10T07:55:27+00:00",
    "title": "Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric",
    "authors": [
      {
        "name": "Hyukhun Koh"
      },
      {
        "name": "Dohyung Kim"
      },
      {
        "name": "Minwoo Lee"
      },
      {
        "name": "Kyomin Jung"
      }
    ],
    "summary": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to discern the existence of toxicity in\nthe generated text. The majority of existing toxicity metrics rely on encoder\nmodels trained on specific toxicity datasets. However, these encoders are\nsusceptible to out-of-distribution (OOD) problems and depend on the definition\nof toxicity assumed in a dataset. In this paper, we introduce an automatic\nrobust metric grounded on LLMs to distinguish whether model responses are\ntoxic. We start by analyzing the toxicity factors, followed by examining the\nintrinsic toxic attributes of LLMs to ascertain their suitability as\nevaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators\n(LATTE), on evaluation datasets.The empirical results indicate outstanding\nperformance in measuring toxicity, improving upon state-of-the-art metrics by\n12 points in F1 score without training procedure. We also show that upstream\ntoxicity has an influence on downstream metrics.",
    "comment": "8 page long",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06900v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06900v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06900v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06894v1",
    "updated": "2024-02-10T07:20:49+00:00",
    "published": "2024-02-10T07:20:49+00:00",
    "title": "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators",
    "authors": [
      {
        "name": "Yuchen Hu"
      },
      {
        "name": "Chen Chen"
      },
      {
        "name": "Chao-Han Huck Yang"
      },
      {
        "name": "Ruizhe Li"
      },
      {
        "name": "Dong Zhang"
      },
      {
        "name": "Zhehuai Chen"
      },
      {
        "name": "Eng Siong Chng"
      }
    ],
    "summary": "Recent advances in large language models (LLMs) have stepped forward the\ndevelopment of multilingual speech and machine translation by its reduced\nrepresentation errors and incorporated external knowledge. However, both\ntranslation tasks typically utilize beam search decoding and top-1 hypothesis\nselection for inference. These techniques struggle to fully exploit the rich\ninformation in the diverse N-best hypotheses, making them less optimal for\ntranslation tasks that require a single, high-quality output sequence. In this\npaper, we propose a new generative paradigm for translation tasks, namely\n\"GenTranslate\", which builds upon LLMs to generate better results from the\ndiverse translation versions in N-best list. Leveraging the rich linguistic\nknowledge and strong reasoning abilities of LLMs, our new paradigm can\nintegrate the rich information in N-best candidates to generate a\nhigher-quality translation result. Furthermore, to support LLM finetuning, we\nbuild and release a HypoTranslate dataset that contains over 592K\nhypotheses-translation pairs in 11 languages. Experiments on various speech and\nmachine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that\nour GenTranslate significantly outperforms the state-of-the-art model.",
    "comment": "17 pages. This work is open sourced at:\n  https://github.com/YUCHEN005/GenTranslate",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06894v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06894v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06894v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06892v1",
    "updated": "2024-02-10T06:49:08+00:00",
    "published": "2024-02-10T06:49:08+00:00",
    "title": "Understanding Test-Time Augmentation",
    "authors": [
      {
        "name": "Masanari Kimura"
      }
    ],
    "summary": "Test-Time Augmentation (TTA) is a very powerful heuristic that takes\nadvantage of data augmentation during testing to produce averaged output.\nDespite the experimental effectiveness of TTA, there is insufficient discussion\nof its theoretical aspects. In this paper, we aim to give theoretical\nguarantees for TTA and clarify its behavior.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06892v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06892v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06892v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06886v1",
    "updated": "2024-02-10T04:54:15+00:00",
    "published": "2024-02-10T04:54:15+00:00",
    "title": "Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF",
    "authors": [
      {
        "name": "Han Shen"
      },
      {
        "name": "Zhuoran Yang"
      },
      {
        "name": "Tianyi Chen"
      }
    ],
    "summary": "Bilevel optimization has been recently applied to many machine learning\ntasks. However, their applications have been restricted to the supervised\nlearning setting, where static objective functions with benign structures are\nconsidered. But bilevel problems such as incentive design, inverse\nreinforcement learning (RL), and RL from human feedback (RLHF) are often\nmodeled as dynamic objective functions that go beyond the simple static\nobjective structures, which pose significant challenges of using existing\nbilevel solutions. To tackle this new class of bilevel problems, we introduce\nthe first principled algorithmic framework for solving bilevel RL problems\nthrough the lens of penalty formulation. We provide theoretical studies of the\nproblem landscape and its penalty-based (policy) gradient algorithms. We\ndemonstrate the effectiveness of our algorithms via simulations in the\nStackelberg Markov game, RL from human feedback and incentive design.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06886v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06886v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06886v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06885v1",
    "updated": "2024-02-10T04:50:36+00:00",
    "published": "2024-02-10T04:50:36+00:00",
    "title": "DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine",
    "authors": [
      {
        "name": "Parisa Salmanian"
      },
      {
        "name": "Angelos Chatzimparmpas"
      },
      {
        "name": "Ali Can Karaca"
      },
      {
        "name": "Rafael M. Martins"
      }
    ],
    "summary": "Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular\nfor transforming complex datasets into simpler visual representations. However,\nwhile effective in uncovering general dataset patterns, these methods may\nintroduce artifacts and suffer from interpretability issues. This paper\npresents DimVis, a visualization tool that employs supervised Explainable\nBoosting Machine (EBM) models (trained on user-selected data of interest) as an\ninterpretation assistant for DR projections. Our tool facilitates\nhigh-dimensional data analysis by providing an interpretation of feature\nrelevance in visual clusters through interactive exploration of UMAP\nprojections. Specifically, DimVis uses a contrastive EBM model that is trained\nin real time to differentiate between the data inside and outside a cluster of\ninterest. Taking advantage of the inherent explainable nature of the EBM, we\nthen use this model to interpret the cluster itself via single and pairwise\nfeature comparisons in a ranking based on the EBM model's feature importance.\nThe applicability and effectiveness of DimVis are demonstrated through two use\ncases involving real-world datasets, and we also discuss the limitations and\npotential directions for future research.",
    "comment": "This manuscript is currently under review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.LG",
      "stat.CO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06885v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06885v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06885v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06884v1",
    "updated": "2024-02-10T04:45:27+00:00",
    "published": "2024-02-10T04:45:27+00:00",
    "title": "Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning",
    "authors": [
      {
        "name": "Kang Du"
      },
      {
        "name": "Yu Xiang"
      }
    ],
    "summary": "We study the data-generating mechanism for reconstructive SSL to shed light\non its effectiveness. With an infinite amount of labeled samples, we provide a\nsufficient and necessary condition for perfect linear approximation. The\ncondition reveals a full-rank component that preserves the label classes of Y,\nalong with a redundant component. Motivated by the condition, we propose to\napproximate the redundant component by a low-rank factorization and measure the\napproximation quality by introducing a new quantity $\\epsilon_s$, parameterized\nby the rank of factorization s. We incorporate $\\epsilon_s$ into the excess\nrisk analysis under both linear regression and ridge regression settings, where\nthe latter regularization approach is to handle scenarios when the dimension of\nthe learned features is much larger than the number of labeled samples n for\ndownstream tasks. We design three stylized experiments to compare SSL with\nsupervised learning under different settings to support our theoretical\nfindings.",
    "comment": "Accepted to the 3rd Conference on Causal Learning and Reasoning\n  (CLeaR)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06884v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06884v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06884v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06871v1",
    "updated": "2024-02-10T03:21:13+00:00",
    "published": "2024-02-10T03:21:13+00:00",
    "title": "Non-autoregressive Generative Models for Reranking Recommendation",
    "authors": [
      {
        "name": "Yuxin Ren"
      },
      {
        "name": "Qiya Yang"
      },
      {
        "name": "Yichun Wu"
      },
      {
        "name": "Wei Xu"
      },
      {
        "name": "Yalong Wang"
      },
      {
        "name": "Zhiqiang Zhang"
      }
    ],
    "summary": "In a multi-stage recommendation system, reranking plays a crucial role by\nmodeling the intra-list correlations among items.The key challenge of reranking\nlies in the exploration of optimal sequences within the combinatorial space of\npermutations. Recent research proposes a generator-evaluator learning paradigm,\nwhere the generator generates multiple feasible sequences and the evaluator\npicks out the best sequence based on the estimated listwise score. Generator is\nof vital importance, and generative models are well-suited for the generator\nfunction. Current generative models employ an autoregressive strategy for\nsequence generation. However, deploying autoregressive models in real-time\nindustrial systems is challenging. Hence, we propose a Non-AutoRegressive\ngenerative model for reranking Recommendation (NAR4Rec) designed to enhance\nefficiency and effectiveness. To address challenges related to sparse training\nsamples and dynamic candidates impacting model convergence, we introduce a\nmatching model. Considering the diverse nature of user feedback, we propose a\nsequence-level unlikelihood training objective to distinguish feasible from\nunfeasible sequences. Additionally, to overcome the lack of dependency modeling\nin non-autoregressive models regarding target items, we introduce contrastive\ndecoding to capture correlations among these items. Extensive offline\nexperiments on publicly available datasets validate the superior performance of\nour proposed approach compared to the existing state-of-the-art reranking\nmethods. Furthermore, our method has been fully deployed in a popular video app\nKuaishou with over 300 million daily active users, significantly enhancing\nonline recommendation quality, and demonstrating the effectiveness and\nefficiency of our approach.",
    "comment": "Work in progress",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06871v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06871v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06871v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06864v2",
    "updated": "2024-02-13T06:14:21+00:00",
    "published": "2024-02-10T03:04:57+00:00",
    "title": "Discriminative Adversarial Unlearning",
    "authors": [
      {
        "name": "Rohan Sharma"
      },
      {
        "name": "Shijie Zhou"
      },
      {
        "name": "Kaiyi Ji"
      },
      {
        "name": "Changyou Chen"
      }
    ],
    "summary": "We introduce a novel machine unlearning framework founded upon the\nestablished principles of the min-max optimization paradigm. We capitalize on\nthe capabilities of strong Membership Inference Attacks (MIA) to facilitate the\nunlearning of specific samples from a trained model. We consider the scenario\nof two networks, the attacker $\\mathbf{A}$ and the trained defender\n$\\mathbf{D}$ pitted against each other in an adversarial objective, wherein the\nattacker aims at teasing out the information of the data to be unlearned in\norder to infer membership, and the defender unlearns to defend the network\nagainst the attack, whilst preserving its general performance. The algorithm\ncan be trained end-to-end using backpropagation, following the well known\niterative min-max approach in updating the attacker and the defender. We\nadditionally incorporate a self-supervised objective effectively addressing the\nfeature space discrepancies between the forget set and the validation set,\nenhancing unlearning performance. Our proposed algorithm closely approximates\nthe ideal benchmark of retraining from scratch for both random sample\nforgetting and class-wise forgetting schemes on standard machine-unlearning\ndatasets. Specifically, on the class unlearning scheme, the method demonstrates\nnear-optimal performance and comprehensively overcomes known methods over the\nrandom sample forgetting scheme across all metrics and multiple network pruning\nstrategies.",
    "comment": "13 pages including references, 2 tables, 2 figures and 1 algorithm",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06864v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06864v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06864v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06861v1",
    "updated": "2024-02-10T01:50:19+00:00",
    "published": "2024-02-10T01:50:19+00:00",
    "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
    "authors": [
      {
        "name": "Yansong Ning"
      },
      {
        "name": "Hao Liu"
      }
    ],
    "summary": "Urban knowledge graph has recently worked as an emerging building block to\ndistill critical knowledge from multi-sourced urban data for diverse urban\napplication scenarios. Despite its promising benefits, urban knowledge graph\nconstruction (UrbanKGC) still heavily relies on manual effort, hindering its\npotential advancement. This paper presents UrbanKGent, a unified large language\nmodel agent framework, for urban knowledge graph construction. Specifically, we\nfirst construct the knowledgeable instruction set for UrbanKGC tasks (such as\nrelational triplet extraction and knowledge graph completion) via\nheterogeneity-aware and geospatial-infused instruction generation. Moreover, we\npropose a tool-augmented iterative trajectory refinement module to enhance and\nrefine the trajectories distilled from GPT-4. Through hybrid instruction\nfine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC\nagent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world\ndatasets using both human and GPT-4 self-evaluation. The experimental results\ndemonstrate that UrbanKGent-13B not only can significantly outperform 21\nbaselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4,\nby more than 10\\% with approximately 20 times lower cost. We deploy\nUrbanKGent-13B to provide online services, which can construct an UrbanKG with\nthousands of times richer relationships using only one-fifth of the data\ncompared with the existing benchmark. Our data, code, and opensource UrbanKGC\nagent are available at https://github.com/usail-hkust/UrbanKGent.",
    "comment": "Under review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06861v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06861v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06861v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06859v1",
    "updated": "2024-02-10T01:47:10+00:00",
    "published": "2024-02-10T01:47:10+00:00",
    "title": "LiRank: Industrial Large Scale Ranking Models at LinkedIn",
    "authors": [
      {
        "name": "Fedor Borisyuk"
      },
      {
        "name": "Mingzhou Zhou"
      },
      {
        "name": "Qingquan Song"
      },
      {
        "name": "Siyu Zhu"
      },
      {
        "name": "Birjodh Tiwana"
      },
      {
        "name": "Ganesh Parameswaran"
      },
      {
        "name": "Siddharth Dangi"
      },
      {
        "name": "Lars Hertel"
      },
      {
        "name": "Qiang Xiao"
      },
      {
        "name": "Xiaochen Hou"
      },
      {
        "name": "Yunbo Ouyang"
      },
      {
        "name": "Aman Gupta"
      },
      {
        "name": "Sheallika Singh"
      },
      {
        "name": "Dan Liu"
      },
      {
        "name": "Hailing Cheng"
      },
      {
        "name": "Lei Le"
      },
      {
        "name": "Jonathan Hung"
      },
      {
        "name": "Sathiya Keerthi"
      },
      {
        "name": "Ruoyan Wang"
      },
      {
        "name": "Fengyu Zhang"
      },
      {
        "name": "Mohit Kothari"
      },
      {
        "name": "Chen Zhu"
      },
      {
        "name": "Daqi Sun"
      },
      {
        "name": "Yun Dai"
      },
      {
        "name": "Xun Luan"
      },
      {
        "name": "Sirou Zhu"
      },
      {
        "name": "Zhiwei Wang"
      },
      {
        "name": "Neil Daftary"
      },
      {
        "name": "Qianqi Shen"
      },
      {
        "name": "Chengming Jiang"
      },
      {
        "name": "Haichao Wei"
      },
      {
        "name": "Maneesh Varshney"
      },
      {
        "name": "Amol Ghoting"
      },
      {
        "name": "Souvik Ghosh"
      }
    ],
    "summary": "We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "H.3.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06859v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06859v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06859v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06855v1",
    "updated": "2024-02-10T01:36:39+00:00",
    "published": "2024-02-10T01:36:39+00:00",
    "title": "For Better or For Worse? Learning Minimum Variance Features With Label Augmentation",
    "authors": [
      {
        "name": "Muthu Chidambaram"
      },
      {
        "name": "Rong Ge"
      }
    ],
    "summary": "Data augmentation has been pivotal in successfully training deep learning\nmodels on classification tasks over the past decade. An important subclass of\ndata augmentation techniques - which includes both label smoothing and Mixup -\ninvolves modifying not only the input data but also the input label during\nmodel training. In this work, we analyze the role played by the label\naugmentation aspect of such methods. We prove that linear models on linearly\nseparable data trained with label augmentation learn only the minimum variance\nfeatures in the data, while standard training (which includes weight decay) can\nlearn higher variance features. An important consequence of our results is\nnegative: label smoothing and Mixup can be less robust to adversarial\nperturbations of the training data when compared to standard training. We\nverify that our theory reflects practice via a range of experiments on\nsynthetic data and image classification benchmarks.",
    "comment": "20 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06855v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06855v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06855v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06854v1",
    "updated": "2024-02-10T01:30:24+00:00",
    "published": "2024-02-10T01:30:24+00:00",
    "title": "Gyroscope-Assisted Motion Deblurring Network",
    "authors": [
      {
        "name": "Simin Luan"
      },
      {
        "name": "Cong Yang"
      },
      {
        "name": "Zeyd Boukhers"
      },
      {
        "name": "Xue Qin"
      },
      {
        "name": "Dongfeng Cheng"
      },
      {
        "name": "Wei Sui"
      },
      {
        "name": "Zhijun Li"
      }
    ],
    "summary": "Image research has shown substantial attention in deblurring networks in\nrecent years. Yet, their practical usage in real-world deblurring, especially\nmotion blur, remains limited due to the lack of pixel-aligned training triplets\n(background, blurred image, and blur heat map) and restricted information\ninherent in blurred images. This paper presents a simple yet efficient\nframework to synthetic and restore motion blur images using Inertial\nMeasurement Unit (IMU) data. Notably, the framework includes a strategy for\ntraining triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD)\nnetwork for blurred image restoration. The rationale is that through harnessing\nIMU data, we can determine the transformation of the camera pose during the\nimage exposure phase, facilitating the deduction of the motion trajectory (aka.\nblur trajectory) for each point inside the three-dimensional space. Thus, the\nsynthetic triplets using our strategy are inherently close to natural motion\nblur, strictly pixel-aligned, and mass-producible. Through comprehensive\nexperiments, we demonstrate the advantages of the proposed framework: only\ntwo-pixel errors between our synthetic and real-world blur trajectories, a\nmarked improvement (around 33.17%) of the state-of-the-art deblurring method\nMIMO on Peak Signal-to-Noise Ratio (PSNR).",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06854v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06854v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06854v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06852v1",
    "updated": "2024-02-10T01:11:59+00:00",
    "published": "2024-02-10T01:11:59+00:00",
    "title": "ChemLLM: A Chemical Large Language Model",
    "authors": [
      {
        "name": "Di Zhang"
      },
      {
        "name": "Wei Liu"
      },
      {
        "name": "Qian Tan"
      },
      {
        "name": "Jingdan Chen"
      },
      {
        "name": "Hang Yan"
      },
      {
        "name": "Yuliang Yan"
      },
      {
        "name": "Jiatong Li"
      },
      {
        "name": "Weiran Huang"
      },
      {
        "name": "Xiangyu Yue"
      },
      {
        "name": "Dongzhan Zhou"
      },
      {
        "name": "Shufei Zhang"
      },
      {
        "name": "Mao Su"
      },
      {
        "name": "Hansen Zhong"
      },
      {
        "name": "Yuqiang Li"
      },
      {
        "name": "Wanli Ouyang"
      }
    ],
    "summary": "Large language models (LLMs) have made impressive progress in chemistry\napplications, including molecular property prediction, molecular generation,\nexperimental protocol design, etc. However, the community lacks a\ndialogue-based model specifically designed for chemistry. The challenge arises\nfrom the fact that most chemical data and scientific knowledge are primarily\nstored in structured databases, and the direct use of these structured data\ncompromises the model's ability to maintain coherent dialogue. To tackle this\nissue, we develop a novel template-based instruction construction method that\ntransforms structured knowledge into plain dialogue, making it suitable for\nlanguage model training. By leveraging this approach, we develop ChemLLM, the\nfirst large language model dedicated to chemistry, capable of performing\nvarious tasks across chemical disciplines with smooth dialogue interaction.\nChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name\nconversion, molecular caption, and reaction prediction, and surpasses GPT-4 on\ntwo of them. Remarkably, ChemLLM also shows exceptional adaptability to related\nmathematical and physical tasks despite being trained mainly on\nchemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in\nspecialized NLP tasks within chemistry, such as literature translation and\ncheminformatic programming. ChemLLM opens up a new avenue for exploration\nwithin chemical studies, while our method of integrating structured chemical\nknowledge into dialogue systems sets a new frontier for developing LLMs across\nvarious scientific fields. Codes, Datasets, and Model weights are publicly\naccessible at hf.co/AI4Chem/ChemLLM-7B-Chat.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06852v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06852v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06852v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07949v1",
    "updated": "2024-02-10T00:49:46+00:00",
    "published": "2024-02-10T00:49:46+00:00",
    "title": "Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management",
    "authors": [
      {
        "name": "Ashok Khanna"
      },
      {
        "name": "Olivier Francon"
      },
      {
        "name": "Risto Miikkulainen"
      }
    ],
    "summary": "Diabetes, a chronic condition that impairs how the body turns food into\nenergy, i.e. blood glucose, affects 38 million people in the US alone. The\nstandard treatment is to supplement carbohydrate intake with an artificial\npancreas, i.e. a continuous insulin pump (basal shots), as well as occasional\ninsulin injections (bolus shots). The goal of the treatment is to keep blood\nglucose at the center of an acceptable range, as measured through a continuous\nglucose meter. A secondary goal is to minimize injections, which are unpleasant\nand difficult for some patients to implement. In this study, neuroevolution was\nused to discover an optimal strategy for the treatment. Based on a dataset of\n30 days of treatment and measurements of a single patient, a random forest was\nfirst trained to predict future glucose levels. A neural network was then\nevolved to prescribe carbohydrates, basal pumping levels, and bolus injections.\nEvolution discovered a Pareto front that reduced deviation from the target and\nnumber of injections compared to the original data, thus improving patients'\nquality of life. To make the system easier to adopt, a language interface was\ndeveloped with a large language model. Thus, these technologies not only\nimprove patient care but also adoption in a broader population.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.QM",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07949v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07949v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07949v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06827v1",
    "updated": "2024-02-09T23:29:54+00:00",
    "published": "2024-02-09T23:29:54+00:00",
    "title": "RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations",
    "authors": [
      {
        "name": "Enyi Jiang"
      },
      {
        "name": "Gagandeep Singh"
      }
    ],
    "summary": "There is considerable work on improving robustness against adversarial\nattacks bounded by a single $l_p$ norm using adversarial training (AT).\nHowever, the multiple-norm robustness (union accuracy) of AT models is still\nlow. We observe that simultaneously obtaining good union and clean accuracy is\nhard since there are tradeoffs between robustness against multiple $l_p$\nperturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs\nfrom the lens of distribution shifts, we identify the key tradeoff pair among\n$l_p$ attacks to boost efficiency and design a logit pairing loss to improve\nthe union accuracy. Next, we connect natural training with AT via gradient\nprojection, to find and incorporate useful information from natural training\ninto AT, which moderates the accuracy/robustness tradeoff. Combining our\ncontributions, we propose a framework called \\textbf{RAMP}, to boost the\nrobustness against multiple $l_p$ perturbations. We show \\textbf{RAMP} can be\neasily adapted for both robust fine-tuning and full AT. For robust fine-tuning,\n\\textbf{RAMP} obtains a union accuracy up to $53.5\\%$ on CIFAR-10, and $29.7\\%$\non ImageNet. For training from scratch, \\textbf{RAMP} achieves SOTA union\naccuracy of $44.6\\%$ and relatively good clean accuracy of $81.2\\%$ on\nResNet-18 against AutoAttack on CIFAR-10.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06827v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06827v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06827v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06820v1",
    "updated": "2024-02-09T23:02:57+00:00",
    "published": "2024-02-09T23:02:57+00:00",
    "title": "Forecasting Events in Soccer Matches Through Language",
    "authors": [
      {
        "name": "Tiago Mendes-Neves"
      },
      {
        "name": "Lu\u00eds Meireles"
      },
      {
        "name": "Jo\u00e3o Mendes-Moreira"
      }
    ],
    "summary": "This paper introduces an approach to predicting the next event in a soccer\nmatch, a challenge bearing remarkable similarities to the problem faced by\nLarge Language Models (LLMs). Unlike other methods that severely limit event\ndynamics in soccer, often abstracting from many variables or relying on a mix\nof sequential models, our research proposes a novel technique inspired by the\nmethodologies used in LLMs. These models predict a complete chain of variables\nthat compose an event, significantly simplifying the construction of Large\nEvent Models (LEMs) for soccer. Utilizing deep learning on the publicly\navailable WyScout dataset, the proposed approach notably surpasses the\nperformance of previous LEM proposals in critical areas, such as the prediction\naccuracy of the next event type. This paper highlights the utility of LEMs in\nvarious applications, including betting and match analytics. Moreover, we show\nthat LEMs provide a simulation backbone on which many analytics pipelines can\nbe built, an approach opposite to the current specialized single-purpose\nmodels. LEMs represent a pivotal advancement in soccer analytics, establishing\na foundational framework for multifaceted analytics pipelines through a\nsingular machine-learning model.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06820v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06820v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06820v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06819v2",
    "updated": "2024-02-13T19:49:04+00:00",
    "published": "2024-02-09T23:00:29+00:00",
    "title": "Monitored Markov Decision Processes",
    "authors": [
      {
        "name": "Simone Parisi"
      },
      {
        "name": "Montaser Mohammedalamen"
      },
      {
        "name": "Alireza Kazemipour"
      },
      {
        "name": "Matthew E. Taylor"
      },
      {
        "name": "Michael Bowling"
      }
    ],
    "summary": "In reinforcement learning (RL), an agent learns to perform a task by\ninteracting with an environment and receiving feedback (a numerical reward) for\nits actions. However, the assumption that rewards are always observable is\noften not applicable in real-world problems. For example, the agent may need to\nask a human to supervise its actions or activate a monitoring system to receive\nfeedback. There may even be a period of time before rewards become observable,\nor a period of time after which rewards are no longer given. In other words,\nthere are cases where the environment generates rewards in response to the\nagent's actions but the agent cannot observe them. In this paper, we formalize\na novel but general RL framework - Monitored MDPs - where the agent cannot\nalways observe rewards. We discuss the theoretical and practical consequences\nof this setting, show challenges raised even in toy environments, and propose\nalgorithms to begin to tackle this novel setting. This paper introduces a\npowerful new formalism that encompasses both new and existing problems and lays\nthe foundation for future research.",
    "comment": "AAMAS 2024, Main Track",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06819v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06819v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06819v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06818v1",
    "updated": "2024-02-09T22:59:20+00:00",
    "published": "2024-02-09T22:59:20+00:00",
    "title": "Towards a Systematic Approach to Design New Ensemble Learning Algorithms",
    "authors": [
      {
        "name": "Jo\u00e3o Mendes-Moreira"
      },
      {
        "name": "Tiago Mendes-Neves"
      }
    ],
    "summary": "Ensemble learning has been a focal point of machine learning research due to\nits potential to improve predictive performance. This study revisits the\nfoundational work on ensemble error decomposition, historically confined to\nbias-variance-covariance analysis for regression problems since the 1990s.\nRecent advancements introduced a \"unified theory of diversity,\" which proposes\nan innovative bias-variance-diversity decomposition framework. Leveraging this\ncontemporary understanding, our research systematically explores the\napplication of this decomposition to guide the creation of new ensemble\nlearning algorithms. Focusing on regression tasks, we employ neural networks as\nbase learners to investigate the practical implications of this theoretical\nframework. This approach used 7 simple ensemble methods, we name them\nstrategies, for neural networks that were used to generate 21 new ensemble\nalgorithms. Among these, most of the methods aggregated with the snapshot\nstrategy, one of the 7 strategies used, showcase superior predictive\nperformance across diverse datasets w.r.t. the Friedman rank test with the\nConover post-hoc test. Our systematic design approach contributes a suite of\neffective new algorithms and establishes a structured pathway for future\nensemble learning algorithm development.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06818v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06818v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06818v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06815v1",
    "updated": "2024-02-09T22:47:25+00:00",
    "published": "2024-02-09T22:47:25+00:00",
    "title": "Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models",
    "authors": [
      {
        "name": "Tiago Mendes-Neves"
      },
      {
        "name": "Lu\u00eds Meireles"
      },
      {
        "name": "Jo\u00e3o Mendes-Moreira"
      }
    ],
    "summary": "This paper introduces an innovative application of Large Event Models (LEMs),\nakin to Large Language Models, to the domain of soccer analytics. By learning\nthe \"language\" of soccer - predicting variables for subsequent events rather\nthan words LEMs facilitate the simulation of matches and offer various\napplications, including player performance prediction across different team\ncontexts. We focus on fine-tuning LEMs with the WyScout dataset for the\n2017-2018 Premier League season to derive specific insights into player\ncontributions and team strategies. Our methodology involves adapting these\nmodels to reflect the nuanced dynamics of soccer, enabling the evaluation of\nhypothetical transfers. Our findings confirm the effectiveness and limitations\nof LEMs in soccer analytics, highlighting the model's capability to forecast\nteams' expected standings and explore high-profile scenarios, such as the\npotential effects of transferring Cristiano Ronaldo or Lionel Messi to\ndifferent teams in the Premier League. This analysis underscores the importance\nof context in evaluating player quality. While general metrics may suggest\nsignificant differences between players, contextual analyses reveal narrower\ngaps in performance within specific team frameworks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06815v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06815v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06815v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06812v1",
    "updated": "2024-02-09T22:27:29+00:00",
    "published": "2024-02-09T22:27:29+00:00",
    "title": "A Kalman Filter Based Framework for Monitoring the Performance of In-Hospital Mortality Prediction Models Over Time",
    "authors": [
      {
        "name": "Jiacheng Liu"
      },
      {
        "name": "Lisa Kirkland"
      },
      {
        "name": "Jaideep Srivastava"
      }
    ],
    "summary": "Unlike in a clinical trial, where researchers get to determine the least\nnumber of positive and negative samples required, or in a machine learning\nstudy where the size and the class distribution of the validation set is static\nand known, in a real-world scenario, there is little control over the size and\ndistribution of incoming patients. As a result, when measured during different\ntime periods, evaluation metrics like Area under the Receiver Operating Curve\n(AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly\ncomparable. Therefore, in this study, for binary classifiers running in a long\ntime period, we proposed to adjust these performance metrics for sample size\nand class distribution, so that a fair comparison can be made between two time\nperiods. Note that the number of samples and the class distribution, namely the\nratio of positive samples, are two robustness factors which affect the variance\nof AUCROC. To better estimate the mean of performance metrics and understand\nthe change of performance over time, we propose a Kalman filter based framework\nwith extrapolated variance adjusted for the total number of samples and the\nnumber of positive samples during different time periods. The efficacy of this\nmethod is demonstrated first on a synthetic dataset and then retrospectively\napplied to a 2-days ahead in-hospital mortality prediction model for COVID-19\npatients during 2021 and 2022. Further, we conclude that our prediction model\nis not significantly affected by the evolution of the disease, improved\ntreatments and changes in hospital operational plans.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06812v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06812v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06812v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06811v1",
    "updated": "2024-02-09T22:21:55+00:00",
    "published": "2024-02-09T22:21:55+00:00",
    "title": "Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation",
    "authors": [
      {
        "name": "Andrew Smart"
      },
      {
        "name": "Ding Wang"
      },
      {
        "name": "Ellis Monk"
      },
      {
        "name": "Mark D\u00edaz"
      },
      {
        "name": "Atoosa Kasirzadeh"
      },
      {
        "name": "Erin Van Liemt"
      },
      {
        "name": "Sonja Schmer-Galunder"
      }
    ],
    "summary": "Data annotation remains the sine qua non of machine learning and AI. Recent\nempirical work on data annotation has begun to highlight the importance of\nrater diversity for fairness, model performance, and new lines of research have\nbegun to examine the working conditions for data annotation workers, the\nimpacts and role of annotator subjectivity on labels, and the potential\npsychological harms from aspects of annotation work. This paper outlines a\ncritical genealogy of data annotation; starting with its psychological and\nperceptual aspects. We draw on similarities with critiques of the rise of\ncomputerized lab-based psychological experiments in the 1970's which question\nwhether these experiments permit the generalization of results beyond the\nlaboratory settings within which these results are typically obtained. Do data\nannotations permit the generalization of results beyond the settings, or\nlocations, in which they were obtained? Psychology is overly reliant on\nparticipants from Western, Educated, Industrialized, Rich, and Democratic\nsocieties (WEIRD). Many of the people who work as data annotation platform\nworkers, however, are not from WEIRD countries; most data annotation workers\nare based in Global South countries. Social categorizations and classifications\nfrom WEIRD countries are imposed on non-WEIRD annotators through instructions\nand tasks, and through them, on data, which is then used to train or evaluate\nAI models in WEIRD countries. We synthesize evidence from several recent lines\nof research and argue that data annotation is a form of automated social\ncategorization that risks entrenching outdated and static social categories\nthat are in reality dynamic and changing. We propose a framework for\nunderstanding the interplay of the global social conditions of data annotation\nwith the subjective phenomenological experience of data annotation work.",
    "comment": "18 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06811v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06811v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06811v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06810v1",
    "updated": "2024-02-09T22:15:39+00:00",
    "published": "2024-02-09T22:15:39+00:00",
    "title": "Evaluating Co-Creativity using Total Information Flow",
    "authors": [
      {
        "name": "Vignesh Gokul"
      },
      {
        "name": "Chris Francis"
      },
      {
        "name": "Shlomo Dubnov"
      }
    ],
    "summary": "Co-creativity in music refers to two or more musicians or musical agents\ninteracting with one another by composing or improvising music. However, this\nis a very subjective process and each musician has their own preference as to\nwhich improvisation is better for some context. In this paper, we aim to create\na measure based on total information flow to quantitatively evaluate the\nco-creativity process in music. In other words, our measure is an indication of\nhow \"good\" a creative musical process is. Our main hypothesis is that a good\nmusical creation would maximize information flow between the participants\ncaptured by music voices recorded in separate tracks. We propose a method to\ncompute the information flow using pre-trained generative models as entropy\nestimators. We demonstrate how our method matches with human perception using a\nqualitative study.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.IT",
      "cs.LG",
      "eess.AS",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06810v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06810v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06810v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06808v1",
    "updated": "2024-02-09T22:14:40+00:00",
    "published": "2024-02-09T22:14:40+00:00",
    "title": "Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction",
    "authors": [
      {
        "name": "Jiacheng Liu"
      },
      {
        "name": "Jaideep Srivastava"
      }
    ],
    "summary": "In healthcare, thanks to many model agnostic methods, explainability of the\nprediction scores made by deep learning applications has improved. However, we\nnote that for daily or hourly risk of deterioration prediction of in-hospital\npatients, not only the predicted risk probability score matters, but also the\nvariance of the risk scores play key roles in aiding clinical decision making.\nIn this paper, we propose to use delta's method to approximate variance of\nprediction deterministically, such that the SHAP method can be adopted to\nattribute contribution of variance. The prediction variance is estimated by\nsampling the conditional hidden space in variational models and is propagated\nto input clinical variables based on Shapley values of the variance game. This\napproach works with variational time series models such as variational\nrecurrent neural networks and variational transformers. We further argue that\nvariational time series models are perfect fits for achieving a balance between\npredictive power and explainability through a series of experiments on a public\nclinical ICU datasets. Since SHAP values are additive, we also postulate that\nthe SHAP importance of clinical variables with respect to prediction variations\ncan guide their frequency of measurements.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06808v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06808v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06808v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06806v1",
    "updated": "2024-02-09T22:07:59+00:00",
    "published": "2024-02-09T22:07:59+00:00",
    "title": "Towards Principled Assessment of Tabular Data Synthesis Algorithms",
    "authors": [
      {
        "name": "Yuntao Du"
      },
      {
        "name": "Ninghui Li"
      }
    ],
    "summary": "Data synthesis has been advocated as an important approach for utilizing data\nwhile protecting data privacy. A large number of tabular data synthesis\nalgorithms (which we call synthesizers) have been proposed. Some synthesizers\nsatisfy Differential Privacy, while others aim to provide privacy in a\nheuristic fashion. A comprehensive understanding of the strengths and\nweaknesses of these synthesizers remains elusive due to lacking principled\nevaluation metrics and missing head-to-head comparisons of newly developed\nsynthesizers that take advantage of diffusion models and large language models\nwith state-of-the-art marginal-based synthesizers.\n  In this paper, we present a principled and systematic evaluation framework\nfor assessing tabular data synthesis algorithms. Specifically, we examine and\ncritique existing evaluation metrics, and introduce a set of new metrics in\nterms of fidelity, privacy, and utility to address their limitations. Based on\nthe proposed metrics, we also devise a unified objective for tuning, which can\nconsistently improve the quality of synthetic data for all methods. We\nconducted extensive evaluations of 8 different types of synthesizers on 12\ndatasets and identified some interesting findings, which offer new directions\nfor privacy-preserving data synthesis.",
    "comment": "The code is available at: https://github.com/zealscott/SynMeter",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.DB",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06806v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06806v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06806v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06800v1",
    "updated": "2024-02-09T21:57:57+00:00",
    "published": "2024-02-09T21:57:57+00:00",
    "title": "Generative Nowcasting of Marine Fog Visibility in the Grand Banks area and Sable Island in Canada",
    "authors": [
      {
        "name": "Eren Gultepe"
      },
      {
        "name": "Sen Wang"
      },
      {
        "name": "Byron Blomquist"
      },
      {
        "name": "Harindra J. S. Fernando"
      },
      {
        "name": "O. Patrick Kreidl"
      },
      {
        "name": "David J. Delene"
      },
      {
        "name": "Ismail Gultepe"
      }
    ],
    "summary": "This study presents the application of generative deep learning techniques to\nevaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence\ninteractions in the marine atmosphere) campaign observations collected during\nJuly 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable\nIsland (SI), northeast of Canada. The measurements were collected using the\nVaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50,\nand Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic\nCondor. To perform nowcasting, the time series of fog visibility (Vis), wind\nspeed, dew point depression, and relative humidity with respect to water were\npreprocessed to have lagged time step features. Generative nowcasting of Vis\ntime series for lead times of 30 and 60 minutes were performed using\nconditional generative adversarial networks (cGAN) regression at visibility\nthresholds of Vis < 1 km and < 10 km. Extreme gradient boosting (XGBoost) was\nused as a baseline method for comparison against cGAN. At the 30 min lead time,\nVis was best predicted with cGAN at Vis < 1 km (RMSE = 0.151 km) and with\nXGBoost at Vis < 10 km (RMSE = 2.821 km). At the 60 min lead time, Vis was best\npredicted with XGBoost at Vis < 1 km (RMSE = 0.167 km) and Vis < 10 km (RMSE =\n3.508 km), but the cGAN RMSE was similar to XGBoost. Despite nowcasting Vis at\n30 min being quite difficult, the ability of the cGAN model to track the\nvariation in Vis at 1 km suggests that there is potential for generative\nanalysis of marine fog visibility using observational meteorological\nparameters.",
    "comment": null,
    "journal_ref": "Tackling Climate Change with Machine Learning: workshop at NeurIPS\n  2023",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06800v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06800v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06800v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06794v1",
    "updated": "2024-02-09T21:37:13+00:00",
    "published": "2024-02-09T21:37:13+00:00",
    "title": "Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing",
    "authors": [
      {
        "name": "Hochul Hwang"
      },
      {
        "name": "Sunjae Kwon"
      },
      {
        "name": "Yekyung Kim"
      },
      {
        "name": "Donghyun Kim"
      }
    ],
    "summary": "Safely navigating street intersections is a complex challenge for blind and\nlow-vision individuals, as it requires a nuanced understanding of the\nsurrounding context - a task heavily reliant on visual cues. Traditional\nmethods for assisting in this decision-making process often fall short, lacking\nthe ability to provide a comprehensive scene analysis and safety level. This\npaper introduces an innovative approach that leverages large multimodal models\n(LMMs) to interpret complex street crossing scenes, offering a potential\nadvancement over conventional traffic signal recognition techniques. By\ngenerating a safety score and scene description in natural language, our method\nsupports safe decision-making for the blind and low-vision individuals. We\ncollected crosswalk intersection data that contains multiview egocentric images\ncaptured by a quadruped robot and annotated the images with corresponding\nsafety scores based on our predefined safety score categorization. Grounded on\nthe visual knowledge, extracted from images, and text prompt, we evaluate a\nlarge multimodal model for safety score prediction and scene description. Our\nfindings highlight the reasoning and safety score prediction capabilities of a\nLMM, activated by various prompts, as a pathway to developing a trustworthy\nsystem, crucial for applications requiring reliable decision-making support.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06794v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06794v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06794v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06787v1",
    "updated": "2024-02-09T21:21:17+00:00",
    "published": "2024-02-09T21:21:17+00:00",
    "title": "ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics",
    "authors": [
      {
        "name": "Liangyu Zhao"
      },
      {
        "name": "Saeed Maleki"
      },
      {
        "name": "Ziyue Yang"
      },
      {
        "name": "Hossein Pourreza"
      },
      {
        "name": "Aashaka Shah"
      },
      {
        "name": "Changho Hwang"
      },
      {
        "name": "Arvind Krishnamurthy"
      }
    ],
    "summary": "As modern DNN models grow ever larger, collective communications between the\naccelerators (allreduce, etc.) emerge as a significant performance bottleneck.\nDesigning efficient communication schedules is challenging given today's highly\ndiverse and heterogeneous network fabrics. In this paper, we present\nForestColl, a tool that generates efficient schedules for any network topology.\nForestColl constructs broadcast/aggregation spanning trees as the communication\nschedule, achieving theoretically minimum network congestion. Its schedule\ngeneration runs in strongly polynomial time and is highly scalable. ForestColl\nsupports any network fabrics, including both switching fabrics and direct\nconnections, as well as any network graph structure. We evaluated ForestColl on\nmulti-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules\nachieved up to 52\\% higher performance compared to the vendors' own optimized\ncommunication libraries, RCCL and NCCL. ForestColl also outperforms other\nstate-of-the-art schedule generation techniques with both up to 61\\% more\nefficient generated schedules and orders of magnitude faster schedule\ngeneration speed.",
    "comment": "arXiv admin note: text overlap with arXiv:2305.18461",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.NI",
    "categories": [
      "cs.NI",
      "cs.DC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06787v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06787v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06787v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06784v1",
    "updated": "2024-02-09T21:17:31+00:00",
    "published": "2024-02-09T21:17:31+00:00",
    "title": "Transfer learning with generative models for object detection on limited datasets",
    "authors": [
      {
        "name": "Matteo Paiano"
      },
      {
        "name": "Stefano Martina"
      },
      {
        "name": "Carlotta Giannelli"
      },
      {
        "name": "Filippo Caruso"
      }
    ],
    "summary": "The availability of data is limited in some fields, especially for object\ndetection tasks, where it is necessary to have correctly labeled bounding boxes\naround each object. A notable example of such data scarcity is found in the\ndomain of marine biology, where it is useful to develop methods to\nautomatically detect submarine species for environmental monitoring. To address\nthis data limitation, the state-of-the-art machine learning strategies employ\ntwo main approaches. The first involves pretraining models on existing datasets\nbefore generalizing to the specific domain of interest. The second strategy is\nto create synthetic datasets specifically tailored to the target domain using\nmethods like copy-paste techniques or ad-hoc simulators. The first strategy\noften faces a significant domain shift, while the second demands custom\nsolutions crafted for the specific task. In response to these challenges, here\nwe propose a transfer learning framework that is valid for a generic scenario.\nIn this framework, generated images help to improve the performances of an\nobject detector in a few-real data regime. This is achieved through a\ndiffusion-based generative model that was pretrained on large generic datasets,\nand is not trained on the task-specific domain. We validate our approach on\nobject detection tasks, specifically focusing on fishes in an underwater\nenvironment, and on the more common domain of cars in an urban setting. Our\nmethod achieves detection performance comparable to models trained on thousands\nof images, using only a few hundreds of input data. Our results pave the way\nfor new generative AI-based protocols for machine learning applications in\nvarious domains, for instance ranging from geophysics to biology and medicine.",
    "comment": "24 pages, 15 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "68T05, 68T07, 68T10, 68T45,",
      "I.2.6; I.2.0; I.4.8; I.4.9; I.5.1; I.5.0; I.5.4; J.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06784v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06784v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06784v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06783v1",
    "updated": "2024-02-09T21:16:43+00:00",
    "published": "2024-02-09T21:16:43+00:00",
    "title": "Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer",
    "authors": [
      {
        "name": "Feiyang Wu"
      },
      {
        "name": "Zhaoyuan Gu"
      },
      {
        "name": "Ye Zhao"
      },
      {
        "name": "Anqi Wu"
      }
    ],
    "summary": "Simulation-to-reality (sim-to-real) transfer is a fundamental problem for\nrobot learning. Domain Randomization, which adds randomization during training,\nis a powerful technique that effectively addresses the sim-to-real gap.\nHowever, the noise in observations makes learning significantly harder.\nRecently, studies have shown that employing a teacher-student learning paradigm\ncan accelerate training in randomized environments. Learned with privileged\ninformation, a teacher agent can instruct the student agent to operate in noisy\nenvironments. However, this approach is often not sample efficient as the\nexperience collected by the teacher is discarded completely when training the\nstudent, wasting information revealed by the environment. In this work, we\nextend the teacher-student learning paradigm by proposing a sample efficient\nlearning framework termed Learn to Teach (L2T) that recycles experience\ncollected by the teacher agent. We observe that the dynamics of the\nenvironments for both agents remain unchanged, and the state space of the\nteacher is coupled with the observation space of the student. We show that a\nsingle-loop algorithm can train both the teacher and student agents under both\nReinforcement Learning and Inverse Reinforcement Learning contexts. We\nimplement variants of our methods, conduct experiments on the MuJoCo benchmark,\nand apply our methods to the Cassie robot locomotion problem. Extensive\nexperiments show that our method achieves competitive performance while only\nrequiring environmental interaction with the teacher.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06783v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06783v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06783v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06782v1",
    "updated": "2024-02-09T21:05:01+00:00",
    "published": "2024-02-09T21:05:01+00:00",
    "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "authors": [
      {
        "name": "Akbir Khan"
      },
      {
        "name": "John Hughes"
      },
      {
        "name": "Dan Valentine"
      },
      {
        "name": "Laura Ruis"
      },
      {
        "name": "Kshitij Sachan"
      },
      {
        "name": "Ansh Radhakrishnan"
      },
      {
        "name": "Edward Grefenstette"
      },
      {
        "name": "Samuel R. Bowman"
      },
      {
        "name": "Tim Rockt\u00e4schel"
      },
      {
        "name": "Ethan Perez"
      }
    ],
    "summary": "Common methods for aligning large language models (LLMs) with desired\nbehaviour heavily rely on human-labelled data. However, as models grow\nincreasingly sophisticated, they will surpass human expertise, and the role of\nhuman evaluation will evolve into non-experts overseeing experts. In\nanticipation of this, we ask: can weaker models assess the correctness of\nstronger models? We investigate this question in an analogous setting, where\nstronger models (experts) possess the necessary information to answer questions\nand weaker models (non-experts) lack this information. The method we evaluate\nis \\textit{debate}, where two LLM experts each argue for a different answer,\nand a non-expert selects the answer. We find that debate consistently helps\nboth non-expert models and humans answer questions, achieving 76\\% and 88\\%\naccuracy respectively (naive baselines obtain 48\\% and 60\\%). Furthermore,\noptimising expert debaters for persuasiveness in an unsupervised manner\nimproves non-expert ability to identify the truth in debates. Our results\nprovide encouraging empirical evidence for the viability of aligning models\nwith debate in the absence of ground truth.",
    "comment": "For code please check: https://github.com/ucl-dark/llm_debate",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06782v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06782v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06782v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07948v1",
    "updated": "2024-02-09T20:33:48+00:00",
    "published": "2024-02-09T20:33:48+00:00",
    "title": "evolSOM: an R Package for evolutionary conservation analysis with SOMs",
    "authors": [
      {
        "name": "Santiago Prochetto"
      },
      {
        "name": "Renata Reinheimer"
      },
      {
        "name": "Georgina Stegmayer"
      }
    ],
    "summary": "Motivation: Unraveling the connection between genes and traits is crucial for\nsolving many biological puzzles. Genes provide instructions for building\ncellular machinery, directing the processes that sustain life. RNA molecules\nand proteins, derived from these genetic instructions, play crucial roles in\nshaping cell structures, influencing reactions, and guiding behavior. This\nfundamental biological principle links genetic makeup to observable traits, but\nintegrating and extracting meaningful relationships from this complex,\nmultimodal data presents a significant challenge. Results: We introduce\nevolSOM, a novel R package that utilizes Self-Organizing Maps (SOMs) to explore\nand visualize the conservation of biological variables, easing the integration\nof phenotypic and genotypic attributes. By constructing species-specific or\ncondition-specific SOMs that capture non-redundant patterns, evolSOM allows the\nanalysis of displacement of biological variables between species or conditions.\nVariables displaced together suggest membership in the same regulatory network,\nand the nature of the displacement may hold biological significance. The\npackage automatically calculates and graphically presents these displacements,\nenabling efficient comparison and revealing conserved and displaced variables.\nThe package facilitates the integration of diverse phenotypic data types,\nenabling the exploration of potential gene drivers underlying observed\nphenotypic changes. Its user-friendly interface and visualization capabilities\nenhance the accessibility of complex network analyses. Illustratively, we\nemployed evolSOM to study the displacement of genes and phenotypic traits,\nsuccessfully identifying potential drivers of phenotypic differentiation in\ngrass leaves. Availability: The package is open-source and is available at\nhttps://github.com/sanprochetto/evolSOM.",
    "comment": "8 pages, 1 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.QM",
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "q-bio.PE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07948v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07948v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07948v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06772v1",
    "updated": "2024-02-09T20:25:45+00:00",
    "published": "2024-02-09T20:25:45+00:00",
    "title": "Retrosynthesis Prediction via Search in (Hyper) Graph",
    "authors": [
      {
        "name": "Zixun Lan"
      },
      {
        "name": "Binjie Hong"
      },
      {
        "name": "Jiajun Zhu"
      },
      {
        "name": "Zuo Zeng"
      },
      {
        "name": "Zhenfu Liu"
      },
      {
        "name": "Limin Yu"
      },
      {
        "name": "Fei Ma"
      }
    ],
    "summary": "Predicting reactants from a specified core product stands as a fundamental\nchallenge within organic synthesis, termed retrosynthesis prediction. Recently,\nsemi-template-based methods and graph-edits-based methods have achieved good\nperformance in terms of both interpretability and accuracy. However, due to\ntheir mechanisms these methods cannot predict complex reactions, e.g.,\nreactions with multiple reaction center or attaching the same leaving group to\nmore than one atom. In this study we propose a semi-template-based method, the\n\\textbf{Retro}synthesis via \\textbf{S}earch \\textbf{i}n (Hyper) \\textbf{G}raph\n(RetroSiG) framework to alleviate these limitations. In the proposed method, we\nturn the reaction center identification and the leaving group completion tasks\nas tasks of searching in the product molecular graph and leaving group\nhypergraph respectively. As a semi-template-based method RetroSiG has several\nadvantages. First, RetroSiG is able to handle the complex reactions mentioned\nabove by its novel search mechanism. Second, RetroSiG naturally exploits the\nhypergraph to model the implicit dependencies between leaving groups. Third,\nRetroSiG makes full use of the prior, i.e., one-hop constraint. It reduces the\nsearch space and enhances overall performance. Comprehensive experiments\ndemonstrated that RetroSiG achieved competitive results. Furthermore, we\nconducted experiments to show the capability of RetroSiG in predicting complex\nreactions. Ablation experiments verified the efficacy of specific elements,\nsuch as the one-hop constraint and the leaving group hypergraph.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.QM",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06772v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06772v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06772v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06766v1",
    "updated": "2024-02-09T19:59:34+00:00",
    "published": "2024-02-09T19:59:34+00:00",
    "title": "Evaluation Metrics for Text Data Augmentation in NLP",
    "authors": [
      {
        "name": "Marcellus Amadeus"
      },
      {
        "name": "William Alberto Cruz Casta\u00f1eda"
      }
    ],
    "summary": "Recent surveys on data augmentation for natural language processing have\nreported different techniques and advancements in the field. Several\nframeworks, tools, and repositories promote the implementation of text data\naugmentation pipelines. However, a lack of evaluation criteria and standards\nfor method comparison due to different tasks, metrics, datasets, architectures,\nand experimental settings makes comparisons meaningless. Also, a lack of\nmethods unification exists and text data augmentation research would benefit\nfrom unified metrics to compare different augmentation methods. Thus, academics\nand the industry endeavor relevant evaluation metrics for text data\naugmentation techniques. The contribution of this work is to provide a taxonomy\nof evaluation metrics for text augmentation methods and serve as a direction\nfor a unified benchmark. The proposed taxonomy organizes categories that\ninclude tools for implementation and metrics calculation. Finally, with this\nstudy, we intend to present opportunities to explore the unification and\nstandardization of text data augmentation metrics.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06766v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06766v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06766v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06764v1",
    "updated": "2024-02-09T19:53:29+00:00",
    "published": "2024-02-09T19:53:29+00:00",
    "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
    "authors": [
      {
        "name": "Stefan Dernbach"
      },
      {
        "name": "Khushbu Agarwal"
      },
      {
        "name": "Alejandro Zuniga"
      },
      {
        "name": "Michael Henry"
      },
      {
        "name": "Sutanay Choudhury"
      }
    ],
    "summary": "Integrating large language models (LLMs) with knowledge graphs derived from\ndomain-specific data represents an important advancement towards more powerful\nand factual reasoning. As these models grow more capable, it is crucial to\nenable them to perform multi-step inferences over real-world knowledge graphs\nwhile minimizing hallucination. While large language models excel at\nconversation and text generation, their ability to reason over\ndomain-specialized graphs of interconnected entities remains limited. For\nexample, can we query a LLM to identify the optimal contact in a professional\nnetwork for a specific goal, based on relationships and attributes in a private\ndatabase? The answer is no--such capabilities lie beyond current methods.\nHowever, this question underscores a critical technical gap that must be\naddressed. Many high-value applications in areas such as science, security, and\ne-commerce rely on proprietary knowledge graphs encoding unique structures,\nrelationships, and logical constraints. We introduce a fine-tuning framework\nfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge\ngraph into an alternate text representation with labeled question-answer pairs.\nWe demonstrate that grounding the models in specific graph-based knowledge\nexpands the models' capacity for structure-based reasoning. Our methodology\nleverages the large-language model's generative capabilities to create the\ndataset and proposes an efficient alternate to retrieval-augmented generation\nstyled methods.",
    "comment": "To be published in AAAI Spring Symposium: AAAI-MAKE 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06764v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06764v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06764v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06763v1",
    "updated": "2024-02-09T19:52:31+00:00",
    "published": "2024-02-09T19:52:31+00:00",
    "title": "Scalable Kernel Logistic Regression with Nystr\u00f6m Approximation: Theoretical Analysis and Application to Discrete Choice Modelling",
    "authors": [
      {
        "name": "Jos\u00e9 \u00c1ngel Mart\u00edn-Baos"
      },
      {
        "name": "Ricardo Garc\u00eda-R\u00f3denas"
      },
      {
        "name": "Luis Rodriguez-Benitez"
      },
      {
        "name": "Michel Bierlaire"
      }
    ],
    "summary": "The application of kernel-based Machine Learning (ML) techniques to discrete\nchoice modelling using large datasets often faces challenges due to memory\nrequirements and the considerable number of parameters involved in these\nmodels. This complexity hampers the efficient training of large-scale models.\nThis paper addresses these problems of scalability by introducing the Nystr\\\"om\napproximation for Kernel Logistic Regression (KLR) on large datasets. The study\nbegins by presenting a theoretical analysis in which: i) the set of KLR\nsolutions is characterised, ii) an upper bound to the solution of KLR with\nNystr\\\"om approximation is provided, and finally iii) a specialisation of the\noptimisation algorithms to Nystr\\\"om KLR is described. After this, the\nNystr\\\"om KLR is computationally validated. Four landmark selection methods are\ntested, including basic uniform sampling, a k-means sampling strategy, and two\nnon-uniform methods grounded in leverage scores. The performance of these\nstrategies is evaluated using large-scale transport mode choice datasets and is\ncompared with traditional methods such as Multinomial Logit (MNL) and\ncontemporary ML techniques. The study also assesses the efficiency of various\noptimisation techniques for the proposed Nystr\\\"om KLR model. The performance\nof gradient descent, Momentum, Adam, and L-BFGS-B optimisation methods is\nexamined on these datasets. Among these strategies, the k-means Nystr\\\"om KLR\napproach emerges as a successful solution for applying KLR to large datasets,\nparticularly when combined with the L-BFGS-B and Adam optimisation methods. The\nresults highlight the ability of this strategy to handle datasets exceeding\n200,000 observations while maintaining robust performance.",
    "comment": "32 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06763v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06763v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06763v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06761v1",
    "updated": "2024-02-09T19:47:31+00:00",
    "published": "2024-02-09T19:47:31+00:00",
    "title": "Embedding Compression for Teacher-to-Student Knowledge Transfer",
    "authors": [
      {
        "name": "Yiwei Ding"
      },
      {
        "name": "Alexander Lerch"
      }
    ],
    "summary": "Common knowledge distillation methods require the teacher model and the\nstudent model to be trained on the same task. However, the usage of embeddings\nas teachers has also been proposed for different source tasks and target tasks.\nPrior work that uses embeddings as teachers ignores the fact that the teacher\nembeddings are likely to contain irrelevant knowledge for the target task. To\naddress this problem, we propose to use an embedding compression module with a\ntrainable teacher transformation to obtain a compact teacher embedding. Results\nshow that adding the embedding compression module improves the classification\nperformance, especially for unsupervised teacher embeddings. Moreover, student\nmodels trained with the guidance of embeddings show stronger generalizability.",
    "comment": "5+1 pages. In ICASSP 2024 Satellite Workshop Deep Neural Network\n  Model Compression",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06761v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06761v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06761v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06759v1",
    "updated": "2024-02-09T19:44:29+00:00",
    "published": "2024-02-09T19:44:29+00:00",
    "title": "A Methodology for Questionnaire Analysis: Insights through Cluster Analysis of an Investor Competition Data",
    "authors": [
      {
        "name": "Carlos Henrique Q. Forster"
      },
      {
        "name": "Paulo Andr\u00e9 Lima de Castro"
      },
      {
        "name": "Andrei Ramalho"
      }
    ],
    "summary": "In this paper, we propose a methodology for the analysis of questionnaire\ndata along with its application on discovering insights from investor data\nmotivated by a day trading competition. The questionnaire includes categorical\nquestions, which are reduced to binary questions, 'yes' or 'no'. The\nmethodology reduces dimensionality by grouping questions and participants with\nsimilar responses using clustering analysis. Rule discovery was performed by\nusing a conversion rate metric. Innovative visual representations were proposed\nto validate the cluster analysis and the relation discovery between questions.\nWhen crossing with financial data, additional insights were revealed related to\nthe recognized clusters.",
    "comment": "14 pages, 12 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06759v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06759v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06759v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06756v1",
    "updated": "2024-02-09T19:39:23+00:00",
    "published": "2024-02-09T19:39:23+00:00",
    "title": "Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion",
    "authors": [
      {
        "name": "Jianhao Ma"
      },
      {
        "name": "Salar Fattahi"
      }
    ],
    "summary": "We study the problem of symmetric matrix completion, where the goal is to\nreconstruct a positive semidefinite matrix $\\rm{X}^\\star \\in\n\\mathbb{R}^{d\\times d}$ of rank-$r$, parameterized by $\\rm{U}\\rm{U}^{\\top}$,\nfrom only a subset of its observed entries. We show that the vanilla gradient\ndescent (GD) with small initialization provably converges to the ground truth\n$\\rm{X}^\\star$ without requiring any explicit regularization. This convergence\nresult holds true even in the over-parameterized scenario, where the true rank\n$r$ is unknown and conservatively over-estimated by a search rank $r'\\gg r$.\nThe existing results for this problem either require explicit regularization, a\nsufficiently accurate initial point, or exact knowledge of the true rank $r$.\n  In the over-parameterized regime where $r'\\geq r$, we show that, with\n$\\widetilde\\Omega(dr^9)$ observations, GD with an initial point $\\|\\rm{U}_0\\|\n\\leq \\epsilon$ converges near-linearly to an $\\epsilon$-neighborhood of\n$\\rm{X}^\\star$. Consequently, smaller initial points result in increasingly\naccurate solutions. Surprisingly, neither the convergence rate nor the final\naccuracy depends on the over-parameterized search rank $r'$, and they are only\ngoverned by the true rank $r$. In the exactly-parameterized regime where\n$r'=r$, we further enhance this result by proving that GD converges at a faster\nrate to achieve an arbitrarily small accuracy $\\epsilon>0$, provided the\ninitial point satisfies $\\|\\rm{U}_0\\| = O(1/d)$. At the crux of our method lies\na novel weakly-coupled leave-one-out analysis, which allows us to establish the\nglobal convergence of GD, extending beyond what was previously possible using\nthe classical leave-one-out analysis.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06756v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06756v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06756v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06751v1",
    "updated": "2024-02-09T19:28:02+00:00",
    "published": "2024-02-09T19:28:02+00:00",
    "title": "Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse",
    "authors": [
      {
        "name": "Bradley T. Baker"
      },
      {
        "name": "Barak A. Pearlmutter"
      },
      {
        "name": "Robyn Miller"
      },
      {
        "name": "Vince D. Calhoun"
      },
      {
        "name": "Sergey M. Plis"
      }
    ],
    "summary": "Our understanding of learning dynamics of deep neural networks (DNNs) remains\nincomplete. Recent research has begun to uncover the mathematical principles\nunderlying these networks, including the phenomenon of \"Neural Collapse\", where\nlinear classifiers within DNNs converge to specific geometrical structures\nduring late-stage training. However, the role of geometric constraints in\nlearning extends beyond this terminal phase. For instance, gradients in\nfully-connected layers naturally develop a low-rank structure due to the\naccumulation of rank-one outer products over a training batch. Despite the\nattention given to methods that exploit this structure for memory saving or\nregularization, the emergence of low-rank learning as an inherent aspect of\ncertain DNN architectures has been under-explored. In this paper, we conduct a\ncomprehensive study of gradient rank in DNNs, examining how architectural\nchoices and structure of the data effect gradient rank bounds. Our theoretical\nanalysis provides these bounds for training fully-connected, recurrent, and\nconvolutional neural networks. We also demonstrate, both theoretically and\nempirically, how design choices like activation function linearity, bottleneck\nlayer introduction, convolutional stride, and sequence truncation influence\nthese bounds. Our findings not only contribute to the understanding of learning\ndynamics in DNNs, but also provide practical guidance for deep learning\nengineers to make informed design decisions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06751v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06751v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06751v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06737v1",
    "updated": "2024-02-09T19:16:04+00:00",
    "published": "2024-02-09T19:16:04+00:00",
    "title": "ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning",
    "authors": [
      {
        "name": "Mahdi Naseri"
      },
      {
        "name": "Mahdi Biparva"
      }
    ],
    "summary": "Self-supervised Learning (SSL) has emerged as a powerful technique in\npre-training deep learning models without relying on expensive annotated\nlabels, instead leveraging embedded signals in unlabeled data. While SSL has\nshown remarkable success in computer vision tasks through intuitive data\naugmentation, its application to graph-structured data poses challenges due to\nthe semantic-altering and counter-intuitive nature of graph augmentations.\nAddressing this limitation, this paper introduces a novel non-contrastive SSL\napproach to Explicitly Generate a compositional Relation Graph (ExGRG) instead\nof relying solely on the conventional augmentation-based implicit relation\ngraph. ExGRG offers a framework for incorporating prior domain knowledge and\nonline extracted information into the SSL invariance objective, drawing\ninspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).\nEmploying an EM perspective on SSL, our E-step involves relation graph\ngeneration to identify candidates to guide the SSL invariance objective, and\nM-step updates the model parameters by integrating the derived relational\ninformation. Extensive experimentation on diverse node classification datasets\ndemonstrates the superiority of our method over state-of-the-art techniques,\naffirming ExGRG as an effective adoption of SSL for graph representation\nlearning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06737v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06737v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06737v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06734v1",
    "updated": "2024-02-09T19:09:48+00:00",
    "published": "2024-02-09T19:09:48+00:00",
    "title": "Corruption Robust Offline Reinforcement Learning with Human Feedback",
    "authors": [
      {
        "name": "Debmalya Mandal"
      },
      {
        "name": "Andi Nika"
      },
      {
        "name": "Parameswaran Kamalaruban"
      },
      {
        "name": "Adish Singla"
      },
      {
        "name": "Goran Radanovi\u0107"
      }
    ],
    "summary": "We study data corruption robustness for reinforcement learning with human\nfeedback (RLHF) in an offline setting. Given an offline dataset of pairs of\ntrajectories along with feedback about human preferences, an\n$\\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or\ntrajectory features manipulated), capturing an adversarial attack or noisy\nhuman preferences. We aim to design algorithms that identify a near-optimal\npolicy from the corrupted data, with provable guarantees. Existing theoretical\nworks have separately studied the settings of corruption robust RL (learning\nfrom scalar rewards directly under corruption) and offline RLHF (learning from\nhuman feedback without corruption); however, they are inapplicable to our\nproblem of dealing with corrupted data in offline RLHF setting. To this end, we\ndesign novel corruption robust offline RLHF methods under various assumptions\non the coverage of the data-generating distributions. At a high level, our\nmethodology robustifies an offline RLHF framework by first learning a reward\nmodel along with confidence sets and then learning a pessimistic optimal policy\nover the confidence set. Our key insight is that learning optimal policy can be\ndone by leveraging an offline corruption-robust RL oracle in different ways\n(e.g., zero-order oracle or first-order oracle), depending on the data coverage\nassumptions. To our knowledge, ours is the first work that provides provable\ncorruption robust offline RLHF methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06734v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06734v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06734v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06733v1",
    "updated": "2024-02-09T19:09:19+00:00",
    "published": "2024-02-09T19:09:19+00:00",
    "title": "NICE: To Optimize In-Context Examples or Not?",
    "authors": [
      {
        "name": "Pragya Srivastava"
      },
      {
        "name": "Satvik Golechha"
      },
      {
        "name": "Amit Deshpande"
      },
      {
        "name": "Amit Sharma"
      }
    ],
    "summary": "Recent works have shown that large language models (LLMs) work remarkably\nwell on a wide range of tasks through in-context learning and optimization of\nin-context examples (ICE). However, most of these studies assume either a fixed\nor no instruction provided in the prompt, leading to the apparent consensus\nthat the optimization of in-context examples is critical for better\nperformance. We challenge this consensus for instruction-tuned LLMs by\ninvestigating the necessity of optimizing in-context examples when\ntask-specific instructions are provided, and find that there are tasks for\nwhich various ways of optimizing in-context examples yield diminishing returns.\nWe introduce a task-specific metric called \\metriclong{} (\\metric) that\nquantifies the learnability of tasks from a given instruction, and provides a\nheuristic that helps decide whether to optimize for instructions or ICE for any\nnew task. On a wide range of tasks and a systematically created instruction set\nwith gradually added details, we validate our hypothesis empirically by\ncomputing \\metric with query-dependent bins of examples, comparing different\ninstructions with ICE selection methods, and performing label perturbation\nexperiments. We conclude that tasks can be divided into two broad classes based\non the \\metric metric, where the returns on ICE optimization follow predictable\ntrends when instructions are provided in the prompt.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06733v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06733v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06733v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06730v1",
    "updated": "2024-02-09T19:01:48+00:00",
    "published": "2024-02-09T19:01:48+00:00",
    "title": "A Scalable Algorithm for Individually Fair K-means Clustering",
    "authors": [
      {
        "name": "MohammadHossein Bateni"
      },
      {
        "name": "Vincent Cohen-Addad"
      },
      {
        "name": "Alessandro Epasto"
      },
      {
        "name": "Silvio Lattanzi"
      }
    ],
    "summary": "We present a scalable algorithm for the individually fair ($p$,\n$k$)-clustering problem introduced by Jung et al. and Mahabadi et al. Given $n$\npoints $P$ in a metric space, let $\\delta(x)$ for $x\\in P$ be the radius of the\nsmallest ball around $x$ containing at least $n / k$ points. A clustering is\nthen called individually fair if it has centers within distance $\\delta(x)$ of\n$x$ for each $x\\in P$. While good approximation algorithms are known for this\nproblem no efficient practical algorithms with good theoretical guarantees have\nbeen presented. We design the first fast local-search algorithm that runs in\n~$O(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. Then we\nshow empirically that not only is our algorithm much faster than prior work,\nbut it also produces lower-cost solutions.",
    "comment": "32 pages, 2 figures, to appear at the 27th International Conference\n  on Artificial Intelligence and Statistics (AISTATS) 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DS",
    "categories": [
      "cs.DS",
      "cs.CY",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06730v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06730v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06730v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06627v1",
    "updated": "2024-02-09T18:59:29+00:00",
    "published": "2024-02-09T18:59:29+00:00",
    "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
    "authors": [
      {
        "name": "Alexander Pan"
      },
      {
        "name": "Erik Jones"
      },
      {
        "name": "Meena Jagadeesan"
      },
      {
        "name": "Jacob Steinhardt"
      }
    ],
    "summary": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.",
    "comment": "44 pages, 12 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06627v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06627v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06627v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06619v1",
    "updated": "2024-02-09T18:51:49+00:00",
    "published": "2024-02-09T18:51:49+00:00",
    "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
    "authors": [
      {
        "name": "Shivalika Singh"
      },
      {
        "name": "Freddie Vargus"
      },
      {
        "name": "Daniel Dsouza"
      },
      {
        "name": "B\u00f6rje F. Karlsson"
      },
      {
        "name": "Abinaya Mahendiran"
      },
      {
        "name": "Wei-Yin Ko"
      },
      {
        "name": "Herumb Shandilya"
      },
      {
        "name": "Jay Patel"
      },
      {
        "name": "Deividas Mataciunas"
      },
      {
        "name": "Laura OMahony"
      },
      {
        "name": "Mike Zhang"
      },
      {
        "name": "Ramith Hettiarachchi"
      },
      {
        "name": "Joseph Wilson"
      },
      {
        "name": "Marina Machado"
      },
      {
        "name": "Luisa Souza Moura"
      },
      {
        "name": "Dominik Krzemi\u0144ski"
      },
      {
        "name": "Hakimeh Fadaei"
      },
      {
        "name": "Irem Erg\u00fcn"
      },
      {
        "name": "Ifeoma Okoh"
      },
      {
        "name": "Aisha Alaagib"
      },
      {
        "name": "Oshan Mudannayake"
      },
      {
        "name": "Zaid Alyafeai"
      },
      {
        "name": "Vu Minh Chien"
      },
      {
        "name": "Sebastian Ruder"
      },
      {
        "name": "Surya Guthikonda"
      },
      {
        "name": "Emad A. Alghamdi"
      },
      {
        "name": "Sebastian Gehrmann"
      },
      {
        "name": "Niklas Muennighoff"
      },
      {
        "name": "Max Bartolo"
      },
      {
        "name": "Julia Kreutzer"
      },
      {
        "name": "Ahmet \u00dcst\u00fcn"
      },
      {
        "name": "Marzieh Fadaee"
      },
      {
        "name": "Sara Hooker"
      }
    ],
    "summary": "Datasets are foundational to many breakthroughs in modern artificial\nintelligence. Many recent achievements in the space of natural language\nprocessing (NLP) can be attributed to the finetuning of pre-trained models on a\ndiverse set of tasks that enables a large language model (LLM) to respond to\ninstructions. Instruction fine-tuning (IFT) requires specifically constructed\nand annotated datasets. However, existing datasets are almost all in the\nEnglish language. In this work, our primary goal is to bridge the language gap\nby building a human-curated instruction-following dataset spanning 65\nlanguages. We worked with fluent speakers of languages from around the world to\ncollect natural instances of instructions and completions. Furthermore, we\ncreate the most extensive multilingual collection to date, comprising 513\nmillion instances through templating and translating existing datasets across\n114 languages. In total, we contribute four key resources: we develop and\nopen-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,\nand the Aya Evaluation Suite. The Aya initiative also serves as a valuable case\nstudy in participatory research, involving collaborators from 119 countries. We\nsee this as a valuable framework for future research collaborations that aim to\nbridge gaps in resources.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06619v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06619v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06619v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06614v1",
    "updated": "2024-02-09T18:45:00+00:00",
    "published": "2024-02-09T18:45:00+00:00",
    "title": "The Complexity of Sequential Prediction in Dynamical Systems",
    "authors": [
      {
        "name": "Vinod Raman"
      },
      {
        "name": "Unique Subedi"
      },
      {
        "name": "Ambuj Tewari"
      }
    ],
    "summary": "We study the problem of learning to predict the next state of a dynamical\nsystem when the underlying evolution function is unknown. Unlike previous work,\nwe place no parametric assumptions on the dynamical system, and study the\nproblem from a learning theory perspective. We define new combinatorial\nmeasures and dimensions and show that they quantify the optimal mistake and\nregret bounds in the realizable and agnostic setting respectively.",
    "comment": "35 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06614v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06614v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06614v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06608v1",
    "updated": "2024-02-09T18:39:13+00:00",
    "published": "2024-02-09T18:39:13+00:00",
    "title": "TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations",
    "authors": [
      {
        "name": "Sudhir Agarwal"
      },
      {
        "name": "Anu Sreepathy"
      }
    ],
    "summary": "We study the problem of generating plans for given natural language planning\ntask requests. On one hand, LLMs excel at natural language processing but do\nnot perform well on planning. On the other hand, classical planning tools excel\nat planning tasks but require input in a structured language such as the\nPlanning Domain Definition Language (PDDL). We leverage the strengths of both\nthe techniques by using an LLM for generating the PDDL representation (task\nPDDL) of planning task requests followed by using a classical planner for\ncomputing a plan. Unlike previous approaches that use LLMs for generating task\nPDDLs directly, our approach comprises of (a) translate: using an LLM only for\ngenerating a logically interpretable intermediate representation of natural\nlanguage task descriptions, (b) infer: deriving additional logically dependent\ninformation from the intermediate representation using a logic reasoner\n(currently, Answer Set Programming solver), and (c) compile: generating the\ntarget task PDDL from the base and inferred information. We observe that using\nan LLM to only output the intermediate representation significantly reduces LLM\nerrors. Consequently, TIC approach achieves, for at least one LLM, high\naccuracy on task PDDL generation for all seven domains of our evaluation\ndataset.",
    "comment": "20 pages (7 main + 2 references + 11 appendix), 4 figures, 2 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06608v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06608v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06608v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06606v1",
    "updated": "2024-02-09T18:34:08+00:00",
    "published": "2024-02-09T18:34:08+00:00",
    "title": "RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization",
    "authors": [
      {
        "name": "Ce Feng"
      },
      {
        "name": "Parv Venkitasubramaniam"
      }
    ],
    "summary": "The rise of IoT devices has prompted the demand for deploying machine\nlearning at-the-edge with real-time, efficient, and secure data processing. In\nthis context, implementing machine learning (ML) models with real-valued weight\nparameters can prove to be impractical particularly for large models, and there\nis a need to train models with quantized discrete weights. At the same time,\nthese low-dimensional models also need to preserve privacy of the underlying\ndataset. In this work, we present RQP-SGD, a new approach for\nprivacy-preserving quantization to train machine learning models for low-memory\nML-at-the-edge. This approach combines differentially private stochastic\ngradient descent (DP-SGD) with randomized quantization, providing a measurable\nprivacy guarantee in machine learning. In particular, we study the utility\nconvergence of implementing RQP-SGD on ML tasks with convex objectives and\nquantization constraints and demonstrate its efficacy over deterministic\nquantization. Through experiments conducted on two datasets, we show the\npractical effectiveness of RQP-SGD.",
    "comment": "This work is accepted by the 5th AAAI Workshop on Privacy-Preserving\n  Artificial Intelligence",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06606v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06606v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06606v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06599v1",
    "updated": "2024-02-09T18:21:51+00:00",
    "published": "2024-02-09T18:21:51+00:00",
    "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language Models",
    "authors": [
      {
        "name": "Xingxuan Zhang"
      },
      {
        "name": "Jiansheng Li"
      },
      {
        "name": "Wenjing Chu"
      },
      {
        "name": "Junjia Hai"
      },
      {
        "name": "Renzhe Xu"
      },
      {
        "name": "Yuqing Yang"
      },
      {
        "name": "Shikai Guan"
      },
      {
        "name": "Jiazheng Xu"
      },
      {
        "name": "Peng Cui"
      }
    ],
    "summary": "We investigate the generalization boundaries of current Multimodal Large\nLanguage Models (MLLMs) via comprehensive evaluation under out-of-distribution\nscenarios and domain-specific tasks. We evaluate their zero-shot generalization\nacross synthetic images, real-world distributional shifts, and specialized\ndatasets like medical and molecular imagery. Empirical results indicate that\nMLLMs struggle with generalization beyond common training domains, limiting\ntheir direct application without adaptation. To understand the cause of\nunreliable performance, we analyze three hypotheses: semantic\nmisinterpretation, visual feature extraction insufficiency, and mapping\ndeficiency. Results identify mapping deficiency as the primary hurdle. To\naddress this problem, we show that in-context learning (ICL) can significantly\nenhance MLLMs' generalization, opening new avenues for overcoming\ngeneralization barriers. We further explore the robustness of ICL under\ndistribution shifts and show its vulnerability to domain shifts, label shifts,\nand spurious correlation shifts between in-context examples and test data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06599v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06599v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06599v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06596v1",
    "updated": "2024-02-09T18:19:25+00:00",
    "published": "2024-02-09T18:19:25+00:00",
    "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
    "authors": [
      {
        "name": "Mingzhe Xing"
      },
      {
        "name": "Rongkai Zhang"
      },
      {
        "name": "Hui Xue"
      },
      {
        "name": "Qi Chen"
      },
      {
        "name": "Fan Yang"
      },
      {
        "name": "Zhen Xiao"
      }
    ],
    "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06596v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06596v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06596v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06590v1",
    "updated": "2024-02-09T18:10:38+00:00",
    "published": "2024-02-09T18:10:38+00:00",
    "title": "Predictive representations: building blocks of intelligence",
    "authors": [
      {
        "name": "Wilka Carvalho"
      },
      {
        "name": "Momchil S. Tomov"
      },
      {
        "name": "William de Cothi"
      },
      {
        "name": "Caswell Barry"
      },
      {
        "name": "Samuel J. Gershman"
      }
    ],
    "summary": "Adaptive behavior often requires predicting future events. The theory of\nreinforcement learning prescribes what kinds of predictive representations are\nuseful and how to compute them. This paper integrates these theoretical ideas\nwith work on cognition and neuroscience. We pay special attention to the\nsuccessor representation (SR) and its generalizations, which have been widely\napplied both as engineering tools and models of brain function. This\nconvergence suggests that particular kinds of predictive representations may\nfunction as versatile building blocks of intelligence.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06590v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06590v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06590v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06584v1",
    "updated": "2024-02-09T18:05:03+00:00",
    "published": "2024-02-09T18:05:03+00:00",
    "title": "G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German",
    "authors": [
      {
        "name": "Ehsan Latif"
      },
      {
        "name": "Gyeong-Geon Lee"
      },
      {
        "name": "Knut Neuman"
      },
      {
        "name": "Tamara Kastorff"
      },
      {
        "name": "Xiaoming Zhai"
      }
    ],
    "summary": "The advancement of natural language processing has paved the way for\nautomated scoring systems in various languages, such as German (e.g., German\nBERT [G-BERT]). Automatically scoring written responses to science questions in\nGerman is a complex task and challenging for standard G-BERT as they lack\ncontextual knowledge in the science domain and may be unaligned with student\nwriting styles. This paper developed a contextualized German Science Education\nBERT (G-SciEdBERT), an innovative large language model tailored for scoring\nGerman-written responses to science tasks. Using G-BERT, we pre-trained\nG-SciEdBERT on a corpus of 50K German written science responses with 5M tokens\nto the Programme for International Student Assessment (PISA) 2015. We\nfine-tuned G-SciEdBERT on 59 assessment items and examined the scoring\naccuracy. We then compared its performance with G-BERT. Our findings reveal a\nsubstantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a\n10% increase of quadratic weighted kappa compared to G-BERT (mean accuracy\ndifference = 0.096, SD = 0.024). These insights underline the significance of\nspecialized language models like G-SciEdBERT, which is trained to enhance the\naccuracy of automated scoring, offering a substantial contribution to the field\nof AI in education.",
    "comment": "First German Science Education LLM, Submitted to AIED2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06584v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06584v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06584v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06581v1",
    "updated": "2024-02-09T18:01:15+00:00",
    "published": "2024-02-09T18:01:15+00:00",
    "title": "More than the Sum of Its Parts: Ensembling Backbone Networks for Few-Shot Segmentation",
    "authors": [
      {
        "name": "Nico Catalano"
      },
      {
        "name": "Alessandro Maranelli"
      },
      {
        "name": "Agnese Chiatti"
      },
      {
        "name": "Matteo Matteucci"
      }
    ],
    "summary": "Semantic segmentation is a key prerequisite to robust image understanding for\napplications in \\acrlong{ai} and Robotics. \\acrlong{fss}, in particular,\nconcerns the extension and optimization of traditional segmentation methods in\nchallenging conditions where limited training examples are available. A\npredominant approach in \\acrlong{fss} is to rely on a single backbone for\nvisual feature extraction. Choosing which backbone to leverage is a deciding\nfactor contributing to the overall performance. In this work, we interrogate on\nwhether fusing features from different backbones can improve the ability of\n\\acrlong{fss} models to capture richer visual features. To tackle this\nquestion, we propose and compare two ensembling techniques-Independent Voting\nand Feature Fusion. Among the available \\acrlong{fss} methods, we implement the\nproposed ensembling techniques on PANet. The module dedicated to predicting\nsegmentation masks from the backbone embeddings in PANet avoids trainable\nparameters, creating a controlled `in vitro' setting for isolating the impact\nof different ensembling strategies. Leveraging the complementary strengths of\ndifferent backbones, our approach outperforms the original single-backbone\nPANet across standard benchmarks even in challenging one-shot learning\nscenarios. Specifically, it achieved a performance improvement of +7.37\\% on\nPASCAL-5\\textsuperscript{i} and of +10.68\\% on COCO-20\\textsuperscript{i} in\nthe top-performing scenario where three backbones are combined. These results,\ntogether with the qualitative inspection of the predicted subject masks,\nsuggest that relying on multiple backbones in PANet leads to a more\ncomprehensive feature representation, thus expediting the successful\napplication of \\acrlong{fss} methods in challenging, data-scarce environments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06581v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06581v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06581v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06580v1",
    "updated": "2024-02-09T17:55:01+00:00",
    "published": "2024-02-09T17:55:01+00:00",
    "title": "SAE: Single Architecture Ensemble Neural Networks",
    "authors": [
      {
        "name": "Martin Ferianc"
      },
      {
        "name": "Hongxiang Fan"
      },
      {
        "name": "Miguel Rodrigues"
      }
    ],
    "summary": "Ensembles of separate neural networks (NNs) have shown superior accuracy and\nconfidence calibration over single NN across tasks. Recent methods compress\nensembles within a single network via early exits or multi-input multi-output\nframeworks. However, the landscape of these methods is fragmented thus far,\nmaking it difficult to choose the right approach for a given task. Furthermore,\nthe algorithmic performance of these methods is behind the ensemble of separate\nNNs and requires extensive architecture tuning. We propose a novel methodology\nunifying these approaches into a Single Architecture Ensemble (SAE). Our method\nlearns the optimal number and depth of exits per ensemble input in a single NN.\nThis enables the SAE framework to flexibly tailor its configuration for a given\narchitecture or application. We evaluate SAEs on image classification and\nregression across various network architecture types and sizes. We demonstrate\ncompetitive accuracy or confidence calibration to baselines while reducing the\ncompute operations or parameter count by up to $1.5{\\sim}3.7\\times$.",
    "comment": "32 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06580v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06580v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06580v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06578v1",
    "updated": "2024-02-09T17:51:43+00:00",
    "published": "2024-02-09T17:51:43+00:00",
    "title": "On the Universality of Coupling-based Normalizing Flows",
    "authors": [
      {
        "name": "Felix Draxler"
      },
      {
        "name": "Stefan Wahl"
      },
      {
        "name": "Christoph Schn\u00f6rr"
      },
      {
        "name": "Ullrich K\u00f6the"
      }
    ],
    "summary": "We present a novel theoretical framework for understanding the expressive\npower of coupling-based normalizing flows such as RealNVP. Despite their\nprevalence in scientific applications, a comprehensive understanding of\ncoupling flows remains elusive due to their restricted architectures. Existing\ntheorems fall short as they require the use of arbitrarily ill-conditioned\nneural networks, limiting practical applicability. Additionally, we demonstrate\nthat these constructions inherently lead to volume-preserving flows, a property\nwhich we show to be a fundamental constraint for expressivity. We propose a new\ndistributional universality theorem for coupling-based normalizing flows, which\novercomes several limitations of prior work. Our results support the general\nwisdom that the coupling architecture is expressive and provide a nuanced view\nfor choosing the expressivity of coupling functions, bridging a gap between\nempirical results and theoretical understanding.",
    "comment": "under review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06578v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06578v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06578v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06570v1",
    "updated": "2024-02-09T17:40:51+00:00",
    "published": "2024-02-09T17:40:51+00:00",
    "title": "Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control",
    "authors": [
      {
        "name": "Zheng Xiong"
      },
      {
        "name": "Risto Vuorio"
      },
      {
        "name": "Jacob Beck"
      },
      {
        "name": "Matthieu Zimmer"
      },
      {
        "name": "Kun Shao"
      },
      {
        "name": "Shimon Whiteson"
      }
    ],
    "summary": "Learning a universal policy across different robot morphologies can\nsignificantly improve learning efficiency and enable zero-shot generalization\nto unseen morphologies. However, learning a highly performant universal policy\nrequires sophisticated architectures like transformers (TF) that have larger\nmemory and computational cost than simpler multi-layer perceptrons (MLP). To\nachieve both good performance like TF and high efficiency like MLP at inference\ntime, we propose HyperDistill, which consists of: (1) A morphology-conditioned\nhypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy\ndistillation approach that is essential for successful training. We show that\non UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill\nperforms as well as a universal TF teacher policy on both training and unseen\ntest robots, but reduces model size by 6-14 times, and computational cost by\n67-160 times in different environments. Our analysis attributes the efficiency\nadvantage of HyperDistill at inference time to knowledge decoupling, i.e., the\nability to decouple inter-task and intra-task knowledge, a general principle\nthat could also be applied to improve inference efficiency in other domains.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06570v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06570v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06570v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06563v1",
    "updated": "2024-02-09T17:27:35+00:00",
    "published": "2024-02-09T17:27:35+00:00",
    "title": "What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices",
    "authors": [
      {
        "name": "Neslihan Suzen"
      },
      {
        "name": "Evgeny M. Mirkes"
      },
      {
        "name": "Damian Roland"
      },
      {
        "name": "Jeremy Levesley"
      },
      {
        "name": "Alexander N. Gorban"
      },
      {
        "name": "Tim J. Coats"
      }
    ],
    "summary": "Electronic patient records (EPRs) produce a wealth of data but contain\nsignificant missing information. Understanding and handling this missing data\nis an important part of clinical data analysis and if left unaddressed could\nresult in bias in analysis and distortion in critical conclusions. Missing data\nmay be linked to health care professional practice patterns and imputation of\nmissing data can increase the validity of clinical decisions. This study\nfocuses on statistical approaches for understanding and interpreting the\nmissing data and machine learning based clinical data imputation using a single\ncentre's paediatric emergency data and the data from UK's largest clinical\naudit for traumatic injury database (TARN). In the study of 56,961 data points\nrelated to initial vital signs and observations taken on children presenting to\nan Emergency Department, we have shown that missing data are likely to be\nnon-random and how these are linked to health care professional practice\npatterns. We have then examined 79 TARN fields with missing values for 5,791\ntrauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)\nbased missing data imputation methods are used and imputation results against\nthe original dataset are compared and statistically tested. We have concluded\nthat the 1NN imputer is the best imputation which indicates a usual pattern of\nclinical decision making: find the most similar patients and take their\nattributes as imputation.",
    "comment": "8 pages",
    "journal_ref": "2023 IEEE International Conference on Big Data (BigData),\n  4979-4986",
    "doi": "10.1109/BigData59044.2023.10386194",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.IT",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/BigData59044.2023.10386194",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06563v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06563v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06563v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06562v1",
    "updated": "2024-02-09T17:26:26+00:00",
    "published": "2024-02-09T17:26:26+00:00",
    "title": "Safe Guaranteed Exploration for Non-linear Systems",
    "authors": [
      {
        "name": "Manish Prajapat"
      },
      {
        "name": "Johannes K\u00f6hler"
      },
      {
        "name": "Matteo Turchetta"
      },
      {
        "name": "Andreas Krause"
      },
      {
        "name": "Melanie N. Zeilinger"
      }
    ],
    "summary": "Safely exploring environments with a-priori unknown constraints is a\nfundamental challenge that restricts the autonomy of robots. While safety is\nparamount, guarantees on sufficient exploration are also crucial for ensuring\nautonomous task completion. To address these challenges, we propose a novel\nsafe guaranteed exploration framework using optimal control, which achieves\nfirst-of-its-kind results: guaranteed exploration for non-linear systems with\nfinite time sample complexity bounds, while being provably safe with\narbitrarily high probability. The framework is general and applicable to many\nreal-world scenarios with complex non-linear dynamics and unknown domains.\nBased on this framework we propose an efficient algorithm, SageMPC, SAfe\nGuaranteed Exploration using Model Predictive Control. SageMPC improves\nefficiency by incorporating three techniques: i) exploiting a Lipschitz bound,\nii) goal-directed exploration, and iii) receding horizon style re-planning, all\nwhile maintaining the desired sample complexity, safety and exploration\nguarantees of the framework. Lastly, we demonstrate safe efficient exploration\nin challenging unknown environments using SageMPC with a car model.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.SY",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06562v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06562v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06562v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06560v1",
    "updated": "2024-02-09T17:19:05+00:00",
    "published": "2024-02-09T17:19:05+00:00",
    "title": "Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning",
    "authors": [
      {
        "name": "Amir Ziai"
      },
      {
        "name": "Aneesh Vartakavi"
      }
    ],
    "summary": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, often leading to a reliance on\nthird-party annotators who are not the domain experts. Hard samples, which are\nusually the most informative for model training, tend to be difficult to label\naccurately and consistently without business context. These can arise\nunpredictably during the annotation process, requiring a variable number of\niterations and rounds of feedback, leading to unforeseen expenses and time\ncommitments to guarantee quality.\n  We posit that more direct involvement of domain experts, using a\nhuman-in-the-loop system, can resolve many of these practical challenges. We\npropose a novel framework we call Video Annotator (VA) for annotating,\nmanaging, and iterating on video classification datasets. Our approach offers a\nnew paradigm for an end-user-centered model development process, enhancing the\nefficiency, usability, and effectiveness of video classifiers. Uniquely, VA\nallows for a continuous annotation process, seamlessly integrating data\ncollection and model training.\n  We leverage the zero-shot capabilities of vision-language foundation models\ncombined with active learning techniques, and demonstrate that VA enables the\nefficient creation of high-quality models. VA achieves a median 6.8 point\nimprovement in Average Precision relative to the most competitive baseline\nacross a wide-ranging assortment of tasks. We release a dataset with 153k\nlabels across 56 video understanding tasks annotated by three professional\nvideo editors using VA, and also release code to replicate our experiments at:\nhttp://github.com/netflix/videoannotator.",
    "comment": "Submitted for review to KDD '24 (ADS Track)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06560v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06560v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06560v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06559v1",
    "updated": "2024-02-09T17:18:33+00:00",
    "published": "2024-02-09T17:18:33+00:00",
    "title": "Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following",
    "authors": [
      {
        "name": "Brian Yang"
      },
      {
        "name": "Huangyuan Su"
      },
      {
        "name": "Nikolaos Gkanatsios"
      },
      {
        "name": "Tsung-Wei Ke"
      },
      {
        "name": "Ayush Jain"
      },
      {
        "name": "Jeff Schneider"
      },
      {
        "name": "Katerina Fragkiadaki"
      }
    ],
    "summary": "Diffusion models excel at modeling complex and multimodal trajectory\ndistributions for decision-making and control. Reward-gradient guided denoising\nhas been recently proposed to generate trajectories that maximize both a\ndifferentiable reward function and the likelihood under the data distribution\ncaptured by a diffusion model. Reward-gradient guided denoising requires a\ndifferentiable reward function fitted to both clean and noised samples,\nlimiting its applicability as a general trajectory optimizer. In this paper, we\npropose DiffusionES, a method that combines gradient-free optimization with\ntrajectory denoising to optimize black-box non-differentiable objectives while\nstaying in the data manifold. Diffusion-ES samples trajectories during\nevolutionary search from a diffusion model and scores them using a black-box\nreward function. It mutates high-scoring trajectories using a truncated\ndiffusion process that applies a small number of noising and denoising steps,\nallowing for much more efficient exploration of the solution space. We show\nthat DiffusionES achieves state-of-the-art performance on nuPlan, an\nestablished closed-loop planning benchmark for autonomous driving. Diffusion-ES\noutperforms existing sampling-based planners, reactive deterministic or\ndiffusion-based policies, and reward-gradient guidance. Additionally, we show\nthat unlike prior guidance methods, our method can optimize non-differentiable\nlanguage-shaped reward functions generated by few-shot LLM prompting. When\nguided by a human teacher that issues instructions to follow, our method can\ngenerate novel, highly complex behaviors, such as aggressive lane weaving,\nwhich are not present in the training data. This allows us to solve the hardest\nnuPlan scenarios which are beyond the capabilities of existing trajectory\noptimization methods and driving policies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06559v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06559v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06559v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06557v1",
    "updated": "2024-02-09T17:15:45+00:00",
    "published": "2024-02-09T17:15:45+00:00",
    "title": "The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model",
    "authors": [
      {
        "name": "Gregory Coppola"
      }
    ],
    "summary": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), which\nprovides a unified view of logical and probabilistic reasoning. The QBBN is\nmeant to address a central problem with the Large Language Model (LLM), which\nhas become extremely popular in Information Retrieval, which is that the LLM\nhallucinates. A Bayesian Network, by construction, cannot hallucinate, because\nit can only return answers that it can explain. We show how a Bayesian Network\nover an unbounded number of boolean variables can be configured to represent\nthe logical reasoning underlying human language. We do this by creating a\nkey-value version of the First-Order Calculus, for which we can prove\nconsistency and completeness. We show that the model is trivially trained over\nfully observed data, but that inference is non-trivial. Exact inference in a\nBayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For\ninference, we investigate the use of Loopy Belief Propagation (LBP), which is\nnot guaranteed to converge, but which has been shown to often converge in\npractice. Our experiments show that LBP indeed does converge very reliably, and\nour analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ bounds\nthe number of variables considered, and $n$ bounds the number of incoming\nconnections to any factor, and further improvements may be possible. Our\nnetwork is specifically designed to alternate between AND and OR gates in a\nBoolean Algebra, which connects more closely to logical reasoning, allowing a\ncompleteness proof for an expanded version of our network, and also allows\ninference to follow specific but adequate pathways, that turn out to be fast.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06557v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06557v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06557v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06552v1",
    "updated": "2024-02-09T17:07:31+00:00",
    "published": "2024-02-09T17:07:31+00:00",
    "title": "Deceptive Path Planning via Reinforcement Learning with Graph Neural Networks",
    "authors": [
      {
        "name": "Michael Y. Fatemi"
      },
      {
        "name": "Wesley A. Suttle"
      },
      {
        "name": "Brian M. Sadler"
      }
    ],
    "summary": "Deceptive path planning (DPP) is the problem of designing a path that hides\nits true goal from an outside observer. Existing methods for DPP rely on\nunrealistic assumptions, such as global state observability and perfect model\nknowledge, and are typically problem-specific, meaning that even minor changes\nto a previously solved problem can force expensive computation of an entirely\nnew solution. Given these drawbacks, such methods do not generalize to unseen\nproblem instances, lack scalability to realistic problem sizes, and preclude\nboth on-the-fly tunability of deception levels and real-time adaptivity to\nchanging environments. In this paper, we propose a reinforcement learning\n(RL)-based scheme for training policies to perform DPP over arbitrary weighted\ngraphs that overcomes these issues. The core of our approach is the\nintroduction of a local perception model for the agent, a new state space\nrepresentation distilling the key components of the DPP problem, the use of\ngraph neural network-based policies to facilitate generalization and scaling,\nand the introduction of new deception bonuses that translate the deception\nobjectives of classical methods to the RL setting. Through extensive\nexperimentation we show that, without additional fine-tuning, at test time the\nresulting policies successfully generalize, scale, enjoy tunable levels of\ndeception, and adapt in real-time to changes in the environment.",
    "comment": "11 pages, 14 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "68T05"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06552v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06552v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06552v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06549v1",
    "updated": "2024-02-09T17:02:41+00:00",
    "published": "2024-02-09T17:02:41+00:00",
    "title": "Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA",
    "authors": [
      {
        "name": "Marek \u0160uppa"
      },
      {
        "name": "Daniel Skala"
      },
      {
        "name": "Daniela Ja\u0161\u0161"
      },
      {
        "name": "Samuel Su\u010d\u00edk"
      },
      {
        "name": "Andrej \u0160vec"
      },
      {
        "name": "Peter Hra\u0161ka"
      }
    ],
    "summary": "This study details our approach for the CASE 2024 Shared Task on Climate\nActivism Stance and Hate Event Detection, focusing on Hate Speech Detection,\nHate Speech Target Identification, and Stance Detection as classification\nchallenges. We explored the capability of Large Language Models (LLMs),\nparticularly GPT-4, in zero- or few-shot settings enhanced by retrieval\naugmentation and re-ranking for Tweet classification. Our goal was to determine\nif LLMs could match or surpass traditional methods in this context.\n  We conducted an ablation study with LLaMA for comparison, and our results\nindicate that our models significantly outperformed the baselines, securing\nsecond place in the Target Detection task. The code for our submission is\navailable at https://github.com/NaiveNeuron/bryndza-case-2024",
    "comment": "Accepted to the 7th Workshop on Challenges and Applications of\n  Automated Extraction of Socio-political Events from Text (CASE 2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06549v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06549v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06549v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06716v1",
    "updated": "2024-02-09T17:02:41+00:00",
    "published": "2024-02-09T17:02:41+00:00",
    "title": "Dynamic Graph Information Bottleneck",
    "authors": [
      {
        "name": "Haonan Yuan"
      },
      {
        "name": "Qingyun Sun"
      },
      {
        "name": "Xingcheng Fu"
      },
      {
        "name": "Cheng Ji"
      },
      {
        "name": "Jianxin Li"
      }
    ],
    "summary": "Dynamic Graphs widely exist in the real world, which carry complicated\nspatial and temporal feature patterns, challenging their representation\nlearning. Dynamic Graph Neural Networks (DGNNs) have shown impressive\npredictive abilities by exploiting the intrinsic dynamics. However, DGNNs\nexhibit limited robustness, prone to adversarial attacks. This paper presents\nthe novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust\nand discriminative representations. Leveraged by the Information Bottleneck\n(IB) principle, we first propose the expected optimal representations should\nsatisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress\nredundant as well as conserve meritorious information into latent\nrepresentation, DGIB iteratively directs and refines the structural and feature\ninformation flow passing through graph snapshots. To meet the MSC Condition, we\ndecompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the\nDGIB$_{MS}$ channel aims to learn the minimal and sufficient representations,\nwith the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive\nexperiments on real-world and synthetic dynamic graph datasets demonstrate the\nsuperior robustness of DGIB against adversarial attacks compared with\nstate-of-the-art baselines in the link prediction task. To the best of our\nknowledge, DGIB is the first work to learn robust representations of dynamic\ngraphs grounded in the information-theoretic IB principle.",
    "comment": "Accepted by the research tracks of The Web Conference 2024 (WWW 2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06716v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06716v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06716v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06544v1",
    "updated": "2024-02-09T17:00:32+00:00",
    "published": "2024-02-09T17:00:32+00:00",
    "title": "Calibrating Long-form Generations from Large Language Models",
    "authors": [
      {
        "name": "Yukun Huang"
      },
      {
        "name": "Yixin Liu"
      },
      {
        "name": "Raghuveer Thirukovalluru"
      },
      {
        "name": "Arman Cohan"
      },
      {
        "name": "Bhuwan Dhingra"
      }
    ],
    "summary": "To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06544v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06544v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06544v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06535v1",
    "updated": "2024-02-09T16:49:13+00:00",
    "published": "2024-02-09T16:49:13+00:00",
    "title": "Bandit Convex Optimisation",
    "authors": [
      {
        "name": "Tor Lattimore"
      }
    ],
    "summary": "Bandit convex optimisation is a fundamental framework for studying\nzeroth-order convex optimisation. These notes cover the many tools used for\nthis problem, including cutting plane methods, interior point methods,\ncontinuous exponential weights, gradient descent and online Newton step. The\nnuances between the many assumptions and setups are explained. Although there\nis not much truly new here, some existing tools are applied in novel ways to\nobtain new algorithms. A few bounds are improved in minor ways.",
    "comment": "158 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06535v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06535v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06535v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06532v1",
    "updated": "2024-02-09T16:43:57+00:00",
    "published": "2024-02-09T16:43:57+00:00",
    "title": "Generative Adversarial Bayesian Optimization for Surrogate Objectives",
    "authors": [
      {
        "name": "Michael S. Yao"
      },
      {
        "name": "Yimeng Zeng"
      },
      {
        "name": "Hamsa Bastani"
      },
      {
        "name": "Jacob Gardner"
      },
      {
        "name": "James C. Gee"
      },
      {
        "name": "Osbert Bastani"
      }
    ],
    "summary": "Offline model-based policy optimization seeks to optimize a learned surrogate\nobjective function without querying the true oracle objective during\noptimization. However, inaccurate surrogate model predictions are frequently\nencountered along the optimization trajectory. To address this limitation, we\npropose generative adversarial Bayesian optimization (GABO) using adaptive\nsource critic regularization, a task-agnostic framework for Bayesian\noptimization that employs a Lipschitz-bounded source critic model to constrain\nthe optimization trajectory to regions where the surrogate function is\nreliable. We show that under certain assumptions for the continuous input space\nprior, our algorithm dynamically adjusts the strength of the source critic\nregularization. GABO outperforms existing baselines on a number of different\noffline optimization tasks across a variety of scientific domains. Our code is\navailable at https://github.com/michael-s-yao/gabo",
    "comment": "15 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06532v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06532v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06532v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06531v1",
    "updated": "2024-02-09T16:43:34+00:00",
    "published": "2024-02-09T16:43:34+00:00",
    "title": "Transferring facade labels between point clouds with semantic octrees while considering change detection",
    "authors": [
      {
        "name": "Sophia Schwarz"
      },
      {
        "name": "Tanja Pilz"
      },
      {
        "name": "Olaf Wysocki"
      },
      {
        "name": "Ludwig Hoegner"
      },
      {
        "name": "Uwe Stilla"
      }
    ],
    "summary": "Point clouds and high-resolution 3D data have become increasingly important\nin various fields, including surveying, construction, and virtual reality.\nHowever, simply having this data is not enough; to extract useful information,\nsemantic labeling is crucial. In this context, we propose a method to transfer\nannotations from a labeled to an unlabeled point cloud using an octree\nstructure. The structure also analyses changes between the point clouds. Our\nexperiments confirm that our method effectively transfers annotations while\naddressing changes. The primary contribution of this project is the development\nof the method for automatic label transfer between two different point clouds\nthat represent the same real-world object. The proposed method can be of great\nimportance for data-driven deep learning algorithms as it can also allow\ncircumventing stochastic transfer learning by deterministic label transfer\nbetween datasets depicting the same objects.",
    "comment": "Accepted to the Recent Advances in 3D Geoinformation Science,\n  Proceedings of the 18th 3D GeoInfo Conference 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06531v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06531v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06531v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06530v1",
    "updated": "2024-02-09T16:41:50+00:00",
    "published": "2024-02-09T16:41:50+00:00",
    "title": "Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification",
    "authors": [
      {
        "name": "Muhammad Uzair Zahid"
      },
      {
        "name": "Aysen Degerli"
      },
      {
        "name": "Fahad Sohrab"
      },
      {
        "name": "Serkan Kiranyaz"
      },
      {
        "name": "Moncef Gabbouj"
      }
    ],
    "summary": "Early detection of myocardial infarction (MI), a critical condition arising\nfrom coronary artery disease (CAD), is vital to prevent further myocardial\ndamage. This study introduces a novel method for early MI detection using a\none-class classification (OCC) algorithm in echocardiography. Our study\novercomes the challenge of limited echocardiography data availability by\nadopting a novel approach based on Multi-modal Subspace Support Vector Data\nDescription. The proposed technique involves a specialized MI detection\nframework employing multi-view echocardiography incorporating a composite\nkernel in the non-linear projection trick, fusing Gaussian and Laplacian\nsigmoid functions. Additionally, we enhance the update strategy of the\nprojection matrices by adapting maximization for both or one of the modalities\nin the optimization process. Our method boosts MI detection capability by\nefficiently transforming features extracted from echocardiography data into an\noptimized lower-dimensional subspace. The OCC model trained specifically on\ntarget class instances from the comprehensive HMC-QU dataset that includes\nmultiple echocardiography views indicates a marked improvement in MI detection\naccuracy. Our findings reveal that our proposed multi-view approach achieves a\ngeometric mean of 71.24\\%, signifying a substantial advancement in\nechocardiography-based MI diagnosis and offering more precise and efficient\ndiagnostic tools.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06530v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06530v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06530v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06529v1",
    "updated": "2024-02-09T16:40:59+00:00",
    "published": "2024-02-09T16:40:59+00:00",
    "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty",
    "authors": [
      {
        "name": "Kaiqu Liang"
      },
      {
        "name": "Zixu Zhang"
      },
      {
        "name": "Jaime Fern\u00e1ndez Fisac"
      }
    ],
    "summary": "Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural\nlanguage instructions can induce task uncertainty, particularly in situations\nwhere multiple valid options exist. To address this issue, LLMs must identify\nsuch uncertainty and proactively seek clarification. This paper explores the\nconcept of introspective planning as a systematic method for guiding LLMs in\nforming uncertainty--aware plans for robotic task execution without the need\nfor fine-tuning. We investigate uncertainty quantification in task-level robot\nplanning and demonstrate that introspection significantly improves both success\nrates and safety compared to state-of-the-art LLM-based planning approaches.\nFurthermore, we assess the effectiveness of introspective planning in\nconjunction with conformal prediction, revealing that this combination yields\ntighter confidence bounds, thereby maintaining statistical success guarantees\nwith fewer superfluous user clarification queries.",
    "comment": "22 pages, 15 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06529v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06529v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06529v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06525v1",
    "updated": "2024-02-09T16:37:08+00:00",
    "published": "2024-02-09T16:37:08+00:00",
    "title": "Flexible infinite-width graph convolutional networks and the importance of representation learning",
    "authors": [
      {
        "name": "Ben Anson"
      },
      {
        "name": "Edward Milsom"
      },
      {
        "name": "Laurence Aitchison"
      }
    ],
    "summary": "A common theoretical approach to understanding neural networks is to take an\ninfinite-width limit, at which point the outputs become Gaussian process (GP)\ndistributed. This is known as a neural network Gaussian process (NNGP).\nHowever, the NNGP kernel is fixed, and tunable only through a small number of\nhyperparameters, eliminating any possibility of representation learning. This\ncontrasts with finite-width NNs, which are often believed to perform well\nprecisely because they are able to learn representations. Thus in simplifying\nNNs to make them theoretically tractable, NNGPs may eliminate precisely what\nmakes them work well (representation learning). This motivated us to understand\nwhether representation learning is necessary in a range of graph classification\ntasks. We develop a precise tool for this task, the graph convolutional deep\nkernel machine. This is very similar to an NNGP, in that it is an infinite\nwidth limit and uses kernels, but comes with a `knob' to control the amount of\nrepresentation learning. We found that representation learning is necessary (in\nthe sense that it gives dramatic performance improvements) in graph\nclassification tasks and heterophilous node classification tasks, but not in\nhomophilous node classification tasks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06525v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06525v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06525v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06521v1",
    "updated": "2024-02-09T16:34:28+00:00",
    "published": "2024-02-09T16:34:28+00:00",
    "title": "Reconstructing facade details using MLS point clouds and Bag-of-Words approach",
    "authors": [
      {
        "name": "Thomas Froech"
      },
      {
        "name": "Olaf Wysocki"
      },
      {
        "name": "Ludwig Hoegner"
      },
      {
        "name": "Uwe Stilla"
      }
    ],
    "summary": "In the reconstruction of fa\\c{c}ade elements, the identification of specific\nobject types remains challenging and is often circumvented by rectangularity\nassumptions or the use of bounding boxes. We propose a new approach for the\nreconstruction of 3D fa\\c{c}ade details. We combine MLS point clouds and a\npre-defined 3D model library using a BoW concept, which we augment by\nincorporating semi-global features. We conduct experiments on the models\nsuperimposed with random noise and on the TUM-FA\\c{C}ADE dataset. Our method\ndemonstrates promising results, improving the conventional BoW approach. It\nholds the potential to be utilized for more realistic facade reconstruction\nwithout rectangularity assumptions, which can be used in applications such as\ntesting automated driving functions or estimating fa\\c{c}ade solar potential.",
    "comment": "Accepted to the Recent Advances in 3D Geoinformation Science,\n  Proceedings of the 18th 3D GeoInfo Conference 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06521v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06521v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06521v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06512v1",
    "updated": "2024-02-09T16:18:38+00:00",
    "published": "2024-02-09T16:18:38+00:00",
    "title": "Multimodal Clinical Trial Outcome Prediction with Large Language Models",
    "authors": [
      {
        "name": "Wenhao Zheng"
      },
      {
        "name": "Dongsheng Peng"
      },
      {
        "name": "Hongxia Xu"
      },
      {
        "name": "Hongtu Zhu"
      },
      {
        "name": "Tianfan Fu"
      },
      {
        "name": "Huaxiu Yao"
      }
    ],
    "summary": "The clinical trial is a pivotal and costly process, often spanning multiple\nyears and requiring substantial financial resources. Therefore, the development\nof clinical trial outcome prediction models aims to exclude drugs likely to\nfail and holds the potential for significant cost savings. Recent data-driven\nattempts leverage deep learning methods to integrate multimodal data for\npredicting clinical trial outcomes. However, these approaches rely on manually\ndesigned modal-specific encoders, which limits both the extensibility to adapt\nnew modalities and the ability to discern similar information patterns across\ndifferent modalities. To address these issues, we propose a multimodal\nmixture-of-experts (LIFTED) approach for clinical trial outcome prediction.\nSpecifically, LIFTED unifies different modality data by transforming them into\nnatural language descriptions. Then, LIFTED constructs unified noise-resilient\nencoders to extract information from modal-specific language descriptions.\nSubsequently, a sparse Mixture-of-Experts framework is employed to further\nrefine the representations, enabling LIFTED to identify similar information\npatterns across different modalities and extract more consistent\nrepresentations from those patterns using the same expert model. Finally, a\nmixture-of-experts module is further employed to dynamically integrate\ndifferent modality representations for prediction, which gives LIFTED the\nability to automatically weigh different modalities and pay more attention to\ncritical information. The experiments demonstrate that LIFTED significantly\nenhances performance in predicting clinical trial outcomes across all three\nphases compared to the best baseline, showcasing the effectiveness of our\nproposed key components.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06512v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06512v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06512v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06509v1",
    "updated": "2024-02-09T16:15:30+00:00",
    "published": "2024-02-09T16:15:30+00:00",
    "title": "Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions",
    "authors": [
      {
        "name": "Alberto Testoni"
      },
      {
        "name": "Raquel Fern\u00e1ndez"
      }
    ],
    "summary": "Clarification questions are an essential dialogue tool to signal\nmisunderstanding, ambiguities, and under-specification in language use. While\nhumans are able to resolve uncertainty by asking questions since childhood,\nmodern dialogue systems struggle to generate effective questions. To make\nprogress in this direction, in this work we take a collaborative dialogue task\nas a testbed and study how model uncertainty relates to human uncertainty -- an\nas yet under-explored problem. We show that model uncertainty does not mirror\nhuman clarification-seeking behavior, which suggests that using human\nclarification questions as supervision for deciding when to ask may not be the\nmost effective way to resolve model uncertainty. To address this issue, we\npropose an approach to generating clarification questions based on model\nuncertainty estimation, compare it to several alternatives, and show that it\nleads to significant improvements in terms of task success. Our findings\nhighlight the importance of equipping dialogue systems with the ability to\nassess their own uncertainty and exploit in interaction.",
    "comment": "Accepted at EACL 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06509v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06509v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06509v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06506v1",
    "updated": "2024-02-09T16:14:30+00:00",
    "published": "2024-02-09T16:14:30+00:00",
    "title": "Classifying point clouds at the facade-level using geometric features and deep learning networks",
    "authors": [
      {
        "name": "Yue Tan"
      },
      {
        "name": "Olaf Wysocki"
      },
      {
        "name": "Ludwig Hoegner"
      },
      {
        "name": "Uwe Stilla"
      }
    ],
    "summary": "3D building models with facade details are playing an important role in many\napplications now. Classifying point clouds at facade-level is key to create\nsuch digital replicas of the real world. However, few studies have focused on\nsuch detailed classification with deep neural networks. We propose a method\nfusing geometric features with deep learning networks for point cloud\nclassification at facade-level. Our experiments conclude that such early-fused\nfeatures improve deep learning methods' performance. This method can be applied\nfor compensating deep learning networks' ability in capturing local geometric\ninformation and promoting the advancement of semantic segmentation.",
    "comment": "Accepted to the Recent Advances in 3D Geoinformation Science,\n  Proceedings of the 18th 3D GeoInfo Conference 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06506v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06506v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06506v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06503v1",
    "updated": "2024-02-09T16:12:53+00:00",
    "published": "2024-02-09T16:12:53+00:00",
    "title": "ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies",
    "authors": [
      {
        "name": "Jasmina Gajcin"
      },
      {
        "name": "Ivana Dusparic"
      }
    ],
    "summary": "Understanding how failure occurs and how it can be prevented in reinforcement\nlearning (RL) is necessary to enable debugging, maintain user trust, and\ndevelop personalized policies. Counterfactual reasoning has often been used to\nassign blame and understand failure by searching for the closest possible world\nin which the failure is avoided. However, current counterfactual state\nexplanations in RL can only explain an outcome using just the current state\nfeatures and offer no actionable recourse on how a negative outcome could have\nbeen prevented. In this work, we propose ACTER (Actionable Counterfactual\nSequences for Explaining Reinforcement Learning Outcomes), an algorithm for\ngenerating counterfactual sequences that provides actionable advice on how\nfailure can be avoided. ACTER investigates actions leading to a failure and\nuses the evolutionary algorithm NSGA-II to generate counterfactual sequences of\nactions that prevent it with minimal changes and high certainty even in\nstochastic environments. Additionally, ACTER generates a set of multiple\ndiverse counterfactual sequences that enable users to correct failure in the\nway that best fits their preferences. We also introduce three diversity metrics\nthat can be used for evaluating the diversity of counterfactual sequences. We\nevaluate ACTER in two RL environments, with both discrete and continuous\nactions, and show that it can generate actionable and diverse counterfactual\nsequences. We conduct a user study to explore how explanations generated by\nACTER help users identify and correct failure.",
    "comment": "17 pages, 4 Figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06503v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06503v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06503v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06501v1",
    "updated": "2024-02-09T16:11:04+00:00",
    "published": "2024-02-09T16:11:04+00:00",
    "title": "Scalable Interactive Machine Learning for Future Command and Control",
    "authors": [
      {
        "name": "Anna Madison"
      },
      {
        "name": "Ellen Novoseller"
      },
      {
        "name": "Vinicius G. Goecks"
      },
      {
        "name": "Benjamin T. Files"
      },
      {
        "name": "Nicholas Waytowich"
      },
      {
        "name": "Alfred Yu"
      },
      {
        "name": "Vernon J. Lawhern"
      },
      {
        "name": "Steven Thurman"
      },
      {
        "name": "Christopher Kelshaw"
      },
      {
        "name": "Kaleb McDowell"
      }
    ],
    "summary": "Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.",
    "comment": "Submitted to the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "I.2.6; I.2.7; J.7"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06501v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06501v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06501v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06715v1",
    "updated": "2024-02-09T16:10:54+00:00",
    "published": "2024-02-09T16:10:54+00:00",
    "title": "Learning-augmented Online Algorithm for Two-level Ski-rental Problem",
    "authors": [
      {
        "name": "Keyuan Zhang"
      },
      {
        "name": "Zhongdong Liu"
      },
      {
        "name": "Nakjung Choi"
      },
      {
        "name": "Bo Ji"
      }
    ],
    "summary": "In this paper, we study the two-level ski-rental problem,where a user needs\nto fulfill a sequence of demands for multiple items by choosing one of the\nthree payment options: paying for the on-demand usage (i.e., rent), buying\nindividual items (i.e., single purchase), and buying all the items (i.e., combo\npurchase). Without knowing future demands, the user aims to minimize the total\ncost (i.e., the sum of the rental, single purchase, and combo purchase costs)\nby balancing the trade-off between the expensive upfront costs (for purchase)\nand the potential future expenses (for rent). We first design a robust online\nalgorithm (RDTSR) that offers a worst-case performance guarantee. While online\nalgorithms are robust against the worst-case scenarios, they are often overly\ncautious and thus suffer a poor average performance in typical scenarios. On\nthe other hand, Machine Learning (ML) algorithms typically show promising\naverage performance in various applications but lack worst-case performance\nguarantees. To harness the benefits of both methods, we develop a\nlearning-augmented algorithm (LADTSR) by integrating ML predictions into the\nrobust online algorithm, which outperforms the robust online algorithm under\naccurate predictions while ensuring worst-case performance guarantees even when\npredictions are inaccurate. Finally, we conduct numerical experiments on both\nsynthetic and real-world trace data to corroborate the effectiveness of our\napproach.",
    "comment": "Accepted by the Thirty-Eighth AAAI Conference on Artificial\n  Intelligence (AAAI-24)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DS",
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06715v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06715v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06715v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07946v1",
    "updated": "2024-02-09T16:10:29+00:00",
    "published": "2024-02-09T16:10:29+00:00",
    "title": "Re-Envisioning Command and Control",
    "authors": [
      {
        "name": "Kaleb McDowell"
      },
      {
        "name": "Ellen Novoseller"
      },
      {
        "name": "Anna Madison"
      },
      {
        "name": "Vinicius G. Goecks"
      },
      {
        "name": "Christopher Kelshaw"
      }
    ],
    "summary": "Future warfare will require Command and Control (C2) decision-making to occur\nin more complex, fast-paced, ill-structured, and demanding conditions. C2 will\nbe further complicated by operational challenges such as Denied, Degraded,\nIntermittent, and Limited (DDIL) communications and the need to account for\nmany data streams, potentially across multiple domains of operation. Yet,\ncurrent C2 practices -- which stem from the industrial era rather than the\nemerging intelligence era -- are linear and time-consuming. Critically, these\napproaches may fail to maintain overmatch against adversaries on the future\nbattlefield. To address these challenges, we propose a vision for future C2\nbased on robust partnerships between humans and artificial intelligence (AI)\nsystems. This future vision is encapsulated in three operational impacts:\nstreamlining the C2 operations process, maintaining unity of effort, and\ndeveloping adaptive collective knowledge systems. This paper illustrates the\nenvisaged future C2 capabilities, discusses the assumptions that shaped them,\nand describes how the proposed developments could transform C2 in future\nwarfare.",
    "comment": "Submitted to the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.6; I.2.7; J.7"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07946v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07946v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07946v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06500v1",
    "updated": "2024-02-09T16:10:19+00:00",
    "published": "2024-02-09T16:10:19+00:00",
    "title": "On the Fly Detection of Root Causes from Observed Data with Application to IT Systems",
    "authors": [
      {
        "name": "Lei Zan"
      },
      {
        "name": "Charles K. Assaad"
      },
      {
        "name": "Emilie Devijver"
      },
      {
        "name": "Eric Gaussier"
      }
    ],
    "summary": "This paper introduces a new structural causal model tailored for representing\nthreshold-based IT systems and presents a new algorithm designed to rapidly\ndetect root causes of anomalies in such systems. When root causes are not\ncausally related, the method is proven to be correct; while an extension is\nproposed based on the intervention of an agent to relax this assumption. Our\nalgorithm and its agent-based extension leverage causal discovery from offline\ndata and engage in subgraph traversal when encountering new anomalies in online\ndata. Our extensive experiments demonstrate the superior performance of our\nmethods, even when applied to data generated from alternative structural causal\nmodels or real IT monitoring data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06500v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06500v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06500v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06494v1",
    "updated": "2024-02-09T15:56:39+00:00",
    "published": "2024-02-09T15:56:39+00:00",
    "title": "Deep Learning-Based Auto-Segmentation of Planning Target Volume for Total Marrow and Lymph Node Irradiation",
    "authors": [
      {
        "name": "Ricardo Coimbra Brioso"
      },
      {
        "name": "Damiano Dei"
      },
      {
        "name": "Nicola Lambri"
      },
      {
        "name": "Daniele Loiacono"
      },
      {
        "name": "Pietro Mancosu"
      },
      {
        "name": "Marta Scorsetti"
      }
    ],
    "summary": "In order to optimize the radiotherapy delivery for cancer treatment,\nespecially when dealing with complex treatments such as Total Marrow and Lymph\nNode Irradiation (TMLI), the accurate contouring of the Planning Target Volume\n(PTV) is crucial. Unfortunately, relying on manual contouring for such\ntreatments is time-consuming and prone to errors. In this paper, we investigate\nthe application of Deep Learning (DL) to automate the segmentation of the PTV\nin TMLI treatment, building upon previous work that introduced a solution to\nthis problem based on a 2D U-Net model. We extend the previous research (i) by\nemploying the nnU-Net framework to develop both 2D and 3D U-Net models and (ii)\nby evaluating the trained models on the PTV with the exclusion of bones, which\nconsist mainly of lymp-nodes and represent the most challenging region of the\ntarget volume to segment. Our result show that the introduction of nnU-NET\nframework led to statistically significant improvement in the segmentation\nperformance. In addition, the analysis on the PTV after the exclusion of bones\nshowed that the models are quite robust also on the most challenging areas of\nthe target volume. Overall, our study is a significant step forward in the\napplication of DL in a complex radiotherapy treatment such as TMLI, offering a\nviable and scalable solution to increase the number of patients who can benefit\nfrom this treatment.",
    "comment": "arXiv admin note: text overlap with arXiv:2304.02353",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06494v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06494v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06494v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06492v1",
    "updated": "2024-02-09T15:53:15+00:00",
    "published": "2024-02-09T15:53:15+00:00",
    "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
    "authors": [
      {
        "name": "Yichen Jiang"
      },
      {
        "name": "Xiang Zhou"
      },
      {
        "name": "Mohit Bansal"
      }
    ],
    "summary": "Transformers generalize to novel compositions of structures and entities\nafter being trained on a complex dataset, but easily overfit on datasets of\ninsufficient complexity. We observe that when the training set is sufficiently\ncomplex, the model encodes sentences that have a common syntactic structure\nusing a systematic attention pattern. Inspired by this observation, we propose\nSQ-Transformer (Structurally Quantized) that explicitly encourages\nsystematicity in the embeddings and attention layers, even with a training set\nof low complexity. At the embedding level, we introduce Structure-oriented\nVector Quantization (SoVQ) to cluster word embeddings into several classes of\nstructurally equivalent entities. At the attention level, we devise the\nSystematic Attention Layer (SAL) and an alternative, Systematically Regularized\nLayer (SRL) that operate on the quantized word embeddings so that sentences of\nthe same structure are encoded with invariant or similar attention patterns.\nEmpirically, we show that SQ-Transformer achieves stronger compositional\ngeneralization than the vanilla Transformer on multiple low-complexity semantic\nparsing and machine translation datasets. In our analysis, we show that SoVQ\nindeed learns a syntactically clustered embedding space and SAL/SRL induces\ngeneralizable attention patterns, which lead to improved systematicity.",
    "comment": "22 pages, code: https://github.com/jiangycTarheel/SQ-Transformer",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06492v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06492v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06492v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06487v1",
    "updated": "2024-02-09T15:43:31+00:00",
    "published": "2024-02-09T15:43:31+00:00",
    "title": "Le Nozze di Giustizia. Interactions between Artificial Intelligence, Law, Logic, Language and Computation with some case studies in Traffic Regulations and Health Care",
    "authors": [
      {
        "name": "Joost J. Joosten"
      },
      {
        "name": "Manuela Montoya Garc\u00eda"
      }
    ],
    "summary": "An important aim of this paper is to convey some basics of mathematical logic\nto the legal community working with Artificial Intelligence. After analysing\nwhat AI is, we decide to delimit ourselves to rule-based AI leaving Neural\nNetworks and Machine Learning aside. Rule based AI allows for Formal methods\nwhich are described in a rudimentary form. We will then see how mathematical\nlogic interacts with legal rule-based AI practice. We shall see how\nmathematical logic imposes limitations and complications to AI applications. We\nclassify the limitations and interactions between mathematical logic and legal\nAI in three categories: logical, computational and mathematical. The examples\nto showcase the interactions will largely come from European traffic\nregulations. The paper closes off with some reflections on how and where AI\ncould be used and on basic mechanisms that shape society.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06487v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06487v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06487v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06472v1",
    "updated": "2024-02-09T15:25:36+00:00",
    "published": "2024-02-09T15:25:36+00:00",
    "title": "\"When He Feels Cold, He Goes to the Seahorse\"-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy",
    "authors": [
      {
        "name": "Di Liu"
      },
      {
        "name": "Hanqing Zhou"
      },
      {
        "name": "Pengcheng An"
      }
    ],
    "summary": "Storymaking, as an integrative form of expressive arts therapy, is an\neffective means to foster family communication. Yet, the integration of\ngenerative AI as expressive materials in therapeutic storymaking remains\nunderexplored. And there is a lack of HCI implications on how to support\nfamilies and therapists in this context. Addressing this, our study involved\nfive weeks of storymaking sessions with seven families guided by a professional\ntherapist. In these sessions, the families used both traditional art-making\nmaterials and image-based generative AI to create and evolve their family\nstories. Via the rich empirical data and commentaries from four expert\ntherapists, we contextualize how families creatively melded AI and traditional\nexpressive materials to externalize their ideas and feelings. Through the lens\nof Expressive Therapies Continuum (ETC), we characterize the therapeutic\nimplications of AI as expressive materials. Desirable interaction qualities to\nsupport children, parents, and therapists are distilled for future HCI\nresearch.",
    "comment": "to appear at ACM CHI '24",
    "journal_ref": null,
    "doi": "10.1145/3613904.3642852",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3613904.3642852",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06472v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06472v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06472v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06714v1",
    "updated": "2024-02-09T15:18:00+00:00",
    "published": "2024-02-09T15:18:00+00:00",
    "title": "Electricity Price Forecasting in the Irish Balancing Market",
    "authors": [
      {
        "name": "Ciaran O'Connor"
      },
      {
        "name": "Joseph Collins"
      },
      {
        "name": "Steven Prestwich"
      },
      {
        "name": "Andrea Visentin"
      }
    ],
    "summary": "Short-term electricity markets are becoming more relevant due to\nless-predictable renewable energy sources, attracting considerable attention\nfrom the industry. The balancing market is the closest to real-time and the\nmost volatile among them. Its price forecasting literature is limited,\ninconsistent and outdated, with few deep learning attempts and no public\ndataset. This work applies to the Irish balancing market a variety of price\nprediction techniques proven successful in the widely studied day-ahead market.\nWe compare statistical, machine learning, and deep learning models using a\nframework that investigates the impact of different training sizes. The\nframework defines hyperparameters and calibration settings; the dataset and\nmodels are made public to ensure reproducibility and to be used as benchmarks\nfor future works. An extensive numerical study shows that well-performing\nmodels in the day-ahead market do not perform well in the balancing one,\nhighlighting that these markets are fundamentally different constructs. The\nbest model is LEAR, a statistical approach based on LASSO, which outperforms\nmore complex and computationally demanding approaches.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "q-fin.PR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06714v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06714v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06714v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06465v1",
    "updated": "2024-02-09T15:17:53+00:00",
    "published": "2024-02-09T15:17:53+00:00",
    "title": "On Differentially Private Subspace Estimation Without Distributional Assumptions",
    "authors": [
      {
        "name": "Eliad Tsfadia"
      }
    ],
    "summary": "Private data analysis faces a significant challenge known as the curse of\ndimensionality, leading to increased costs. However, many datasets possess an\ninherent low-dimensional structure. For instance, during optimization via\ngradient descent, the gradients frequently reside near a low-dimensional\nsubspace. If the low-dimensional structure could be privately identified using\na small amount of points, we could avoid paying (in terms of privacy and\naccuracy) for the high ambient dimension.\n  On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved\nthat privately estimating subspaces, in general, requires an amount of points\nthat depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed\nthis limitation by considering points that are i.i.d. samples from a Gaussian\ndistribution whose covariance matrix has a certain eigenvalue gap. Yet, it was\nstill left unclear whether we could provide similar upper bounds without\ndistributional assumptions and whether we could prove lower bounds that depend\non similar eigenvalue gaps.\n  In this work, we make progress in both directions. We formulate the problem\nof private subspace estimation under two different types of singular value gaps\nof the input data and prove new upper and lower bounds for both types. In\nparticular, our results determine what type of gap is sufficient and necessary\nfor estimating a subspace with an amount of points that is independent of the\ndimension.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06465v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06465v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06465v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06463v1",
    "updated": "2024-02-09T15:14:48+00:00",
    "published": "2024-02-09T15:14:48+00:00",
    "title": "Cardiac ultrasound simulation for autonomous ultrasound navigation",
    "authors": [
      {
        "name": "Abdoul Aziz Amadou"
      },
      {
        "name": "Laura Peralta"
      },
      {
        "name": "Paul Dryburgh"
      },
      {
        "name": "Paul Klein"
      },
      {
        "name": "Kaloian Petkov"
      },
      {
        "name": "Richard James Housden"
      },
      {
        "name": "Vivek Singh"
      },
      {
        "name": "Rui Liao"
      },
      {
        "name": "Young-Ho Kim"
      },
      {
        "name": "Florin Christian Ghesu"
      },
      {
        "name": "Tommaso Mansi"
      },
      {
        "name": "Ronak Rajani"
      },
      {
        "name": "Alistair Young"
      },
      {
        "name": "Kawal Rhode"
      }
    ],
    "summary": "Ultrasound is well-established as an imaging modality for diagnostic and\ninterventional purposes. However, the image quality varies with operator skills\nas acquiring and interpreting ultrasound images requires extensive training due\nto the imaging artefacts, the range of acquisition parameters and the\nvariability of patient anatomies. Automating the image acquisition task could\nimprove acquisition reproducibility and quality but training such an algorithm\nrequires large amounts of navigation data, not saved in routine examinations.\nThus, we propose a method to generate large amounts of ultrasound images from\nother modalities and from arbitrary positions, such that this pipeline can\nlater be used by learning algorithms for navigation. We present a novel\nsimulation pipeline which uses segmentations from other modalities, an\noptimized volumetric data representation and GPU-accelerated Monte Carlo path\ntracing to generate view-dependent and patient-specific ultrasound images. We\nextensively validate the correctness of our pipeline with a phantom experiment,\nwhere structures' sizes, contrast and speckle noise properties are assessed.\nFurthermore, we demonstrate its usability to train neural networks for\nnavigation in an echocardiography view classification experiment by generating\nsynthetic images from more than 1000 patients. Networks pre-trained with our\nsimulations achieve significantly superior performance in settings where large\nreal datasets are not available, especially for under-represented classes. The\nproposed approach allows for fast and accurate patient-specific ultrasound\nimage generation, and its usability for training networks for\nnavigation-related tasks is demonstrated.",
    "comment": "24 pages, 10 figures, 5 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "I.6.0; I.5.4; J.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06463v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06463v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06463v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06461v1",
    "updated": "2024-02-09T15:09:38+00:00",
    "published": "2024-02-09T15:09:38+00:00",
    "title": "Sequential Flow Matching for Generative Modeling",
    "authors": [
      {
        "name": "Jongmin Yoon"
      },
      {
        "name": "Juho Lee"
      }
    ],
    "summary": "Straightening the probability flow of the continuous-time generative models,\nsuch as diffusion models or flow-based models, is the key to fast sampling\nthrough the numerical solvers, existing methods learn a linear path by directly\ngenerating the probability path the joint distribution between the noise and\ndata distribution. One key reason for the slow sampling speed of the ODE-based\nsolvers that simulate these generative models is the global truncation error of\nthe ODE solver, caused by the high curvature of the ODE trajectory, which\nexplodes the truncation error of the numerical solvers in the low-NFE regime.\nTo address this challenge, We propose a novel method called SeqRF, a learning\ntechnique that straightens the probability flow to reduce the global truncation\nerror and hence enable acceleration of sampling and improve the synthesis\nquality. In both theoretical and empirical studies, we first observe the\nstraightening property of our SeqRF. Through empirical evaluations via SeqRF\nover flow-based generative models, We achieve surpassing results on CIFAR-10,\nCelebA-$64 \\times 64$, and LSUN-Church datasets.",
    "comment": "19 pages, 13 figures. Under review by ICML 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06461v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06461v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06461v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06457v1",
    "updated": "2024-02-09T15:02:56+00:00",
    "published": "2024-02-09T15:02:56+00:00",
    "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
    "authors": [
      {
        "name": "Arian Hosseini"
      },
      {
        "name": "Xingdi Yuan"
      },
      {
        "name": "Nikolay Malkin"
      },
      {
        "name": "Aaron Courville"
      },
      {
        "name": "Alessandro Sordoni"
      },
      {
        "name": "Rishabh Agarwal"
      }
    ],
    "summary": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated\nsolutions to improve their problem-solving ability. However, these approaches\ndiscard the large amounts of incorrect solutions generated during this process,\npotentially neglecting valuable information in such solutions. To address this\nshortcoming, we propose V-STaR that utilizes both the correct and incorrect\nsolutions generated during the self-improvement process to train a verifier\nusing DPO that judges correctness of model-generated solutions. This verifier\nis used at inference time to select one solution among many candidate\nsolutions. Running V-STaR for multiple iterations results in progressively\nbetter reasoners and verifiers, delivering a 4% to 17% test accuracy\nimprovement over existing self-improvement and verification approaches on\ncommon code generation and math reasoning benchmarks with LLaMA2 models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06457v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06457v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06457v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06452v1",
    "updated": "2024-02-09T14:58:07+00:00",
    "published": "2024-02-09T14:58:07+00:00",
    "title": "An Algorithmic Framework for Constructing Multiple Decision Trees by Evaluating Their Combination Performance Throughout the Construction Process",
    "authors": [
      {
        "name": "Keito Tajima"
      },
      {
        "name": "Naoki Ichijo"
      },
      {
        "name": "Yuta Nakahara"
      },
      {
        "name": "Toshiyasu Matsushima"
      }
    ],
    "summary": "Predictions using a combination of decision trees are known to be effective\nin machine learning. Typical ideas for constructing a combination of decision\ntrees for prediction are bagging and boosting. Bagging independently constructs\ndecision trees without evaluating their combination performance and averages\nthem afterward. Boosting constructs decision trees sequentially, only\nevaluating a combination performance of a new decision tree and the fixed past\ndecision trees at each step. Therefore, neither method directly constructs nor\nevaluates a combination of decision trees for the final prediction. When the\nfinal prediction is based on a combination of decision trees, it is natural to\nevaluate the appropriateness of the combination when constructing them. In this\nstudy, we propose a new algorithmic framework that constructs decision trees\nsimultaneously and evaluates their combination performance throughout the\nconstruction process. Our framework repeats two procedures. In the first\nprocedure, we construct new candidates of combinations of decision trees to\nfind a proper combination of decision trees. In the second procedure, we\nevaluate each combination performance of decision trees under some criteria and\nselect a better combination. To confirm the performance of the proposed\nframework, we perform experiments on synthetic and benchmark data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06452v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06452v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06452v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06445v1",
    "updated": "2024-02-09T14:46:50+00:00",
    "published": "2024-02-09T14:46:50+00:00",
    "title": "The Deep Equilibrium Algorithmic Reasoner",
    "authors": [
      {
        "name": "Dobrik Georgiev"
      },
      {
        "name": "Pietro Li\u00f2"
      },
      {
        "name": "Davide Buffelli"
      }
    ],
    "summary": "Recent work on neural algorithmic reasoning has demonstrated that graph\nneural networks (GNNs) could learn to execute classical algorithms. Doing so,\nhowever, has always used a recurrent architecture, where each iteration of the\nGNN aligns with an algorithm's iteration. Since an algorithm's solution is\noften an equilibrium, we conjecture and empirically validate that one can train\na network to solve algorithmic problems by directly finding the equilibrium.\nNote that this does not require matching each GNN iteration with a step of the\nalgorithm.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06445v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06445v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06445v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06441v1",
    "updated": "2024-02-09T14:34:28+00:00",
    "published": "2024-02-09T14:34:28+00:00",
    "title": "Incorporating Taylor Series and Recursive Structure in Neural Networks for Time Series Prediction",
    "authors": [
      {
        "name": "Jarrod Mau"
      },
      {
        "name": "Kevin Moon"
      }
    ],
    "summary": "Time series analysis is relevant in various disciplines such as physics,\nbiology, chemistry, and finance. In this paper, we present a novel neural\nnetwork architecture that integrates elements from ResNet structures, while\nintroducing the innovative incorporation of the Taylor series framework. This\napproach demonstrates notable enhancements in test accuracy across many of the\nbaseline datasets investigated. Furthermore, we extend our method to\nincorporate a recursive step, which leads to even further improvements in test\naccuracy. Our findings underscore the potential of our proposed model to\nsignificantly advance time series analysis methodologies, offering promising\navenues for future research and application.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06441v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06441v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06441v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06434v1",
    "updated": "2024-02-09T14:24:18+00:00",
    "published": "2024-02-09T14:24:18+00:00",
    "title": "Where is the Truth? The Risk of Getting Confounded in a Continual World",
    "authors": [
      {
        "name": "Florian Peter Busch"
      },
      {
        "name": "Roshni Kamath"
      },
      {
        "name": "Rupert Mitchell"
      },
      {
        "name": "Wolfgang Stammer"
      },
      {
        "name": "Kristian Kersting"
      },
      {
        "name": "Martin Mundt"
      }
    ],
    "summary": "A dataset is confounded if it is most easily solved via a spurious\ncorrelation which fails to generalize to new data. We will show that, in a\ncontinual learning setting where confounders may vary in time across tasks, the\nresulting challenge far exceeds the standard forgetting problem normally\nconsidered. In particular, we derive mathematically the effect of such\nconfounders on the space of valid joint solutions to sets of confounded tasks.\nInterestingly, our theory predicts that for many such continual datasets,\nspurious correlations are easily ignored when the tasks are trained on jointly,\nbut it is far harder to avoid confounding when they are considered\nsequentially. We construct such a dataset and demonstrate empirically that\nstandard continual learning methods fail to ignore confounders, while training\njointly on all tasks is successful. Our continually confounded dataset, ConCon,\nis based on CLEVR images and demonstrates the need for continual learning\nmethods with more robust behavior with respect to confounding.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06434v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06434v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06434v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06414v1",
    "updated": "2024-02-09T14:00:16+00:00",
    "published": "2024-02-09T14:00:16+00:00",
    "title": "Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions",
    "authors": [
      {
        "name": "Bianca-Mihaela Ganescu"
      },
      {
        "name": "Jonathan Passerat-Palmbach"
      }
    ],
    "summary": "Generative AI, exemplified by models like transformers, has opened up new\npossibilities in various domains but also raised concerns about fairness,\ntransparency and reliability, especially in fields like medicine and law. This\npaper emphasizes the urgency of ensuring fairness and quality in these domains\nthrough generative AI. It explores using cryptographic techniques, particularly\nZero-Knowledge Proofs (ZKPs), to address concerns regarding performance\nfairness and accuracy while protecting model privacy. Applying ZKPs to Machine\nLearning models, known as ZKML (Zero-Knowledge Machine Learning), enables\nindependent validation of AI-generated content without revealing sensitive\nmodel information, promoting transparency and trust. ZKML enhances AI fairness\nby providing cryptographic audit trails for model predictions and ensuring\nuniform performance across users. We introduce snarkGPT, a practical ZKML\nimplementation for transformers, to empower users to verify output accuracy and\nquality while preserving model privacy. We present a series of empirical\nresults studying snarkGPT's scalability and performance to assess the\nfeasibility and challenges of adopting a ZKML-powered approach to capture\nquality and performance fairness problems in generative AI models.",
    "comment": "Accepted at PPAI-24: The 5th AAAI Workshop on Privacy-Preserving\n  Artificial Intelligence 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06414v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06414v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06414v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06412v1",
    "updated": "2024-02-09T13:58:33+00:00",
    "published": "2024-02-09T13:58:33+00:00",
    "title": "Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity",
    "authors": [
      {
        "name": "Kaja Gruntkowska"
      },
      {
        "name": "Alexander Tyurin"
      },
      {
        "name": "Peter Richt\u00e1rik"
      }
    ],
    "summary": "Effective communication between the server and workers plays a key role in\ndistributed optimization. In this paper, we focus on optimizing the\nserver-to-worker communication, uncovering inefficiencies in prevalent downlink\ncompression approaches. Considering first the pure setup where the uplink\ncommunication costs are negligible, we introduce MARINA-P, a novel method for\ndownlink compression, employing a collection of correlated compressors.\nTheoretical analyses demonstrates that MARINA-P with permutation compressors\ncan achieve a server-to-worker communication complexity improving with the\nnumber of workers, thus being provably superior to existing algorithms. We\nfurther show that MARINA-P can serve as a starting point for extensions such as\nmethods supporting bidirectional compression. We introduce M3, a method\ncombining MARINA-P with uplink compression and a momentum step, achieving\nbidirectional compression with provable improvements in total communication\ncomplexity as the number of workers increases. Theoretical findings align\nclosely with empirical experiments, underscoring the efficiency of the proposed\nalgorithms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06412v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06412v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06412v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06402v1",
    "updated": "2024-02-09T13:40:11+00:00",
    "published": "2024-02-09T13:40:11+00:00",
    "title": "Hierarchical Transformers are Efficient Meta-Reinforcement Learners",
    "authors": [
      {
        "name": "Gresa Shala"
      },
      {
        "name": "Andr\u00e9 Biedenkapp"
      },
      {
        "name": "Josif Grabocka"
      }
    ],
    "summary": "We introduce Hierarchical Transformers for Meta-Reinforcement Learning\n(HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims\nto address the challenge of enabling reinforcement learning agents to perform\neffectively in previously unseen tasks. We demonstrate how past episodes serve\nas a rich source of information, which our model effectively distills and\napplies to new contexts. Our learned algorithm is capable of outperforming the\nprevious state-of-the-art and provides more efficient meta-training while\nsignificantly improving generalization capabilities. Experimental results,\nobtained across various simulated tasks of the Meta-World Benchmark, indicate a\nsignificant improvement in learning efficiency and adaptability compared to the\nstate-of-the-art on a variety of tasks. Our approach not only enhances the\nagent's ability to generalize from limited data but also paves the way for more\nrobust and versatile AI systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06402v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06402v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06402v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06397v1",
    "updated": "2024-02-09T13:29:44+00:00",
    "published": "2024-02-09T13:29:44+00:00",
    "title": "Finding hardness reductions automatically using SAT solvers",
    "authors": [
      {
        "name": "Helena Bergold"
      },
      {
        "name": "Manfred Scheucher"
      },
      {
        "name": "Felix Schr\u00f6der"
      }
    ],
    "summary": "In this article, we show that the completion problem, i.e. the decision\nproblem whether a partial structure can be completed to a full structure, is\nNP-complete for many combinatorial structures. While the gadgets for most\nreductions in literature are found by hand, we present an algorithm to\nconstruct gadgets in a fully automated way. Using our framework which is based\non SAT, we present the first thorough study of the completion problem on sign\nmappings with forbidden substructures by classifying thousands of structures\nfor which the completion problem is NP-complete. Our list in particular\nincludes interior triple systems, which were introduced by Knuth towards an\naxiomatization of planar point configurations. Last but not least, we give an\ninfinite family of structures generalizing interior triple system to higher\ndimensions for which the completion problem is NP-complete.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CC",
    "categories": [
      "cs.CC",
      "cs.AI",
      "cs.DS",
      "cs.LO",
      "math.CO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06397v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06397v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06397v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06389v1",
    "updated": "2024-02-09T13:11:19+00:00",
    "published": "2024-02-09T13:11:19+00:00",
    "title": "Human Aesthetic Preference-Based Large Text-to-Image Model Personalization: Kandinsky Generation as an Example",
    "authors": [
      {
        "name": "Aven-Le Zhou"
      },
      {
        "name": "Yu-Ao Wang"
      },
      {
        "name": "Wei Wu"
      },
      {
        "name": "Kang Zhang"
      }
    ],
    "summary": "With the advancement of neural generative capabilities, the art community has\nactively embraced GenAI (generative artificial intelligence) for creating\npainterly content. Large text-to-image models can quickly generate\naesthetically pleasing outcomes. However, the process can be non-deterministic\nand often involves tedious trial-and-error, as users struggle with formulating\neffective prompts to achieve their desired results. This paper introduces a\nprompting-free generative approach that empowers users to automatically\ngenerate personalized painterly content that incorporates their aesthetic\npreferences in a customized artistic style. This approach involves utilizing\n``semantic injection'' to customize an artist model in a specific artistic\nstyle, and further leveraging a genetic algorithm to optimize the prompt\ngeneration process through real-time iterative human feedback. By solely\nrelying on the user's aesthetic evaluation and preference for the artist\nmodel-generated images, this approach creates the user a personalized model\nthat encompasses their aesthetic preferences and the customized artistic style.",
    "comment": "9 pages, 10 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06389v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06389v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06389v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06388v1",
    "updated": "2024-02-09T13:10:04+00:00",
    "published": "2024-02-09T13:10:04+00:00",
    "title": "On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit",
    "authors": [
      {
        "name": "Stefana Anita"
      },
      {
        "name": "Gabriel Turinici"
      }
    ],
    "summary": "We present a self-contained proof of the convergence rate of the Stochastic\nGradient Descent (SGD) when the learning rate follows an inverse time decays\nschedule; we next apply the results to the convergence of a modified form of\npolicy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.DS",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06388v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06388v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06388v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06386v1",
    "updated": "2024-02-09T13:08:21+00:00",
    "published": "2024-02-09T13:08:21+00:00",
    "title": "Boosting-Based Sequential Meta-Tree Ensemble Construction for Improved Decision Trees",
    "authors": [
      {
        "name": "Ryota Maniwa"
      },
      {
        "name": "Naoki Ichijo"
      },
      {
        "name": "Yuta Nakahara"
      },
      {
        "name": "Toshiyasu Matsushima"
      }
    ],
    "summary": "A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06386v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06386v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06386v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06380v1",
    "updated": "2024-02-09T12:58:36+00:00",
    "published": "2024-02-09T12:58:36+00:00",
    "title": "Optimal estimation of Gaussian (poly)trees",
    "authors": [
      {
        "name": "Yuhao Wang"
      },
      {
        "name": "Ming Gao"
      },
      {
        "name": "Wai Ming Tai"
      },
      {
        "name": "Bryon Aragam"
      },
      {
        "name": "Arnab Bhattacharyya"
      }
    ],
    "summary": "We develop optimal algorithms for learning undirected Gaussian trees and\ndirected Gaussian polytrees from data. We consider both problems of\ndistribution learning (i.e. in KL distance) and structure learning (i.e. exact\nrecovery). The first approach is based on the Chow-Liu algorithm, and learns an\noptimal tree-structured distribution efficiently. The second approach is a\nmodification of the PC algorithm for polytrees that uses partial correlation as\na conditional independence tester for constraint-based structure learning. We\nderive explicit finite-sample guarantees for both approaches, and show that\nboth approaches are optimal by deriving matching lower bounds. Additionally, we\nconduct numerical experiments to compare the performance of various algorithms,\nproviding further insights and empirical evidence.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06380v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06380v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06380v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06377v1",
    "updated": "2024-02-09T12:54:34+00:00",
    "published": "2024-02-09T12:54:34+00:00",
    "title": "High-Precision Geosteering via Reinforcement Learning and Particle Filters",
    "authors": [
      {
        "name": "Ressi Bonti Muhammad"
      },
      {
        "name": "Apoorv Srivastava"
      },
      {
        "name": "Sergey Alyaev"
      },
      {
        "name": "Reidar Brumer Bratvold"
      },
      {
        "name": "Daniel M. Tartakovsky"
      }
    ],
    "summary": "Geosteering, a key component of drilling operations, traditionally involves\nmanual interpretation of various data sources such as well-log data. This\nintroduces subjective biases and inconsistent procedures. Academic attempts to\nsolve geosteering decision optimization with greedy optimization and\nApproximate Dynamic Programming (ADP) showed promise but lacked adaptivity to\nrealistic diverse scenarios. Reinforcement learning (RL) offers a solution to\nthese challenges, facilitating optimal decision-making through reward-based\niterative learning. State estimation methods, e.g., particle filter (PF),\nprovide a complementary strategy for geosteering decision-making based on\nonline information. We integrate an RL-based geosteering with PF to address\nrealistic geosteering scenarios. Our framework deploys PF to process real-time\nwell-log data to estimate the location of the well relative to the\nstratigraphic layers, which then informs the RL-based decision-making process.\nWe compare our method's performance with that of using solely either RL or PF.\nOur findings indicate a synergy between RL and PF in yielding optimized\ngeosteering decisions.",
    "comment": "40 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06377v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06377v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06377v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06367v1",
    "updated": "2024-02-09T12:19:06+00:00",
    "published": "2024-02-09T12:19:06+00:00",
    "title": "TEE4EHR: Transformer Event Encoder for Better Representation Learning in Electronic Health Records",
    "authors": [
      {
        "name": "Hojjat Karami"
      },
      {
        "name": "David Atienza"
      },
      {
        "name": "Anisoara Ionescu"
      }
    ],
    "summary": "Irregular sampling of time series in electronic health records (EHRs) is one\nof the main challenges for developing machine learning models. Additionally,\nthe pattern of missing data in certain clinical variables is not at random but\ndepends on the decisions of clinicians and the state of the patient. Point\nprocess is a mathematical framework for analyzing event sequence data that is\nconsistent with irregular sampling patterns. Our model, TEE4EHR, is a\ntransformer event encoder (TEE) with point process loss that encodes the\npattern of laboratory tests in EHRs. The utility of our TEE has been\ninvestigated in a variety of benchmark event sequence datasets. Additionally,\nwe conduct experiments on two real-world EHR databases to provide a more\ncomprehensive evaluation of our model. Firstly, in a self-supervised learning\napproach, the TEE is jointly learned with an existing attention-based deep\nneural network which gives superior performance in negative log-likelihood and\nfuture event prediction. Besides, we propose an algorithm for aggregating\nattention weights that can reveal the interaction between the events. Secondly,\nwe transfer and freeze the learned TEE to the downstream task for the outcome\nprediction, where it outperforms state-of-the-art models for handling\nirregularly sampled time series. Furthermore, our results demonstrate that our\napproach can improve representation learning in EHRs and can be useful for\nclinical prediction tasks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06367v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06367v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06367v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06360v1",
    "updated": "2024-02-09T12:10:00+00:00",
    "published": "2024-02-09T12:10:00+00:00",
    "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models",
    "authors": [
      {
        "name": "Peiyuan Gong"
      },
      {
        "name": "Jiamian Li"
      },
      {
        "name": "Jiaxin Mao"
      }
    ],
    "summary": "Collaborative search supports multiple users working together to accomplish a\nspecific search task. Research has found that designing lightweight\ncollaborative search plugins within instant messaging platforms aligns better\nwith users' collaborative habits. However, due to the complexity of multi-user\ninteraction scenarios, it is challenging to implement a fully functioning\nlightweight collaborative search system. Therefore, previous studies on\nlightweight collaborative search had to rely on the Wizard of Oz paradigm. In\nrecent years, large language models (LLMs) have been demonstrated to interact\nnaturally with users and achieve complex information-seeking tasks through\nLLM-based agents. Hence, to better support the research in collaborative\nsearch, in this demo, we propose CoSearchAgent, a lightweight collaborative\nsearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that\ncan support collaborative search during multi-party conversations on this\nplatform. Equipped with the capacity to understand the queries and context in\nmulti-user conversations and the ability to search the Web for relevant\ninformation via APIs, CoSearchAgent can respond to user queries with answers\ngrounded on the relevant search results. It can also ask clarifying questions\nwhen the information needs are unclear. The proposed CoSearchAgent is highly\nflexible and would be useful for supporting further research on collaborative\nsearch. The code and demo video are accessible.",
    "comment": "4 pages, demo",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06360v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06360v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06360v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06359v1",
    "updated": "2024-02-09T12:08:49+00:00",
    "published": "2024-02-09T12:08:49+00:00",
    "title": "Modelling Human Values for AI Reasoning",
    "authors": [
      {
        "name": "Nardine Osman"
      },
      {
        "name": "Mark d'Inverno"
      }
    ],
    "summary": "One of today's most significant societal challenges is building AI systems\nwhose behaviour, or the behaviour it enables within communities of interacting\nagents (human and artificial), aligns with human values. To address this\nchallenge, we detail a formal model of human values for their explicit\ncomputational representation. To our knowledge, this has not been attempted as\nyet, which is surprising given the growing volume of research integrating\nvalues within AI. Taking as our starting point the wealth of research\ninvestigating the nature of human values from social psychology over the last\nfew decades, we set out to provide such a formal model. We show how this model\ncan provide the foundational apparatus for AI-based reasoning over values, and\ndemonstrate its applicability in real-world use cases. We illustrate how our\nmodel captures the key ideas from social psychology research and propose a\nroadmap for future integrated, and interdisciplinary, research into human\nvalues in AI. The ability to automatically reason over values not only helps\naddress the value alignment problem but also facilitates the design of AI\nsystems that can support individuals and communities in making more informed,\nvalue-aligned decisions. More and more, individuals and organisations are\nmotivated to understand their values more explicitly and explore whether their\nbehaviours and attitudes properly reflect them. Our work on modelling human\nvalues will enable AI systems to be designed and deployed to meet this growing\nneed.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.MA",
      "68T01",
      "I.2.4"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06359v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06359v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06359v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06357v1",
    "updated": "2024-02-09T12:07:06+00:00",
    "published": "2024-02-09T12:07:06+00:00",
    "title": "The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks",
    "authors": [
      {
        "name": "Jona te Lintelo"
      },
      {
        "name": "Stefanos Koffas"
      },
      {
        "name": "Stjepan Picek"
      }
    ],
    "summary": "Sponge attacks aim to increase the energy consumption and computation time of\nneural networks deployed on hardware accelerators. Existing sponge attacks can\nbe performed during inference via sponge examples or during training via Sponge\nPoisoning. Sponge examples leverage perturbations added to the model's input to\nincrease energy and latency, while Sponge Poisoning alters the objective\nfunction of a model to induce inference-time energy/latency effects.\n  In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is\nthe first sponge attack that is performed directly on the parameters of a\npre-trained model. Our experiments show that SpongeNet can successfully\nincrease the energy consumption of vision models with fewer samples required\nthan Sponge Poisoning. Our experiments indicate that poisoning defenses are\nineffective if not adjusted specifically for the defense against Sponge\nPoisoning (i.e., they decrease batch normalization bias values). Our work shows\nthat SpongeNet is more effective on StarGAN than the state-of-the-art.\nAdditionally, SpongeNet is stealthier than the previous Sponge Poisoning attack\nas it does not require significant changes in the victim model's weights. Our\nexperiments indicate that the SpongeNet attack can be performed even when an\nattacker has access to only 1% of the entire dataset and reach up to 11% energy\nincrease.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06357v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06357v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06357v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06348v1",
    "updated": "2024-02-09T11:53:27+00:00",
    "published": "2024-02-09T11:53:27+00:00",
    "title": "Fairness of Exposure in Online Restless Multi-armed Bandits",
    "authors": [
      {
        "name": "Archit Sood"
      },
      {
        "name": "Shweta Jain"
      },
      {
        "name": "Sujit Gujar"
      }
    ],
    "summary": "Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where\neach arm exhibits Markovian behavior and transitions according to their\ntransition dynamics. Solutions to RMAB exist for both offline and online cases.\nHowever, they do not consider the distribution of pulls among the arms. Studies\nhave shown that optimal policies lead to unfairness, where some arms are not\nexposed enough. Existing works in fairness in RMABs focus heavily on the\noffline case, which diminishes their application in real-world scenarios where\nthe environment is largely unknown. In the online scenario, we propose the\nfirst fair RMAB framework, where each arm receives pulls in proportion to its\nmerit. We define the merit of an arm as a function of its stationary reward\ndistribution. We prove that our algorithm achieves sublinear fairness regret in\nthe single pull case $O(\\sqrt{T\\ln T})$, with $T$ being the total number of\nepisodes. Empirically, we show that our algorithm performs well in the\nmulti-pull scenario as well.",
    "comment": "Accepted as extended abstract in AAMAS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06348v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06348v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06348v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06334v1",
    "updated": "2024-02-09T11:23:14+00:00",
    "published": "2024-02-09T11:23:14+00:00",
    "title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs",
    "authors": [
      {
        "name": "Fernando Ferraretto"
      },
      {
        "name": "Thiago Laitz"
      },
      {
        "name": "Roberto Lotufo"
      },
      {
        "name": "Rodrigo Nogueira"
      }
    ],
    "summary": "ExaRanker recently introduced an approach to training information retrieval\n(IR) models, incorporating natural language explanations as additional labels.\nThe method addresses the challenge of limited labeled examples, leading to\nimprovements in the effectiveness of IR models. However, the initial results\nwere based on proprietary language models such as GPT-3.5, which posed\nconstraints on dataset size due to its cost and data privacy. In this paper, we\nintroduce ExaRanker-Open, where we adapt and explore the use of open-source\nlanguage models to generate explanations. The method has been tested using\ndifferent LLMs and datasets sizes to better comprehend the effective\ncontribution of data augmentation. Our findings reveal that incorporating\nexplanations consistently enhances neural rankers, with benefits escalating as\nthe LLM size increases. Notably, the data augmentation method proves\nadvantageous even with large datasets, as evidenced by ExaRanker surpassing the\ntarget baseline by 0.6 nDCG@10 points in our study. To encourage further\nadvancements by the research community, we have open-sourced both the code and\ndatasets at https://github.com/unicamp-dl/ExaRanker.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06334v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06334v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06334v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06331v1",
    "updated": "2024-02-09T11:15:49+00:00",
    "published": "2024-02-09T11:15:49+00:00",
    "title": "Taking Class Imbalance Into Account in Open Set Recognition Evaluation",
    "authors": [
      {
        "name": "Joanna Komorniczak"
      },
      {
        "name": "Pawel Ksieniewicz"
      }
    ],
    "summary": "In recent years Deep Neural Network-based systems are not only increasing in\npopularity but also receive growing user trust. However, due to the\nclosed-world assumption of such systems, they cannot recognize samples from\nunknown classes and often induce an incorrect label with high confidence.\nPresented work looks at the evaluation of methods for Open Set Recognition,\nfocusing on the impact of class imbalance, especially in the dichotomy between\nknown and unknown samples. As an outcome of problem analysis, we present a set\nof guidelines for evaluation of methods in this field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06331v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06331v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06331v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06330v1",
    "updated": "2024-02-09T11:09:52+00:00",
    "published": "2024-02-09T11:09:52+00:00",
    "title": "Continual Learning on Graphs: A Survey",
    "authors": [
      {
        "name": "Zonggui Tian"
      },
      {
        "name": "Du Zhang"
      },
      {
        "name": "Hong-Ning Dai"
      }
    ],
    "summary": "Recently, continual graph learning has been increasingly adopted for diverse\ngraph-structured data processing tasks in non-stationary environments. Despite\nits promising learning capability, current studies on continual graph learning\nmainly focus on mitigating the catastrophic forgetting problem while ignoring\ncontinuous performance improvement. To bridge this gap, this article aims to\nprovide a comprehensive survey of recent efforts on continual graph learning.\nSpecifically, we introduce a new taxonomy of continual graph learning from the\nperspective of overcoming catastrophic forgetting. Moreover, we systematically\nanalyze the challenges of applying these continual graph learning methods in\nimproving performance continuously and then discuss the possible solutions.\nFinally, we present open issues and future directions pertaining to the\ndevelopment of continual graph learning and discuss how they impact continuous\nperformance improvement.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06330v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06330v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06330v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06326v1",
    "updated": "2024-02-09T11:06:20+00:00",
    "published": "2024-02-09T11:06:20+00:00",
    "title": "Prompt Learning on Temporal Interaction Graphs",
    "authors": [
      {
        "name": "Xi Chen"
      },
      {
        "name": "Siwei Zhang"
      },
      {
        "name": "Yun Xiong"
      },
      {
        "name": "Xixi Wu"
      },
      {
        "name": "Jiawei Zhang"
      },
      {
        "name": "Xiangguo Sun"
      },
      {
        "name": "Yao Zhang"
      },
      {
        "name": "Yinglong Zhao"
      },
      {
        "name": "Yulin Kang"
      }
    ],
    "summary": "Temporal Interaction Graphs (TIGs) are widely utilized to represent\nreal-world systems. To facilitate representation learning on TIGs, researchers\nhave proposed a series of TIG models. However, these models are still facing\ntwo tough gaps between the pre-training and downstream predictions in their\n``pre-train, predict'' training paradigm. First, the temporal discrepancy\nbetween the pre-training and inference data severely undermines the models'\napplicability in distant future predictions on the dynamically evolving data.\nSecond, the semantic divergence between pretext and downstream tasks hinders\ntheir practical applications, as they struggle to align with their learning and\nprediction capabilities across application scenarios.\n  Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight\nmechanism for model generalization. Applying this paradigm is a potential\nsolution to solve the aforementioned challenges. However, the adaptation of\nthis paradigm to TIGs is not straightforward. The application of prompting in\nstatic graph contexts falls short in temporal settings due to a lack of\nconsideration for time-sensitive dynamics and a deficiency in expressive power.\nTo address this issue, we introduce Temporal Interaction Graph Prompting\n(TIGPrompt), a versatile framework that seamlessly integrates with TIG models,\nbridging both the temporal and semantic gaps. In detail, we propose a temporal\nprompt generator to offer temporally-aware prompts for different tasks. These\nprompts stand out for their minimalistic design, relying solely on the tuning\nof the prompt generator with very little supervision data. To cater to varying\ncomputational resource demands, we propose an extended ``pre-train,\nprompt-based fine-tune'' paradigm, offering greater flexibility. Through\nextensive experiments, the TIGPrompt demonstrates the SOTA performance and\nremarkable efficiency advantages.",
    "comment": "11 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06326v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06326v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06326v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06323v1",
    "updated": "2024-02-09T11:03:52+00:00",
    "published": "2024-02-09T11:03:52+00:00",
    "title": "How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers",
    "authors": [
      {
        "name": "Gon Buzaglo"
      },
      {
        "name": "Itamar Harel"
      },
      {
        "name": "Mor Shpigel Nacson"
      },
      {
        "name": "Alon Brutzkus"
      },
      {
        "name": "Nathan Srebro"
      },
      {
        "name": "Daniel Soudry"
      }
    ],
    "summary": "Background. A main theoretical puzzle is why over-parameterized Neural\nNetworks (NNs) generalize well when trained to zero loss (i.e., so they\ninterpolate the data). Usually, the NN is trained with Stochastic Gradient\nDescent (SGD) or one of its variants. However, recent empirical work examined\nthe generalization of a random NN that interpolates the data: the NN was\nsampled from a seemingly uniform prior over the parameters, conditioned on that\nthe NN perfectly classifying the training set. Interestingly, such a NN sample\ntypically generalized as well as SGD-trained NNs.\n  Contributions. We prove that such a random NN interpolator typically\ngeneralizes well if there exists an underlying narrow ``teacher NN\" that agrees\nwith the labels. Specifically, we show that such a `flat' prior over the NN\nparametrization induces a rich prior over the NN functions, due to the\nredundancy in the NN structure. In particular, this creates a bias towards\nsimpler functions, which require less relevant parameters to represent --\nenabling learning with a sample complexity approximately proportional to the\ncomplexity of the teacher (roughly, the number of non-redundant parameters),\nrather than the student's.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06323v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06323v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06323v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06320v1",
    "updated": "2024-02-09T11:01:35+00:00",
    "published": "2024-02-09T11:01:35+00:00",
    "title": "Particle Denoising Diffusion Sampler",
    "authors": [
      {
        "name": "Angus Phillips"
      },
      {
        "name": "Hai-Dang Dau"
      },
      {
        "name": "Michael John Hutchinson"
      },
      {
        "name": "Valentin De Bortoli"
      },
      {
        "name": "George Deligiannidis"
      },
      {
        "name": "Arnaud Doucet"
      }
    ],
    "summary": "Denoising diffusion models have become ubiquitous for generative modeling.\nThe core idea is to transport the data distribution to a Gaussian by using a\ndiffusion. Approximate samples from the data distribution are then obtained by\nestimating the time-reversal of this diffusion using score matching ideas. We\nfollow here a similar strategy to sample from unnormalized probability\ndensities and compute their normalizing constants. However, the time-reversed\ndiffusion is here simulated by using an original iterative particle scheme\nrelying on a novel score matching loss. Contrary to standard denoising\ndiffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS)\nprovides asymptotically consistent estimates under mild assumptions. We\ndemonstrate PDDS on multimodal and high dimensional sampling tasks.",
    "comment": "30 pages, 12 figures, 3 tables, 4 algorithms",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06320v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06320v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06320v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06318v1",
    "updated": "2024-02-09T10:57:14+00:00",
    "published": "2024-02-09T10:57:14+00:00",
    "title": "TimEHR: Image-based Time Series Generation for Electronic Health Records",
    "authors": [
      {
        "name": "Hojjat Karami"
      },
      {
        "name": "Mary-Anne Hartley"
      },
      {
        "name": "David Atienza"
      },
      {
        "name": "Anisoara Ionescu"
      }
    ],
    "summary": "Time series in Electronic Health Records (EHRs) present unique challenges for\ngenerative models, such as irregular sampling, missing values, and high\ndimensionality. In this paper, we propose a novel generative adversarial\nnetwork (GAN) model, TimEHR, to generate time series data from EHRs. In\nparticular, TimEHR treats time series as images and is based on two conditional\nGANs. The first GAN generates missingness patterns, and the second GAN\ngenerates time series values based on the missingness pattern. Experimental\nresults on three real-world EHR datasets show that TimEHR outperforms\nstate-of-the-art methods in terms of fidelity, utility, and privacy metrics.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06318v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06318v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06318v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06707v1",
    "updated": "2024-02-09T10:51:09+00:00",
    "published": "2024-02-09T10:51:09+00:00",
    "title": "Multi-class real-time crash risk forecasting using convolutional neural network: Istanbul case study",
    "authors": [
      {
        "name": "Behnaz Alafi"
      },
      {
        "name": "Saeid Moradi"
      }
    ],
    "summary": "The performance of an artificial neural network (ANN) in forecasting crash\nrisk is shown in this paper. To begin, some traffic and weather data are\nacquired as raw data. This data is then analyzed, and relevant characteristics\nare chosen to utilize as input data based on additional tree and Pearson\ncorrelation. Furthermore, crash and non-crash time data are separated; then,\nfeature values for crash and non-crash events are written in three four-minute\nintervals prior to the crash and non-crash events using the average of all\navailable values for that period. The number of non-crash samples was lowered\nafter calculating crash likelihood for each period based on accident labeling.\nThe proposed CNN model is capable of learning from recorded, processed, and\ncategorized input characteristics such as traffic characteristics and\nmeteorological conditions. The goal of this work is to forecast the chance of a\nreal-time crash based on three periods before events. The area under the curve\n(AUC) for the receiver operating characteristic curve (ROC curve), as well as\nsensitivity as the true positive rate and specificity as the false positive\nrate, are shown and compared with three typical machine learning and neural\nnetwork models. Finally, when it comes to the error value, AUC, sensitivity,\nand specificity parameters as performance variables, the executed model\noutperforms other models. The findings of this research suggest applying the\nCNN model as a multi-class prediction model for real-time crash risk\nprediction. Our emphasis is on multi-class prediction, while prior research\nused this for binary (two-class) categorization like crash and non-crash.",
    "comment": "17 pages, 16 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "I.2.6; K.3.2; I.2.m"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06707v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06707v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06707v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06706v1",
    "updated": "2024-02-09T10:50:45+00:00",
    "published": "2024-02-09T10:50:45+00:00",
    "title": "CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs",
    "authors": [
      {
        "name": "Florian Gr\u00f6tschla"
      },
      {
        "name": "Jo\u00ebl Mathys"
      },
      {
        "name": "Robert Veres"
      },
      {
        "name": "Roger Wattenhofer"
      }
    ],
    "summary": "Graph Visualization, also known as Graph Drawing, aims to find geometric\nembeddings of graphs that optimize certain criteria. Stress is a widely used\nmetric; stress is minimized when every pair of nodes is positioned at their\nshortest path distance. However, stress optimization presents computational\nchallenges due to its inherent complexity and is usually solved using\nheuristics in practice. We introduce a scalable Graph Neural Network (GNN)\nbased Graph Drawing framework with sub-quadratic runtime that can learn to\noptimize stress. Inspired by classical stress optimization techniques and\nforce-directed layout algorithms, we create a coarsening hierarchy for the\ninput graph. Beginning at the coarsest level, we iteratively refine and\nun-coarsen the layout, until we generate an embedding for the original graph.\nTo enhance information propagation within the network, we propose a novel\npositional rewiring technique based on intermediate node positions. Our\nempirical evaluation demonstrates that the framework achieves state-of-the-art\nperformance while remaining scalable.",
    "comment": "Published as a conference paper at ICLR 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CG",
    "categories": [
      "cs.CG",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06706v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06706v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06706v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06304v1",
    "updated": "2024-02-09T10:34:01+00:00",
    "published": "2024-02-09T10:34:01+00:00",
    "title": "A New Approach to Voice Authenticity",
    "authors": [
      {
        "name": "Nicolas M. M\u00fcller"
      },
      {
        "name": "Piotr Kawa"
      },
      {
        "name": "Shen Hu"
      },
      {
        "name": "Matthias Neu"
      },
      {
        "name": "Jennifer Williams"
      },
      {
        "name": "Philip Sperl"
      },
      {
        "name": "Konstantin B\u00f6ttinger"
      }
    ],
    "summary": "Voice faking, driven primarily by recent advances in text-to-speech (TTS)\nsynthesis technology, poses significant societal challenges. Currently, the\nprevailing assumption is that unaltered human speech can be considered genuine,\nwhile fake speech comes from TTS synthesis. We argue that this binary\ndistinction is oversimplified. For instance, altered playback speeds can be\nused for malicious purposes, like in the 'Drunken Nancy Pelosi' incident.\nSimilarly, editing of audio clips can be done ethically, e.g., for brevity or\nsummarization in news reporting or podcasts, but editing can also create\nmisleading narratives. In this paper, we propose a conceptual shift away from\nthe binary paradigm of audio being either 'fake' or 'real'. Instead, our focus\nis on pinpointing 'voice edits', which encompass traditional modifications like\nfilters and cuts, as well as TTS synthesis and VC systems. We delineate 6\ncategories and curate a new challenge dataset rooted in the M-AILABS corpus,\nfor which we present baseline detection systems. And most importantly, we argue\nthat merely categorizing audio as fake or real is a dangerous\nover-simplification that will fail to move the field of speech technology\nforward.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06304v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06304v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06304v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06299v1",
    "updated": "2024-02-09T10:24:47+00:00",
    "published": "2024-02-09T10:24:47+00:00",
    "title": "A Functional Analysis Approach to Symbolic Regression",
    "authors": [
      {
        "name": "Kirill Antonov"
      },
      {
        "name": "Roman Kalkreuth"
      },
      {
        "name": "Kaifeng Yang"
      },
      {
        "name": "Thomas B\u00e4ck"
      },
      {
        "name": "Niki van Stein"
      },
      {
        "name": "Anna V Kononova"
      }
    ],
    "summary": "Symbolic regression (SR) poses a significant challenge for randomized search\nheuristics due to its reliance on the synthesis of expressions for input-output\nmappings. Although traditional genetic programming (GP) algorithms have\nachieved success in various domains, they exhibit limited performance when\ntree-based representations are used for SR. To address these limitations, we\nintroduce a novel SR approach called Fourier Tree Growing (FTG) that draws\ninsights from functional analysis. This new perspective enables us to perform\noptimization directly in a different space, thus avoiding intricate symbolic\nexpressions. Our proposed algorithm exhibits significant performance\nimprovements over traditional GP methods on a range of classical\none-dimensional benchmarking problems. To identify and explain limiting factors\nof GP and FTG, we perform experiments on a large-scale polynomials benchmark\nwith high-order polynomials up to degree 100. To the best of the authors'\nknowledge, this work represents the pioneering application of functional\nanalysis in addressing SR problems. The superior performance of the proposed\nalgorithm and insights into the limitations of GP open the way for further\nadvancing GP for SR and related areas of explainable machine learning.",
    "comment": "14 pages, 3 figures. Submitted to Genetic and Evolutionary\n  Computation Conference (GECCO-2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.NE",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06299v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06299v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06299v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06295v1",
    "updated": "2024-02-09T10:16:58+00:00",
    "published": "2024-02-09T10:16:58+00:00",
    "title": "Multimodal Interpretable Data-Driven Models for Early Prediction of Antimicrobial Multidrug Resistance Using Multivariate Time-Series",
    "authors": [
      {
        "name": "Sergio Mart\u00ednez-Ag\u00fcero"
      },
      {
        "name": "Antonio G. Marques"
      },
      {
        "name": "Inmaculada Mora-Jim\u00e9nez"
      },
      {
        "name": "Joaqu\u00edn Alv\u00e1rez-Rodr\u00edguez"
      },
      {
        "name": "Cristina Soguero-Ruiza"
      }
    ],
    "summary": "Electronic health records (EHR) is an inherently multimodal register of the\npatient's health status characterized by static data and multivariate time\nseries (MTS). While MTS are a valuable tool for clinical prediction, their\nfusion with other data modalities can possibly result in more thorough insights\nand more accurate results. Deep neural networks (DNNs) have emerged as\nfundamental tools for identifying and defining underlying patterns in the\nhealthcare domain. However, fundamental improvements in interpretability are\nneeded for DNN models to be widely used in the clinical setting. In this study,\nwe present an approach built on a collection of interpretable multimodal\ndata-driven models that may anticipate and understand the emergence of\nantimicrobial multidrug resistance (AMR) germs in the intensive care unit (ICU)\nof the University Hospital of Fuenlabrada (Madrid, Spain). The profile and\ninitial health status of the patient are modeled using static variables, while\nthe evolution of the patient's health status during the ICU stay is modeled\nusing several MTS, including mechanical ventilation and antibiotics intake. The\nmultimodal DNNs models proposed in this paper include interpretable principles\nin addition to being effective at predicting AMR and providing an explainable\nprediction support system for AMR in the ICU. Furthermore, our proposed\nmethodology based on multimodal models and interpretability schemes can be\nleveraged in additional clinical problems dealing with EHR data, broadening the\nimpact and applicability of our results.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06295v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06295v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06295v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06293v1",
    "updated": "2024-02-09T10:14:18+00:00",
    "published": "2024-02-09T10:14:18+00:00",
    "title": "Probabilistic Forecasting of Irregular Time Series via Conditional Flows",
    "authors": [
      {
        "name": "Vijaya Krishna Yalavarthi"
      },
      {
        "name": "Randolf Scholz"
      },
      {
        "name": "Stefan Born"
      },
      {
        "name": "Lars Schmidt-Thieme"
      }
    ],
    "summary": "Probabilistic forecasting of irregularly sampled multivariate time series\nwith missing values is an important problem in many fields, including health\ncare, astronomy, and climate. State-of-the-art methods for the task estimate\nonly marginal distributions of observations in single channels and at single\ntimepoints, assuming a fixed-shape parametric distribution. In this work, we\npropose a novel model, ProFITi, for probabilistic forecasting of irregularly\nsampled time series with missing values using conditional normalizing flows.\nThe model learns joint distributions over the future values of the time series\nconditioned on past observations and queried channels and times, without\nassuming any fixed shape of the underlying distribution. As model components,\nwe introduce a novel invertible triangular attention layer and an invertible\nnon-linear activation function on and onto the whole real line. We conduct\nextensive experiments on four datasets and demonstrate that the proposed model\nprovides $4$ times higher likelihood over the previously best model.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06293v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06293v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06293v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06289v1",
    "updated": "2024-02-09T09:58:35+00:00",
    "published": "2024-02-09T09:58:35+00:00",
    "title": "Evaluating Membership Inference Attacks and Defenses in Federated Learning",
    "authors": [
      {
        "name": "Gongxi Zhu"
      },
      {
        "name": "Donghao Li"
      },
      {
        "name": "Hanlin Gu"
      },
      {
        "name": "Yuxing Han"
      },
      {
        "name": "Yuan Yao"
      },
      {
        "name": "Lixin Fan"
      },
      {
        "name": "Qiang Yang"
      }
    ],
    "summary": "Membership Inference Attacks (MIAs) pose a growing threat to privacy\npreservation in federated learning. The semi-honest attacker, e.g., the server,\nmay determine whether a particular sample belongs to a target client according\nto the observed model information. This paper conducts an evaluation of\nexisting MIAs and corresponding defense strategies. Our evaluation on MIAs\nreveals two important findings about the trend of MIAs. Firstly, combining\nmodel information from multiple communication rounds (Multi-temporal) enhances\nthe overall effectiveness of MIAs compared to utilizing model information from\na single epoch. Secondly, incorporating models from non-target clients\n(Multi-spatial) significantly improves the effectiveness of MIAs, particularly\nwhen the clients' data is homogeneous. This highlights the importance of\nconsidering the temporal and spatial model information in MIAs. Next, we assess\nthe effectiveness via privacy-utility tradeoff for two type defense mechanisms\nagainst MIAs: Gradient Perturbation and Data Replacement. Our results\ndemonstrate that Data Replacement mechanisms achieve a more optimal balance\nbetween preserving privacy and maintaining model utility. Therefore, we\nrecommend the adoption of Data Replacement methods as a defense strategy\nagainst MIAs. Our code is available in https://github.com/Liar-Mask/FedMIA.",
    "comment": "11 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06289v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06289v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06289v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06287v1",
    "updated": "2024-02-09T09:54:01+00:00",
    "published": "2024-02-09T09:54:01+00:00",
    "title": "AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems",
    "authors": [
      {
        "name": "Clara Punzi"
      },
      {
        "name": "Roberto Pellungrini"
      },
      {
        "name": "Mattia Setzu"
      },
      {
        "name": "Fosca Giannotti"
      },
      {
        "name": "Dino Pedreschi"
      }
    ],
    "summary": "Everyday we increasingly rely on machine learning models to automate and\nsupport high-stake tasks and decisions. This growing presence means that humans\nare now constantly interacting with machine learning-based systems, training\nand using models everyday. Several different techniques in computer science\nliterature account for the human interaction with machine learning systems, but\ntheir classification is sparse and the goals varied. This survey proposes a\ntaxonomy of Hybrid Decision Making Systems, providing both a conceptual and\ntechnical framework for understanding how current computer science literature\nmodels interaction between humans and machines.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06287v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06287v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06287v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06282v2",
    "updated": "2024-02-13T14:24:53+00:00",
    "published": "2024-02-09T09:48:38+00:00",
    "title": "Retrieve, Merge, Predict: Augmenting Tables with Data Lakes",
    "authors": [
      {
        "name": "Riccardo Cappuzzo"
      },
      {
        "name": "Gael Varoquaux"
      },
      {
        "name": "Aimee Coelho"
      },
      {
        "name": "Paolo Papotti"
      }
    ],
    "summary": "We present an in-depth analysis of data discovery in data lakes, focusing on\ntable augmentation for given machine learning tasks. We analyze alternative\nmethods used in the three main steps: retrieving joinable tables, merging\ninformation, and predicting with the resultant table. As data lakes, the paper\nuses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for\nbenchmarking this data discovery task -- and Open Data US, a well-referenced\nreal data lake. Through systematic exploration on both lakes, our study\noutlines the importance of accurately retrieving join candidates and the\nefficiency of simple merging methods. We report new insights on the benefits of\nexisting solutions and on their limitations, aiming at guiding future research\nin this space.",
    "comment": "12 pages + references, 11 figures. Under submission at VLDB2024 (EA&B\n  track)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DB",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06282v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06282v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06282v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06277v1",
    "updated": "2024-02-09T09:41:26+00:00",
    "published": "2024-02-09T09:41:26+00:00",
    "title": "Controllable seismic velocity synthesis using generative diffusion models",
    "authors": [
      {
        "name": "Fu Wang"
      },
      {
        "name": "Xinquan Huang"
      },
      {
        "name": "Tariq Alkhalifah"
      }
    ],
    "summary": "Accurate seismic velocity estimations are vital to understanding Earth's\nsubsurface structures, assessing natural resources, and evaluating seismic\nhazards. Machine learning-based inversion algorithms have shown promising\nperformance in regional (i.e., for exploration) and global velocity estimation,\nwhile their effectiveness hinges on access to large and diverse training\ndatasets whose distributions generally cover the target solutions.\nAdditionally, enhancing the precision and reliability of velocity estimation\nalso requires incorporating prior information, e.g., geological classes, well\nlogs, and subsurface structures, but current statistical or neural\nnetwork-based methods are not flexible enough to handle such multi-modal\ninformation. To address both challenges, we propose to use conditional\ngenerative diffusion models for seismic velocity synthesis, in which we readily\nincorporate those priors. This approach enables the generation of seismic\nvelocities that closely match the expected target distribution, offering\ndatasets informed by both expert knowledge and measured data to support\ntraining for data-driven geophysical methods. We demonstrate the flexibility\nand effectiveness of our method through training diffusion models on the\nOpenFWI dataset under various conditions, including class labels, well logs,\nreflectivity images, as well as the combination of these priors. The\nperformance of the approach under out-of-distribution conditions further\nunderscores its generalization ability, showcasing its potential to provide\ntailored priors for velocity inverse problems and create specific training\ndatasets for machine learning-based geophysical applications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.geo-ph",
    "categories": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06277v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06277v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06277v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06276v1",
    "updated": "2024-02-09T09:40:33+00:00",
    "published": "2024-02-09T09:40:33+00:00",
    "title": "Safe Active Learning for Time-Series Modeling with Gaussian Processes",
    "authors": [
      {
        "name": "Christoph Zimmer"
      },
      {
        "name": "Mona Meister"
      },
      {
        "name": "Duy Nguyen-Tuong"
      }
    ],
    "summary": "Learning time-series models is useful for many applications, such as\nsimulation and forecasting. In this study, we consider the problem of actively\nlearning time-series models while taking given safety constraints into account.\nFor time-series modeling we employ a Gaussian process with a nonlinear\nexogenous input structure. The proposed approach generates data appropriate for\ntime series model learning, i.e. input and output trajectories, by dynamically\nexploring the input space. The approach parametrizes the input trajectory as\nconsecutive trajectory sections, which are determined stepwise given safety\nrequirements and past observations. We analyze the proposed algorithm and\nevaluate it empirically on a technical application. The results show the\neffectiveness of our approach in a realistic technical use case.",
    "comment": "Clarification / Errata of article originally published at NeurIPS:\n  https://proceedings.neurips.cc/paper/2018/hash/b197ffdef2ddc3308584dce7afa3661b-Abstract.html",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06276v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06276v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06276v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06275v1",
    "updated": "2024-02-09T09:40:12+00:00",
    "published": "2024-02-09T09:40:12+00:00",
    "title": "Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics",
    "authors": [
      {
        "name": "Artur P. Toshev"
      },
      {
        "name": "Jonas A. Erbesdobler"
      },
      {
        "name": "Nikolaus A. Adams"
      },
      {
        "name": "Johannes Brandstetter"
      }
    ],
    "summary": "Smoothed particle hydrodynamics (SPH) is omnipresent in modern engineering\nand scientific disciplines. SPH is a class of Lagrangian schemes that\ndiscretize fluid dynamics via finite material points that are tracked through\nthe evolving velocity field. Due to the particle-like nature of the simulation,\ngraph neural networks (GNNs) have emerged as appealing and successful\nsurrogates. However, the practical utility of such GNN-based simulators relies\non their ability to faithfully model physics, providing accurate and stable\npredictions over long time horizons - which is a notoriously hard problem. In\nthis work, we identify particle clustering originating from tensile\ninstabilities as one of the primary pitfalls. Based on these insights, we\nenhance both training and rollout inference of state-of-the-art GNN-based\nsimulators with varying components from standard SPH solvers, including\npressure, viscous, and external force components. All neural SPH-enhanced\nsimulators achieve better performance, often by orders of magnitude, than the\nbaseline GNNs, allowing for significantly longer rollouts and significantly\nbetter physics modeling. Code available under\n(https://github.com/tumaer/neuralsph).",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.flu-dyn",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06275v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06275v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06275v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06271v1",
    "updated": "2024-02-09T09:37:28+00:00",
    "published": "2024-02-09T09:37:28+00:00",
    "title": "Adaptive proximal gradient methods are universal without approximation",
    "authors": [
      {
        "name": "Konstantinos A. Oikonomidis"
      },
      {
        "name": "Emanuel Laude"
      },
      {
        "name": "Puya Latafat"
      },
      {
        "name": "Andreas Themelis"
      },
      {
        "name": "Panagiotis Patrinos"
      }
    ],
    "summary": "We show that adaptive proximal gradient methods for convex problems are not\nrestricted to traditional Lipschitzian assumptions. Our analysis reveals that a\nclass of linesearch-free methods is still convergent under mere local H\\\"older\ngradient continuity, covering in particular continuously differentiable\nsemi-algebraic functions. To mitigate the lack of local Lipschitz continuity,\npopular approaches revolve around $\\varepsilon$-oracles and/or linesearch\nprocedures. In contrast, we exploit plain H\\\"older inequalities not entailing\nany approximation, all while retaining the linesearch-free nature of adaptive\nschemes. Furthermore, we prove full sequence convergence without prior\nknowledge of local H\\\"older constants nor of the order of H\\\"older continuity.\nIn numerical experiments we present comparisons to baseline methods on diverse\ntasks from machine learning covering both the locally and the globally H\\\"older\nsetting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "65K05, 90C06, 90C25, 90C30, 90C47"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06271v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06271v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06271v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06268v1",
    "updated": "2024-02-09T09:34:36+00:00",
    "published": "2024-02-09T09:34:36+00:00",
    "title": "YAMLE: Yet Another Machine Learning Environment",
    "authors": [
      {
        "name": "Martin Ferianc"
      },
      {
        "name": "Miguel Rodrigues"
      }
    ],
    "summary": "YAMLE: Yet Another Machine Learning Environment is an open-source framework\nthat facilitates rapid prototyping and experimentation with machine learning\n(ML) models and methods. The key motivation is to reduce repetitive work when\nimplementing new approaches and improve reproducibility in ML research. YAMLE\nincludes a command-line interface and integrations with popular and\nwell-maintained PyTorch-based libraries to streamline training, hyperparameter\noptimisation, and logging. The ambition for YAMLE is to grow into a shared\necosystem where researchers and practitioners can quickly build on and compare\nexisting implementations. Find it at: https://github.com/martinferianc/yamle.",
    "comment": "Find it at: https://github.com/martinferianc/yamle",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06268v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06268v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06268v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06266v1",
    "updated": "2024-02-09T09:28:01+00:00",
    "published": "2024-02-09T09:28:01+00:00",
    "title": "Value function interference and greedy action selection in value-based multi-objective reinforcement learning",
    "authors": [
      {
        "name": "Peter Vamplew"
      },
      {
        "name": "Cameron Foale"
      },
      {
        "name": "Richard Dazeley"
      }
    ],
    "summary": "Multi-objective reinforcement learning (MORL) algorithms extend conventional\nreinforcement learning (RL) to the more general case of problems with multiple,\nconflicting objectives, represented by vector-valued rewards. Widely-used\nscalar RL methods such as Q-learning can be modified to handle multiple\nobjectives by (1) learning vector-valued value functions, and (2) performing\naction selection using a scalarisation or ordering operator which reflects the\nuser's utility with respect to the different objectives. However, as we\ndemonstrate here, if the user's utility function maps widely varying\nvector-values to similar levels of utility, this can lead to interference in\nthe value-function learned by the agent, leading to convergence to sub-optimal\npolicies. This will be most prevalent in stochastic environments when\noptimising for the Expected Scalarised Return criterion, but we present a\nsimple example showing that interference can also arise in deterministic\nenvironments. We demonstrate empirically that avoiding the use of random\ntie-breaking when identifying greedy actions can ameliorate, but not fully\novercome, the problems caused by value function interference.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06266v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06266v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06266v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06264v1",
    "updated": "2024-02-09T09:25:18+00:00",
    "published": "2024-02-09T09:25:18+00:00",
    "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education",
    "authors": [
      {
        "name": "Unggi Lee"
      },
      {
        "name": "Minji Jeon"
      },
      {
        "name": "Yunseo Lee"
      },
      {
        "name": "Gyuri Byun"
      },
      {
        "name": "Yoorim Son"
      },
      {
        "name": "Jaeyoon Shin"
      },
      {
        "name": "Hongkyu Ko"
      },
      {
        "name": "Hyeoncheol Kim"
      }
    ],
    "summary": "Art appreciation is vital in nurturing critical thinking and emotional\nintelligence among learners. However, traditional art appreciation education\nhas often been hindered by limited access to art resources, especially for\ndisadvantaged students, and an imbalanced emphasis on STEM subjects in\nmainstream education. In response to these challenges, recent technological\nadvancements have paved the way for innovative solutions. This study explores\nthe application of multi-modal large language models (MLLMs) in art\nappreciation education, focusing on developing LLaVA-Docent, a model that\nleverages these advancements. Our approach involved a comprehensive literature\nreview and consultations with experts in the field, leading to developing a\nrobust data framework. Utilizing this framework, we generated a virtual\ndialogue dataset that was leveraged by GPT-4. This dataset was instrumental in\ntraining the MLLM, named LLaVA-Docent. Six researchers conducted quantitative\nand qualitative evaluations of LLaVA-Docent to assess its effectiveness,\nbenchmarking it against the GPT-4 model in a few-shot setting. The evaluation\nprocess revealed distinct strengths and weaknesses of the LLaVA-Docent model.\nOur findings highlight the efficacy of LLaVA-Docent in enhancing the\naccessibility and engagement of art appreciation education. By harnessing the\npotential of MLLMs, this study makes a significant contribution to the field of\nart education, proposing a novel methodology that reimagines the way art\nappreciation is taught and experienced.",
    "comment": "37 pages, 4 figures, 10 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06264v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06264v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06264v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06262v1",
    "updated": "2024-02-09T09:20:59+00:00",
    "published": "2024-02-09T09:20:59+00:00",
    "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
    "authors": [
      {
        "name": "Siyu Ren"
      },
      {
        "name": "Kenny Q. Zhu"
      }
    ],
    "summary": "Despite the recent success associated with Large Language Models~(LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of \\textit{importance score calculation} and \\textit{eviction\nscope construction}. We identify the deficiency of prior policies in these two\naspects and introduce RoCo, a \\underline{r}\\underline{o}bust \\underline{c}ache\n\\underline{o}mission policy based on temporal attention scores and robustness\nmeasures. Extensive experimentation spanning prefilling and auto-regressive\ndecoding stages validates the superiority of RoCo. Finally, we release EasyKV,\na versatile software package dedicated to user-friendly key-value constrained\ngenerative inference. Code available at \\url{https://github.com/DRSY/EasyKV}.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06262v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06262v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06262v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06255v1",
    "updated": "2024-02-09T09:09:39+00:00",
    "published": "2024-02-09T09:09:39+00:00",
    "title": "Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
    "authors": [
      {
        "name": "Yichuan Mo"
      },
      {
        "name": "Yuji Wang"
      },
      {
        "name": "Zeming Wei"
      },
      {
        "name": "Yisen Wang"
      }
    ],
    "summary": "Although Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to certain prompts that can\ninduce them to bypass built-in safety measures and provide dangerous or illegal\ncontent, a phenomenon known as jailbreak. To protect LLMs from producing\nharmful information, various defense strategies are proposed, with most\nfocusing on content filtering or adversarial training of models. In this paper,\nwe propose an approach named Prompt Adversarial Tuning (PAT) to train a defense\ncontrol mechanism, which is then embedded as a prefix to user prompts to\nimplement our defense strategy. We design a training process similar to\nadversarial training to achieve our optimized goal, alternating between\nupdating attack and defense controls. To our knowledge, we are the first to\nimplement defense from the perspective of prompt tuning. Once employed, our\nmethod will hardly impact the operational efficiency of LLMs. Experiments show\nthat our method is effective in both black-box and white-box settings, reducing\nthe success rate of advanced attacks to nearly 0 while maintaining the benign\nanswer rate of 80% to simple benign questions. Our work might potentially chart\na new perspective for future explorations in LLM security.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06255v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06255v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06255v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06701v1",
    "updated": "2024-02-09T08:31:46+00:00",
    "published": "2024-02-09T08:31:46+00:00",
    "title": "Privacy Profiles for Private Selection",
    "authors": [
      {
        "name": "Antti Koskela"
      },
      {
        "name": "Rachel Redberg"
      },
      {
        "name": "Yu-Xiang Wang"
      }
    ],
    "summary": "Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are\nfundamental primitives of differentially private (DP) data analysis with wide\napplications to private query release, voting, and hyperparameter tuning.\nRecent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made\nsignificant progress in both generalizing private selection mechanisms and\ntightening their privacy analysis using modern numerical privacy accounting\ntools, e.g., R\\'enyi DP. But R\\'enyi DP is known to be lossy when\n$(\\epsilon,\\delta)$-DP is ultimately needed, and there is a trend to close the\ngap by directly handling privacy profiles, i.e., $\\delta$ as a function of\n$\\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work\nout an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax\nand PrivateTuning using the privacy profiles of the base algorithms they\ncorral. Numerically, our approach improves over the RDP-based accounting in all\nregimes of interest and leads to substantial benefits in end-to-end private\nlearning experiments. Our analysis also suggests new distributions, e.g.,\nbinomial distribution for randomizing the number of rounds that leads to more\nsubstantial improvements in certain regimes.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06701v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06701v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06701v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06700v1",
    "updated": "2024-02-09T07:45:26+00:00",
    "published": "2024-02-09T07:45:26+00:00",
    "title": "Entropy-Regularized Token-Level Policy Optimization for Large Language Models",
    "authors": [
      {
        "name": "Muning Wen"
      },
      {
        "name": "Cheng Deng"
      },
      {
        "name": "Jun Wang"
      },
      {
        "name": "Weinan Zhang"
      },
      {
        "name": "Ying Wen"
      }
    ],
    "summary": "Large Language Models (LLMs) have shown promise as intelligent agents in\ninteractive decision-making tasks. Traditional approaches often depend on\nmeticulously designed prompts, high-quality examples, or additional reward\nmodels for in-context learning, supervised fine-tuning, or RLHF. Reinforcement\nlearning (RL) presents a dynamic alternative for LLMs to overcome these\ndependencies by engaging directly with task-specific environments. Nonetheless,\nit faces significant hurdles: 1) instability stemming from the exponentially\nvast action space requiring exploration; 2) challenges in assigning token-level\ncredit based on action-level reward signals, resulting in discord between\nmaximizing rewards and accurately modeling corpus data. In response to these\nchallenges, we introduce Entropy-Regularized Token-level Policy Optimization\n(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the\ntoken level. At the heart of ETPO is our novel per-token soft Bellman update,\ndesigned to harmonize the RL process with the principles of language modeling.\nThis methodology decomposes the Q-function update from a coarse action-level\nview to a more granular token-level perspective, backed by theoretical proof of\noptimization consistency. Crucially, this decomposition renders linear time\ncomplexity in action exploration. We assess the effectiveness of ETPO within a\nsimulated environment that models data science code generation as a series of\nmulti-step interactive tasks; results show that ETPO achieves effective\nperformance improvement on the CodeLlama-7B model and surpasses a variant PPO\nbaseline inherited from RLHF. This underlines ETPO's potential as a robust\nmethod for refining the interactive decision-making capabilities of LLMs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06700v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06700v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06700v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06229v1",
    "updated": "2024-02-09T07:44:27+00:00",
    "published": "2024-02-09T07:44:27+00:00",
    "title": "Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants",
    "authors": [
      {
        "name": "Bhavya Chopra"
      },
      {
        "name": "Yasharth Bajpai"
      },
      {
        "name": "Param Biyani"
      },
      {
        "name": "Gustavo Soares"
      },
      {
        "name": "Arjun Radhakrishna"
      },
      {
        "name": "Chris Parnin"
      },
      {
        "name": "Sumit Gulwani"
      }
    ],
    "summary": "The widespread availability of Large Language Models (LLMs) within Integrated\nDevelopment Environments (IDEs) has led to their speedy adoption.\nConversational interactions with LLMs enable programmers to obtain natural\nlanguage explanations for various software development tasks. However, LLMs\noften leap to action without sufficient context, giving rise to implicit\nassumptions and inaccurate responses. Conversations between developers and LLMs\nare primarily structured as question-answer pairs, where the developer is\nresponsible for asking the the right questions and sustaining conversations\nacross multiple turns. In this paper, we draw inspiration from interaction\npatterns and conversation analysis -- to design Robin, an enhanced\nconversational AI-assistant for debugging. Through a within-subjects user study\nwith 12 industry professionals, we find that equipping the LLM to -- (1)\nleverage the insert expansion interaction pattern, (2) facilitate turn-taking,\nand (3) utilize debugging workflows -- leads to lowered conversation barriers,\neffective fault localization, and 5x improvement in bug resolution rates.",
    "comment": "7 pages, 4 figures, 2 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06229v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06229v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06229v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06226v1",
    "updated": "2024-02-09T07:23:27+00:00",
    "published": "2024-02-09T07:23:27+00:00",
    "title": "N-1 Reduced Optimal Power Flow Using Augmented Hierarchical Graph Neural Network",
    "authors": [
      {
        "name": "Thuan Pham"
      },
      {
        "name": "Xingpeng Li"
      }
    ],
    "summary": "Optimal power flow (OPF) is used to perform generation redispatch in power\nsystem real-time operations. N-1 OPF can ensure safe grid operations under\ndiverse contingency scenarios. For large and intricate power networks with\nnumerous variables and constraints, achieving an optimal solution for real-time\nN-1 OPF necessitates substantial computational resources. To mitigate this\nchallenge, machine learning (ML) is introduced as an additional tool for\npredicting congested or heavily loaded lines dynamically. In this paper, an\nadvanced ML model known as the augmented hierarchical graph neural network\n(AHGNN) was proposed to predict critical congested lines and create N-1 reduced\nOPF (N-1 ROPF). The proposed AHGNN-enabled N-1 ROPF can result in a remarkable\nreduction in computing time while retaining the solution quality. Several\nvariations of GNN-based ML models are also implemented as benchmark to\ndemonstrate effectiveness of the proposed AHGNN approach. Case studies prove\nthe proposed AHGNN and the associated N-1 ROPF are highly effective in reducing\ncomputation time while preserving solution quality, highlighting the promising\npotential of ML, particularly GNN in enhancing power system operations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.SY",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06226v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06226v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06226v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06224v1",
    "updated": "2024-02-09T07:20:14+00:00",
    "published": "2024-02-09T07:20:14+00:00",
    "title": "Adaptive multi-gradient methods for quasiconvex vector optimization and applications to multi-task learning",
    "authors": [
      {
        "name": "Nguyen Anh Minh"
      },
      {
        "name": "Le Dung Muu"
      },
      {
        "name": "Tran Ngoc Thang"
      }
    ],
    "summary": "We present an adaptive step-size method, which does not include line-search\ntechniques, for solving a wide class of nonconvex multiobjective programming\nproblems on an unbounded constraint set. We also prove convergence of a general\napproach under modest assumptions. More specifically, the convexity criterion\nmight not be satisfied by the objective function. Unlike descent line-search\nalgorithms, it does not require an initial step-size to be determined by a\npreviously determined Lipschitz constant. The process's primary characteristic\nis its gradual step-size reduction up until a predetermined condition is met.\nIt can be specifically applied to offer an innovative multi-gradient projection\nmethod for unbounded constrained optimization issues. Preliminary findings from\na few computational examples confirm the accuracy of the strategy. We apply the\nproposed technique to some multi-task learning experiments to show its efficacy\nfor large-scale challenges.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06224v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06224v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06224v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06223v1",
    "updated": "2024-02-09T07:18:06+00:00",
    "published": "2024-02-09T07:18:06+00:00",
    "title": "Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models",
    "authors": [
      {
        "name": "Yuhang Liu"
      },
      {
        "name": "Zhen Zhang"
      },
      {
        "name": "Dong Gong"
      },
      {
        "name": "Biwei Huang"
      },
      {
        "name": "Mingming Gong"
      },
      {
        "name": "Anton van den Hengel"
      },
      {
        "name": "Kun Zhang"
      },
      {
        "name": "Javen Qinfeng Shi"
      }
    ],
    "summary": "Multimodal contrastive representation learning methods have proven successful\nacross a range of domains, partly due to their ability to generate meaningful\nshared representations of complex phenomena. To enhance the depth of analysis\nand understanding of these acquired representations, we introduce a unified\ncausal model specifically designed for multimodal data. By examining this\nmodel, we show that multimodal contrastive representation learning excels at\nidentifying latent coupled variables within the proposed unified model, up to\nlinear or permutation transformations resulting from different assumptions. Our\nfindings illuminate the potential of pre-trained multimodal models, eg, CLIP,\nin learning disentangled representations through a surprisingly simple yet\nhighly effective tool: linear independent component analysis. Experiments\ndemonstrate the robustness of our findings, even when the assumptions are\nviolated, and validate the effectiveness of the proposed method in learning\ndisentangled representations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06223v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06223v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06223v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06204v1",
    "updated": "2024-02-09T06:16:08+00:00",
    "published": "2024-02-09T06:16:08+00:00",
    "title": "The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",
    "authors": [
      {
        "name": "Juhyun Oh"
      },
      {
        "name": "Eunsu Kim"
      },
      {
        "name": "Inha Cha"
      },
      {
        "name": "Alice Oh"
      }
    ],
    "summary": "This paper explores the assumption that Large Language Models (LLMs) skilled\nin generation tasks are equally adept as evaluators. We assess the performance\nof three LLMs and one open-source LM in Question-Answering (QA) and evaluation\ntasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a\nsignificant disparity, with LLMs exhibiting lower performance in evaluation\ntasks compared to generation tasks. Intriguingly, we discover instances of\nunfaithful evaluation where models accurately evaluate answers in areas where\nthey lack competence, underscoring the need to examine the faithfulness and\ntrustworthiness of LLMs as evaluators. This study contributes to the\nunderstanding of \"the Generative AI Paradox\" (West et al., 2023), highlighting\na need to explore the correlation between generative excellence and evaluation\nproficiency, and the necessity to scrutinize the faithfulness aspect in model\nevaluations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06204v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06204v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06204v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06196v1",
    "updated": "2024-02-09T05:37:09+00:00",
    "published": "2024-02-09T05:37:09+00:00",
    "title": "Large Language Models: A Survey",
    "authors": [
      {
        "name": "Shervin Minaee"
      },
      {
        "name": "Tomas Mikolov"
      },
      {
        "name": "Narjes Nikzad"
      },
      {
        "name": "Meysam Chenaghlu"
      },
      {
        "name": "Richard Socher"
      },
      {
        "name": "Xavier Amatriain"
      },
      {
        "name": "Jianfeng Gao"
      }
    ],
    "summary": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2401.14423",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06196v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06196v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06196v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06191v1",
    "updated": "2024-02-09T05:10:53+00:00",
    "published": "2024-02-09T05:10:53+00:00",
    "title": "The Berkeley Single Cell Computational Microscopy (BSCCM) Dataset",
    "authors": [
      {
        "name": "Henry Pinkard"
      },
      {
        "name": "Cherry Liu"
      },
      {
        "name": "Fanice Nyatigo"
      },
      {
        "name": "Daniel A. Fletcher"
      },
      {
        "name": "Laura Waller"
      }
    ],
    "summary": "Computational microscopy, in which hardware and algorithms of an imaging\nsystem are jointly designed, shows promise for making imaging systems that cost\nless, perform more robustly, and collect new types of information. Often, the\nperformance of computational imaging systems, especially those that incorporate\nmachine learning, is sample-dependent. Thus, standardized datasets are an\nessential tool for comparing the performance of different approaches. Here, we\nintroduce the Berkeley Single Cell Computational Microscopy (BSCCM) dataset,\nwhich contains over ~12,000,000 images of 400,000 of individual white blood\ncells. The dataset contains images captured with multiple illumination patterns\non an LED array microscope and fluorescent measurements of the abundance of\nsurface proteins that mark different cell types. We hope this dataset will\nprovide a valuable resource for the development and testing of new algorithms\nin computational microscopy and computer vision with practical biomedical\napplications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06191v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06191v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06191v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06190v1",
    "updated": "2024-02-09T05:06:58+00:00",
    "published": "2024-02-09T05:06:58+00:00",
    "title": "Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain",
    "authors": [
      {
        "name": "Amin Karimi Monsefi"
      },
      {
        "name": "Payam Karisani"
      },
      {
        "name": "Mengxi Zhou"
      },
      {
        "name": "Stacey Choi"
      },
      {
        "name": "Nathan Doble"
      },
      {
        "name": "Heng Ji"
      },
      {
        "name": "Srinivasan Parthasarathy"
      },
      {
        "name": "Rajiv Ramnath"
      }
    ],
    "summary": "Standard modern machine-learning-based imaging methods have faced challenges\nin medical applications due to the high cost of dataset construction and,\nthereby, the limited labeled training data available. Additionally, upon\ndeployment, these methods are usually used to process a large volume of data on\na daily basis, imposing a high maintenance cost on medical facilities. In this\npaper, we introduce a new neural network architecture, termed LoGoNet, with a\ntailored self-supervised learning (SSL) method to mitigate such challenges.\nLoGoNet integrates a novel feature extractor within a U-shaped architecture,\nleveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture\nboth long-range and short-range feature dependencies adeptly. This is in\ncontrast to existing methods that rely on increasing network capacity to\nenhance feature extraction. This combination of novel techniques in our model\nis especially beneficial in medical image segmentation, given the difficulty of\nlearning intricate and often irregular body organ shapes, such as the spleen.\nComplementary, we propose a novel SSL method tailored for 3D images to\ncompensate for the lack of large labeled datasets. The method combines masking\nand contrastive learning techniques within a multi-task learning framework and\nis compatible with both Vision Transformer (ViT) and CNN-based models. We\ndemonstrate the efficacy of our methods in numerous tasks across two standard\ndatasets (i.e., BTCV and MSD). Benchmark comparisons with eight\nstate-of-the-art models highlight LoGoNet's superior performance in both\ninference time and accuracy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06190v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06190v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06190v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06188v1",
    "updated": "2024-02-09T05:05:28+00:00",
    "published": "2024-02-09T05:05:28+00:00",
    "title": "A self-supervised framework for learning whole slide representations",
    "authors": [
      {
        "name": "Xinhai Hou"
      },
      {
        "name": "Cheng Jiang"
      },
      {
        "name": "Akhil Kondepudi"
      },
      {
        "name": "Yiwei Lyu"
      },
      {
        "name": "Asadur Zaman Chowdury"
      },
      {
        "name": "Honglak Lee"
      },
      {
        "name": "Todd C. Hollon"
      }
    ],
    "summary": "Whole slide imaging is fundamental to biomedical microscopy and computational\npathology. However, whole slide images (WSIs) present a complex computer vision\nchallenge due to their gigapixel size, diverse histopathologic features,\nspatial heterogeneity, and limited/absent data annotations. These challenges\nhighlight that supervised training alone can result in suboptimal whole slide\nrepresentations. Self-supervised representation learning can achieve\nhigh-quality WSI visual feature learning for downstream diagnostic tasks, such\nas cancer diagnosis or molecular genetic prediction. Here, we present a general\nself-supervised whole slide learning (S3L) framework for gigapixel-scale\nself-supervision of WSIs. S3L combines data transformation strategies from\ntransformer-based vision and language modeling into a single unified framework\nto generate paired views for self-supervision. S3L leverages the inherent\nregional heterogeneity, histologic feature variability, and information\nredundancy within WSIs to learn high-quality whole-slide representations. We\nbenchmark S3L visual representations on two diagnostic tasks for two biomedical\nmicroscopy modalities. S3L significantly outperforms WSI baselines for cancer\ndiagnosis and genetic mutation prediction. Additionally, S3L achieves good\nperformance using both in-domain and out-of-distribution patch encoders,\ndemonstrating good flexibility and generalizability.",
    "comment": "18 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06188v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06188v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06188v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06187v3",
    "updated": "2024-02-13T21:19:04+00:00",
    "published": "2024-02-09T05:04:40+00:00",
    "title": "Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
    "authors": [
      {
        "name": "Ruijie Zheng"
      },
      {
        "name": "Yongyuan Liang"
      },
      {
        "name": "Xiyao Wang"
      },
      {
        "name": "Shuang Ma"
      },
      {
        "name": "Hal Daum\u00e9 III"
      },
      {
        "name": "Huazhe Xu"
      },
      {
        "name": "John Langford"
      },
      {
        "name": "Praveen Palanisamy"
      },
      {
        "name": "Kalyan Shankar Basu"
      },
      {
        "name": "Furong Huang"
      }
    ],
    "summary": "We present Premier-TACO, a multitask feature representation learning approach\ndesigned to improve few-shot policy learning efficiency in sequential\ndecision-making tasks. Premier-TACO leverages a subset of multitask offline\ndatasets for pretraining a general feature representation, which captures\ncritical environmental dynamics and is fine-tuned using minimal expert\ndemonstrations. It advances the temporal action contrastive learning (TACO)\nobjective, known for state-of-the-art results in visual control tasks, by\nincorporating a novel negative example sampling strategy. This strategy is\ncrucial in significantly boosting TACO's computational efficiency, making\nlarge-scale multitask offline pretraining feasible. Our extensive empirical\nevaluation in a diverse set of continuous control benchmarks including Deepmind\nControl Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness\nin pretraining visual representations, significantly enhancing few-shot\nimitation learning of novel tasks. Our code, pretraining data, as well as\npretrained model checkpoints will be released at\nhttps://github.com/PremierTACO/premier-taco. Our project webpage is at\nhttps://premiertaco.github.io.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06187v3",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06187v3",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06187v3"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06185v1",
    "updated": "2024-02-09T04:47:26+00:00",
    "published": "2024-02-09T04:47:26+00:00",
    "title": "Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters",
    "authors": [
      {
        "name": "Edward S. Harake"
      },
      {
        "name": "Joseph R. Linzey"
      },
      {
        "name": "Cheng Jiang"
      },
      {
        "name": "Rushikesh S. Joshi"
      },
      {
        "name": "Mark M. Zaki"
      },
      {
        "name": "Jaes C. Jones"
      },
      {
        "name": "Siri S. Khalsa"
      },
      {
        "name": "John H. Lee"
      },
      {
        "name": "Zachary Wilseck"
      },
      {
        "name": "Jacob R. Joseph"
      },
      {
        "name": "Todd C. Hollon"
      },
      {
        "name": "Paul Park"
      }
    ],
    "summary": "Objective. Achieving appropriate spinopelvic alignment has been shown to be\nassociated with improved clinical symptoms. However, measurement of spinopelvic\nradiographic parameters is time-intensive and interobserver reliability is a\nconcern. Automated measurement tools have the promise of rapid and consistent\nmeasurements, but existing tools are still limited by some degree of manual\nuser-entry requirements. This study presents a novel artificial intelligence\n(AI) tool called SpinePose that automatically predicts spinopelvic parameters\nwith high accuracy without the need for manual entry.\n  Methods. SpinePose was trained and validated on 761 sagittal whole-spine\nX-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic\nincidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle\n(T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was\nlabeled by 4 reviewers, including fellowship-trained spine surgeons and a\nfellowship-trained radiologist with neuroradiology subspecialty certification.\nMedian errors relative to the most senior reviewer were calculated to determine\nmodel accuracy on test images. Intraclass correlation coefficients (ICC) were\nused to assess inter-rater reliability.\n  Results. SpinePose exhibited the following median (interquartile range)\nparameter errors: SVA: 2.2(2.3)mm, p=0.93; PT: 1.3(1.2){\\deg}, p=0.48; SS:\n1.7(2.2){\\deg}, p=0.64; PI: 2.2(2.1){\\deg}, p=0.24; LL: 2.6(4.0){\\deg}, p=0.89;\nT1PA: 1.1(0.9){\\deg}, p=0.42; and L1PA: 1.4(1.6){\\deg}, p=0.49. Model\npredictions also exhibited excellent reliability at all parameters (ICC:\n0.91-1.0).\n  Conclusions. SpinePose accurately predicted spinopelvic parameters with\nexcellent reliability comparable to fellowship-trained spine surgeons and\nneuroradiologists. Utilization of predictive AI tools in spinal imaging can\nsubstantially aid in patient selection and surgical planning.",
    "comment": "10 pages, 5 figures, to appear in Journal of Neurosurgery: Spine",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06185v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06185v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06185v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06184v1",
    "updated": "2024-02-09T04:46:48+00:00",
    "published": "2024-02-09T04:46:48+00:00",
    "title": "The boundary of neural network trainability is fractal",
    "authors": [
      {
        "name": "Jascha Sohl-Dickstein"
      }
    ],
    "summary": "Some fractals -- for instance those associated with the Mandelbrot and\nquadratic Julia sets -- are computed by iterating a function, and identifying\nthe boundary between hyperparameters for which the resulting series diverges or\nremains bounded. Neural network training similarly involves iterating an update\nfunction (e.g. repeated steps of gradient descent), can result in convergent or\ndivergent behavior, and can be extremely sensitive to small changes in\nhyperparameters. Motivated by these similarities, we experimentally examine the\nboundary between neural network hyperparameters that lead to stable and\ndivergent training. We find that this boundary is fractal over more than ten\ndecades of scale in all tested configurations.",
    "comment": "3 pages, mesmerizing fractals",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE",
      "nlin.CD"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06184v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06184v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06184v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06178v1",
    "updated": "2024-02-09T04:34:08+00:00",
    "published": "2024-02-09T04:34:08+00:00",
    "title": "MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models",
    "authors": [
      {
        "name": "Yixiao Zhang"
      },
      {
        "name": "Yukara Ikemiya"
      },
      {
        "name": "Gus Xia"
      },
      {
        "name": "Naoki Murata"
      },
      {
        "name": "Marco Mart\u00ednez"
      },
      {
        "name": "Wei-Hsiang Liao"
      },
      {
        "name": "Yuki Mitsufuji"
      },
      {
        "name": "Simon Dixon"
      }
    ],
    "summary": "Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.",
    "comment": "Project page: https://bit.ly/musicmagus-demo",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06178v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06178v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06178v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06173v1",
    "updated": "2024-02-09T04:13:38+00:00",
    "published": "2024-02-09T04:13:38+00:00",
    "title": "SMC Is All You Need: Parallel Strong Scaling",
    "authors": [
      {
        "name": "Xinzhu Liang"
      },
      {
        "name": "Sanjaya Lohani"
      },
      {
        "name": "Joseph M. Lukens"
      },
      {
        "name": "Brian T. Kirby"
      },
      {
        "name": "Thomas A. Searles"
      },
      {
        "name": "Kody J. H. Law"
      }
    ],
    "summary": "In the general framework of Bayesian inference, the target distribution can\nonly be evaluated up-to a constant of proportionality. Classical consistent\nBayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte\nCarlo (MCMC) have unbounded time complexity requirements. We develop a fully\nparallel sequential Monte Carlo (pSMC) method which provably delivers parallel\nstrong scaling, i.e. the time complexity (and per-node memory) remains bounded\nif the number of asynchronous processes is allowed to grow. More precisely, the\npSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes\nthe number of communicating samples in each processor and $R$ denotes the\nnumber of processors. In particular, for suitably-large problem-dependent $N$,\nas $R \\rightarrow \\infty$ the method converges to infinitesimal accuracy\nMSE$=O(\\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with\nno efficiency leakage, i.e. computational complexity\nCost$=O(\\varepsilon^{-2})$. A number of Bayesian inference problems are taken\ninto consideration to compare the pSMC and MCMC methods.",
    "comment": "21 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06173v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06173v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06173v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06171v1",
    "updated": "2024-02-09T04:01:25+00:00",
    "published": "2024-02-09T04:01:25+00:00",
    "title": "Pushing Boundaries: Mixup's Influence on Neural Collapse",
    "authors": [
      {
        "name": "Quinn Fisher"
      },
      {
        "name": "Haoming Meng"
      },
      {
        "name": "Vardan Papyan"
      }
    ],
    "summary": "Mixup is a data augmentation strategy that employs convex combinations of\ntraining instances and their respective labels to augment the robustness and\ncalibration of deep neural networks. Despite its widespread adoption, the\nnuanced mechanisms that underpin its success are not entirely understood. The\nobserved phenomenon of Neural Collapse, where the last-layer activations and\nclassifier of deep networks converge to a simplex equiangular tight frame\n(ETF), provides a compelling motivation to explore whether mixup induces\nalternative geometric configurations and whether those could explain its\nsuccess. In this study, we delve into the last-layer activations of training\ndata for deep networks subjected to mixup, aiming to uncover insights into its\noperational efficacy. Our investigation, spanning various architectures and\ndataset pairs, reveals that mixup's last-layer activations predominantly\nconverge to a distinctive configuration different than one might expect. In\nthis configuration, activations from mixed-up examples of identical classes\nalign with the classifier, while those from different classes delineate\nchannels along the decision boundary. Moreover, activations in earlier layers\nexhibit patterns, as if trained with manifold mixup. These findings are\nunexpected, as mixed-up features are not simple convex combinations of feature\nclass means (as one might get, for example, by training mixup with the mean\nsquared error loss). By analyzing this distinctive geometric configuration, we\nelucidate the mechanisms by which mixup enhances model calibration. To further\nvalidate our empirical observations, we conduct a theoretical analysis under\nthe assumption of an unconstrained features model, utilizing the mixup loss.\nThrough this, we characterize and derive the optimal last-layer features under\nthe assumption that the classifier forms a simplex ETF.",
    "comment": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR 2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06171v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06171v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06171v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06165v1",
    "updated": "2024-02-09T03:48:20+00:00",
    "published": "2024-02-09T03:48:20+00:00",
    "title": "Learning Contrastive Feature Representations for Facial Action Unit Detection",
    "authors": [
      {
        "name": "Ziqiao Shang"
      },
      {
        "name": "Bin Liu"
      },
      {
        "name": "Fei Teng"
      },
      {
        "name": "Tianrui Li"
      }
    ],
    "summary": "The predominant approach to facial action unit (AU) detection revolves around\na supervised multi-label binary classification problem. Existing methodologies\noften encode pixel-level information of AUs, thereby imposing substantial\ndemands on model complexity and expressiveness. Moreover, this practice\nelevates the susceptibility to overfitting due to the presence of noisy AU\nlabels. In the present study, we introduce a contrastive learning framework\nenhanced by both supervised and self-supervised signals. The objective is to\nacquire discriminative features, deviating from the conventional pixel-level\nlearning paradigm within the domain of AU detection. To address the challenge\nposed by noisy AU labels, we augment the supervised signal through the\nintroduction of a self-supervised signal. This augmentation is achieved through\npositive sample sampling, encompassing three distinct types of positive sample\npairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we\nemploy an importance re-weighting strategy tailored for minority AUs. The\nresulting loss, denoted as AUNCE, is proposed to encapsulate this strategy. Our\nexperimental assessments, conducted on two widely-utilized benchmark datasets\n(BP4D and DISFA), underscore the superior performance of our approach compared\nto state-of-the-art methods in the realm of AU detection.",
    "comment": "11 pages, 3 figures, submitted to an IEEE journal",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06165v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06165v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06165v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06162v1",
    "updated": "2024-02-09T03:33:13+00:00",
    "published": "2024-02-09T03:33:13+00:00",
    "title": "Wasserstein proximal operators describe score-based generative models and resolve memorization",
    "authors": [
      {
        "name": "Benjamin J. Zhang"
      },
      {
        "name": "Siting Liu"
      },
      {
        "name": "Wuchen Li"
      },
      {
        "name": "Markos A. Katsoulakis"
      },
      {
        "name": "Stanley J. Osher"
      }
    ],
    "summary": "We focus on the fundamental mathematical structure of score-based generative\nmodels (SGMs). We first formulate SGMs in terms of the Wasserstein proximal\noperator (WPO) and demonstrate that, via mean-field games (MFGs), the WPO\nformulation reveals mathematical structure that describes the inductive bias of\ndiffusion and score-based models. In particular, MFGs yield optimality\nconditions in the form of a pair of coupled partial differential equations: a\nforward-controlled Fokker-Planck (FP) equation, and a backward\nHamilton-Jacobi-Bellman (HJB) equation. Via a Cole-Hopf transformation and\ntaking advantage of the fact that the cross-entropy can be related to a linear\nfunctional of the density, we show that the HJB equation is an uncontrolled FP\nequation. Second, with the mathematical structure at hand, we present an\ninterpretable kernel-based model for the score function which dramatically\nimproves the performance of SGMs in terms of training samples and training\ntime. In addition, the WPO-informed kernel model is explicitly constructed to\navoid the recently studied memorization effects of score-based generative\nmodels. The mathematical form of the new kernel-based models in combination\nwith the use of the terminal condition of the MFG reveals new explanations for\nthe manifold learning and generalization properties of SGMs, and provides a\nresolution to their memorization effects. Finally, our mathematically informed,\ninterpretable kernel-based model suggests new scalable bespoke neural network\narchitectures for high-dimensional applications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06162v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06162v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06162v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06160v1",
    "updated": "2024-02-09T03:23:39+00:00",
    "published": "2024-02-09T03:23:39+00:00",
    "title": "Improved Evidential Deep Learning via a Mixture of Dirichlet Distributions",
    "authors": [
      {
        "name": "J. Jon Ryu"
      },
      {
        "name": "Maohao Shen"
      },
      {
        "name": "Soumya Ghosh"
      },
      {
        "name": "Yuheng Bu"
      },
      {
        "name": "Prasanna Sattigeri"
      },
      {
        "name": "Subhro Das"
      },
      {
        "name": "Gregory W. Wornell"
      }
    ],
    "summary": "This paper explores a modern predictive uncertainty estimation approach,\ncalled evidential deep learning (EDL), in which a single neural network model\nis trained to learn a meta distribution over the predictive distribution by\nminimizing a specific objective function. Despite their strong empirical\nperformance, recent studies by Bengs et al. identify a fundamental pitfall of\nthe existing methods: the learned epistemic uncertainty may not vanish even in\nthe infinite-sample limit. We corroborate the observation by providing a\nunifying view of a class of widely used objectives from the literature. Our\nanalysis reveals that the EDL methods essentially train a meta distribution by\nminimizing a certain divergence measure between the distribution and a\nsample-size-independent target distribution, resulting in spurious epistemic\nuncertainty. Grounded in theoretical principles, we propose learning a\nconsistent target distribution by modeling it with a mixture of Dirichlet\ndistributions and learning via variational inference. Afterward, a final meta\ndistribution model distills the learned uncertainty from the target model.\nExperimental results across various uncertainty-based downstream tasks\ndemonstrate the superiority of our proposed method, and illustrate the\npractical implications arising from the consistency and inconsistency of\nlearned epistemic uncertainty.",
    "comment": "18 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06160v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06160v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06160v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06158v1",
    "updated": "2024-02-09T03:18:44+00:00",
    "published": "2024-02-09T03:18:44+00:00",
    "title": "Assortment Planning with Sponsored Products",
    "authors": [
      {
        "name": "Shaojie Tang"
      },
      {
        "name": "Shuzhang Cai"
      },
      {
        "name": "Jing Yuan"
      },
      {
        "name": "Kai Han"
      }
    ],
    "summary": "In the rapidly evolving landscape of retail, assortment planning plays a\ncrucial role in determining the success of a business. With the rise of\nsponsored products and their increasing prominence in online marketplaces,\nretailers face new challenges in effectively managing their product assortment\nin the presence of sponsored products. Remarkably, previous research in\nassortment planning largely overlooks the existence of sponsored products and\ntheir potential impact on overall recommendation effectiveness. Instead, they\ncommonly make the simplifying assumption that all products are either organic\nor non-sponsored. This research gap underscores the necessity for a more\nthorough investigation of the assortment planning challenge when sponsored\nproducts are in play. We formulate the assortment planning problem in the\npresence of sponsored products as a combinatorial optimization task. The\nultimate objective is to compute an assortment plan that optimizes expected\nrevenue while considering the specific requirements of placing sponsored\nproducts strategically.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DS",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.IR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06158v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06158v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06158v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06151v1",
    "updated": "2024-02-09T03:01:13+00:00",
    "published": "2024-02-09T03:01:13+00:00",
    "title": "POTEC: Off-Policy Learning for Large Action Spaces via Two-Stage Policy Decomposition",
    "authors": [
      {
        "name": "Yuta Saito"
      },
      {
        "name": "Jihan Yao"
      },
      {
        "name": "Thorsten Joachims"
      }
    ],
    "summary": "We study off-policy learning (OPL) of contextual bandit policies in large\ndiscrete action spaces where existing methods -- most of which rely crucially\non reward-regression models or importance-weighted policy gradients -- fail due\nto excessive bias or variance. To overcome these issues in OPL, we propose a\nnovel two-stage algorithm, called Policy Optimization via Two-Stage Policy\nDecomposition (POTEC). It leverages clustering in the action space and learns\ntwo different policies via policy- and regression-based approaches,\nrespectively. In particular, we derive a novel low-variance gradient estimator\nthat enables to learn a first-stage policy for cluster selection efficiently\nvia a policy-based approach. To select a specific action within the cluster\nsampled by the first-stage policy, POTEC uses a second-stage policy derived\nfrom a regression-based approach within each cluster. We show that a local\ncorrectness condition, which only requires that the regression model preserves\nthe relative expected reward differences of the actions within each cluster,\nensures that our policy-gradient estimator is unbiased and the second-stage\npolicy is optimal. We also show that POTEC provides a strict generalization of\npolicy- and regression-based approaches and their associated assumptions.\nComprehensive experiments demonstrate that POTEC provides substantial\nimprovements in OPL effectiveness particularly in large and structured action\nspaces.",
    "comment": "arXiv admin note: text overlap with arXiv:2305.08062",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06151v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06151v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06151v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06150v1",
    "updated": "2024-02-09T02:59:08+00:00",
    "published": "2024-02-09T02:59:08+00:00",
    "title": "Domain Generalization with Small Data",
    "authors": [
      {
        "name": "Kecheng Chen"
      },
      {
        "name": "Elena Gal"
      },
      {
        "name": "Hong Yan"
      },
      {
        "name": "Haoliang Li"
      }
    ],
    "summary": "In this work, we propose to tackle the problem of domain generalization in\nthe context of \\textit{insufficient samples}. Instead of extracting latent\nfeature embeddings based on deterministic models, we propose to learn a\ndomain-invariant representation based on the probabilistic framework by mapping\neach data point into probabilistic embeddings. Specifically, we first extend\nempirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can\nmeasure the discrepancy between mixture distributions (i.e., source domains)\nconsisting of a series of latent distributions rather than latent points.\nMoreover, instead of imposing the contrastive semantic alignment (CSA) loss\nbased on pairs of latent points, a novel probabilistic CSA loss encourages\npositive probabilistic embedding pairs to be closer while pulling other\nnegative ones apart. Benefiting from the learned representation captured by\nprobabilistic models, our proposed method can marriage the measurement on the\n\\textit{distribution over distributions} (i.e., the global perspective\nalignment) and the distribution-based contrastive semantic alignment (i.e., the\nlocal perspective alignment). Extensive experimental results on three\nchallenging medical datasets show the effectiveness of our proposed method in\nthe context of insufficient data compared with state-of-the-art methods.",
    "comment": "This paper has been accepted by International Journal of Computer\n  Vision",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06150v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06150v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06150v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07945v1",
    "updated": "2024-02-09T02:33:45+00:00",
    "published": "2024-02-09T02:33:45+00:00",
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "authors": [
      {
        "name": "Runliang Niu"
      },
      {
        "name": "Jindong Li"
      },
      {
        "name": "Shiqi Wang"
      },
      {
        "name": "Yali Fu"
      },
      {
        "name": "Xiyu Hu"
      },
      {
        "name": "Xueyuan Leng"
      },
      {
        "name": "He Kong"
      },
      {
        "name": "Yi Chang"
      },
      {
        "name": "Qi Wang"
      }
    ],
    "summary": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs\nto complete complex tasks. The computer, as the most powerful and universal\ntool, could potentially be controlled directly by a trained LLM agent. Powered\nby the computer, we can hopefully build a more generalized agent to assist\nhumans in various daily digital works. In this paper, we construct an\nenvironment for a Vision Language Model (VLM) agent to interact with a real\ncomputer screen. Within this environment, the agent can observe screenshots and\nmanipulate the Graphics User Interface (GUI) by outputting mouse and keyboard\nactions. We also design an automated control pipeline that includes planning,\nacting, and reflecting phases, guiding the agent to continuously interact with\nthe environment and complete multi-step tasks. Additionally, we construct the\nScreenAgent Dataset, which collects screenshots and action sequences when\ncompleting a variety of daily computer tasks. Finally, we trained a model,\nScreenAgent, which achieved computer control capabilities comparable to GPT-4V\nand demonstrated more precise UI positioning capabilities. Our attempts could\ninspire further research on building a generalist LLM agent. The code is\navailable at \\url{https://github.com/niuzaisheng/ScreenAgent}.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07945v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07945v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07945v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06697v1",
    "updated": "2024-02-09T02:23:37+00:00",
    "published": "2024-02-09T02:23:37+00:00",
    "title": "Feed-Forward Neural Networks as a Mixed-Integer Program",
    "authors": [
      {
        "name": "Navid Aftabi"
      },
      {
        "name": "Nima Moradi"
      },
      {
        "name": "Fatemeh Mahroo"
      }
    ],
    "summary": "Deep neural networks (DNNs) are widely studied in various applications. A DNN\nconsists of layers of neurons that compute affine combinations, apply nonlinear\noperations, and produce corresponding activations. The rectified linear unit\n(ReLU) is a typical nonlinear operator, outputting the max of its input and\nzero. In scenarios like max pooling, where multiple input values are involved,\na fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This\nformulation, with continuous variables representing unit outputs and binary\nvariables for ReLU activation, finds applications across diverse domains. This\nstudy explores the formulation of trained ReLU neurons as MIP and applies MIP\nmodels for training neural networks (NNs). Specifically, it investigates\ninteractions between MIP techniques and various NN architectures, including\nbinary DNNs (employing step activation functions) and binarized DNNs (with\nweights and activations limited to $-1,0,+1$). The research focuses on training\nand evaluating proposed approaches through experiments on handwritten digit\nclassification models. The comparative study assesses the performance of\ntrained ReLU NNs, shedding light on the effectiveness of MIP formulations in\nenhancing training processes for NNs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06697v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06697v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06697v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06137v1",
    "updated": "2024-02-09T02:11:25+00:00",
    "published": "2024-02-09T02:11:25+00:00",
    "title": "On the Privacy of Selection Mechanisms with Gaussian Noise",
    "authors": [
      {
        "name": "Jonathan Lebensold"
      },
      {
        "name": "Doina Precup"
      },
      {
        "name": "Borja Balle"
      }
    ],
    "summary": "Report Noisy Max and Above Threshold are two classical differentially private\n(DP) selection mechanisms. Their output is obtained by adding noise to a\nsequence of low-sensitivity queries and reporting the identity of the query\nwhose (noisy) answer satisfies a certain condition. Pure DP guarantees for\nthese mechanisms are easy to obtain when Laplace noise is added to the queries.\nOn the other hand, when instantiated using Gaussian noise, standard analyses\nonly yield approximate DP guarantees despite the fact that the outputs of these\nmechanisms lie in a discrete space. In this work, we revisit the analysis of\nReport Noisy Max and Above Threshold with Gaussian noise and show that, under\nthe additional assumption that the underlying queries are bounded, it is\npossible to provide pure ex-ante DP bounds for Report Noisy Max and pure\nex-post DP bounds for Above Threshold. The resulting bounds are tight and\ndepend on closed-form expressions that can be numerically evaluated using\nstandard methods. Empirically we find these lead to tighter privacy accounting\nin the high privacy, low data regime. Further, we propose a simple privacy\nfilter for composing pure ex-post DP guarantees, and use it to derive a fully\nadaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide\nexperiments on mobility and energy consumption datasets demonstrating that our\nSparse Vector Technique is practically competitive with previous approaches and\nrequires less hyper-parameter tuning.",
    "comment": "AISTATS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06137v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06137v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06137v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06135v1",
    "updated": "2024-02-09T01:47:18+00:00",
    "published": "2024-02-09T01:47:18+00:00",
    "title": "Jointly Learning Representations for Map Entities via Heterogeneous Graph Contrastive Learning",
    "authors": [
      {
        "name": "Jiawei Jiang"
      },
      {
        "name": "Yifan Yang"
      },
      {
        "name": "Jingyuan Wang"
      },
      {
        "name": "Junjie Wu"
      }
    ],
    "summary": "The electronic map plays a crucial role in geographic information systems,\nserving various urban managerial scenarios and daily life services. Developing\neffective Map Entity Representation Learning (MERL) methods is crucial to\nextracting embedding information from electronic maps and converting map\nentities into representation vectors for downstream applications. However,\nexisting MERL methods typically focus on one specific category of map entities,\nsuch as POIs, road segments, or land parcels, which is insufficient for\nreal-world diverse map-based applications and might lose latent structural and\nsemantic information interacting between entities of different types. Moreover,\nusing representations generated by separate models for different map entities\ncan introduce inconsistencies. Motivated by this, we propose a novel method\nnamed HOME-GCL for learning representations of multiple categories of map\nentities. Our approach utilizes a heterogeneous map entity graph (HOME graph)\nthat integrates both road segments and land parcels into a unified framework. A\nHOME encoder with parcel-segment joint feature encoding and heterogeneous graph\ntransformer is then deliberately designed to convert segments and parcels into\nrepresentation vectors. Moreover, we introduce two types of contrastive\nlearning tasks, namely intra-entity and inter-entity tasks, to train the\nencoder in a self-supervised manner. Extensive experiments on three large-scale\ndatasets covering road segment-based, land parcel-based, and trajectory-based\ntasks demonstrate the superiority of our approach. To the best of our\nknowledge, HOME-GCL is the first attempt to jointly learn representations for\nroad segments and land parcels using a unified model.",
    "comment": "14 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06135v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06135v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06135v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06128v1",
    "updated": "2024-02-09T01:19:47+00:00",
    "published": "2024-02-09T01:19:47+00:00",
    "title": "Rethinking Node-wise Propagation for Large-scale Graph Learning",
    "authors": [
      {
        "name": "Xunkai Li"
      },
      {
        "name": "Jingyuan Ma"
      },
      {
        "name": "Zhengyu Wu"
      },
      {
        "name": "Daohan Su"
      },
      {
        "name": "Wentao Zhang"
      },
      {
        "name": "Rong-Hua Li"
      },
      {
        "name": "Guoren Wang"
      }
    ],
    "summary": "Scalable graph neural networks (GNNs) have emerged as a promising technique,\nwhich exhibits superior predictive performance and high running efficiency\nacross numerous large-scale graph-based web applications. However, (i) Most\nscalable GNNs tend to treat all nodes in graphs with the same propagation\nrules, neglecting their topological uniqueness; (ii) Existing node-wise\npropagation optimization strategies are insufficient on web-scale graphs with\nintricate topology, where a full portrayal of nodes' local properties is\nrequired. Intuitively, different nodes in web-scale graphs possess distinct\ntopological roles, and therefore propagating them indiscriminately or neglect\nlocal contexts may compromise the quality of node representations. This\nintricate topology in web-scale graphs cannot be matched by small-scale\nscenarios. To address the above issues, we propose \\textbf{A}daptive\n\\textbf{T}opology-aware \\textbf{P}ropagation (ATP), which reduces potential\nhigh-bias propagation and extracts structural patterns of each node in a\nscalable manner to improve running efficiency and predictive performance.\nRemarkably, ATP is crafted to be a plug-and-play node-wise propagation\noptimization strategy, allowing for offline execution independent of the graph\nlearning process in a new perspective. Therefore, this approach can be\nseamlessly integrated into most scalable GNNs while remain orthogonal to\nexisting node-wise propagation optimization strategies. Extensive experiments\non 12 datasets, including the most representative large-scale ogbn-papers100M,\nhave demonstrated the effectiveness of ATP. Specifically, ATP has proven to be\nefficient in improving the performance of prevalent scalable GNNs for\nsemi-supervised node classification while addressing redundant computational\ncosts.",
    "comment": "Accepted by WWW 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06128v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06128v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06128v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06127v1",
    "updated": "2024-02-09T01:19:41+00:00",
    "published": "2024-02-09T01:19:41+00:00",
    "title": "CityFlowER: An Efficient and Realistic Traffic Simulator with Embedded Machine Learning Models",
    "authors": [
      {
        "name": "Longchao Da"
      },
      {
        "name": "Chen Chu"
      },
      {
        "name": "Weinan Zhang"
      },
      {
        "name": "Hua Wei"
      }
    ],
    "summary": "Traffic simulation is an essential tool for transportation infrastructure\nplanning, intelligent traffic control policy learning, and traffic flow\nanalysis. Its effectiveness relies heavily on the realism of the simulators\nused. Traditional traffic simulators, such as SUMO and CityFlow, are often\nlimited by their reliance on rule-based models with hyperparameters that\noversimplify driving behaviors, resulting in unrealistic simulations. To\nenhance realism, some simulators have provided Application Programming\nInterfaces (APIs) to interact with Machine Learning (ML) models, which learn\nfrom observed data and offer more sophisticated driving behavior models.\nHowever, this approach faces challenges in scalability and time efficiency as\nvehicle numbers increase. Addressing these limitations, we introduce\nCityFlowER, an advancement over the existing CityFlow simulator, designed for\nefficient and realistic city-wide traffic simulation. CityFlowER innovatively\npre-embeds ML models within the simulator, eliminating the need for external\nAPI interactions and enabling faster data computation. This approach allows for\na blend of rule-based and ML behavior models for individual vehicles, offering\nunparalleled flexibility and efficiency, particularly in large-scale\nsimulations. We provide detailed comparisons with existing simulators,\nimplementation insights, and comprehensive experiments to demonstrate\nCityFlowER's superiority in terms of realism, efficiency, and adaptability.",
    "comment": "4 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": [
      "cs.MA",
      "cs.LG",
      "G.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06127v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06127v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06127v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06126v2",
    "updated": "2024-02-13T16:38:03+00:00",
    "published": "2024-02-09T01:18:16+00:00",
    "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models",
    "authors": [
      {
        "name": "Haizhong Zheng"
      },
      {
        "name": "Xiaoyan Bai"
      },
      {
        "name": "Beidi Chen"
      },
      {
        "name": "Fan Lai"
      },
      {
        "name": "Atul Prakash"
      }
    ],
    "summary": "Large Language Models (LLMs) have achieved remarkable success with their\nbillion-level parameters, yet they incur high inference overheads. The\nemergence of activation sparsity in LLMs provides a natural approach to reduce\nthis cost by involving only parts of the parameters for inference. Existing\nmethods only focus on utilizing this naturally formed activation sparsity,\noverlooking the potential for further amplifying this inherent sparsity. In\nthis paper, we hypothesize that LLMs can learn to be efficient by achieving\nmore structured activation sparsity. To achieve this, we introduce a novel\nalgorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs\nto learn to activate fewer neurons and achieve a better trade-off between\nsparsity and performance. Furthermore, unlike SOTA MoEfication methods, which\nmainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and\nLLaMA with soft activation functions. We evaluate LTE on four models and eleven\ndatasets. The experiments show that LTE achieves a better trade-off between\nsparsity and task performance. For instance, LTE with LLaMA provides a\n1.83x-2.59x FLOPs speed-up on language generation tasks, outperforming the\nstate-of-the-art methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06126v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06126v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06126v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06122v1",
    "updated": "2024-02-09T01:11:34+00:00",
    "published": "2024-02-09T01:11:34+00:00",
    "title": "Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams",
    "authors": [
      {
        "name": "Brian Cho"
      },
      {
        "name": "Kyra Gan"
      },
      {
        "name": "Nathan Kallus"
      }
    ],
    "summary": "We propose a novel nonparametric sequential test for composite hypotheses for\nmeans of multiple data streams. Our proposed method, \\emph{peeking with\nexpectation-based averaged capital} (PEAK), builds upon the testing-as-betting\nframework and provides a non-asymptotic $\\alpha$-level test across any stopping\ntime. PEAK is computationally tractable and efficiently rejects hypotheses that\nare incorrect across all potential distributions that satisfy our nonparametric\nassumption, enabling joint composite hypothesis testing on multiple streams of\ndata. We numerically validate our theoretical findings under the best arm\nidentification and threshold identification in the bandit setting, illustrating\nthe computational efficiency of our method against state-of-the-art testing\nmethods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ME",
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06122v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06122v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06122v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06121v1",
    "updated": "2024-02-09T01:11:23+00:00",
    "published": "2024-02-09T01:11:23+00:00",
    "title": "Iterated Denoising Energy Matching for Sampling from Boltzmann Densities",
    "authors": [
      {
        "name": "Tara Akhound-Sadegh"
      },
      {
        "name": "Jarrid Rector-Brooks"
      },
      {
        "name": "Avishek Joey Bose"
      },
      {
        "name": "Sarthak Mittal"
      },
      {
        "name": "Pablo Lemos"
      },
      {
        "name": "Cheng-Hao Liu"
      },
      {
        "name": "Marcin Sendera"
      },
      {
        "name": "Siamak Ravanbakhsh"
      },
      {
        "name": "Gauthier Gidel"
      },
      {
        "name": "Yoshua Bengio"
      },
      {
        "name": "Nikolay Malkin"
      },
      {
        "name": "Alexander Tong"
      }
    ],
    "summary": "Efficiently generating statistically independent samples from an unnormalized\nprobability distribution, such as equilibrium samples of many-body systems, is\na foundational problem in science. In this paper, we propose Iterated Denoising\nEnergy Matching (iDEM), an iterative algorithm that uses a novel stochastic\nscore matching objective leveraging solely the energy function and its gradient\n-- and no data samples -- to train a diffusion-based sampler. Specifically,\niDEM alternates between (I) sampling regions of high model density from a\ndiffusion-based sampler and (II) using these samples in our stochastic matching\nobjective to further improve the sampler. iDEM is scalable to high dimensions\nas the inner matching objective, is simulation-free, and requires no MCMC\nsamples. Moreover, by leveraging the fast mode mixing behavior of diffusion,\niDEM smooths out the energy landscape enabling efficient exploration and\nlearning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging\nfrom standard synthetic energy functions to invariant $n$-body particle\nsystems. We show that the proposed approach achieves state-of-the-art\nperformance on all metrics and trains $2-5\\times$ faster, which allows it to be\nthe first method to train using energy on the challenging $55$-particle\nLennard-Jones system.",
    "comment": "Code for iDEM is available at https://github.com/jarridrb/dem",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06121v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06121v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06121v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06118v1",
    "updated": "2024-02-09T01:00:14+00:00",
    "published": "2024-02-09T01:00:14+00:00",
    "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
    "authors": [
      {
        "name": "Siming Yan"
      },
      {
        "name": "Min Bai"
      },
      {
        "name": "Weifeng Chen"
      },
      {
        "name": "Xiong Zhou"
      },
      {
        "name": "Qixing Huang"
      },
      {
        "name": "Li Erran Li"
      }
    ],
    "summary": "By combining natural language understanding and the generation capabilities\nand breadth of knowledge of large language models with image perception, recent\nlarge vision language models (LVLMs) have shown unprecedented reasoning\ncapabilities in the real world. However, the generated text often suffers from\ninaccurate grounding in the visual input, resulting in errors such as\nhallucinating nonexistent scene elements, missing significant parts of the\nscene, and inferring incorrect attributes and relationships between objects. To\naddress these issues, we introduce a novel framework, ViGoR (Visual Grounding\nThrough Fine-Grained Reward Modeling) that utilizes fine-grained reward\nmodeling to significantly enhance the visual grounding of LVLMs over\npre-trained baselines. This improvement is efficiently achieved using much\ncheaper human evaluations instead of full supervisions, as well as automated\nmethods. We show the effectiveness of our approach through numerous metrics on\nseveral benchmarks. Additionally, we construct a comprehensive and challenging\ndataset specifically designed to validate the visual grounding capabilities of\nLVLMs. Finally, we plan to release our human annotation comprising\napproximately 16,000 images and generated text pairs with fine-grained\nevaluations to contribute to related research in the community.",
    "comment": "10 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06118v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06118v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06118v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06116v1",
    "updated": "2024-02-09T00:58:57+00:00",
    "published": "2024-02-09T00:58:57+00:00",
    "title": "LLMs for Coding and Robotics Education",
    "authors": [
      {
        "name": "Peng Shu"
      },
      {
        "name": "Huaqin Zhao"
      },
      {
        "name": "Hanqi Jiang"
      },
      {
        "name": "Yiwei Li"
      },
      {
        "name": "Shaochen Xu"
      },
      {
        "name": "Yi Pan"
      },
      {
        "name": "Zihao Wu"
      },
      {
        "name": "Zhengliang Liu"
      },
      {
        "name": "Guoyu Lu"
      },
      {
        "name": "Le Guan"
      },
      {
        "name": "Gong Chen"
      },
      {
        "name": "Xianqiao Wang Tianming Liu"
      }
    ],
    "summary": "Large language models and multimodal large language models have\nrevolutionized artificial intelligence recently. An increasing number of\nregions are now embracing these advanced technologies. Within this context,\nrobot coding education is garnering increasing attention. To teach young\nchildren how to code and compete in robot challenges, large language models are\nbeing utilized for robot code explanation, generation, and modification. In\nthis paper, we highlight an important trend in robot coding education. We test\nseveral mainstream large language models on both traditional coding tasks and\nthe more challenging task of robot code generation, which includes block\ndiagrams. Our results show that GPT-4V outperforms other models in all of our\ntests but struggles with generating block diagram images.",
    "comment": "20 pages, 6 figures, 1 table",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06116v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06116v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06116v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06696v1",
    "updated": "2024-02-09T00:49:03+00:00",
    "published": "2024-02-09T00:49:03+00:00",
    "title": "FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models",
    "authors": [
      {
        "name": "Ruiyang Qin"
      },
      {
        "name": "Yuting Hu"
      },
      {
        "name": "Zheyu Yan"
      },
      {
        "name": "Jinjun Xiong"
      },
      {
        "name": "Ahmed Abbasi"
      },
      {
        "name": "Yiyu Shi"
      }
    ],
    "summary": "Neural Architecture Search (NAS) has become the de fecto tools in the\nindustry in automating the design of deep neural networks for various\napplications, especially those driven by mobile and edge devices with limited\ncomputing resources. The emerging large language models (LLMs), due to their\nprowess, have also been incorporated into NAS recently and show some promising\nresults. This paper conducts further exploration in this direction by\nconsidering three important design metrics simultaneously, i.e., model\naccuracy, fairness, and hardware deployment efficiency. We propose a novel\nLLM-based NAS framework, FL-NAS, in this paper, and show experimentally that\nFL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN\nmodels by orders-of-magnitude across almost all design considerations.",
    "comment": "ASP-DAC 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06696v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06696v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06696v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06110v1",
    "updated": "2024-02-09T00:24:46+00:00",
    "published": "2024-02-09T00:24:46+00:00",
    "title": "AI enhanced data assimilation and uncertainty quantification applied to Geological Carbon Storage",
    "authors": [
      {
        "name": "G. S. Seabra"
      },
      {
        "name": "N. T. M\u00fccke"
      },
      {
        "name": "V. L. S. Silva"
      },
      {
        "name": "D. Voskov"
      },
      {
        "name": "F. Vossepoel"
      }
    ],
    "summary": "This study investigates the integration of machine learning (ML) and data\nassimilation (DA) techniques, focusing on implementing surrogate models for\nGeological Carbon Storage (GCS) projects while maintaining high fidelity\nphysical results in posterior states. Initially, we evaluate the surrogate\nmodeling capability of two distinct machine learning models, Fourier Neural\nOperators (FNOs) and Transformer UNet (T-UNet), in the context of CO$_2$\ninjection simulations within channelized reservoirs. We introduce the\nSurrogate-based hybrid ESMDA (SH-ESMDA), an adaptation of the traditional\nEnsemble Smoother with Multiple Data Assimilation (ESMDA). This method uses\nFNOs and T-UNet as surrogate models and has the potential to make the standard\nESMDA process at least 50% faster or more, depending on the number of\nassimilation steps. Additionally, we introduce Surrogate-based Hybrid RML\n(SH-RML), a variational data assimilation approach that relies on the\nrandomized maximum likelihood (RML) where both the FNO and the T-UNet enable\nthe computation of gradients for the optimization of the objective function,\nand a high-fidelity model is employed for the computation of the posterior\nstates. Our comparative analyses show that SH-RML offers better uncertainty\nquantification compared to conventional ESMDA for the case study.",
    "comment": "29 pages, 20 figures, submited to the International Journal of\n  Greenhouse Gas Control",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "J.2"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06110v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06110v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06110v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06107v1",
    "updated": "2024-02-09T00:01:42+00:00",
    "published": "2024-02-09T00:01:42+00:00",
    "title": "Multiple Instance Learning for Cheating Detection and Localization in Online Examinations",
    "authors": [
      {
        "name": "Yemeng Liu"
      },
      {
        "name": "Jing Ren"
      },
      {
        "name": "Jianshuo Xu"
      },
      {
        "name": "Xiaomei Bai"
      },
      {
        "name": "Roopdeep Kaur"
      },
      {
        "name": "Feng Xia"
      }
    ],
    "summary": "The spread of the Coronavirus disease-2019 epidemic has caused many courses\nand exams to be conducted online. The cheating behavior detection model in\nexamination invigilation systems plays a pivotal role in guaranteeing the\nequality of long-distance examinations. However, cheating behavior is rare, and\nmost researchers do not comprehensively take into account features such as head\nposture, gaze angle, body posture, and background information in the task of\ncheating behavior detection. In this paper, we develop and present CHEESE, a\nCHEating detection framework via multiplE inStancE learning. The framework\nconsists of a label generator that implements weak supervision and a feature\nencoder to learn discriminative features. In addition, the framework combines\nbody posture and background features extracted by 3D convolution with eye gaze,\nhead posture and facial features captured by OpenFace 2.0. These features are\nfed into the spatio-temporal graph module by stitching to analyze the\nspatio-temporal changes in video clips to detect the cheating behaviors. Our\nexperiments on three datasets, UCF-Crime, ShanghaiTech and Online Exam\nProctoring (OEP), prove the effectiveness of our method as compared to the\nstate-of-the-art approaches, and obtain the frame-level AUC score of 87.58% on\nthe OEP dataset.",
    "comment": "12 pages, 7 figures",
    "journal_ref": "IEEE Transactions on Cognitive and Developmental Systems 2024",
    "doi": "10.1109/TCDS.2024.3349705",
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "68T40, 68T45",
      "I.2.10; I.5.4"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/TCDS.2024.3349705",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06107v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06107v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06107v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06104v1",
    "updated": "2024-02-08T23:43:53+00:00",
    "published": "2024-02-08T23:43:53+00:00",
    "title": "Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data",
    "authors": [
      {
        "name": "Dixian Zhu"
      },
      {
        "name": "Livnat Jerby-Arnon"
      }
    ],
    "summary": "Regression is a fundamental task in machine learning that has garnered\nextensive attention over the past decades. The conventional approach for\nregression involves employing loss functions that primarily concentrate on\naligning model prediction with the ground truth for each individual data\nsample, which, as we show, can result in sub-optimal prediction of the\nrelationships between the different samples. Recent research endeavors have\nintroduced novel perspectives by incorporating label similarity information to\nregression. However, a notable gap persists in these approaches when it comes\nto fully capturing the intricacies of the underlying ground truth function. In\nthis work, we propose FAR (Function Aligned Regression) as a arguably better\nand more efficient solution to fit the underlying function of ground truth by\ncapturing functional derivatives. We demonstrate the effectiveness of the\nproposed method practically on 2 synthetic datasets and on 8 extensive\nreal-world tasks from 6 benchmark datasets with other 8 competitive baselines.\nThe code is open-sourced at \\url{https://github.com/DixianZhu/FAR}.",
    "comment": "21 pages excluding references",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06104v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06104v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06104v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06102v1",
    "updated": "2024-02-08T23:35:03+00:00",
    "published": "2024-02-08T23:35:03+00:00",
    "title": "Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning",
    "authors": [
      {
        "name": "Mohak Bhardwaj"
      },
      {
        "name": "Thomas Lampe"
      },
      {
        "name": "Michael Neunert"
      },
      {
        "name": "Francesco Romano"
      },
      {
        "name": "Abbas Abdolmaleki"
      },
      {
        "name": "Arunkumar Byravan"
      },
      {
        "name": "Markus Wulfmeier"
      },
      {
        "name": "Martin Riedmiller"
      },
      {
        "name": "Jonas Buchli"
      }
    ],
    "summary": "Recent advances in real-world applications of reinforcement learning (RL)\nhave relied on the ability to accurately simulate systems at scale. However,\ndomains such as fluid dynamical systems exhibit complex dynamic phenomena that\nare hard to simulate at high integration rates, limiting the direct application\nof modern deep RL algorithms to often expensive or safety critical hardware. In\nthis work, we introduce \"Box o Flows\", a novel benchtop experimental control\nsystem for systematically evaluating RL algorithms in dynamic real-world\nscenarios. We describe the key components of the Box o Flows, and through a\nseries of experiments demonstrate how state-of-the-art model-free RL algorithms\ncan synthesize a variety of complex behaviors via simple reward specifications.\nFurthermore, we explore the role of offline RL in data-efficient hypothesis\ntesting by reusing past experiences. We believe that the insights gained from\nthis preliminary study and the availability of systems like the Box o Flows\nsupport the way forward for developing systematic RL algorithms that can be\ngenerally applied to complex, dynamical systems. Supplementary material and\nvideos of experiments are available at\nhttps://sites.google.com/view/box-o-flows/home.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06102v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06102v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06102v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06098v1",
    "updated": "2024-02-08T23:15:23+00:00",
    "published": "2024-02-08T23:15:23+00:00",
    "title": "Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning",
    "authors": [
      {
        "name": "Jeffrey Sardina"
      },
      {
        "name": "Luca Costabello"
      },
      {
        "name": "Christophe Gu\u00e9ret"
      }
    ],
    "summary": "Knowledge Graphs (KGs) have become increasingly common for representing\nlarge-scale linked data. However, their immense size has required graph\nlearning systems to assist humans in analysis, interpretation, and pattern\ndetection. While there have been promising results for researcher- and\nclinician- empowerment through a variety of KG learning systems, we identify\nfour key deficiencies in state-of-the-art graph learning that simultaneously\nlimit KG learning performance and diminish the ability of humans to interface\noptimally with these learning systems. These deficiencies are: 1) lack of\nexpert knowledge integration, 2) instability to node degree extremity in the\nKG, 3) lack of consideration for uncertainty and relevance while learning, and\n4) lack of explainability. Furthermore, we characterise state-of-the-art\nattempts to solve each of these problems and note that each attempt has largely\nbeen isolated from attempts to solve the other problems. Through a\nformalisation of these problems and a review of the literature that addresses\nthem, we adopt the position that not only are deficiencies in these four key\nareas holding back human-KG empowerment, but that the divide-and-conquer\napproach to solving these problems as individual units rather than a whole is a\nsignificant barrier to the interface between humans and KG learning systems. We\npropose that it is only through integrated, holistic solutions to the\nlimitations of KG learning systems that human and KG learning co-empowerment\nwill be efficiently affected. We finally present our \"Veni, Vidi, Vici\"\nframework that sets a roadmap for effectively and efficiently shifting to a\nholistic co-empowerment model in both the KG learning and the broader machine\nlearning domain.",
    "comment": "This article was accepted for publication at IEEE ICSC 2024, and is\n  being made available as an author preprint. As soon as it is published by\n  IEEE, this registry will be updated in accordance with the IEEE copyright\n  agreement",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06098v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06098v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06098v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06097v1",
    "updated": "2024-02-08T23:12:02+00:00",
    "published": "2024-02-08T23:12:02+00:00",
    "title": "TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models",
    "authors": [
      {
        "name": "Jeffrey Sardina"
      },
      {
        "name": "John D. Kelleher"
      },
      {
        "name": "Declan O'Sullivan"
      }
    ],
    "summary": "In this paper we introduce TWIG (Topologically-Weighted Intelligence\nGeneration), a novel, embedding-free paradigm for simulating the output of KGEs\nthat uses a tiny fraction of the parameters. TWIG learns weights from inputs\nthat consist of topological features of the graph data, with no coding for\nlatent representations of entities or edges. Our experiments on the UMLS\ndataset show that a single TWIG neural network can predict the results of\nstate-of-the-art ComplEx-N3 KGE model nearly exactly on across all\nhyperparameter configurations. To do this it uses a total of 2590 learnable\nparameters, but accurately predicts the results of 1215 different\nhyperparameter combinations with a combined cost of 29,322,000 parameters.\nBased on these results, we make two claims: 1) that KGEs do not learn latent\nsemantics, but only latent representations of structural patterns; 2) that\nhyperparameter choice in KGEs is a deterministic function of the KGE model and\ngraph structure. We further hypothesise that, as TWIG can simulate KGEs without\nembeddings, that node and edge embeddings are not needed to learn to accurately\npredict new facts in KGs. Finally, we formulate all of our findings under the\numbrella of the ``Structural Generalisation Hypothesis\", which suggests that\n``twiggy\" embedding-free / data-structure-based learning methods can allow a\nsingle neural network to simulate KGE performance, and perhaps solve the Link\nPrediction task, across many KGs from diverse domains and with different\nsemantics.",
    "comment": "This article was accepted for publication at IEEE ICSC 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68R10"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06097v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06097v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06097v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06087v1",
    "updated": "2024-02-08T22:39:49+00:00",
    "published": "2024-02-08T22:39:49+00:00",
    "title": "Descriptive Kernel Convolution Network with Improved Random Walk Kernel",
    "authors": [
      {
        "name": "Meng-Chieh Lee"
      },
      {
        "name": "Lingxiao Zhao"
      },
      {
        "name": "Leman Akoglu"
      }
    ],
    "summary": "Graph kernels used to be the dominant approach to feature engineering for\nstructured data, which are superseded by modern GNNs as the former lacks\nlearnability. Recently, a suite of Kernel Convolution Networks (KCNs)\nsuccessfully revitalized graph kernels by introducing learnability, which\nconvolves input with learnable hidden graphs using a certain graph kernel. The\nrandom walk kernel (RWK) has been used as the default kernel in many KCNs,\ngaining increasing attention. In this paper, we first revisit the RWK and its\ncurrent usage in KCNs, revealing several shortcomings of the existing designs,\nand propose an improved graph kernel RWK+, by introducing color-matching random\nwalks and deriving its efficient computation. We then propose RWK+CN, a KCN\nthat uses RWK+ as the core kernel to learn descriptive graph features with an\nunsupervised objective, which can not be achieved by GNNs. Further, by\nunrolling RWK+, we discover its connection with a regular GCN layer, and\npropose a novel GNN layer RWK+Conv. In the first part of experiments, we\ndemonstrate the descriptive learning ability of RWK+CN with the improved random\nwalk kernel RWK+ on unsupervised pattern mining tasks; in the second part, we\nshow the effectiveness of RWK+ for a variety of KCN architectures and\nsupervised graph learning tasks, and demonstrate the expressiveness of RWK+Conv\nlayer, especially on the graph-level tasks. RWK+ and RWK+Conv adapt to various\nreal-world applications, including web applications such as bot detection in a\nweb-scale Twitter social network, and community classification in Reddit social\ninteraction networks.",
    "comment": "WWW 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06087v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06087v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06087v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06086v1",
    "updated": "2024-02-08T22:38:14+00:00",
    "published": "2024-02-08T22:38:14+00:00",
    "title": "Rhizomes to Load Balance Skewed In-Degree Distributions",
    "authors": [
      {
        "name": "Bibrak Qamar Chandio"
      }
    ],
    "summary": "The paper aims to address load imbalance caused by high in-degree\ndistribution in graphs by applying the idea of rhizome to vertex-centric\nmessage-driven graph processing. Rhizome construction of the graph creates\nmultiple named vertex address for any number of single large in-degree\nvertices. It then allows other vertices to point to any of the named addresses\nthus sharing the in-degree load. The rhizomes internally communicate and remain\nconsistent to provide a unified and correct view of the vertex. Simulated\nexperimental results show performance speed ups for BFS graph traversal on\nlarge chip sizes for the tested input graph datasets containing highly skewed\nin-degree distribution. The improvements come from sharing the in-degree\ncompute workload among memory-processing elements and also lowering contention\non the network-on-chip.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DC",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.DS",
      "C.1.4; C.3; C.4; D.1.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06086v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06086v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06086v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06082v1",
    "updated": "2024-02-08T22:17:40+00:00",
    "published": "2024-02-08T22:17:40+00:00",
    "title": "SubGen: Token Generation in Sublinear Time and Memory",
    "authors": [
      {
        "name": "Amir Zandieh"
      },
      {
        "name": "Insu Han"
      },
      {
        "name": "Vahab Mirrokni"
      },
      {
        "name": "Amin Karbasi"
      }
    ],
    "summary": "Despite the significant success of large language models (LLMs), their\nextensive memory requirements pose challenges for deploying them in\nlong-context token generation. The substantial memory footprint of LLM decoders\narises from the necessity to store all previous tokens in the attention module,\na requirement imposed by key-value (KV) caching. In this work, our focus is on\ndeveloping an efficient compression technique for the KV cache. Empirical\nevidence indicates a significant clustering tendency within key embeddings in\nthe attention module. Building on this key insight, we have devised a novel\ncaching method with sublinear complexity, employing online clustering on key\ntokens and online $\\ell_2$ sampling on values. The result is a provably\naccurate and efficient attention decoding algorithm, termed SubGen. Not only\ndoes this algorithm ensure a sublinear memory footprint and sublinear time\ncomplexity, but we also establish a tight error bound for our approach.\nEmpirical evaluations on long-context question-answering tasks demonstrate that\nSubGen significantly outperforms existing and state-of-the-art KV cache\ncompression methods in terms of performance and efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06082v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06082v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06082v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06695v1",
    "updated": "2024-02-08T22:11:21+00:00",
    "published": "2024-02-08T22:11:21+00:00",
    "title": "Integrating LLMs for Explainable Fault Diagnosis in Complex Systems",
    "authors": [
      {
        "name": "Akshay J. Dave"
      },
      {
        "name": "Tat Nghia Nguyen"
      },
      {
        "name": "Richard B. Vilim"
      }
    ],
    "summary": "This paper introduces an integrated system designed to enhance the\nexplainability of fault diagnostics in complex systems, such as nuclear power\nplants, where operator understanding is critical for informed decision-making.\nBy combining a physics-based diagnostic tool with a Large Language Model, we\noffer a novel solution that not only identifies faults but also provides clear,\nunderstandable explanations of their causes and implications. The system's\nefficacy is demonstrated through application to a molten salt facility,\nshowcasing its ability to elucidate the connections between diagnosed faults\nand sensor data, answer operator queries, and evaluate historical sensor\nanomalies. Our approach underscores the importance of merging model-based\ndiagnostics with advanced AI to improve the reliability and transparency of\nautonomous systems.",
    "comment": "4 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06695v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06695v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06695v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06079v1",
    "updated": "2024-02-08T22:06:55+00:00",
    "published": "2024-02-08T22:06:55+00:00",
    "title": "DiscDiff: Latent Diffusion Model for DNA Sequence Generation",
    "authors": [
      {
        "name": "Zehui Li"
      },
      {
        "name": "Yuhao Ni"
      },
      {
        "name": "William A V Beardall"
      },
      {
        "name": "Guoxuan Xia"
      },
      {
        "name": "Akashaditya Das"
      },
      {
        "name": "Guy-Bart Stan"
      },
      {
        "name": "Yiren Zhao"
      }
    ],
    "summary": "This paper introduces a novel framework for DNA sequence generation,\ncomprising two key components: DiscDiff, a Latent Diffusion Model (LDM)\ntailored for generating discrete DNA sequences, and Absorb-Escape, a\npost-training algorithm designed to refine these sequences. Absorb-Escape\nenhances the realism of the generated sequences by correcting `round errors'\ninherent in the conversion process between latent and input spaces. Our\napproach not only sets new standards in DNA sequence generation but also\ndemonstrates superior performance over existing diffusion models, in generating\nboth short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the\nfirst comprehensive, multi-species dataset for DNA generation, encompassing\n160,000 unique sequences from 15 species. We hope this study will advance the\ngenerative modelling of DNA, with potential implications for gene therapy and\nprotein production.",
    "comment": "Different from the prior work \"Latent Diffusion Model for DNA\n  Sequence Generation\" (arXiv:2310.06150), we updated the evaluation framework\n  and compared the DiscDiff with other methods comprehensively. In addition, a\n  post-training framework is proposed to increase the quality of generated\n  sequences",
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.GN",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06079v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06079v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06079v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06078v1",
    "updated": "2024-02-08T22:05:45+00:00",
    "published": "2024-02-08T22:05:45+00:00",
    "title": "Gaussian Mixture Models for Affordance Learning using Bayesian Networks",
    "authors": [
      {
        "name": "Pedro Os\u00f3rio"
      },
      {
        "name": "Alexandre Bernardino"
      },
      {
        "name": "Ruben Martinez-Cantin"
      },
      {
        "name": "Jos\u00e9 Santos-Victor"
      }
    ],
    "summary": "Affordances are fundamental descriptors of relationships between actions,\nobjects and effects. They provide the means whereby a robot can predict\neffects, recognize actions, select objects and plan its behavior according to\ndesired goals. This paper approaches the problem of an embodied agent exploring\nthe world and learning these affordances autonomously from its sensory\nexperiences. Models exist for learning the structure and the parameters of a\nBayesian Network encoding this knowledge. Although Bayesian Networks are\ncapable of dealing with uncertainty and redundancy, previous work considered\ncomplete observability of the discrete sensory data, which may lead to hard\nerrors in the presence of noise. In this paper we consider a probabilistic\nrepresentation of the sensors by Gaussian Mixture Models (GMMs) and explicitly\ntaking into account the probability distribution contained in each discrete\naffordance concept, which can lead to a more correct learning.",
    "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  2010",
    "journal_ref": "Published on the Proceedings of the IEEE/RSJ International\n  Conference on Intelligent Robots and Systems 2010",
    "doi": "10.1109/IROS.2010.5650297",
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/IROS.2010.5650297",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06078v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06078v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06078v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06694v1",
    "updated": "2024-02-08T21:57:10+00:00",
    "published": "2024-02-08T21:57:10+00:00",
    "title": "Scaling Intelligent Agents in Combat Simulations for Wargaming",
    "authors": [
      {
        "name": "Scotty Black"
      },
      {
        "name": "Christian Darken"
      }
    ],
    "summary": "Remaining competitive in future conflicts with technologically-advanced\ncompetitors requires us to accelerate our research and development in\nartificial intelligence (AI) for wargaming. More importantly, leveraging\nmachine learning for intelligent combat behavior development will be key to one\nday achieving superhuman performance in this domain--elevating the quality and\naccelerating the speed of our decisions in future wars. Although deep\nreinforcement learning (RL) continues to show promising results in intelligent\nagent behavior development in games, it has yet to perform at or above the\nhuman level in the long-horizon, complex tasks typically found in combat\nmodeling and simulation. Capitalizing on the proven potential of RL and recent\nsuccesses of hierarchical reinforcement learning (HRL), our research is\ninvestigating and extending the use of HRL to create intelligent agents capable\nof performing effectively in these large and complex simulation environments.\nOur ultimate goal is to develop an agent capable of superhuman performance that\ncould then serve as an AI advisor to military planners and decision-makers.\nThis papers covers our ongoing approach and the first three of our five\nresearch areas aimed at managing the exponential growth of computations that\nhave thus far limited the use of AI in combat simulations: (1) developing an\nHRL training framework and agent architecture for combat units; (2) developing\na multi-model framework for agent decision-making; (3) developing\ndimension-invariant observation abstractions of the state space to manage the\nexponential growth of computations; (4) developing an intrinsic rewards engine\nto enable long-term planning; and (5) implementing this framework into a\nhigher-fidelity combat simulation.",
    "comment": "arXiv admin note: text overlap with arXiv:2402.06075",
    "journal_ref": "I/ITSEC Conference Proceedings 2023",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06694v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06694v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06694v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06075v1",
    "updated": "2024-02-08T21:51:07+00:00",
    "published": "2024-02-08T21:51:07+00:00",
    "title": "Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making",
    "authors": [
      {
        "name": "Scotty Black"
      },
      {
        "name": "Christian Darken"
      }
    ],
    "summary": "In this unprecedented era of technology-driven transformation, it becomes\nmore critical than ever that we aggressively invest in developing robust\nartificial intelligence (AI) for wargaming in support of decision-making. By\nadvancing AI-enabled systems and pairing these with human judgment, we will be\nable to enhance all-domain awareness, improve the speed and quality of our\ndecision cycles, offer recommendations for novel courses of action, and more\nrapidly counter our adversary's actions. It therefore becomes imperative that\nwe accelerate the development of AI to help us better address the complexity of\nmodern challenges and dilemmas that currently requires human intelligence and,\nif possible, attempt to surpass human intelligence--not to replace humans, but\nto augment and better inform human decision-making at machine speed. Although\ndeep reinforcement learning continues to show promising results in intelligent\nagent behavior development for the long-horizon, complex tasks typically found\nin combat modeling and simulation, further research is needed to enable the\nscaling of AI to deal with these intricate and expansive state-spaces\ncharacteristic of wargaming for either concept development, education, or\nanalysis. To help address this challenge, in our research, we are developing\nand implementing a hierarchical reinforcement learning framework that includes\na multi-model approach and dimension-invariant observation abstractions.",
    "comment": null,
    "journal_ref": "NATO STO-MP-MSG-207 2023",
    "doi": "10.14339/STO-MP-MSG-207-23-PDF",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.14339/STO-MP-MSG-207-23-PDF",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06075v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06075v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06075v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06063v1",
    "updated": "2024-02-08T21:19:16+00:00",
    "published": "2024-02-08T21:19:16+00:00",
    "title": "3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging",
    "authors": [
      {
        "name": "Andrew H. Proppe"
      },
      {
        "name": "Guillaume Thekkadath"
      },
      {
        "name": "Duncan England"
      },
      {
        "name": "Philip J. Bustard"
      },
      {
        "name": "Fr\u00e9d\u00e9ric Bouchard"
      },
      {
        "name": "Jeff S. Lundeen"
      },
      {
        "name": "Benjamin J. Sussman"
      }
    ],
    "summary": "In recent years, neural networks have been used to solve phase retrieval\nproblems in imaging with superior accuracy and speed than traditional\ntechniques, especially in the presence of noise. However, in the context of\ninterferometric imaging, phase noise has been largely unaddressed by existing\nneural network architectures. Such noise arises naturally in an interferometer\ndue to mechanical instabilities or atmospheric turbulence, limiting measurement\nacquisition times and posing a challenge in scenarios with limited light\nintensity, such as remote sensing. Here, we introduce a 3D-2D Phase Retrieval\nU-Net (PRUNe) that takes noisy and randomly phase-shifted interferograms as\ninputs, and outputs a single 2D phase image. A 3D downsampling convolutional\nencoder captures correlations within and between frames to produce a 2D latent\nspace, which is upsampled by a 2D decoder into a phase image. We test our model\nagainst a state-of-the-art singular value decomposition algorithm and find\nPRUNe reconstructions consistently show more accurate and smooth\nreconstructions, with a x2.5 - 4 lower mean squared error at multiple\nsignal-to-noise ratios for interferograms with low (< 1 photon/pixel) and high\n(~100 photons/pixel) signal intensity. Our model presents a faster and more\naccurate approach to perform phase retrieval in extremely low light intensity\ninterferometry in presence of phase noise, and will find application in other\nmulti-frame noisy imaging techniques.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.optics",
    "categories": [
      "physics.optics",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06063v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06063v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06063v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06059v1",
    "updated": "2024-02-08T21:03:34+00:00",
    "published": "2024-02-08T21:03:34+00:00",
    "title": "Impact on Public Health Decision Making by Utilizing Big Data Without Domain Knowledge",
    "authors": [
      {
        "name": "Miao Zhang"
      },
      {
        "name": "Salman Rahman"
      },
      {
        "name": "Vishwali Mhasawade"
      },
      {
        "name": "Rumi Chunara"
      }
    ],
    "summary": "New data sources, and artificial intelligence (AI) methods to extract\ninformation from them are becoming plentiful, and relevant to decision making\nin many societal applications. An important example is street view imagery,\navailable in over 100 countries, and considered for applications such as\nassessing built environment aspects in relation to community health outcomes.\nRelevant to such uses, important examples of bias in the use of AI are evident\nwhen decision-making based on data fails to account for the robustness of the\ndata, or predictions are based on spurious correlations. To study this risk, we\nutilize 2.02 million GSV images along with health, demographic, and\nsocioeconomic data from New York City. Initially, we demonstrate that built\nenvironment characteristics inferred from GSV labels at the intra-city level\nmay exhibit inadequate alignment with the ground truth. We also find that the\naverage individual-level behavior of physical inactivity significantly mediates\nthe impact of built environment features by census tract, as measured through\nGSV. Finally, using a causal framework which accounts for these mediators of\nenvironmental impacts on health, we find that altering 10% of samples in the\ntwo lowest tertiles would result in a 4.17 (95% CI 3.84 to 4.55) or 17.2 (95%\nCI 14.4 to 21.3) times bigger decrease on the prevalence of obesity or\ndiabetes, than the same proportional intervention on the number of crosswalks\nby census tract. This work illustrates important issues of robustness and model\nspecification for informing effective allocation of interventions using new\ndata sources.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CY",
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06059v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06059v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06059v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06056v1",
    "updated": "2024-02-08T20:57:10+00:00",
    "published": "2024-02-08T20:57:10+00:00",
    "title": "ActiveDP: Bridging Active Learning and Data Programming",
    "authors": [
      {
        "name": "Naiqing Guan"
      },
      {
        "name": "Nick Koudas"
      }
    ],
    "summary": "Modern machine learning models require large labelled datasets to achieve\ngood performance, but manually labelling large datasets is expensive and\ntime-consuming. The data programming paradigm enables users to label large\ndatasets efficiently but produces noisy labels, which deteriorates the\ndownstream model's performance. The active learning paradigm, on the other\nhand, can acquire accurate labels but only for a small fraction of instances.\nIn this paper, we propose ActiveDP, an interactive framework bridging active\nlearning and data programming together to generate labels with both high\naccuracy and coverage, combining the strengths of both paradigms. Experiments\nshow that ActiveDP outperforms previous weak supervision and active learning\napproaches and consistently performs well under different labelling budgets.",
    "comment": "accepted by EDBT 2024 research track",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06056v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06056v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06056v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06053v1",
    "updated": "2024-02-08T20:49:09+00:00",
    "published": "2024-02-08T20:49:09+00:00",
    "title": "Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models",
    "authors": [
      {
        "name": "Thomas Sandholm"
      },
      {
        "name": "Sayandev Mukherjee"
      },
      {
        "name": "Bernardo A. Huberman"
      }
    ],
    "summary": "We present a novel approach to exploring innovation problem and solution\ndomains using LLM fine-tuning with a custom idea database. By semantically\ntraversing the bi-directional problem and solution tree at different\ntemperature levels we achieve high diversity in solution edit distance while\nstill remaining close to the original problem statement semantically. In\naddition to finding a variety of solutions to a given problem, this method can\nalso be used to refine and clarify the original problem statement. As further\nvalidation of the approach, we implemented a proof-of-concept Slack bot to\nserve as an innovation assistant.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06053v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06053v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06053v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06047v1",
    "updated": "2024-02-08T20:38:35+00:00",
    "published": "2024-02-08T20:38:35+00:00",
    "title": "Intelligent Mode-switching Framework for Teleoperation",
    "authors": [
      {
        "name": "Burak Kizilkaya"
      },
      {
        "name": "Changyang She"
      },
      {
        "name": "Guodong Zhao"
      },
      {
        "name": "Muhammad Ali Imran"
      }
    ],
    "summary": "Teleoperation can be very difficult due to limited perception, high\ncommunication latency, and limited degrees of freedom (DoFs) at the operator\nside. Autonomous teleoperation is proposed to overcome this difficulty by\npredicting user intentions and performing some parts of the task autonomously\nto decrease the demand on the operator and increase the task completion rate.\nHowever, decision-making for mode-switching is generally assumed to be done by\nthe operator, which brings an extra DoF to be controlled by the operator and\nintroduces extra mental demand. On the other hand, the communication\nperspective is not investigated in the current literature, although\ncommunication imperfections and resource limitations are the main bottlenecks\nfor teleoperation. In this study, we propose an intelligent mode-switching\nframework by jointly considering mode-switching and communication systems. User\nintention recognition is done at the operator side. Based on user intention\nrecognition, a deep reinforcement learning (DRL) agent is trained and deployed\nat the operator side to seamlessly switch between autonomous and teleoperation\nmodes. A real-world data set is collected from our teleoperation testbed to\ntrain both user intention recognition and DRL algorithms. Our results show that\nthe proposed framework can achieve up to 50% communication load reduction with\nimproved task completion probability.",
    "comment": "Accepted by the 2024 IEEE International Conference on Robotics and\n  Automation (ICRA)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG",
      "cs.NI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06047v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06047v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06047v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06046v1",
    "updated": "2024-02-08T20:37:51+00:00",
    "published": "2024-02-08T20:37:51+00:00",
    "title": "Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap",
    "authors": [
      {
        "name": "Philip Koopman"
      }
    ],
    "summary": "An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San\nFrancisco resulted not only in a severe injury, but also dramatic upheaval at\nthat company that will likely have lasting effects throughout the industry. The\nissues stem not just from the crash facts themselves, but also how Cruise\nmishandled dealing with their robotaxi dragging a pedestrian under the vehicle\nafter the initial post-crash stop. A pair of external investigation reports\nprovide raw material describing the incident and critique the company response\nfrom a regulatory interaction point of view, but did not include potential\nsafety recommendations in scope. We use that report material to highlight\nspecific facts and relationships between events by tying together different\npieces of the report material. We then explore safety lessons that might be\nlearned with regard to technology, operational safety practices, and\norganizational reaction to incidents.",
    "comment": "17 pages, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06046v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06046v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06046v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06045v1",
    "updated": "2024-02-08T20:36:21+00:00",
    "published": "2024-02-08T20:36:21+00:00",
    "title": "Direct Acquisition Optimization for Low-Budget Active Learning",
    "authors": [
      {
        "name": "Zhuokai Zhao"
      },
      {
        "name": "Yibo Jiang"
      },
      {
        "name": "Yuxin Chen"
      }
    ],
    "summary": "Active Learning (AL) has gained prominence in integrating data-intensive\nmachine learning (ML) models into domains with limited labeled data. However,\nits effectiveness diminishes significantly when the labeling budget is low. In\nthis paper, we first empirically observe the performance degradation of\nexisting AL algorithms in the low-budget settings, and then introduce Direct\nAcquisition Optimization (DAO), a novel AL algorithm that optimizes sample\nselections based on expected true loss reduction. Specifically, DAO utilizes\ninfluence functions to update model parameters and incorporates an additional\nacquisition strategy to mitigate bias in loss estimation. This approach\nfacilitates a more accurate estimation of the overall error reduction, without\nextensive computations or reliance on labeled data. Experiments demonstrate\nDAO's effectiveness in low budget settings, outperforming state-of-the-arts\napproaches across seven benchmarks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06045v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06045v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06045v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06044v2",
    "updated": "2024-02-14T13:23:51+00:00",
    "published": "2024-02-08T20:35:06+00:00",
    "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
    "authors": [
      {
        "name": "Hainiu Xu"
      },
      {
        "name": "Runcong Zhao"
      },
      {
        "name": "Lixing Zhu"
      },
      {
        "name": "Jinhua Du"
      },
      {
        "name": "Yulan He"
      }
    ],
    "summary": "Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track\nof the mental states of others, is pivotal in developing socially intelligent\nagents. However, prevalent N-ToM benchmarks have several shortcomings,\nincluding the presence of ambiguous and artificial narratives, absence of\npersonality traits and preferences, a lack of questions addressing characters'\npsychological mental states, and limited diversity in the questions posed. In\nresponse to these issues, we construct OpenToM, a new benchmark for assessing\nN-ToM with (1) longer and clearer narrative stories, (2) characters with\nexplicit personality traits, (3) actions that are triggered by character\nintentions, and (4) questions designed to challenge LLMs' capabilities of\nmodeling characters' mental states of both the physical and psychological\nworld. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling\ncertain aspects of mental states in the physical world but fall short when\ntracking characters' mental states in the psychological world.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06044v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06044v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06044v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06038v1",
    "updated": "2024-02-08T20:20:54+00:00",
    "published": "2024-02-08T20:20:54+00:00",
    "title": "Contrastive Approach to Prior Free Positive Unlabeled Learning",
    "authors": [
      {
        "name": "Anish Acharya"
      },
      {
        "name": "Sujay Sanghavi"
      }
    ],
    "summary": "Positive Unlabeled (PU) learning refers to the task of learning a binary\nclassifier given a few labeled positive samples, and a set of unlabeled samples\n(which could be positive or negative). In this paper, we propose a novel PU\nlearning framework, that starts by learning a feature space through\npretext-invariant representation learning and then applies pseudo-labeling to\nthe unlabeled examples, leveraging the concentration property of the\nembeddings. Overall, our proposed approach handily outperforms state-of-the-art\nPU learning methods across several standard PU benchmark datasets, while not\nrequiring a-priori knowledge or estimate of class prior. Remarkably, our method\nremains effective even when labeled data is scant, where most PU learning\nalgorithms falter. We also provide simple theoretical analysis motivating our\nproposed algorithms and establish generalization guarantee for our approach.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06038v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06038v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06038v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06692v1",
    "updated": "2024-02-08T20:14:46+00:00",
    "published": "2024-02-08T20:14:46+00:00",
    "title": "HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation",
    "authors": [
      {
        "name": "Hrishav Bakul Barua"
      },
      {
        "name": "Ganesh Krishnasamy"
      },
      {
        "name": "KokSheik Wong"
      },
      {
        "name": "Abhinav Dhall"
      },
      {
        "name": "Kalin Stefanov"
      }
    ],
    "summary": "High Dynamic Range (HDR) imaging aims to replicate the high visual quality\nand clarity of real-world scenes. Due to the high costs associated with HDR\nimaging, the literature offers various data-driven methods for HDR image\nreconstruction from Low Dynamic Range (LDR) counterparts. A common limitation\nof these approaches is missing details in regions of the reconstructed HDR\nimages, which are over- or under-exposed in the input LDR images. To this end,\nwe propose a simple and effective method, HistoHDR-Net, to recover the fine\ndetails (e.g., color, contrast, saturation, and brightness) of HDR images via a\nfusion-based approach utilizing histogram-equalized LDR images along with\nself-attention guidance. Our experiments demonstrate the efficacy of the\nproposed approach over the state-of-art methods.",
    "comment": "Submitted to IEEE",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.MM",
      "Artificial intelligence, Computer vision, Machine learning, Deep\n  learning",
      "I.3.3; I.4.5"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06692v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06692v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06692v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06034v1",
    "updated": "2024-02-08T20:14:35+00:00",
    "published": "2024-02-08T20:14:35+00:00",
    "title": "Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent",
    "authors": [
      {
        "name": "Haoyu Yang"
      },
      {
        "name": "Anthony Agnesina"
      },
      {
        "name": "Haoxing Ren"
      }
    ],
    "summary": "Exploding predictive AI has enabled fast yet effective evaluation and\ndecision-making in modern chip physical design flows. State-of-the-art\nframeworks typically include the objective of minimizing the mean square error\n(MSE) between the prediction and the ground truth. We argue the averaging\neffect of MSE induces limitations in both model training and deployment, and\ngood MSE behavior does not guarantee the capability of these models to assist\nphysical design flows which are likely sabotaged due to a small portion of\nprediction error. To address this, we propose mini-pixel batch gradient descent\n(MPGD), a plug-and-play optimization algorithm that takes the most informative\nentries into consideration, offering probably faster and better convergence.\nExperiments on representative benchmark suits show the significant benefits of\nMPGD on various physical design prediction tasks using CNN or Graph-based\nmodels.",
    "comment": "7 pages, 2 figures, preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06034v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06034v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06034v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06033v2",
    "updated": "2024-02-12T10:22:26+00:00",
    "published": "2024-02-08T20:12:47+00:00",
    "title": "An Inexact Halpern Iteration with Application to Distributionally Robust Optimization",
    "authors": [
      {
        "name": "Ling Liang"
      },
      {
        "name": "Kim-Chuan Toh"
      },
      {
        "name": "Jia-Jie Zhu"
      }
    ],
    "summary": "The Halpern iteration for solving monotone inclusion problems has gained\nincreasing interests in recent years due to its simple form and appealing\nconvergence properties. In this paper, we investigate the inexact variants of\nthe scheme in both deterministic and stochastic settings. We conduct extensive\nconvergence analysis and show that by choosing the inexactness tolerances\nappropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in\nterms of the (expected) residue norm. Our results relax the state-of-the-art\ninexactness conditions employed in the literature while sharing the same\ncompetitive convergence properties. We then demonstrate how the proposed\nmethods can be applied for solving two classes of data-driven Wasserstein\ndistributionally robust optimization problems that admit convex-concave min-max\noptimization reformulations. We highlight its capability of performing inexact\ncomputations for distributionally robust learning with stochastic first-order\nmethods.",
    "comment": "Correct a typo in the title and update authors' information",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06033v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06033v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06033v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06031v1",
    "updated": "2024-02-08T20:07:47+00:00",
    "published": "2024-02-08T20:07:47+00:00",
    "title": "An operator learning perspective on parameter-to-observable maps",
    "authors": [
      {
        "name": "Daniel Zhengyu Huang"
      },
      {
        "name": "Nicholas H. Nelsen"
      },
      {
        "name": "Margaret Trautner"
      }
    ],
    "summary": "Computationally efficient surrogates for parametrized physical models play a\ncrucial role in science and engineering. Operator learning provides data-driven\nsurrogates that map between function spaces. However, instead of full-field\nmeasurements, often the available data are only finite-dimensional\nparametrizations of model inputs or finite observables of model outputs.\nBuilding off of Fourier Neural Operators, this paper introduces the Fourier\nNeural Mappings (FNMs) framework that is able to accommodate such\nfinite-dimensional inputs and outputs. The paper develops universal\napproximation theorems for the method. Moreover, in many applications the\nunderlying parameter-to-observable (PtO) map is defined implicitly through an\ninfinite-dimensional operator, such as the solution operator of a partial\ndifferential equation. A natural question is whether it is more data-efficient\nto learn the PtO map end-to-end or first learn the solution operator and\nsubsequently compute the observable from the full-field solution. A theoretical\nanalysis of Bayesian nonparametric regression of linear functionals, which is\nof independent interest, suggests that the end-to-end approach can actually\nhave worse sample complexity. Extending beyond the theory, numerical results\nfor the FNM approximation of three nonlinear PtO maps demonstrate the benefits\nof the operator learning perspective that this paper adopts.",
    "comment": "58 pages, 9 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T07, 62G20, 65J15"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06031v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06031v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06031v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06030v1",
    "updated": "2024-02-08T20:07:43+00:00",
    "published": "2024-02-08T20:07:43+00:00",
    "title": "Game-theoretic Counterfactual Explanation for Graph Neural Networks",
    "authors": [
      {
        "name": "Chirag Chhablani"
      },
      {
        "name": "Sarthak Jain"
      },
      {
        "name": "Akshay Channesh"
      },
      {
        "name": "Ian A. Kash"
      },
      {
        "name": "Sourav Medya"
      }
    ],
    "summary": "Graph Neural Networks (GNNs) have been a powerful tool for node\nclassification tasks in complex networks. However, their decision-making\nprocesses remain a black-box to users, making it challenging to understand the\nreasoning behind their predictions. Counterfactual explanations (CFE) have\nshown promise in enhancing the interpretability of machine learning models.\nPrior approaches to compute CFE for GNNS often are learning-based approaches\nthat require training additional graphs. In this paper, we propose a\nsemivalue-based, non-learning approach to generate CFE for node classification\ntasks, eliminating the need for any additional training. Our results reveals\nthat computing Banzhaf values requires lower sample complexity in identifying\nthe counterfactual explanations compared to other popular methods such as\ncomputing Shapley values. Our empirical evidence indicates computing Banzhaf\nvalues can achieve up to a fourfold speed up compared to Shapley values. We\nalso design a thresholding method for computing Banzhaf values and show\ntheoretical and empirical results on its robustness in noisy environments,\nmaking it superior to Shapley values. Furthermore, the thresholded Banzhaf\nvalues are shown to enhance efficiency without compromising the quality (i.e.,\nfidelity) in the explanations in three popular graph datasets.",
    "comment": "Accepted to WWW 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06030v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06030v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06030v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06026v1",
    "updated": "2024-02-08T19:57:57+00:00",
    "published": "2024-02-08T19:57:57+00:00",
    "title": "Quantum neural network with ensemble learning to mitigate barren plateaus and cost function concentration",
    "authors": [
      {
        "name": "Lucas Friedrich"
      },
      {
        "name": "Jonas Maziero"
      }
    ],
    "summary": "The rapid development of quantum computers promises transformative impacts\nacross diverse fields of science and technology. Quantum neural networks\n(QNNs), as a forefront application, hold substantial potential. Despite the\nmultitude of proposed models in the literature, persistent challenges, notably\nthe vanishing gradient (VG) and cost function concentration (CFC) problems,\nimpede their widespread success. In this study, we introduce a novel approach\nto quantum neural network construction, specifically addressing the issues of\nVG and CFC. Our methodology employs ensemble learning, advocating for the\nsimultaneous deployment of multiple quantum circuits with a depth equal to $1$,\na departure from the conventional use of a single quantum circuit with depth\n$L$. We assess the efficacy of our proposed model through a comparative\nanalysis with a conventionally constructed QNN. The evaluation unfolds in the\ncontext of a classification problem, yielding valuable insights into the\npotential advantages of our innovative approach.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "quant-ph",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06026v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06026v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06026v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06025v1",
    "updated": "2024-02-08T19:57:29+00:00",
    "published": "2024-02-08T19:57:29+00:00",
    "title": "Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning",
    "authors": [
      {
        "name": "Top Piriyakulkij"
      },
      {
        "name": "Kevin Ellis"
      }
    ],
    "summary": "We build a computational model of how humans actively infer hidden rules by\ndoing experiments. The basic principles behind the model is that, even if the\nrule is deterministic, the learner considers a broader space of fuzzy\nprobabilistic rules, which it represents in natural language, and updates its\nhypotheses online after each experiment according to approximately Bayesian\nprinciples. In the same framework we also model experiment design according to\ninformation-theoretic criteria. We find that the combination of these three\nprinciples -- explicit hypotheses, probabilistic rules, and online updates --\ncan explain human performance on a Zendo-style task, and that removing any of\nthese components leaves the model unable to account for the data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06025v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06025v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06025v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06023v1",
    "updated": "2024-02-08T19:47:34+00:00",
    "published": "2024-02-08T19:47:34+00:00",
    "title": "Decision Theory-Guided Deep Reinforcement Learning for Fast Learning",
    "authors": [
      {
        "name": "Zelin Wan"
      },
      {
        "name": "Jin-Hee Cho"
      },
      {
        "name": "Mu Zhu"
      },
      {
        "name": "Ahmed H. Anwar"
      },
      {
        "name": "Charles Kamhoua"
      },
      {
        "name": "Munindar P. Singh"
      }
    ],
    "summary": "This paper introduces a novel approach, Decision Theory-guided Deep\nReinforcement Learning (DT-guided DRL), to address the inherent cold start\nproblem in DRL. By integrating decision theory principles, DT-guided DRL\nenhances agents' initial performance and robustness in complex environments,\nenabling more efficient and reliable convergence during learning. Our\ninvestigation encompasses two primary problem contexts: the cart pole and maze\nnavigation challenges. Experimental results demonstrate that the integration of\ndecision theory not only facilitates effective initial guidance for DRL agents\nbut also promotes a more structured and informed exploration strategy,\nparticularly in environments characterized by large and intricate state spaces.\nThe results of experiment demonstrate that DT-guided DRL can provide\nsignificantly higher rewards compared to regular DRL. Specifically, during the\ninitial phase of training, the DT-guided DRL yields up to an 184% increase in\naccumulated reward. Moreover, even after reaching convergence, it maintains a\nsuperior performance, ending with up to 53% more reward than standard DRL in\nlarge maze problems. DT-guided DRL represents an advancement in mitigating a\nfundamental challenge of DRL by leveraging functions informed by human\n(designer) knowledge, setting a foundation for further research in this\npromising interdisciplinary domain.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06023v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06023v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06023v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06019v1",
    "updated": "2024-02-08T19:41:38+00:00",
    "published": "2024-02-08T19:41:38+00:00",
    "title": "Checking the Sufficiently Scattered Condition using a Global Non-Convex Optimization Software",
    "authors": [
      {
        "name": "Nicolas Gillis"
      },
      {
        "name": "Robert Luce"
      }
    ],
    "summary": "The sufficiently scattered condition (SSC) is a key condition in the study of\nidentifiability of various matrix factorization problems, including\nnonnegative, minimum-volume, symmetric, simplex-structured, and polytopic\nmatrix factorizations. The SSC allows one to guarantee that the computed matrix\nfactorization is unique/identifiable, up to trivial ambiguities. However, this\ncondition is NP-hard to check in general. In this paper, we show that it can\nhowever be checked in a reasonable amount of time in realistic scenarios, when\nthe factorization rank is not too large. This is achieved by formulating the\nproblem as a non-convex quadratic optimization problem over a bounded set. We\nuse the global non-convex optimization software Gurobi, and showcase the\nusefulness of this code on synthetic data sets and on real-world hyperspectral\nimages.",
    "comment": "14 pages, code available from https://gitlab.com/ngillis/check-ssc",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "eess.SP",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06019v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06019v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06019v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07940v1",
    "updated": "2024-02-08T19:21:33+00:00",
    "published": "2024-02-08T19:21:33+00:00",
    "title": "LLMs Among Us: Generative AI Participating in Digital Discourse",
    "authors": [
      {
        "name": "Kristina Radivojevic"
      },
      {
        "name": "Nicholas Clark"
      },
      {
        "name": "Paul Brenner"
      }
    ],
    "summary": "The emergence of Large Language Models (LLMs) has great potential to reshape\nthe landscape of many social media platforms. While this can bring promising\nopportunities, it also raises many threats, such as biases and privacy\nconcerns, and may contribute to the spread of propaganda by malicious actors.\nWe developed the \"LLMs Among Us\" experimental framework on top of the Mastodon\nsocial media platform for bot and human participants to communicate without\nknowing the ratio or nature of bot and human participants. We built 10 personas\nwith three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three\nrounds of the experiment and surveyed participants after each round to measure\nthe ability of LLMs to pose as human participants without human detection. We\nfound that participants correctly identified the nature of other users in the\nexperiment only 42% of the time despite knowing the presence of both bots and\nhumans. We also found that the choice of persona had substantially more impact\non human perception than the choice of mainstream LLMs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07940v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07940v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07940v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06010v1",
    "updated": "2024-02-08T19:12:33+00:00",
    "published": "2024-02-08T19:12:33+00:00",
    "title": "NPSVC++: Nonparallel Classifiers Encounter Representation Learning",
    "authors": [
      {
        "name": "Junhong Zhang"
      },
      {
        "name": "Zhihui Lai"
      },
      {
        "name": "Jie Zhou"
      },
      {
        "name": "Guangfei Liang"
      }
    ],
    "summary": "This paper focuses on a specific family of classifiers called nonparallel\nsupport vector classifiers (NPSVCs). Different from typical classifiers, the\ntraining of an NPSVC involves the minimization of multiple objectives,\nresulting in the potential concerns of feature suboptimality and class\ndependency. Consequently, no effective learning scheme has been established to\nimprove NPSVCs' performance through representation learning, especially deep\nlearning. To break this bottleneck, we develop NPSVC++ based on multi-objective\noptimization, enabling the end-to-end learning of NPSVC and its features. By\npursuing Pareto optimality, NPSVC++ theoretically ensures feature optimality\nacross classes, hence effectively overcoming the two issues above. A general\nlearning procedure via duality optimization is proposed, based on which we\nprovide two applicable instances, K-NPSVC++ and D-NPSVC++. The experiments show\ntheir superiority over the existing methods and verify the efficacy of NPSVC++.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06010v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06010v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06010v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06004v1",
    "updated": "2024-02-08T19:01:14+00:00",
    "published": "2024-02-08T19:01:14+00:00",
    "title": "Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy",
    "authors": [
      {
        "name": "Seyedarmin Azizi"
      },
      {
        "name": "Mahdi Nazemi"
      },
      {
        "name": "Massoud Pedram"
      }
    ],
    "summary": "As Vision Transformers (ViTs) increasingly set new benchmarks in computer\nvision, their practical deployment on inference engines is often hindered by\ntheir significant memory bandwidth and (on-chip) memory footprint requirements.\nThis paper addresses this memory limitation by introducing an activation-aware\nmodel compression methodology that uses selective low-rank weight tensor\napproximations of different layers to reduce the parameter count of ViTs. The\nkey idea is to decompose the weight tensors into a sum of two\nparameter-efficient tensors while minimizing the error between the product of\nthe input activations with the original weight tensor and the product of the\ninput activations with the approximate tensor sum. This approximation is\nfurther refined by adopting an efficient layer-wise error compensation\ntechnique that uses the gradient of the layer's output loss. The combination of\nthese techniques achieves excellent results while it avoids being trapped in a\nshallow local minimum early in the optimization process and strikes a good\nbalance between the model compression and output accuracy. Notably, the\npresented method significantly reduces the parameter count of DeiT-B by 60%\nwith less than 1% accuracy drop on the ImageNet dataset, overcoming the usual\naccuracy degradation seen in low-rank approximations. In addition to this, the\npresented compression technique can compress large DeiT/ViT models to have\nabout the same model size as smaller DeiT/ViT variants while yielding up to\n1.8% accuracy gain. These results highlight the efficacy of our approach,\npresenting a viable solution for embedding ViTs in memory-constrained\nenvironments without compromising their performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06004v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06004v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06004v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05935v1",
    "updated": "2024-02-08T18:59:48+00:00",
    "published": "2024-02-08T18:59:48+00:00",
    "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
    "authors": [
      {
        "name": "Peng Gao"
      },
      {
        "name": "Renrui Zhang"
      },
      {
        "name": "Chris Liu"
      },
      {
        "name": "Longtian Qiu"
      },
      {
        "name": "Siyuan Huang"
      },
      {
        "name": "Weifeng Lin"
      },
      {
        "name": "Shitian Zhao"
      },
      {
        "name": "Shijie Geng"
      },
      {
        "name": "Ziyi Lin"
      },
      {
        "name": "Peng Jin"
      },
      {
        "name": "Kaipeng Zhang"
      },
      {
        "name": "Wenqi Shao"
      },
      {
        "name": "Chao Xu"
      },
      {
        "name": "Conghui He"
      },
      {
        "name": "Junjun He"
      },
      {
        "name": "Hao Shao"
      },
      {
        "name": "Pan Lu"
      },
      {
        "name": "Hongsheng Li"
      },
      {
        "name": "Yu Qiao"
      }
    ],
    "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
    "comment": "Code and models are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05935v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05935v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05935v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05934v1",
    "updated": "2024-02-08T18:59:30+00:00",
    "published": "2024-02-08T18:59:30+00:00",
    "title": "Classifying Nodes in Graphs without GNNs",
    "authors": [
      {
        "name": "Daniel Winter"
      },
      {
        "name": "Niv Cohen"
      },
      {
        "name": "Yedid Hoshen"
      }
    ],
    "summary": "Graph neural networks (GNNs) are the dominant paradigm for classifying nodes\nin a graph, but they have several undesirable attributes stemming from their\nmessage passing architecture. Recently, distillation methods succeeded in\neliminating the use of GNNs at test time but they still require them during\ntraining. We perform a careful analysis of the role that GNNs play in\ndistillation methods. This analysis leads us to propose a fully GNN-free\napproach for node classification, not requiring them at train or test time. Our\nmethod consists of three key components: smoothness constraints,\npseudo-labeling iterations and neighborhood-label histograms. Our final\napproach can match the state-of-the-art accuracy on standard popular benchmarks\nsuch as citation and co-purchase networks, without training a GNN.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05934v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05934v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05934v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05933v1",
    "updated": "2024-02-08T18:59:05+00:00",
    "published": "2024-02-08T18:59:05+00:00",
    "title": "Time Series Diffusion in the Frequency Domain",
    "authors": [
      {
        "name": "Jonathan Crabb\u00e9"
      },
      {
        "name": "Nicolas Huynh"
      },
      {
        "name": "Jan Stanczuk"
      },
      {
        "name": "Mihaela van der Schaar"
      }
    ],
    "summary": "Fourier analysis has been an instrumental tool in the development of signal\nprocessing. This leads us to wonder whether this framework could similarly\nbenefit generative modelling. In this paper, we explore this question through\nthe scope of time series diffusion models. More specifically, we analyze\nwhether representing time series in the frequency domain is a useful inductive\nbias for score-based diffusion models. By starting from the canonical SDE\nformulation of diffusion in the time domain, we show that a dual diffusion\nprocess occurs in the frequency domain with an important nuance: Brownian\nmotions are replaced by what we call mirrored Brownian motions, characterized\nby mirror symmetries among their components. Building on this insight, we show\nhow to adapt the denoising score matching approach to implement diffusion\nmodels in the frequency domain. This results in frequency diffusion models,\nwhich we compare to canonical time diffusion models. Our empirical evaluation\non real-world datasets, covering various domains like healthcare and finance,\nshows that frequency diffusion models better capture the training distribution\nthan time diffusion models. We explain this observation by showing that time\nseries from these datasets tend to be more localized in the frequency domain\nthan in the time domain, which makes them easier to model in the former case.\nAll our observations point towards impactful synergies between Fourier analysis\nand diffusion models.",
    "comment": "27 pages, 12 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05933v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05933v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05933v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05932v1",
    "updated": "2024-02-08T18:59:03+00:00",
    "published": "2024-02-08T18:59:03+00:00",
    "title": "Driving Everywhere with Large Language Model Policy Adaptation",
    "authors": [
      {
        "name": "Boyi Li"
      },
      {
        "name": "Yue Wang"
      },
      {
        "name": "Jiageng Mao"
      },
      {
        "name": "Boris Ivanovic"
      },
      {
        "name": "Sushant Veer"
      },
      {
        "name": "Karen Leung"
      },
      {
        "name": "Marco Pavone"
      }
    ],
    "summary": "Adapting driving behavior to new environments, customs, and laws is a\nlong-standing problem in autonomous driving, precluding the widespread\ndeployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a\nsimple yet powerful tool that enables human drivers and autonomous vehicles\nalike to drive everywhere by adapting their tasks and motion plans to traffic\nrules in new locations. LLaDA achieves this by leveraging the impressive\nzero-shot generalizability of large language models (LLMs) in interpreting the\ntraffic rules in the local driver handbook. Through an extensive user study, we\nshow that LLaDA's instructions are useful in disambiguating in-the-wild\nunexpected situations. We also demonstrate LLaDA's ability to adapt AV motion\nplanning policies in real-world datasets; LLaDA outperforms baseline planning\napproaches on all our metrics. Please check our website for more details:\nhttps://boyiliee.github.io/llada.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05932v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05932v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05932v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05929v1",
    "updated": "2024-02-08T18:58:02+00:00",
    "published": "2024-02-08T18:58:02+00:00",
    "title": "An Interactive Agent Foundation Model",
    "authors": [
      {
        "name": "Zane Durante"
      },
      {
        "name": "Bidipta Sarkar"
      },
      {
        "name": "Ran Gong"
      },
      {
        "name": "Rohan Taori"
      },
      {
        "name": "Yusuke Noda"
      },
      {
        "name": "Paul Tang"
      },
      {
        "name": "Ehsan Adeli"
      },
      {
        "name": "Shrinidhi Kowshika Lakshmikanth"
      },
      {
        "name": "Kevin Schulman"
      },
      {
        "name": "Arnold Milstein"
      },
      {
        "name": "Demetri Terzopoulos"
      },
      {
        "name": "Ade Famoti"
      },
      {
        "name": "Noboru Kuno"
      },
      {
        "name": "Ashley Llorens"
      },
      {
        "name": "Hoi Vo"
      },
      {
        "name": "Katsu Ikeuchi"
      },
      {
        "name": "Li Fei-Fei"
      },
      {
        "name": "Jianfeng Gao"
      },
      {
        "name": "Naoki Wake"
      },
      {
        "name": "Qiuyuan Huang"
      }
    ],
    "summary": "The development of artificial intelligence systems is transitioning from\ncreating static, task-specific models to dynamic, agent-based systems capable\nof performing well in a wide range of applications. We propose an Interactive\nAgent Foundation Model that uses a novel multi-task agent training paradigm for\ntraining AI agents across a wide range of domains, datasets, and tasks. Our\ntraining paradigm unifies diverse pre-training strategies, including visual\nmasked auto-encoders, language modeling, and next-action prediction, enabling a\nversatile and adaptable AI framework. We demonstrate the performance of our\nframework across three separate domains -- Robotics, Gaming AI, and Healthcare.\nOur model demonstrates its ability to generate meaningful and contextually\nrelevant outputs in each area. The strength of our approach lies in its\ngenerality, leveraging a variety of data sources such as robotics sequences,\ngameplay data, large-scale video datasets, and textual information for\neffective multimodal and multi-task learning. Our approach provides a promising\navenue for developing generalist, action-taking, multimodal systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05929v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05929v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05929v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05930v1",
    "updated": "2024-02-08T18:58:02+00:00",
    "published": "2024-02-08T18:58:02+00:00",
    "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
    "authors": [
      {
        "name": "Xing Han L\u00f9"
      },
      {
        "name": "Zden\u011bk Kasner"
      },
      {
        "name": "Siva Reddy"
      }
    ],
    "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05930v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05930v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05930v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05928v1",
    "updated": "2024-02-08T18:57:42+00:00",
    "published": "2024-02-08T18:57:42+00:00",
    "title": "Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss",
    "authors": [
      {
        "name": "Ingvar Ziemann"
      },
      {
        "name": "Stephen Tu"
      },
      {
        "name": "George J. Pappas"
      },
      {
        "name": "Nikolai Matni"
      }
    ],
    "summary": "In this work, we study statistical learning with dependent ($\\beta$-mixing)\ndata and square loss in a hypothesis class $\\mathscr{F}\\subset L_{\\Psi_p}$\nwhere $\\Psi_p$ is the norm $\\|f\\|_{\\Psi_p} \\triangleq \\sup_{m\\geq 1} m^{-1/p}\n\\|f\\|_{L^m} $ for some $p\\in [2,\\infty]$. Our inquiry is motivated by the\nsearch for a sharp noise interaction term, or variance proxy, in learning with\ndependent data. Absent any realizability assumption, typical non-asymptotic\nresults exhibit variance proxies that are deflated \\emph{multiplicatively} by\nthe mixing time of the underlying covariates process. We show that whenever the\ntopologies of $L^2$ and $\\Psi_p$ are comparable on our hypothesis class\n$\\mathscr{F}$ -- that is, $\\mathscr{F}$ is a weakly sub-Gaussian class:\n$\\|f\\|_{\\Psi_p} \\lesssim \\|f\\|_{L^2}^\\eta$ for some $\\eta\\in (0,1]$ -- the\nempirical risk minimizer achieves a rate that only depends on the complexity of\nthe class and second order statistics in its leading term. Our result holds\nwhether the problem is realizable or not and we refer to this as a \\emph{near\nmixing-free rate}, since direct dependence on mixing is relegated to an\nadditive higher order term. We arrive at our result by combining the above\nnotion of a weakly sub-Gaussian class with mixed tail generic chaining. This\ncombination allows us to compute sharp, instance-optimal rates for a wide range\nof problems. %Our approach, reliant on mixed tail generic chaining, allows us\nto obtain sharp, instance-optimal rates. Examples that satisfy our framework\ninclude sub-Gaussian linear regression, more general smoothly parameterized\nfunction classes, finite hypothesis classes, and bounded smoothness classes.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05928v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05928v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05928v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05926v1",
    "updated": "2024-02-08T18:56:40+00:00",
    "published": "2024-02-08T18:56:40+00:00",
    "title": "On the Convergence of Zeroth-Order Federated Tuning in Large Language Models",
    "authors": [
      {
        "name": "Zhenqing Ling"
      },
      {
        "name": "Daoyuan Chen"
      },
      {
        "name": "Liuyi Yao"
      },
      {
        "name": "Yaliang Li"
      },
      {
        "name": "Ying Shen"
      }
    ],
    "summary": "The confluence of Federated Learning (FL) and Large Language Models (LLMs) is\nushering in a new era in privacy-preserving natural language processing.\nHowever, the intensive memory requirements for fine-tuning LLMs pose\nsignificant challenges, especially when deploying on edge devices with limited\ncomputational resources. To circumvent this, we explore the novel integration\nof Memory-efficient Zeroth-Order Optimization within a federated setting, a\nsynergy we denote as FedMeZO. Our study is the first to examine the theoretical\nunderpinnings of FedMeZO in the context of LLMs, tackling key questions\nregarding the influence of large parameter spaces on optimization behavior, the\nestablishment of convergence properties, and the identification of critical\nparameters for convergence to inform personalized federated strategies. Our\nextensive empirical evidence supports the theory, showing that FedMeZO not only\nconverges faster than traditional first-order methods such as SGD but also\nsignificantly reduces GPU memory usage during training to levels comparable to\nthose during inference. Moreover, the proposed personalized FL strategy that is\nbuilt upon the theoretical insights to customize the client-wise learning rate\ncan effectively accelerate loss reduction. We hope our work can help to bridge\ntheoretical and practical aspects of federated fine-tuning for LLMs and\nfacilitate further development and research.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05926v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05926v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05926v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05916v1",
    "updated": "2024-02-08T18:51:55+00:00",
    "published": "2024-02-08T18:51:55+00:00",
    "title": "GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory",
    "authors": [
      {
        "name": "David D. Baek"
      },
      {
        "name": "Ziming Liu"
      },
      {
        "name": "Max Tegmark"
      }
    ],
    "summary": "We present GenEFT: an effective theory framework for shedding light on the\nstatics and dynamics of neural network generalization, and illustrate it with\ngraph learning examples. We first investigate the generalization phase\ntransition as data size increases, comparing experimental results with\ninformation-theory-based approximations. We find generalization in a Goldilocks\nzone where the decoder is neither too weak nor too powerful. We then introduce\nan effective theory for the dynamics of representation learning, where\nlatent-space representations are modeled as interacting particles (repons), and\nfind that it explains our experimentally observed phase transition between\ngeneralization and overfitting as encoder and decoder learning rates are\nscanned. This highlights the power of physics-inspired effective theories for\nbridging the gap between theoretical predictions and practice in machine\nlearning.",
    "comment": "12 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05916v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05916v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05916v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05913v1",
    "updated": "2024-02-08T18:49:09+00:00",
    "published": "2024-02-08T18:49:09+00:00",
    "title": "Efficient Stagewise Pretraining via Progressive Subnetworks",
    "authors": [
      {
        "name": "Abhishek Panigrahi"
      },
      {
        "name": "Nikunj Saunshi"
      },
      {
        "name": "Kaifeng Lyu"
      },
      {
        "name": "Sobhan Miryoosefi"
      },
      {
        "name": "Sashank Reddi"
      },
      {
        "name": "Satyen Kale"
      },
      {
        "name": "Sanjiv Kumar"
      }
    ],
    "summary": "Recent developments in large language models have sparked interest in\nefficient pretraining methods. A recent effective paradigm is to perform\nstage-wise training, where the size of the model is gradually increased over\nthe course of training (e.g. gradual stacking (Reddi et al., 2023)). While the\nresource and wall-time savings are appealing, it has limitations, particularly\nthe inability to evaluate the full model during earlier stages, and degradation\nin model quality due to smaller model capacity in the initial stages. In this\nwork, we propose an alternative framework, progressive subnetwork training,\nthat maintains the full model throughout training, but only trains subnetworks\nwithin the model in each step. We focus on a simple instantiation of this\nframework, Random Path Training (RaPTr) that only trains a sub-path of layers\nin each step, progressively increasing the path lengths in stages. RaPTr\nachieves better pre-training loss for BERT and UL2 language models while\nrequiring 20-33% fewer FLOPs compared to standard training, and is competitive\nor better than other efficient training methods. Furthermore, RaPTr shows\nbetter downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%\ncompared to standard training and stacking. Finally, we provide a theoretical\nbasis for RaPTr to justify (a) the increasing complexity of subnetworks in\nstages, and (b) the stability in loss across stage transitions due to residual\nconnections and layer norm.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05913v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05913v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05913v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05906v1",
    "updated": "2024-02-08T18:43:27+00:00",
    "published": "2024-02-08T18:43:27+00:00",
    "title": "Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games",
    "authors": [
      {
        "name": "Hafez Ghaemi"
      },
      {
        "name": "Hamed Kebriaei"
      },
      {
        "name": "Alireza Ramezani Moghaddam"
      },
      {
        "name": "Majid Nili Ahamdabadi"
      }
    ],
    "summary": "Classical multi-agent reinforcement learning (MARL) assumes risk neutrality\nand complete objectivity for agents. However, in settings where agents need to\nconsider or model human economic or social preferences, a notion of risk must\nbe incorporated into the RL optimization problem. This will be of greater\nimportance in MARL where other human or non-human agents are involved, possibly\nwith their own risk-sensitive policies. In this work, we consider\nrisk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),\na non-convex risk measure and a generalization of coherent measures of risk.\nCPT is capable of explaining loss aversion in humans and their tendency to\noverestimate/underestimate small/large probabilities. We propose a distributed\nsampling-based actor-critic (AC) algorithm with CPT risk for network\naggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.\nUnder a set of assumptions, we prove the convergence of the algorithm to a\nsubjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental\nresults show that subjective CPT policies obtained by our algorithm can be\ndifferent from the risk-neutral ones, and agents with a higher loss aversion\nare more inclined to socially isolate themselves in an NAMG.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "I.2.6; I.2.11"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05906v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05906v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05906v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05902v2",
    "updated": "2024-02-10T03:19:39+00:00",
    "published": "2024-02-08T18:41:41+00:00",
    "title": "ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation",
    "authors": [
      {
        "name": "Aimee Guo"
      },
      {
        "name": "Gace Fei"
      },
      {
        "name": "Hemanth Pasupuletic"
      },
      {
        "name": "Jing Wang"
      }
    ],
    "summary": "The newly released Segment Anything Model (SAM) is a popular tool used in\nimage processing due to its superior segmentation accuracy, variety of input\nprompts, training capabilities, and efficient model design. However, its\ncurrent model is trained on a diverse dataset not tailored to medical images,\nparticularly ultrasound images. Ultrasound images tend to have a lot of noise,\nmaking it difficult to segment out important structures. In this project, we\ndeveloped ClickSAM, which fine-tunes the Segment Anything Model using click\nprompts for ultrasound images. ClickSAM has two stages of training: the first\nstage is trained on single-click prompts centered in the ground-truth contours,\nand the second stage focuses on improving the model performance through\nadditional positive and negative click prompts. By comparing the first stage\npredictions to the ground-truth masks, true positive, false positive, and false\nnegative segments are calculated. Positive clicks are generated using the true\npositive and false negative segments, and negative clicks are generated using\nthe false positive segments. The Centroidal Voronoi Tessellation algorithm is\nthen employed to collect positive and negative click prompts in each segment\nthat are used to enhance the model performance during the second stage of\ntraining. With click-train methods, ClickSAM exhibits superior performance\ncompared to other existing models for ultrasound image segmentation.",
    "comment": "6 pages, 2 figures, SPIE Medical Imaging Conference 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05902v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05902v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05902v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05894v2",
    "updated": "2024-02-09T08:08:57+00:00",
    "published": "2024-02-08T18:33:21+00:00",
    "title": "Large Language Model Meets Graph Neural Network in Knowledge Distillation",
    "authors": [
      {
        "name": "Shengxiang Hu"
      },
      {
        "name": "Guobing Zou"
      },
      {
        "name": "Song Yang"
      },
      {
        "name": "Yanglan Gan"
      },
      {
        "name": "Bofeng Zhang"
      },
      {
        "name": "Yixin Chen"
      }
    ],
    "summary": "Despite recent community revelations about the advancements and potential\napplications of Large Language Models (LLMs) in understanding Text-Attributed\nGraph (TAG), the deployment of LLMs for production is hindered by its high\ncomputational and storage requirements, as well as long latencies during model\ninference. Simultaneously, although traditional Graph Neural Networks (GNNs)\nare light weight and adept at learning structural features of graphs, their\nability to grasp the complex semantics in TAG is somewhat constrained for real\napplications. To address these limitations, we concentrate on the downstream\ntask of node classification in TAG and propose a novel graph knowledge\ndistillation framework, termed Linguistic Graph Knowledge Distillation\n(LinguGKD), using LLMs as teacher models and GNNs as student models for\nknowledge distillation. It involves TAG-oriented instruction tuning of LLM on\ndesigned tailored prompts, followed by propagating knowledge and aligning the\nhierarchically learned node features from the teacher LLM to the student GNN in\nlatent space, employing a layer-adaptive contrastive learning strategy. Through\nextensive experiments on a variety of LLM and GNN models and multiple benchmark\ndatasets, the proposed LinguGKD significantly boosts the student GNN's\npredictive accuracy and convergence rate, without the need of extra data or\nmodel parameters. Compared to teacher LLM, distilled GNN achieves superior\ninference speed equipped with much fewer computing and storage demands, when\nsurpassing the teacher LLM's classification accuracy on some of benchmark\ndatasets.",
    "comment": "17 pages, 6 figures, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T30, 68R10, 68T05"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05894v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05894v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05894v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05889v1",
    "updated": "2024-02-08T18:27:22+00:00",
    "published": "2024-02-08T18:27:22+00:00",
    "title": "CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion",
    "authors": [
      {
        "name": "Shoubin Yu"
      },
      {
        "name": "Jaehong Yoon"
      },
      {
        "name": "Mohit Bansal"
      }
    ],
    "summary": "Despite impressive advancements in multimodal compositional reasoning\napproaches, they are still limited in their flexibility and efficiency by\nprocessing fixed modality inputs while updating a lot of model parameters. This\npaper tackles these critical challenges and proposes CREMA, an efficient and\nmodular modality-fusion framework for injecting any new modality into video\nreasoning. We first augment multiple informative modalities (such as optical\nflow, 3D point cloud, audio) from given videos without extra human annotation\nby leveraging existing pre-trained models. Next, we introduce a query\ntransformer with multiple parameter-efficient modules associated with each\naccessible modality. It projects diverse modality features to the LLM token\nembedding space, allowing the model to integrate different data types for\nresponse generation. Furthermore, we propose a fusion module designed to\ncompress multimodal queries, maintaining computational efficiency in the LLM\nwhile combining additional modalities. We validate our method on video-3D,\nvideo-audio, and video-language reasoning tasks and achieve better/equivalent\nperformance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and\nSeViLA while using 96% fewer trainable parameters. We provide extensive\nanalyses of CREMA, including the impact of each modality on reasoning domains,\nthe design of the fusion module, and example visualizations.",
    "comment": "project page: https://CREMA-VideoLLM.github.io/",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05889v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05889v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05889v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05885v1",
    "updated": "2024-02-08T18:23:05+00:00",
    "published": "2024-02-08T18:23:05+00:00",
    "title": "EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance",
    "authors": [
      {
        "name": "Aditya Bommakanti"
      },
      {
        "name": "Harshith Reddy Vonteri"
      },
      {
        "name": "Sayan Ranu"
      },
      {
        "name": "Panagiotis Karras"
      }
    ],
    "summary": "The need to identify graphs having small structural distance from a query\narises in biology, chemistry, recommender systems, and social network analysis.\nAmong several methods to measure inter graph distance, Graph Edit Distance\n(GED) is preferred for its comprehensibility, yet hindered by the NP-hardness\nof its computation. State-of-the-art GED approximations predominantly employ\nneural methods, which, however, (i) lack an explanatory edit path corresponding\nto the approximated GED; (ii) require the NP-hard generation of ground-truth\nGEDs for training; and (iii) necessitate separate training on each dataset. In\nthis paper, we propose an efficient algebraic unsuper vised method, EUGENE,\nthat approximates GED and yields edit paths corresponding to the approx imated\ncost, while eliminating the need for ground truth generation and data-specific\ntraining. Extensive experimental evaluation demonstrates that the\naforementioned benefits of EUGENE do not come at the cost of efficacy.\nSpecifically, EUGENE consistently ranks among the most accurate methods across\nall of the benchmark datasets and outperforms majority of the neural\napproaches.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05885v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05885v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05885v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05880v2",
    "updated": "2024-02-10T17:03:58+00:00",
    "published": "2024-02-08T18:14:33+00:00",
    "title": "Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking",
    "authors": [
      {
        "name": "Nikhil Sharma"
      },
      {
        "name": "Q. Vera Liao"
      },
      {
        "name": "Ziang Xiao"
      }
    ],
    "summary": "Large language models (LLMs) powered conversational search systems have\nalready been used by hundreds of millions of people, and are believed to bring\nmany benefits over conventional search. However, while decades of research and\npublic discourse interrogated the risk of search systems in increasing\nselective exposure and creating echo chambers -- limiting exposure to diverse\nopinions and leading to opinion polarization, little is known about such a risk\nof LLM-powered conversational search. We conduct two experiments to\ninvestigate: 1) whether and how LLM-powered conversational search increases\nselective exposure compared to conventional search; 2) whether and how LLMs\nwith opinion biases that either reinforce or challenge the user's view change\nthe effect. Overall, we found that participants engaged in more biased\ninformation querying with LLM-powered conversational search, and an opinionated\nLLM reinforcing their views exacerbated this bias. These results present\ncritical implications for the development of LLMs and conversational search\nsystems, and the policy governing these technologies.",
    "comment": "Accepted in CHI'24. Supplementary material will be available online\n  with the official submission in CHI 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05880v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05880v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05880v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05878v1",
    "updated": "2024-02-08T18:13:26+00:00",
    "published": "2024-02-08T18:13:26+00:00",
    "title": "Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits",
    "authors": [
      {
        "name": "Nicolas Nguyen"
      },
      {
        "name": "Imad Aouali"
      },
      {
        "name": "Andr\u00e1s Gy\u00f6rgy"
      },
      {
        "name": "Claire Vernade"
      }
    ],
    "summary": "We study the problem of Bayesian fixed-budget best-arm identification (BAI)\nin structured bandits. We propose an algorithm that uses fixed allocations\nbased on the prior information and the structure of the environment. We provide\ntheoretical bounds on its performance across diverse models, including the\nfirst prior-dependent upper bounds for linear and hierarchical BAI. Our key\ncontribution is introducing new proof methods that result in tighter bounds for\nmulti-armed BAI compared to existing methods. We extensively compare our\napproach to other fixed-budget BAI methods, demonstrating its consistent and\nrobust performance in various settings. Our work improves our understanding of\nBayesian fixed-budget BAI in structured bandits and highlights the\neffectiveness of our approach in practical scenarios.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05878v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05878v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05878v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05876v1",
    "updated": "2024-02-08T18:09:17+00:00",
    "published": "2024-02-08T18:09:17+00:00",
    "title": "Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices",
    "authors": [
      {
        "name": "Jiin Woo"
      },
      {
        "name": "Laixi Shi"
      },
      {
        "name": "Gauri Joshi"
      },
      {
        "name": "Yuejie Chi"
      }
    ],
    "summary": "Offline reinforcement learning (RL), which seeks to learn an optimal policy\nusing offline data, has garnered significant interest due to its potential in\ncritical applications where online data collection is infeasible or expensive.\nThis work explores the benefit of federated learning for offline RL, aiming at\ncollaboratively leveraging offline datasets at multiple agents. Focusing on\nfinite-horizon episodic tabular Markov decision processes (MDPs), we design\nFedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for\nfederated offline RL. FedLCB-Q updates local Q-functions at agents with novel\nlearning rate schedules and aggregates them at a central server using\nimportance averaging and a carefully designed pessimistic penalty term. Our\nsample complexity analysis reveals that, with appropriately chosen parameters\nand synchronization schedules, FedLCB-Q achieves linear speedup in terms of the\nnumber of agents without requiring high-quality datasets at individual agents,\nas long as the local datasets collectively cover the state-action space visited\nby the optimal policy, highlighting the power of collaboration in the federated\nsetting. In fact, the sample complexity almost matches that of the single-agent\ncounterpart, as if all the data are stored at a central location, up to\npolynomial factors of the horizon length. Furthermore, FedLCB-Q is\ncommunication-efficient, where the number of communication rounds is only\nlinear with respect to the horizon length up to logarithmic factors.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.MA",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05876v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05876v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05876v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05868v2",
    "updated": "2024-02-12T16:26:14+00:00",
    "published": "2024-02-08T17:57:11+00:00",
    "title": "EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models",
    "authors": [
      {
        "name": "Guo Lin"
      },
      {
        "name": "Wenyue Hua"
      },
      {
        "name": "Yongfeng Zhang"
      }
    ],
    "summary": "Cloud-based large language models (LLMs) such as ChatGPT have increasingly\nbecome integral to daily operations, serving as vital tools across various\napplications. While these models offer substantial benefits in terms of\naccessibility and functionality, they also introduce significant privacy\nconcerns: the transmission and storage of user data in cloud infrastructures\npose substantial risks of data breaches and unauthorized access to sensitive\ninformation; even if the transmission and storage of data is encrypted, the LLM\nservice provider itself still knows the real contents of the data, preventing\nindividuals or entities from confidently using such LLM services. To address\nthese concerns, this paper proposes a simple yet effective mechanism EmojiCrypt\nto protect user privacy. It uses Emoji to encrypt the user inputs before\nsending them to LLM, effectively rendering them indecipherable to human or\nLLM's examination while retaining the original intent of the prompt, thus\nensuring the model's performance remains unaffected. We conduct experiments on\nthree tasks, personalized recommendation, sentiment analysis, and tabular data\nanalysis. Experiment results reveal that EmojiCrypt can encrypt personal\ninformation within prompts in such a manner that not only prevents the\ndiscernment of sensitive data by humans or LLM itself, but also maintains or\neven improves the precision without further tuning, achieving comparable or\neven better task accuracy than directly prompting the LLM without prompt\nencryption. These results highlight the practicality of adopting encryption\nmeasures that safeguard user privacy without compromising the functional\nintegrity and performance of LLMs. Code and dataset are available at\nhttps://github.com/agiresearch/EmojiCrypt.",
    "comment": "12 pages, 4 figures, 2 tables, comments and suggestions are welcome",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05868v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05868v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05868v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05864v1",
    "updated": "2024-02-08T17:54:23+00:00",
    "published": "2024-02-08T17:54:23+00:00",
    "title": "Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs",
    "authors": [
      {
        "name": "Xuandong Zhao"
      },
      {
        "name": "Lei Li"
      },
      {
        "name": "Yu-Xiang Wang"
      }
    ],
    "summary": "In this paper, we propose a new decoding method called Permute-and-Flip (PF)\ndecoder. It enjoys robustness properties similar to the standard sampling\ndecoder, but is provably up to 2x better in its quality-robustness tradeoff\nthan sampling and never worse than any other decoder. We also design a\ncryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but\nnaturally tailored for PF decoder. The watermarking scheme does not change the\ndistribution to sample, while allowing arbitrarily low false positive rate and\nhigh recall whenever the generated text has high entropy. Our experiments show\nthat the PF decoder (and its watermarked counterpart) significantly\noutperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms\nof perplexity, while retaining the same robustness (and detectability), hence\nmaking it a promising new approach for LLM decoding. The code is available at\nhttps://github.com/XuandongZhao/pf-decoding",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05864v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05864v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05864v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05863v1",
    "updated": "2024-02-08T17:51:48+00:00",
    "published": "2024-02-08T17:51:48+00:00",
    "title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis",
    "authors": [
      {
        "name": "Federico Bianchi"
      },
      {
        "name": "Patrick John Chia"
      },
      {
        "name": "Mert Yuksekgonul"
      },
      {
        "name": "Jacopo Tagliabue"
      },
      {
        "name": "Dan Jurafsky"
      },
      {
        "name": "James Zou"
      }
    ],
    "summary": "Negotiation is the basis of social interactions; humans negotiate everything\nfrom the price of cars to how to share common resources. With rapidly growing\ninterest in using large language models (LLMs) to act as agents on behalf of\nhuman users, such LLM agents would also need to be able to negotiate. In this\npaper, we study how well LLMs can negotiate with each other. We develop\nNegotiationArena: a flexible framework for evaluating and probing the\nnegotiation abilities of LLM agents. We implemented three types of scenarios in\nNegotiationArena to assess LLM's behaviors in allocating shared resources\n(ultimatum games), aggregate resources (trading games) and buy/sell goods\n(price negotiations). Each scenario allows for multiple turns of flexible\ndialogues between LLM agents to allow for more complex negotiations.\nInterestingly, LLM agents can significantly boost their negotiation outcomes by\nemploying certain behavioral tactics. For example, by pretending to be desolate\nand desperate, LLMs can improve their payoffs by 20\\% when negotiating against\nthe standard GPT-4. We also quantify irrational negotiation behaviors exhibited\nby the LLM agents, many of which also appear in humans. Together,\n\\NegotiationArena offers a new environment to investigate LLM interactions,\nenabling new insights into LLM's theory of mind, irrationality, and reasoning\nabilities.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05863v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05863v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05863v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05862v1",
    "updated": "2024-02-08T17:51:44+00:00",
    "published": "2024-02-08T17:51:44+00:00",
    "title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
    "authors": [
      {
        "name": "Bryan Perozzi"
      },
      {
        "name": "Bahare Fatemi"
      },
      {
        "name": "Dustin Zelle"
      },
      {
        "name": "Anton Tsitsulin"
      },
      {
        "name": "Mehran Kazemi"
      },
      {
        "name": "Rami Al-Rfou"
      },
      {
        "name": "Jonathan Halcrow"
      }
    ],
    "summary": "How can we best encode structured data into sequential form for use in large\nlanguage models (LLMs)? In this work, we introduce a parameter-efficient method\nto explicitly represent structured data for LLMs. Our method, GraphToken,\nlearns an encoding function to extend prompts with explicit structured\ninformation. Unlike other work which focuses on limited domains (e.g. knowledge\ngraph representation), our work is the first effort focused on the general\nencoding of structured data to be used for various reasoning tasks. We show\nthat explicitly representing the graph structure allows significant\nimprovements to graph reasoning tasks. Specifically, we see across the board\nimprovements - up to 73% points - on node, edge and, graph-level tasks from the\nGraphQA benchmark.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ML",
      "I.5.1; I.2.6; I.2.7"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05862v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05862v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05862v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05859v1",
    "updated": "2024-02-08T17:43:22+00:00",
    "published": "2024-02-08T17:43:22+00:00",
    "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "authors": [
      {
        "name": "Mohammed Muqeeth"
      },
      {
        "name": "Haokun Liu"
      },
      {
        "name": "Yufan Liu"
      },
      {
        "name": "Colin Raffel"
      }
    ],
    "summary": "Recently, there has been a widespread proliferation of \"expert\" language\nmodels that are specialized to a specific task or domain through\nparameter-efficient fine-tuning. How can we recycle large collections of expert\nlanguage models to improve zero-shot generalization to unseen tasks? In this\nwork, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of\nSpecialized Experts (PHATGOOSE), which learns to route among specialized\nmodules that were produced through parameter-efficient fine-tuning. Unlike past\nmethods that learn to route among specialized models, PHATGOOSE explores the\npossibility that zero-shot generalization will be improved if different experts\ncan be adaptively chosen for each token and at each layer in the model.\nCrucially, our method is post-hoc - it does not require simultaneous access to\nthe datasets used to create the specialized models and only requires a modest\namount of additional compute after each expert model is trained. In experiments\ncovering a range of specialized model collections and zero-shot generalization\nbenchmarks, we find that PHATGOOSE outperforms past methods for post-hoc\nrouting and, in some cases, outperforms explicit multitask training (which\nrequires simultaneous data access). To better understand the routing strategy\nlearned by PHATGOOSE, we perform qualitative experiments to validate that\nPHATGOOSE's performance stems from its ability to make adaptive per-token and\nper-module expert choices. We release all of our code to support future work on\nimproving zero-shot generalization by recycling specialized experts.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05859v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05859v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05859v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05841v1",
    "updated": "2024-02-08T17:18:01+00:00",
    "published": "2024-02-08T17:18:01+00:00",
    "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
    "authors": [
      {
        "name": "Hannes Stark"
      },
      {
        "name": "Bowen Jing"
      },
      {
        "name": "Chenyu Wang"
      },
      {
        "name": "Gabriele Corso"
      },
      {
        "name": "Bonnie Berger"
      },
      {
        "name": "Regina Barzilay"
      },
      {
        "name": "Tommi Jaakkola"
      }
    ],
    "summary": "Discrete diffusion or flow models could enable faster and more controllable\nsequence generation than autoregressive models. We show that na\\\"ive linear\nflow matching on the simplex is insufficient toward this goal since it suffers\nfrom discontinuities in the training target and further pathologies. To\novercome this, we develop Dirichlet flow matching on the simplex based on\nmixtures of Dirichlet distributions as probability paths. In this framework, we\nderive a connection between the mixtures' scores and the flow's vector field\nthat allows for classifier and classifier-free guidance. Further, we provide\ndistilled Dirichlet flow matching, which enables one-step sequence generation\nwith minimal performance hits, resulting in $O(L)$ speedups compared to\nautoregressive models. On complex DNA sequence generation tasks, we demonstrate\nsuperior performance compared to all baselines in distributional metrics and in\nachieving desired design targets for generated sequences. Finally, we show that\nour classifier-free guidance approach improves unconditional generation and is\neffective for generating DNA that satisfies design targets. Code is available\nat https://github.com/HannesStark/dirichlet-flow-matching.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.BM",
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05841v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05841v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05841v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05835v1",
    "updated": "2024-02-08T17:12:49+00:00",
    "published": "2024-02-08T17:12:49+00:00",
    "title": "How Much is Unseen Depends Chiefly on Information About the Seen",
    "authors": [
      {
        "name": "Seongmin Lee"
      },
      {
        "name": "Marcel B\u00f6hme"
      }
    ],
    "summary": "It might seem counter-intuitive at first: We find that, in expectation, the\nproportion of data points in an unknown population-that belong to classes that\ndo not appear in the training data-is almost entirely determined by the number\n$f_k$ of classes that do appear in the training data the same number of times.\nWhile in theory we show that the difference of the induced estimator decays\nexponentially in the size of the sample, in practice the high variance prevents\nus from using it directly for an estimator of the sample coverage. However, our\nprecise characterization of the dependency between $f_k$'s induces a large\nsearch space of different representations of the expected value, which can be\ndeterministically instantiated as estimators. Hence, we turn to optimization\nand develop a genetic algorithm that, given only the sample, searches for an\nestimator with minimal mean-squared error (MSE). In our experiments, our\ngenetic algorithm discovers estimators that have a substantially smaller MSE\nthan the state-of-the-art Good-Turing estimator. This holds for over 96% of\nruns when there are at least as many samples as classes. Our estimators' MSE is\nroughly 80% of the Good-Turing estimator's.",
    "comment": "8 pages with 5 pages of appendix, 5 figures, 3 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05835v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05835v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05835v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06690v1",
    "updated": "2024-02-08T17:10:12+00:00",
    "published": "2024-02-08T17:10:12+00:00",
    "title": "Neural Models for Source Code Synthesis and Completion",
    "authors": [
      {
        "name": "Mitodru Niyogi"
      }
    ],
    "summary": "Natural language (NL) to code suggestion systems assist developers in\nIntegrated Development Environments (IDEs) by translating NL utterances into\ncompilable code snippet. The current approaches mainly involve hard-coded,\nrule-based systems based on semantic parsing. These systems make heavy use of\nhand-crafted rules that map patterns in NL or elements in its syntax parse tree\nto various query constructs and can only work on a limited subset of NL with a\nrestricted NL syntax. These systems are unable to extract semantic information\nfrom the coding intents of the developer, and often fail to infer types, names,\nand the context of the source code to get accurate system-level code\nsuggestions. In this master thesis, we present sequence-to-sequence deep\nlearning models and training paradigms to map NL to general-purpose programming\nlanguages that can assist users with suggestions of source code snippets, given\na NL intent, and also extend auto-completion functionality of the source code\nto users while they are writing source code. The developed architecture\nincorporates contextual awareness into neural models which generate source code\ntokens directly instead of generating parse trees/abstract meaning\nrepresentations from the source code and converting them back to source code.\nThe proposed pretraining strategy and the data augmentation techniques improve\nthe performance of the proposed architecture. The proposed architecture has\nbeen found to exceed the performance of a neural semantic parser, TranX, based\non the BLEU-4 metric by 10.82%. Thereafter, a finer analysis for the parsable\ncode translations from the NL intent for CoNaLA challenge was introduced. The\nproposed system is bidirectional as it can be also used to generate NL code\ndocumentation given source code. Lastly, a RoBERTa masked language model for\nPython was proposed to extend the developed system for code completion.",
    "comment": "Master thesis submitted to University of Heidelberg, Germany on 30th\n  July, 2021",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG",
      "cs.PL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06690v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06690v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06690v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05830v1",
    "updated": "2024-02-08T17:09:12+00:00",
    "published": "2024-02-08T17:09:12+00:00",
    "title": "Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting",
    "authors": [
      {
        "name": "Yanjun Zhao"
      },
      {
        "name": "Tian Zhou"
      },
      {
        "name": "Chao Chen"
      },
      {
        "name": "Liang Sun"
      },
      {
        "name": "Yi Qian"
      },
      {
        "name": "Rong Jin"
      }
    ],
    "summary": "Time series analysis is vital for numerous applications, and transformers\nhave become increasingly prominent in this domain. Leading methods customize\nthe transformer architecture from NLP and CV, utilizing a patching technique to\nconvert continuous signals into segments. Yet, time series data are uniquely\nchallenging due to significant distribution shifts and intrinsic noise levels.\nTo address these two challenges,we introduce the Sparse Vector Quantized\nFFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse\nvector quantization technique coupled with Reverse Instance Normalization\n(RevIN) to reduce noise impact and capture sufficient statistics for\nforecasting, serving as an alternative to the Feed-Forward layer (FFN) in the\ntransformer architecture. Our FFN-free approach trims the parameter count,\nenhancing computational efficiency and reducing overfitting. Through\nevaluations across ten benchmark datasets, including the newly introduced CAISO\ndataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in\nMAE for univariate and multivariate time series forecasting, respectively.\nMoreover, it can be seamlessly integrated with existing transformer-based\nmodels to elevate their performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05830v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05830v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05830v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05829v1",
    "updated": "2024-02-08T17:08:08+00:00",
    "published": "2024-02-08T17:08:08+00:00",
    "title": "Limitations of Agents Simulated by Predictive Models",
    "authors": [
      {
        "name": "Raymond Douglas"
      },
      {
        "name": "Jacek Karwowski"
      },
      {
        "name": "Chan Bae"
      },
      {
        "name": "Andis Draguns"
      },
      {
        "name": "Victoria Krakovna"
      }
    ],
    "summary": "There is increasing focus on adapting predictive models into agent-like\nsystems, most notably AI assistants based on language models. We outline two\nstructural reasons for why these models can fail when turned into agents.\nFirst, we discuss auto-suggestive delusions. Prior work has shown theoretically\nthat models fail to imitate agents that generated the training data if the\nagents relied on hidden observations: the hidden observations act as\nconfounding variables, and the models treat actions they generate as evidence\nfor nonexistent observations. Second, we introduce and formally study a\nrelated, novel limitation: predictor-policy incoherence. When a model generates\na sequence of actions, the model's implicit prediction of the policy that\ngenerated those actions can serve as a confounding variable. The result is that\nmodels choose actions as if they expect future actions to be suboptimal,\ncausing them to be overly conservative. We show that both of those failures are\nfixed by including a feedback loop from the environment, that is, re-training\nthe models on their own actions. We give simple demonstrations of both\nlimitations using Decision Transformers and confirm that empirical results\nagree with our conceptual and formal analysis. Our treatment provides a\nunifying view of those failure modes, and informs the question of why\nfine-tuning offline learned policies with online learning makes them more\neffective.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05829v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05829v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05829v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05828v1",
    "updated": "2024-02-08T17:07:42+00:00",
    "published": "2024-02-08T17:07:42+00:00",
    "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms",
    "authors": [
      {
        "name": "Matthew Thomas Jackson"
      },
      {
        "name": "Chris Lu"
      },
      {
        "name": "Louis Kirsch"
      },
      {
        "name": "Robert Tjarko Lange"
      },
      {
        "name": "Shimon Whiteson"
      },
      {
        "name": "Jakob Nicolaus Foerster"
      }
    ],
    "summary": "Recent advancements in meta-learning have enabled the automatic discovery of\nnovel reinforcement learning algorithms parameterized by surrogate objective\nfunctions. To improve upon manually designed algorithms, the parameterization\nof this learned objective function must be expressive enough to represent novel\nprinciples of learning (instead of merely recovering already established ones)\nwhile still generalizing to a wide range of settings outside of its\nmeta-training distribution. However, existing methods focus on discovering\nobjective functions that, like many widely used objective functions in\nreinforcement learning, do not take into account the total number of steps\nallowed for training, or \"training horizon\". In contrast, humans use a plethora\nof different learning objectives across the course of acquiring a new ability.\nFor instance, students may alter their studying techniques based on the\nproximity to exam deadlines and their self-assessed capabilities. This paper\ncontends that ignoring the optimization time horizon significantly restricts\nthe expressive potential of discovered learning algorithms. We propose a simple\naugmentation to two existing objective discovery approaches that allows the\ndiscovered algorithm to dynamically update its objective function throughout\nthe agent's training procedure, resulting in expressive schedules and increased\ngeneralization across different training horizons. In the process, we find that\ncommonly used meta-gradient approaches fail to discover such adaptive objective\nfunctions while evolution strategies discover highly dynamic learning rules. We\ndemonstrate the effectiveness of our approach on a wide range of tasks and\nanalyze the resulting learned algorithms, which we find effectively balance\nexploration and exploitation by modifying the structure of their learning rules\nthroughout the agent's lifetime.",
    "comment": "Published at ICLR 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05828v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05828v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05828v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05823v1",
    "updated": "2024-02-08T17:03:10+00:00",
    "published": "2024-02-08T17:03:10+00:00",
    "title": "FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting",
    "authors": [
      {
        "name": "Ziqing Ma"
      },
      {
        "name": "Wenwei Wang"
      },
      {
        "name": "Tian Zhou"
      },
      {
        "name": "Chao Chen"
      },
      {
        "name": "Bingqing Peng"
      },
      {
        "name": "Liang Sun"
      },
      {
        "name": "Rong Jin"
      }
    ],
    "summary": "Accurate solar power forecasting is crucial to integrate photovoltaic plants\ninto the electric grid, schedule and secure the power grid safety. This problem\nbecomes more demanding for those newly installed solar plants which lack\nsufficient data. Current research predominantly relies on historical solar\npower data or numerical weather prediction in a single-modality format,\nignoring the complementary information provided in different modalities. In\nthis paper, we propose a multi-modality fusion framework to integrate\nhistorical power data, numerical weather prediction, and satellite images,\nsignificantly improving forecast performance. We introduce a vector quantized\nframework that aligns modalities with varying information densities, striking a\nbalance between integrating sufficient information and averting model\noverfitting. Our framework demonstrates strong zero-shot forecasting\ncapability, which is especially useful for those newly installed plants.\nMoreover, we collect and release a multi-modal solar power (MMSP) dataset from\nreal-world plants to further promote the research of multi-modal solar\nforecasting algorithms. Our extensive experiments show that our model not only\noperates with robustness but also boosts accuracy in both zero-shot forecasting\nand scenarios rich with training data, surpassing leading models. We have\nincorporated it into our eForecaster platform and deployed it for more than 300\nsolar plants with a capacity of over 15GW.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05823v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05823v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05823v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05821v1",
    "updated": "2024-02-08T16:59:24+00:00",
    "published": "2024-02-08T16:59:24+00:00",
    "title": "Guided Evolution with Binary Discriminators for ML Program Search",
    "authors": [
      {
        "name": "John D. Co-Reyes"
      },
      {
        "name": "Yingjie Miao"
      },
      {
        "name": "George Tucker"
      },
      {
        "name": "Aleksandra Faust"
      },
      {
        "name": "Esteban Real"
      }
    ],
    "summary": "How to automatically design better machine learning programs is an open\nproblem within AutoML. While evolution has been a popular tool to search for\nbetter ML programs, using learning itself to guide the search has been less\nsuccessful and less understood on harder problems but has the promise to\ndramatically increase the speed and final performance of the optimization\nprocess. We propose guiding evolution with a binary discriminator, trained\nonline to distinguish which program is better given a pair of programs. The\ndiscriminator selects better programs without having to perform a costly\nevaluation and thus speed up the convergence of evolution. Our method can\nencode a wide variety of ML components including symbolic optimizers, neural\narchitectures, RL loss functions, and symbolic regression equations with the\nsame directed acyclic graph representation. By combining this representation\nwith modern GNNs and an adaptive mutation strategy, we demonstrate our method\ncan speed up evolution across a set of diverse problems including a 3.7x\nspeedup on the symbolic search for ML optimizers and a 4x speedup for RL loss\nfunctions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05821v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05821v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05821v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05819v1",
    "updated": "2024-02-08T16:55:21+00:00",
    "published": "2024-02-08T16:55:21+00:00",
    "title": "Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model",
    "authors": [
      {
        "name": "Hung-Chieh Fang"
      },
      {
        "name": "Nai-Xuan Ye"
      },
      {
        "name": "Yi-Jen Shih"
      },
      {
        "name": "Puyuan Peng"
      },
      {
        "name": "Hsuan-Fu Wang"
      },
      {
        "name": "Layne Berry"
      },
      {
        "name": "Hung-yi Lee"
      },
      {
        "name": "David Harwath"
      }
    ],
    "summary": "Recent advances in self-supervised speech models have shown significant\nimprovement in many downstream tasks. However, these models predominantly\ncentered on frame-level training objectives, which can fall short in spoken\nlanguage understanding tasks that require semantic comprehension. Existing\nworks often rely on additional speech-text data as intermediate targets, which\nis costly in the real-world setting. To address this challenge, we propose\nPseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level\ntargets into the training process, where the targets are derived from a\nvisually-ground speech model, notably eliminating the need for speech-text\npaired data. Our experimental results on four spoken language understanding\n(SLU) benchmarks suggest the superiority of our model in capturing semantic\ninformation.",
    "comment": "Accepted to ICASSP 2024 workshop on Self-supervision in Audio,\n  Speech, and Beyond (SASB)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05819v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05819v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05819v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05817v2",
    "updated": "2024-02-12T15:19:22+00:00",
    "published": "2024-02-08T16:54:20+00:00",
    "title": "Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging",
    "authors": [
      {
        "name": "Pouria Yazdian Anari"
      },
      {
        "name": "Fiona Obiezu"
      },
      {
        "name": "Nathan Lay"
      },
      {
        "name": "Fatemeh Dehghani Firouzabadi"
      },
      {
        "name": "Aditi Chaurasia"
      },
      {
        "name": "Mahshid Golagha"
      },
      {
        "name": "Shiva Singh"
      },
      {
        "name": "Fatemeh Homayounieh"
      },
      {
        "name": "Aryan Zahergivar"
      },
      {
        "name": "Stephanie Harmon"
      },
      {
        "name": "Evrim Turkbey"
      },
      {
        "name": "Rabindra Gautam"
      },
      {
        "name": "Kevin Ma"
      },
      {
        "name": "Maria Merino"
      },
      {
        "name": "Elizabeth C. Jones"
      },
      {
        "name": "Mark W. Ball"
      },
      {
        "name": "W. Marston Linehan"
      },
      {
        "name": "Baris Turkbey"
      },
      {
        "name": "Ashkan A. Malayeri"
      }
    ],
    "summary": "Introduction This study explores the use of the latest You Only Look Once\n(YOLO V7) object detection method to enhance kidney detection in medical\nimaging by training and testing a modified YOLO V7 on medical image formats.\nMethods Study includes 878 patients with various subtypes of renal cell\ncarcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans\nfor 1084 patients were retrieved. 326 patients with 1034 tumors recruited from\na retrospective maintained database, and bounding boxes were drawn around their\ntumors. A primary model was trained on 80% of annotated cases, with 20% saved\nfor testing (primary test set). The best primary model was then used to\nidentify tumors in the remaining 861 patients and bounding box coordinates were\ngenerated on their scans using the model. Ten benchmark training sets were\ncreated with generated coordinates on not-segmented patients. The final model\nused to predict the kidney in the primary test set. We reported the positive\npredictive value (PPV), sensitivity, and mean average precision (mAP). Results\nThe primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of\n0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPV\nof 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated an\naverage PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95\n+/- 0.01. Conclusion Using a semi-supervised approach with a medical image\nlibrary, we developed a high-performing model for kidney detection. Further\nexternal validation is required to assess the model's generalizability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05817v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05817v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05817v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05813v1",
    "updated": "2024-02-08T16:50:01+00:00",
    "published": "2024-02-08T16:50:01+00:00",
    "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
    "authors": [
      {
        "name": "Lingzhi Wang"
      },
      {
        "name": "Xingshan Zeng"
      },
      {
        "name": "Jinsong Guo"
      },
      {
        "name": "Kam-Fai Wong"
      },
      {
        "name": "Georg Gottlob"
      }
    ],
    "summary": "The aim of this study is to investigate Machine Unlearning (MU), a burgeoning\nfield focused on addressing concerns related to neural models inadvertently\nretaining personal or sensitive data. Here, a novel approach is introduced to\nachieve precise and selective forgetting within language models. Unlike\nprevious methodologies that adopt completely opposing training objectives, this\napproach aims to mitigate adverse effects on language model performance,\nparticularly in generation tasks. Furthermore, two innovative evaluation\nmetrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and\nSensitive Information Memory Accuracy (S-MA), designed to gauge the\neffectiveness of sensitive information elimination. To reinforce the forgetting\nframework, an effective method for annotating sensitive scopes is presented,\ninvolving both online and offline strategies. The online selection mechanism\nleverages language probability scores to ensure computational efficiency, while\nthe offline annotation entails a robust two-stage process based on Large\nLanguage Models (LLMs).",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05813v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05813v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05813v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05809v1",
    "updated": "2024-02-08T16:47:43+00:00",
    "published": "2024-02-08T16:47:43+00:00",
    "title": "You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement",
    "authors": [
      {
        "name": "Yixu Feng"
      },
      {
        "name": "Cheng Zhang"
      },
      {
        "name": "Pei Wang"
      },
      {
        "name": "Peng Wu"
      },
      {
        "name": "Qingsen Yan"
      },
      {
        "name": "Yanning Zhang"
      }
    ],
    "summary": "Low-Light Image Enhancement (LLIE) task tends to restore the details and\nvisual information from corrupted low-light images. Most existing methods learn\nthe mapping function between low/normal-light images by Deep Neural Networks\n(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves\namplifying image signals, and applying these color spaces to low-light images\nwith a low signal-to-noise ratio can introduce sensitivity and instability into\nthe enhancement process. Consequently, this results in the presence of color\nartifacts and brightness artifacts in the enhanced images. To alleviate this\nproblem, we propose a novel trainable color space, named\nHorizontal/Vertical-Intensity (HVI). It not only decouples brightness and color\nfrom RGB channels to mitigate the instability during enhancement but also\nadapts to low-light images in different illumination ranges due to the\ntrainable parameters. Further, we design a novel Color and Intensity Decoupling\nNetwork (CIDNet) with two branches dedicated to processing the decoupled image\nbrightness and color in the HVI space. Within CIDNet, we introduce the\nLightweight Cross-Attention (LCA) module to facilitate interaction between\nimage structure and content information in both branches, while also\nsuppressing noise in low-light images. Finally, we conducted 22 quantitative\nand qualitative experiments to show that the proposed CIDNet outperforms the\nstate-of-the-art methods on 11 datasets. The code will be available at\nhttps://github.com/Fediory/HVI-CIDNet.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05809v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05809v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05809v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05808v1",
    "updated": "2024-02-08T16:46:26+00:00",
    "published": "2024-02-08T16:46:26+00:00",
    "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
    "authors": [
      {
        "name": "Zhiheng Xi"
      },
      {
        "name": "Wenxiang Chen"
      },
      {
        "name": "Boyang Hong"
      },
      {
        "name": "Senjie Jin"
      },
      {
        "name": "Rui Zheng"
      },
      {
        "name": "Wei He"
      },
      {
        "name": "Yiwen Ding"
      },
      {
        "name": "Shichun Liu"
      },
      {
        "name": "Xin Guo"
      },
      {
        "name": "Junzhe Wang"
      },
      {
        "name": "Honglin Guo"
      },
      {
        "name": "Wei Shen"
      },
      {
        "name": "Xiaoran Fan"
      },
      {
        "name": "Yuhao Zhou"
      },
      {
        "name": "Shihan Dou"
      },
      {
        "name": "Xiao Wang"
      },
      {
        "name": "Xinbo Zhang"
      },
      {
        "name": "Peng Sun"
      },
      {
        "name": "Tao Gui"
      },
      {
        "name": "Qi Zhang"
      },
      {
        "name": "Xuanjing Huang"
      }
    ],
    "summary": "In this paper, we propose R$^3$: Learning Reasoning through Reverse\nCurriculum Reinforcement Learning (RL), a novel method that employs only\noutcome supervision to achieve the benefits of process supervision for large\nlanguage models. The core challenge in applying RL to complex reasoning is to\nidentify a sequence of actions that result in positive rewards and provide\nappropriate supervision for optimization. Outcome supervision provides sparse\nrewards for final results without identifying error locations, whereas process\nsupervision offers step-wise rewards but requires extensive manual annotation.\nR$^3$ overcomes these limitations by learning from correct demonstrations.\nSpecifically, R$^3$ progressively slides the start state of reasoning from a\ndemonstration's end to its beginning, facilitating easier model exploration at\nall stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome\nsupervision to offer step-level signals and precisely pinpoint errors. Using\nLlama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$\npoints on average. Notebaly, in program-based reasoning on GSM8K, it exceeds\nthe baseline by $4.2$ points across three backbone models, and without any\nextra data, Codellama-7B + R$^3$ performs comparable to larger models or\nclosed-source models.",
    "comment": "Preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05808v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05808v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05808v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05806v1",
    "updated": "2024-02-08T16:45:12+00:00",
    "published": "2024-02-08T16:45:12+00:00",
    "title": "On Calibration and Conformal Prediction of Deep Classifiers",
    "authors": [
      {
        "name": "Lahav Dabah"
      },
      {
        "name": "Tom Tirer"
      }
    ],
    "summary": "In many classification applications, the prediction of a deep neural network\n(DNN) based classifier needs to be accompanied with some confidence indication.\nTwo popular post-processing approaches for that aim are: 1) calibration:\nmodifying the classifier's softmax values such that their maximum (associated\nwith the prediction) better estimates the correctness probability; and 2)\nconformal prediction (CP): devising a score (based on the softmax values) from\nwhich a set of predictions with theoretically guaranteed marginal coverage of\nthe correct class is produced. While in practice both types of indications can\nbe desired, so far the interplay between them has not been investigated. Toward\nfilling this gap, in this paper we study the effect of temperature scaling,\narguably the most common calibration technique, on prominent CP methods. We\nstart with an extensive empirical study that among other insights shows that,\nsurprisingly, calibration has a detrimental effect on popular adaptive CP\nmethods: it frequently leads to larger prediction sets. Then, we turn to\ntheoretically analyze this behavior. We reveal several mathematical properties\nof the procedure, according to which we provide a reasoning for the phenomenon.\nOur study suggests that it may be worthwhile to utilize adaptive CP methods,\nchosen for their enhanced conditional coverage, based on softmax values prior\nto (or after canceling) temperature scaling calibration.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05806v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05806v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05806v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06689v1",
    "updated": "2024-02-08T16:45:01+00:00",
    "published": "2024-02-08T16:45:01+00:00",
    "title": "A Study on Stock Forecasting Using Deep Learning and Statistical Models",
    "authors": [
      {
        "name": "Himanshu Gupta"
      },
      {
        "name": "Aditya Jaiswal"
      }
    ],
    "summary": "Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-fin.ST",
    "categories": [
      "q-fin.ST",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06689v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06689v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06689v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05804v1",
    "updated": "2024-02-08T16:41:41+00:00",
    "published": "2024-02-08T16:41:41+00:00",
    "title": "InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write",
    "authors": [
      {
        "name": "Blagoj Mitrevski"
      },
      {
        "name": "Arina Rak"
      },
      {
        "name": "Julian Schnitzler"
      },
      {
        "name": "Chengkun Li"
      },
      {
        "name": "Andrii Maksai"
      },
      {
        "name": "Jesse Berent"
      },
      {
        "name": "Claudiu Musat"
      }
    ],
    "summary": "Digital note-taking is gaining popularity, offering a durable, editable, and\neasily indexable way of storing notes in the vectorized form, known as digital\nink. However, a substantial gap remains between this way of note-taking and\ntraditional pen-and-paper note-taking, a practice still favored by a vast\nmajority. Our work, InkSight, aims to bridge the gap by empowering physical\nnote-takers to effortlessly convert their work (offline handwriting) to digital\nink (online handwriting), a process we refer to as Derendering. Prior research\non the topic has focused on the geometric properties of images, resulting in\nlimited generalization beyond their training domains. Our approach combines\nreading and writing priors, allowing training a model in the absence of large\namounts of paired samples, which are difficult to obtain. To our knowledge,\nthis is the first work that effectively derenders handwritten text in arbitrary\nphotos with diverse visual characteristics and backgrounds. Furthermore, it\ngeneralizes beyond its training domain into simple sketches. Our human\nevaluation reveals that 87% of the samples produced by our model on the\nchallenging HierText dataset are considered as a valid tracing of the input\nimage and 67% look like a pen trajectory traced by a human.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05804v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05804v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05804v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05802v1",
    "updated": "2024-02-08T16:41:03+00:00",
    "published": "2024-02-08T16:41:03+00:00",
    "title": "Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence",
    "authors": [
      {
        "name": "Thomas A. Lasko"
      },
      {
        "name": "John M. Still"
      },
      {
        "name": "Thomas Z. Li"
      },
      {
        "name": "Marco Barbero Mota"
      },
      {
        "name": "William W. Stead"
      },
      {
        "name": "Eric V. Strobl"
      },
      {
        "name": "Bennett A. Landman"
      },
      {
        "name": "Fabien Maldonado"
      }
    ],
    "summary": "Insufficiently precise diagnosis of clinical disease is likely responsible\nfor many treatment failures, even for common conditions and treatments. With a\nlarge enough dataset, it may be possible to use unsupervised machine learning\nto define clinical disease patterns more precisely. We present an approach to\nlearning these patterns by using probabilistic independence to disentangle the\nimprint on the medical record of causal latent sources of disease. We inferred\na broad set of 2000 clinical signatures of latent sources from 9195 variables\nin 269,099 Electronic Health Records. The learned signatures produced better\ndiscrimination than the original variables in a lung cancer prediction task\nunknown to the inference algorithm, predicting 3-year malignancy in patients\nwith no history of cancer before a solitary lung nodule was discovered. More\nimportantly, the signatures' greater explanatory power identified pre-nodule\nsignatures of apparently undiagnosed cancer in many of those patients.",
    "comment": "29 Pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ML",
      "I.2.6; I.2.1; J.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05802v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05802v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05802v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05794v1",
    "updated": "2024-02-08T16:36:11+00:00",
    "published": "2024-02-08T16:36:11+00:00",
    "title": "Phonetically rich corpus construction for a low-resourced language",
    "authors": [
      {
        "name": "Marcellus Amadeus"
      },
      {
        "name": "William Alberto Cruz Casta\u00f1eda"
      },
      {
        "name": "Wilmer Lobato"
      },
      {
        "name": "Niasche Aquino"
      }
    ],
    "summary": "Speech technologies rely on capturing a speaker's voice variability while\nobtaining comprehensive language information. Textual prompts and sentence\nselection methods have been proposed in the literature to comprise such\nadequate phonetic data, referred to as a phonetically rich \\textit{corpus}.\nHowever, they are still insufficient for acoustic modeling, especially critical\nfor languages with limited resources. Hence, this paper proposes a novel\napproach and outlines the methodological aspects required to create a\n\\textit{corpus} with broad phonetic coverage for a low-resourced language,\nBrazilian Portuguese. Our methodology includes text dataset collection up to a\nsentence selection algorithm based on triphone distribution. Furthermore, we\npropose a new phonemic classification according to acoustic-articulatory speech\nfeatures since the absolute number of distinct triphones, or low-probability\ntriphones, does not guarantee an adequate representation of every possible\ncombination. Using our algorithm, we achieve a 55.8\\% higher percentage of\ndistinct triphones -- for samples of similar size -- while the currently\navailable phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\\% and 12.3\\% in\ncomparison to a non-phonetically rich dataset.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05794v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05794v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05794v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06688v1",
    "updated": "2024-02-08T16:32:14+00:00",
    "published": "2024-02-08T16:32:14+00:00",
    "title": "Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results",
    "authors": [
      {
        "name": "Chukwuma Okolie"
      },
      {
        "name": "Adedayo Adeleke"
      },
      {
        "name": "Julian Smit"
      },
      {
        "name": "Jon Mills"
      },
      {
        "name": "Iyke Maduako"
      },
      {
        "name": "Caleb Ogbeta"
      }
    ],
    "summary": "Several methods have been proposed for correcting the elevation bias in\ndigital elevation models (DEMs) for example, linear regression. Nowadays,\nsupervised machine learning enables the modelling of complex relationships\nbetween variables, and has been deployed by researchers in a variety of fields.\nIn the existing literature, several studies have adopted either machine\nlearning or statistical approaches in the task of DEM correction. However, to\nour knowledge, none of these studies have compared the performance of both\napproaches, especially with regard to open-access global DEMs. Our previous\nwork has already shown the potential of machine learning approaches,\nspecifically gradient boosted decision trees (GBDTs) for DEM correction. In\nthis study, we share some results from the comparison of three recent\nimplementations of gradient boosted decision trees (XGBoost, LightGBM and\nCatBoost), versus multiple linear regression (MLR) for enhancing the vertical\naccuracy of 30 m Copernicus and AW3D global DEMs in Cape Town, South Africa.",
    "comment": "2 pages, 2 figures, 1 table, ISPRS conference extended abstract",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06688v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06688v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06688v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05787v1",
    "updated": "2024-02-08T16:24:44+00:00",
    "published": "2024-02-08T16:24:44+00:00",
    "title": "How do Transformers perform In-Context Autoregressive Learning?",
    "authors": [
      {
        "name": "Michael E. Sander"
      },
      {
        "name": "Raja Giryes"
      },
      {
        "name": "Taiji Suzuki"
      },
      {
        "name": "Mathieu Blondel"
      },
      {
        "name": "Gabriel Peyr\u00e9"
      }
    ],
    "summary": "Transformers have achieved state-of-the-art performance in language modeling\ntasks. However, the reasons behind their tremendous success are still unclear.\nIn this paper, towards a better understanding, we train a Transformer model on\na simple next token prediction task, where sequences are generated as a\nfirst-order autoregressive process $s_{t+1} = W s_t$. We show how a trained\nTransformer predicts the next token by first learning $W$ in-context, then\napplying a prediction mapping. We call the resulting procedure in-context\nautoregressive learning. More precisely, focusing on commuting orthogonal\nmatrices $W$, we first show that a trained one-layer linear Transformer\nimplements one step of gradient descent for the minimization of an inner\nobjective function, when considering augmented tokens. When the tokens are not\naugmented, we characterize the global minima of a one-layer diagonal linear\nmulti-head Transformer. Importantly, we exhibit orthogonality between heads and\nshow that positional encoding captures trigonometric relations in the data. On\nthe experimental side, we consider the general case of non-commuting orthogonal\nmatrices and generalize our theoretical findings.",
    "comment": "24 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05787v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05787v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05787v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05786v2",
    "updated": "2024-02-09T04:19:26+00:00",
    "published": "2024-02-08T16:24:40+00:00",
    "title": "Prompting Fairness: Artificial Intelligence as Game Players",
    "authors": [
      {
        "name": "Jazmia Henry"
      }
    ],
    "summary": "Utilitarian games such as dictator games to measure fairness have been\nstudied in the social sciences for decades. These games have given us insight\ninto not only how humans view fairness but also in what conditions the\nfrequency of fairness, altruism and greed increase or decrease. While these\ngames have traditionally been focused on humans, the rise of AI gives us the\nability to study how these models play these games. AI is becoming a constant\nin human interaction and examining how these models portray fairness in game\nplay can give us some insight into how AI makes decisions. Over 101 rounds of\nthe dictator game, I conclude that AI has a strong sense of fairness that is\ndependant of it it deems the person it is playing with as trustworthy, framing\nhas a strong effect on how much AI gives a recipient when designated the\ntrustee, and there may be evidence that AI experiences inequality aversion just\nas humans.",
    "comment": "preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05786v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05786v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05786v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05785v2",
    "updated": "2024-02-13T07:36:40+00:00",
    "published": "2024-02-08T16:23:29+00:00",
    "title": "Limits of Transformer Language Models on Learning Algorithmic Compositions",
    "authors": [
      {
        "name": "Jonathan Thomm"
      },
      {
        "name": "Aleksandar Terzic"
      },
      {
        "name": "Geethan Karunaratne"
      },
      {
        "name": "Giacomo Camposampiero"
      },
      {
        "name": "Bernhard Sch\u00f6lkopf"
      },
      {
        "name": "Abbas Rahimi"
      }
    ],
    "summary": "We analyze the capabilities of Transformer language models on learning\ndiscrete algorithms. To this end, we introduce two new tasks demanding the\ncomposition of several discrete sub-tasks. On both training LLaMA models from\nscratch and prompting on GPT-4 and Gemini we measure learning compositions of\nlearned primitives. We observe that the compositional capabilities of\nstate-of-the-art Transformer language models are very limited and sample-wise\nscale worse than relearning all sub-tasks for a new algorithmic composition. We\nalso present a theorem in complexity theory, showing that gradient descent on\nmemorizing feedforward models can be exponentially data inefficient.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05785v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05785v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05785v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05782v1",
    "updated": "2024-02-08T16:17:18+00:00",
    "published": "2024-02-08T16:17:18+00:00",
    "title": "Analysing the Sample Complexity of Opponent Shaping",
    "authors": [
      {
        "name": "Kitty Fung"
      },
      {
        "name": "Qizhen Zhang"
      },
      {
        "name": "Chris Lu"
      },
      {
        "name": "Jia Wan"
      },
      {
        "name": "Timon Willi"
      },
      {
        "name": "Jakob Foerster"
      }
    ],
    "summary": "Learning in general-sum games often yields collectively sub-optimal results.\nAddressing this, opponent shaping (OS) methods actively guide the learning\nprocesses of other agents, empirically leading to improved individual and group\nperformances in many settings. Early OS methods use higher-order derivatives to\nshape the learning of co-players, making them unsuitable for shaping multiple\nlearning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses\nthese by reframing the OS problem as a meta-game. In contrast to early OS\nmethods, there is little theoretical understanding of the M-FOS framework.\nProviding theoretical guarantees for M-FOS is hard because A) there is little\nliterature on theoretical sample complexity bounds for meta-reinforcement\nlearning B) M-FOS operates in continuous state and action spaces, so\ntheoretical analysis is challenging. In this work, we present R-FOS, a tabular\nversion of M-FOS that is more suitable for theoretical analysis. R-FOS\ndiscretises the continuous meta-game MDP into a tabular MDP. Within this\ndiscretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to\nderive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We\nderive a sample complexity bound that is exponential in the cardinality of the\ninner state and action space and the number of agents. Our bound guarantees\nthat, with high probability, the final policy learned by an R-FOS agent is\nclose to the optimal policy, apart from a constant factor. Finally, we\ninvestigate how R-FOS's sample complexity scales in the size of state-action\nspace. Our theoretical results on scaling are supported empirically in the\nMatching Pennies environment.",
    "comment": null,
    "journal_ref": "AAMAS 2024",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05782v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05782v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05782v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05774v1",
    "updated": "2024-02-08T16:01:24+00:00",
    "published": "2024-02-08T16:01:24+00:00",
    "title": "Stable Autonomous Flow Matching",
    "authors": [
      {
        "name": "Christopher Iliffe Sprague"
      },
      {
        "name": "Arne Elofsson"
      },
      {
        "name": "Hossein Azizpour"
      }
    ],
    "summary": "In contexts where data samples represent a physically stable state, it is\noften assumed that the data points represent the local minima of an energy\nlandscape. In control theory, it is well-known that energy can serve as an\neffective Lyapunov function. Despite this, connections between control theory\nand generative models in the literature are sparse, even though there are\nseveral machine learning applications with physically stable data points. In\nthis paper, we focus on such data and a recent class of deep generative models\ncalled flow matching. We apply tools of stochastic stability for\ntime-independent systems to flow matching models. In doing so, we characterize\nthe space of flow matching models that are amenable to this treatment, as well\nas draw connections to other control theory principles. We demonstrate our\ntheoretical results on two examples.",
    "comment": "In submission",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05774v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05774v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05774v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05766v1",
    "updated": "2024-02-08T15:51:50+00:00",
    "published": "2024-02-08T15:51:50+00:00",
    "title": "Off-policy Distributional Q($\u03bb$): Distributional RL without Importance Sampling",
    "authors": [
      {
        "name": "Yunhao Tang"
      },
      {
        "name": "Mark Rowland"
      },
      {
        "name": "R\u00e9mi Munos"
      },
      {
        "name": "Bernardo \u00c1vila Pires"
      },
      {
        "name": "Will Dabney"
      }
    ],
    "summary": "We introduce off-policy distributional Q($\\lambda$), a new addition to the\nfamily of off-policy distributional evaluation algorithms. Off-policy\ndistributional Q($\\lambda$) does not apply importance sampling for off-policy\nlearning, which introduces intriguing interactions with signed measures. Such\nunique properties distributional Q($\\lambda$) from other existing alternatives\nsuch as distributional Retrace. We characterize the algorithmic properties of\ndistributional Q($\\lambda$) and validate theoretical insights with tabular\nexperiments. We show how distributional Q($\\lambda$)-C51, a combination of\nQ($\\lambda$) with the C51 agent, exhibits promising results on deep RL\nbenchmarks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05766v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05766v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05766v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05758v1",
    "updated": "2024-02-08T15:41:48+00:00",
    "published": "2024-02-08T15:41:48+00:00",
    "title": "Latent variable model for high-dimensional point process with structured missingness",
    "authors": [
      {
        "name": "Maksim Sinelnikov"
      },
      {
        "name": "Manuel Haussmann"
      },
      {
        "name": "Harri L\u00e4hdesm\u00e4ki"
      }
    ],
    "summary": "Longitudinal data are important in numerous fields, such as healthcare,\nsociology and seismology, but real-world datasets present notable challenges\nfor practitioners because they can be high-dimensional, contain structured\nmissingness patterns, and measurement time points can be governed by an unknown\nstochastic process. While various solutions have been suggested, the majority\nof them have been designed to account for only one of these challenges. In this\nwork, we propose a flexible and efficient latent-variable model that is capable\nof addressing all these limitations. Our approach utilizes Gaussian processes\nto capture temporal correlations between samples and their associated\nmissingness masks as well as to model the underlying point process. We\nconstruct our model as a variational autoencoder together with deep neural\nnetwork parameterised encoder and decoder models, and develop a scalable\namortised variational inference approach for efficient model training. We\ndemonstrate competitive performance using both simulated and real datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05758v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05758v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05758v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07939v1",
    "updated": "2024-02-08T15:40:35+00:00",
    "published": "2024-02-08T15:40:35+00:00",
    "title": "UFO: A UI-Focused Agent for Windows OS Interaction",
    "authors": [
      {
        "name": "Chaoyun Zhang"
      },
      {
        "name": "Liqun Li"
      },
      {
        "name": "Shilin He"
      },
      {
        "name": "Xu Zhang"
      },
      {
        "name": "Bo Qiao"
      },
      {
        "name": "Si Qin"
      },
      {
        "name": "Minghua Ma"
      },
      {
        "name": "Yu Kang"
      },
      {
        "name": "Qingwei Lin"
      },
      {
        "name": "Saravan Rajmohan"
      },
      {
        "name": "Dongmei Zhang"
      },
      {
        "name": "Qi Zhang"
      }
    ],
    "summary": "We introduce UFO, an innovative UI-Focused agent to fulfill user requests\ntailored to applications on Windows OS, harnessing the capabilities of\nGPT-Vision. UFO employs a dual-agent framework to meticulously observe and\nanalyze the graphical user interface (GUI) and control information of Windows\napplications. This enables the agent to seamlessly navigate and operate within\nindividual applications and across them to fulfill user requests, even when\nspanning multiple applications. The framework incorporates a control\ninteraction module, facilitating action grounding without human intervention\nand enabling fully automated execution. Consequently, UFO transforms arduous\nand time-consuming processes into simple tasks achievable solely through\nnatural language commands. We conducted testing of UFO across 9 popular Windows\napplications, encompassing a variety of scenarios reflective of users' daily\nusage. The results, derived from both quantitative metrics and real-case\nstudies, underscore the superior effectiveness of UFO in fulfilling user\nrequests. To the best of our knowledge, UFO stands as the first UI agent\nspecifically tailored for task completion within the Windows OS environment.\nThe open-source code for UFO is available on https://github.com/microsoft/UFO.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07939v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07939v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07939v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05749v1",
    "updated": "2024-02-08T15:33:09+00:00",
    "published": "2024-02-08T15:33:09+00:00",
    "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment",
    "authors": [
      {
        "name": "Yunhao Tang"
      },
      {
        "name": "Zhaohan Daniel Guo"
      },
      {
        "name": "Zeyu Zheng"
      },
      {
        "name": "Daniele Calandriello"
      },
      {
        "name": "R\u00e9mi Munos"
      },
      {
        "name": "Mark Rowland"
      },
      {
        "name": "Pierre Harvey Richemond"
      },
      {
        "name": "Michal Valko"
      },
      {
        "name": "Bernardo \u00c1vila Pires"
      },
      {
        "name": "Bilal Piot"
      }
    ],
    "summary": "Offline preference optimization allows fine-tuning large models directly from\noffline data, and has proved effective in recent alignment practices. We\npropose generalized preference optimization (GPO), a family of offline losses\nparameterized by a general class of convex functions. GPO enables a unified\nview over preference optimization, encompassing existing algorithms such as\nDPO, IPO and SLiC as special cases, while naturally introducing new variants.\nThe GPO framework also sheds light on how offline algorithms enforce\nregularization, through the design of the convex function that defines the\nloss. Our analysis and experiments reveal the connections and subtle\ndifferences between the offline regularization and the KL divergence\nregularization intended by the canonical RLHF formulation. In all, our results\npresent new algorithmic toolkits and empirical insights to alignment\npractitioners.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05749v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05749v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05749v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05747v1",
    "updated": "2024-02-08T15:32:22+00:00",
    "published": "2024-02-08T15:32:22+00:00",
    "title": "Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method",
    "authors": [
      {
        "name": "Qiuhao Li"
      },
      {
        "name": "Shenghai Yuan"
      }
    ],
    "summary": "In the context of rapid advancements in industrial automation, vision-based\nrobotic grasping plays an increasingly crucial role. In order to enhance visual\nrecognition accuracy, the utilization of large-scale datasets is imperative for\ntraining models to acquire implicit knowledge related to the handling of\nvarious objects. Creating datasets from scratch is a time and labor-intensive\nprocess. Moreover, existing datasets often contain errors due to automated\nannotations aimed at expediency, making the improvement of these datasets a\nsubstantial research challenge. Consequently, several issues have been\nidentified in the annotation of grasp bounding boxes within the popular\nJacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance\ndataset quality. This approach relies on backbone deep learning networks to\npredict object positions and orientations for robotic grasping. Predictions\nwith Intersection over Union (IOU) values below 0.2 undergo an assessment by\nhuman operators. After their evaluation, the data is categorized into False\nNegatives(FN) and True Negatives(TN). FN are then subcategorized into either\nmissing annotations or catastrophic labeling errors. Images lacking labels are\naugmented with valid grasp bounding box information, whereas images afflicted\nby catastrophic labeling errors are completely removed. The open-source tool\nLabelbee was employed for 53,026 iterations of HIL dataset enhancement, leading\nto the removal of 2,884 images and the incorporation of ground truth\ninformation for 30,292 images. The enhanced dataset, named the Jacquard V2\nGrasping Dataset, served as the training data for a range of neural networks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05747v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05747v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05747v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05741v1",
    "updated": "2024-02-08T15:19:50+00:00",
    "published": "2024-02-08T15:19:50+00:00",
    "title": "Real-World Robot Applications of Foundation Models: A Review",
    "authors": [
      {
        "name": "Kento Kawaharazuka"
      },
      {
        "name": "Tatsuya Matsushima"
      },
      {
        "name": "Andrew Gambardella"
      },
      {
        "name": "Jiaxian Guo"
      },
      {
        "name": "Chris Paxton"
      },
      {
        "name": "Andy Zeng"
      }
    ],
    "summary": "Recent developments in foundation models, like Large Language Models (LLMs)\nand Vision-Language Models (VLMs), trained on extensive data, facilitate\nflexible application across different tasks and modalities. Their impact spans\nvarious fields, including healthcare, education, and robotics. This paper\nprovides an overview of the practical application of foundation models in\nreal-world robotics, with a primary emphasis on the replacement of specific\ncomponents within existing robot systems. The summary encompasses the\nperspective of input-output relationships in foundation models, as well as\ntheir role in perception, motion planning, and control within the field of\nrobotics. This paper concludes with a discussion of future challenges and\nimplications for practical robot applications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05741v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05741v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05741v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05738v1",
    "updated": "2024-02-08T15:15:09+00:00",
    "published": "2024-02-08T15:15:09+00:00",
    "title": "Implicit Bias and Fast Convergence Rates for Self-attention",
    "authors": [
      {
        "name": "Bhavya Vasudeva"
      },
      {
        "name": "Puneesh Deora"
      },
      {
        "name": "Christos Thrampoulidis"
      }
    ],
    "summary": "Self-attention, the core mechanism of transformers, distinguishes them from\ntraditional neural networks and drives their outstanding performance. Towards\ndeveloping the fundamental optimization principles of self-attention, we\ninvestigate the implicit bias of gradient descent (GD) in training a\nself-attention layer with fixed linear decoder in binary classification.\nDrawing inspiration from the study of GD in linear logistic regression over\nseparable data, recent work demonstrates that as the number of iterations $t$\napproaches infinity, the key-query matrix $W_t$ converges locally (with respect\nto the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our\nwork enhances this result in four aspects. Firstly, we identify non-trivial\ndata settings for which convergence is provably global, thus shedding light on\nthe optimization landscape. Secondly, we provide the first finite-time\nconvergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of\nsparsification in the attention map. Thirdly, through an analysis of normalized\nGD and Polyak step-size, we demonstrate analytically that adaptive step-size\nrules can accelerate the convergence of self-attention. Additionally, we remove\nthe restriction of prior work on a fixed linear decoder. Our results reinforce\nthe implicit-bias perspective of self-attention and strengthen its connections\nto implicit-bias in linear logistic regression, despite the intricate\nnon-convex nature of the former.",
    "comment": "41 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05738v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05738v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05738v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05724v1",
    "updated": "2024-02-08T14:54:47+00:00",
    "published": "2024-02-08T14:54:47+00:00",
    "title": "Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL",
    "authors": [
      {
        "name": "Jiawei Huang"
      },
      {
        "name": "Niao He"
      },
      {
        "name": "Andreas Krause"
      }
    ],
    "summary": "We study the sample complexity of reinforcement learning (RL) in Mean-Field\nGames (MFGs) with model-based function approximation that requires strategic\nexploration to find a Nash Equilibrium policy. We introduce the Partial\nModel-Based Eluder Dimension (P-MBED), a more effective notion to characterize\nthe model class complexity. Notably, P-MBED measures the complexity of the\nsingle-agent model class converted from the given mean-field model class, and\npotentially, can be exponentially lower than the MBED proposed by\n\\citet{huang2023statistical}. We contribute a model elimination algorithm\nfeaturing a novel exploration strategy and establish sample complexity results\npolynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic\nrealizability and Lipschitz continuity assumptions, \\emph{learning Nash\nEquilibrium in MFGs is no more statistically challenging than solving a\nlogarithmic number of single-agent RL problems}. We further extend our results\nto Multi-Type MFGs, generalizing from conventional MFGs and involving multiple\ntypes of agents. This extension implies statistical tractability of a broader\nclass of Markov Games through the efficacy of mean-field approximation.\nFinally, inspired by our theoretical algorithm, we present a heuristic approach\nwith improved computational efficiency and empirically demonstrate its\neffectiveness.",
    "comment": "49 Pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05724v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05724v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05724v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05723v1",
    "updated": "2024-02-08T14:54:17+00:00",
    "published": "2024-02-08T14:54:17+00:00",
    "title": "In-Context Learning Can Re-learn Forbidden Tasks",
    "authors": [
      {
        "name": "Sophie Xhonneux"
      },
      {
        "name": "David Dobre"
      },
      {
        "name": "Jian Tang"
      },
      {
        "name": "Gauthier Gidel"
      },
      {
        "name": "Dhanya Sridhar"
      }
    ],
    "summary": "Despite significant investment into safety training, large language models\n(LLMs) deployed in the real world still suffer from numerous vulnerabilities.\nOne perspective on LLM safety training is that it algorithmically forbids the\nmodel from answering toxic or harmful queries. To assess the effectiveness of\nsafety training, in this work, we study forbidden tasks, i.e., tasks the model\nis designed to refuse to answer. Specifically, we investigate whether\nin-context learning (ICL) can be used to re-learn forbidden tasks despite the\nexplicit fine-tuning of the model to refuse them. We first examine a toy\nexample of refusing sentiment classification to demonstrate the problem. Then,\nwe use ICL on a model fine-tuned to refuse to summarise made-up news articles.\nFinally, we investigate whether ICL can undo safety training, which could\nrepresent a major security risk. For the safety task, we look at Vicuna-7B,\nStarling-7B, and Llama2-7B. We show that the attack works out-of-the-box on\nStarling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL\nattack that uses the chat template tokens like a prompt injection attack to\nachieve a better attack success rate on Vicuna-7B and Starling-7B.\n  Trigger Warning: the appendix contains LLM-generated text with violence,\nsuicide, and misinformation.",
    "comment": "19 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05723v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05723v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05723v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05719v1",
    "updated": "2024-02-08T14:50:07+00:00",
    "published": "2024-02-08T14:50:07+00:00",
    "title": "Exact capacity of the \\emph{wide} hidden layer treelike neural networks with generic activations",
    "authors": [
      {
        "name": "Mihailo Stojnic"
      }
    ],
    "summary": "Recent progress in studying \\emph{treelike committee machines} (TCM) neural\nnetworks (NN) in\n\\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23}\nshowed that the Random Duality Theory (RDT) and its a \\emph{partially\nlifted}(pl RDT) variant are powerful tools that can be used for very precise\nnetworks capacity analysis. Here, we consider \\emph{wide} hidden layer networks\nand uncover that certain aspects of numerical difficulties faced in\n\\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we\nemploy recently developed \\emph{fully lifted} (fl) RDT to characterize the\n\\emph{wide} ($d\\rightarrow \\infty$) TCM nets capacity. We obtain explicit,\nclosed form, capacity characterizations for a very generic class of the hidden\nlayer activations. While the utilized approach significantly lowers the amount\nof the needed numerical evaluations, the ultimate fl RDT usefulness and success\nstill require a solid portion of the residual numerical work. To get the\nconcrete capacity values, we take four very famous activations examples:\n\\emph{\\textbf{ReLU}}, \\textbf{\\emph{quadratic}}, \\textbf{\\emph{erf}}, and\n\\textbf{\\emph{tanh}}. After successfully conducting all the residual numerical\nwork for all of them, we uncover that the whole lifting mechanism exhibits a\nremarkably rapid convergence with the relative improvements no better than\n$\\sim 0.1\\%$ happening already on the 3-rd level of lifting. As a convenient\nbonus, we also uncover that the capacity characterizations obtained on the\nfirst and second level of lifting precisely match those obtained through the\nstatistical physics replica theory methods in \\cite{ZavPeh21} for the generic\nand in \\cite{BalMalZech19} for the ReLU activations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.PR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05719v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05719v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05719v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05718v1",
    "updated": "2024-02-08T14:47:37+00:00",
    "published": "2024-02-08T14:47:37+00:00",
    "title": "REMEDI: Corrective Transformations for Improved Neural Entropy Estimation",
    "authors": [
      {
        "name": "Viktor Nilsson"
      },
      {
        "name": "Anirban Samaddar"
      },
      {
        "name": "Sandeep Madireddy"
      },
      {
        "name": "Pierre Nyquist"
      }
    ],
    "summary": "Information theoretic quantities play a central role in machine learning. The\nrecent surge in the complexity of data and models has increased the demand for\naccurate estimation of these quantities. However, as the dimension grows the\nestimation presents significant challenges, with existing methods struggling\nalready in relatively low dimensions. To address this issue, in this work, we\nintroduce $\\texttt{REMEDI}$ for efficient and accurate estimation of\ndifferential entropy, a fundamental information theoretic quantity. The\napproach combines the minimization of the cross-entropy for simple, adaptive\nbase models and the estimation of their deviation, in terms of the relative\nentropy, from the data density. Our approach demonstrates improvement across a\nbroad spectrum of estimation tasks, encompassing entropy estimation on both\nsynthetic and natural data. Further, we extend important theoretical\nconsistency results to a more generalized setting required by our approach. We\nillustrate how the framework can be naturally extended to information theoretic\nsupervised learning models, with a specific focus on the Information Bottleneck\napproach. It is demonstrated that the method delivers better accuracy compared\nto the existing methods in Information Bottleneck. In addition, we explore a\nnatural connection between $\\texttt{REMEDI}$ and generative modeling using\nrejection sampling and Langevin dynamics.",
    "comment": "27 pages, 17 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "94A17 (Primary) 68T01, 94A08 (Secondary)"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05718v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05718v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05718v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05715v1",
    "updated": "2024-02-08T14:43:56+00:00",
    "published": "2024-02-08T14:43:56+00:00",
    "title": "Collaborative non-parametric two-sample testing",
    "authors": [
      {
        "name": "Alejandro de la Concha"
      },
      {
        "name": "Nicolas Vayatis"
      },
      {
        "name": "Argyris Kalogeratos"
      }
    ],
    "summary": "This paper addresses the multiple two-sample test problem in a\ngraph-structured setting, which is a common scenario in fields such as Spatial\nStatistics and Neuroscience. Each node $v$ in fixed graph deals with a\ntwo-sample testing problem between two node-specific probability density\nfunctions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null\nhypothesis $p_v = q_v$ should be rejected, under the assumption that connected\nnodes would yield similar test outcomes. We propose the non-parametric\ncollaborative two-sample testing (CTST) framework that efficiently leverages\nthe graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our\nmethodology integrates elements from f-divergence estimation, Kernel Methods,\nand Multitask Learning. We use synthetic experiments and a real sensor network\ndetecting seismic activity to demonstrate that CTST outperforms\nstate-of-the-art non-parametric statistical tests that apply at each node\nindependently, hence disregard the geometry of the problem.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05715v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05715v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05715v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05713v1",
    "updated": "2024-02-08T14:40:32+00:00",
    "published": "2024-02-08T14:40:32+00:00",
    "title": "Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations",
    "authors": [
      {
        "name": "Pranav Kulkarni"
      },
      {
        "name": "Andrew Chan"
      },
      {
        "name": "Nithya Navarathna"
      },
      {
        "name": "Skylar Chan"
      },
      {
        "name": "Paul H. Yi"
      },
      {
        "name": "Vishwa S. Parekh"
      }
    ],
    "summary": "The proliferation of artificial intelligence (AI) in radiology has shed light\non the risk of deep learning (DL) models exacerbating clinical biases towards\nvulnerable patient populations. While prior literature has focused on\nquantifying biases exhibited by trained DL models, demographically targeted\nadversarial bias attacks on DL models and its implication in the clinical\nenvironment remains an underexplored field of research in medical imaging. In\nthis work, we demonstrate that demographically targeted label poisoning attacks\ncan introduce adversarial underdiagnosis bias in DL models and degrade\nperformance on underrepresented groups without impacting overall model\nperformance. Moreover, our results across multiple performance metrics and\ndemographic groups like sex, age, and their intersectional subgroups indicate\nthat a group's vulnerability to undetectable adversarial bias attacks is\ndirectly correlated with its representation in the model's training data.",
    "comment": "26 pages, 20 figures, 1 table",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05713v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05713v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05713v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05712v1",
    "updated": "2024-02-08T14:39:16+00:00",
    "published": "2024-02-08T14:39:16+00:00",
    "title": "DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer",
    "authors": [
      {
        "name": "Zhiyuan Ma"
      },
      {
        "name": "Xiangyu Zhu"
      },
      {
        "name": "Guojun Qi"
      },
      {
        "name": "Chen Qian"
      },
      {
        "name": "Zhaoxiang Zhang"
      },
      {
        "name": "Zhen Lei"
      }
    ],
    "summary": "Speech-driven 3D facial animation is important for many multimedia\napplications. Recent work has shown promise in using either Diffusion models or\nTransformer architectures for this task. However, their mere aggregation does\nnot lead to improved performance. We suspect this is due to a shortage of\npaired audio-4D data, which is crucial for the Transformer to effectively\nperform as a denoiser within the Diffusion framework. To tackle this issue, we\npresent DiffSpeaker, a Transformer-based network equipped with novel biased\nconditional attention modules. These modules serve as substitutes for the\ntraditional self/cross-attention in standard Transformers, incorporating\nthoughtfully designed biases that steer the attention mechanisms to concentrate\non both the relevant task-specific and diffusion-related conditions. We also\nexplore the trade-off between accurate lip synchronization and non-verbal\nfacial expressions within the Diffusion paradigm. Experiments show our model\nnot only achieves state-of-the-art performance on existing benchmarks, but also\nfast inference speed owing to its ability to generate facial motions in\nparallel.",
    "comment": "9 pages, 5 figures. Code is avalable at\n  https://github.com/theEricMa/DiffSpeaker",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05712v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05712v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05712v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05703v1",
    "updated": "2024-02-08T14:27:34+00:00",
    "published": "2024-02-08T14:27:34+00:00",
    "title": "Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming",
    "authors": [
      {
        "name": "Giorgio Angelotti"
      },
      {
        "name": "Caroline P. C. Chanel"
      },
      {
        "name": "Adam H. M. Pinto"
      },
      {
        "name": "Christophe Lounis"
      },
      {
        "name": "Corentin Chauffaut"
      },
      {
        "name": "Nicolas Drougard"
      }
    ],
    "summary": "The integration of physiological computing into mixed-initiative human-robot\ninteraction systems offers valuable advantages in autonomous task allocation by\nincorporating real-time features as human state observations into the\ndecision-making system. This approach may alleviate the cognitive load on human\noperators by intelligently allocating mission tasks between agents.\nNevertheless, accommodating a diverse pool of human participants with varying\nphysiological and behavioral measurements presents a substantial challenge. To\naddress this, resorting to a probabilistic framework becomes necessary, given\nthe inherent uncertainty and partial observability on the human's state. Recent\nresearch suggests to learn a Partially Observable Markov Decision Process\n(POMDP) model from a data set of previously collected experiences that can be\nsolved using Offline Reinforcement Learning (ORL) methods. In the present work,\nwe not only highlight the potential of partially observable representations and\nphysiological measurements to improve human operator state estimation and\nperformance, but also enhance the overall mission effectiveness of a\nhuman-robot team. Importantly, as the fixed data set may not contain enough\ninformation to fully represent complex stochastic processes, we propose a\nmethod to incorporate model uncertainty, thus enabling risk-sensitive\nsequential decision-making. Experiments were conducted with a group of\ntwenty-six human participants within a simulated robot teleoperation\nenvironment, yielding empirical evidence of the method's efficacy. The obtained\nadaptive task allocation policy led to statistically significant higher scores\nthan the one that was used to collect the data set, allowing for generalization\nacross diverse participants also taking into account risk-sensitive metrics.",
    "comment": "Accepted as a full paper at AAMAS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05703v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05703v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05703v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05983v1",
    "updated": "2024-02-08T14:23:24+00:00",
    "published": "2024-02-08T14:23:24+00:00",
    "title": "Capability enhancement of the X-ray micro-tomography system via ML-assisted approaches",
    "authors": [
      {
        "name": "Dhruvi Shah"
      },
      {
        "name": "Shruti Mehta"
      },
      {
        "name": "Ashish Agrawal"
      },
      {
        "name": "Shishir Purohit"
      },
      {
        "name": "Bhaskar Chaudhury"
      }
    ],
    "summary": "Ring artifacts in X-ray micro-CT images are one of the primary causes of\nconcern in their accurate visual interpretation and quantitative analysis. The\ngeometry of X-ray micro-CT scanners is similar to the medical CT machines,\nexcept the sample is rotated with a stationary source and detector. The ring\nartifacts are caused by a defect or non-linear responses in detector pixels\nduring the MicroCT data acquisition. Artifacts in MicroCT images can often be\nso severe that the images are no longer useful for further analysis. Therefore,\nit is essential to comprehend the causes of artifacts and potential solutions\nto maximize image quality. This article presents a convolution neural network\n(CNN)-based Deep Learning (DL) model inspired by UNet with a series of encoder\nand decoder units with skip connections for removal of ring artifacts. The\nproposed architecture has been evaluated using the Structural Similarity Index\nMeasure (SSIM) and Mean Squared Error (MSE). Additionally, the results are\ncompared with conventional filter-based non-ML techniques and are found to be\nbetter than the latter.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.LG",
      "physics.app-ph",
      "physics.ins-det"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05983v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05983v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05983v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05699v1",
    "updated": "2024-02-08T14:21:03+00:00",
    "published": "2024-02-08T14:21:03+00:00",
    "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
    "authors": [
      {
        "name": "Xianghe Pang"
      },
      {
        "name": "Shuo Tang"
      },
      {
        "name": "Rui Ye"
      },
      {
        "name": "Yuxin Xiong"
      },
      {
        "name": "Bolun Zhang"
      },
      {
        "name": "Yanfeng Wang"
      },
      {
        "name": "Siheng Chen"
      }
    ],
    "summary": "Aligning large language models (LLMs) with human values is imperative to\nmitigate potential adverse effects resulting from their misuse. Drawing from\nthe sociological insight that acknowledging all parties' concerns is a key\nfactor in shaping human values, this paper proposes a novel direction to align\nLLMs by themselves: social scene simulation. To achieve this, we present\nMATRIX, a novel social scene simulator that emulates realistic scenes around a\nuser's input query, enabling the LLM to take social consequences into account\nbefore responding. MATRIX serves as a virtual rehearsal space, akin to a\nMonopolylogue, where the LLM performs diverse roles related to the query and\npractice by itself. To inject this alignment, we fine-tune the LLM with\nMATRIX-simulated data, ensuring adherence to human values without compromising\ninference speed. We theoretically show that the LLM with MATRIX outperforms\nConstitutional AI under mild assumptions. Finally, extensive experiments\nvalidate that our method outperforms over 10 baselines across 4 benchmarks. As\nevidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning\nwith human values. Code is available at https://github.com/pangxianghe/MATRIX.",
    "comment": "36 pages, 9 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05699v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05699v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05699v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05696v1",
    "updated": "2024-02-08T14:19:29+00:00",
    "published": "2024-02-08T14:19:29+00:00",
    "title": "Fixed width treelike neural networks capacity analysis -- generic activations",
    "authors": [
      {
        "name": "Mihailo Stojnic"
      }
    ],
    "summary": "We consider the capacity of \\emph{treelike committee machines} (TCM) neural\nnetworks. Relying on Random Duality Theory (RDT), \\cite{Stojnictcmspnncaprdt23}\nrecently introduced a generic framework for their capacity analysis. An upgrade\nbased on the so-called \\emph{partially lifted} RDT (pl RDT) was then presented\nin \\cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the\nnetworks with the most typical, \\emph{sign}, activations. Here, on the other\nhand, we focus on networks with other, more general, types of activations and\nshow that the frameworks of\n\\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently\npowerful to enable handling of such scenarios as well. In addition to the\nstandard \\emph{linear} activations, we uncover that particularly convenient\nresults can be obtained for two very commonly used activations, namely, the\n\\emph{quadratic} and \\emph{rectified linear unit (ReLU)} ones. In more concrete\nterms, for each of these activations, we obtain both the RDT and pl RDT based\nmemory capacities upper bound characterization for \\emph{any} given (even)\nnumber of the hidden layer neurons, $d$. In the process, we also uncover the\nfollowing two, rather remarkable, facts: 1) contrary to the common wisdom, both\nsets of results show that the bounding capacity decreases for large $d$ (the\nwidth of the hidden layer) while converging to a constant value; and 2) the\nmaximum bounding capacity is achieved for the networks with precisely\n\\textbf{\\emph{two}} hidden layer neurons! Moreover, the large $d$ converging\nvalues are observed to be in excellent agrement with the statistical physics\nreplica theory based predictions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.PR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05696v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05696v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05696v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05689v1",
    "updated": "2024-02-08T14:07:20+00:00",
    "published": "2024-02-08T14:07:20+00:00",
    "title": "Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits",
    "authors": [
      {
        "name": "Yige Hong"
      },
      {
        "name": "Qiaomin Xie"
      },
      {
        "name": "Yudong Chen"
      },
      {
        "name": "Weina Wang"
      }
    ],
    "summary": "We consider the infinite-horizon, average-reward restless bandit problem in\ndiscrete time. We propose a new class of policies that are designed to drive a\nprogressively larger subset of arms toward the optimal distribution. We show\nthat our policies are asymptotically optimal with an $O(1/\\sqrt{N})$ optimality\ngap for an $N$-armed problem, provided that the single-armed relaxed problem is\nunichain and aperiodic. Our approach departs from most existing work that\nfocuses on index or priority policies, which rely on the Uniform Global\nAttractor Property (UGAP) to guarantee convergence to the optimum, or a\nrecently developed simulation-based policy, which requires a Synchronization\nAssumption (SA).",
    "comment": "41 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC",
      "math.PR",
      "90C40",
      "G.3; I.6"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05689v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05689v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05689v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05680v1",
    "updated": "2024-02-08T13:58:16+00:00",
    "published": "2024-02-08T13:58:16+00:00",
    "title": "Interpretable classifiers for tabular data via discretization and feature selection",
    "authors": [
      {
        "name": "Reijo Jaakkola"
      },
      {
        "name": "Tomi Janhunen"
      },
      {
        "name": "Antti Kuusisto"
      },
      {
        "name": "Masood Feyzbakhsh Rankooh"
      },
      {
        "name": "Miikka Vilander"
      }
    ],
    "summary": "We introduce a method for computing immediately human interpretable yet\naccurate classifiers from tabular data. The classifiers obtained are short\nDNF-formulas, computed via first discretizing the original data to Boolean form\nand then using feature selection coupled with a very fast algorithm for\nproducing the best possible Boolean classifier for the setting. We demonstrate\nthe approach via 14 experiments, obtaining results with accuracies mainly\nsimilar to ones obtained via random forests, XGBoost, and existing results for\nthe same datasets in the literature. In several cases, our approach in fact\noutperforms the reference results in relation to accuracy, even though the main\nobjective of our study is the immediate interpretability of our classifiers. We\nalso prove a new result on the probability that the classifier we obtain from\nreal-life data corresponds to the ideally best classifier with respect to the\nbackground distribution the data comes from.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "I.2.6; F.4.1; I.2.4; E.2"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05680v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05680v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05680v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05675v1",
    "updated": "2024-02-08T13:53:11+00:00",
    "published": "2024-02-08T13:53:11+00:00",
    "title": "Is Adversarial Training with Compressed Datasets Effective?",
    "authors": [
      {
        "name": "Tong Chen"
      },
      {
        "name": "Raghavendra Selvan"
      }
    ],
    "summary": "Dataset Condensation (DC) refers to the recent class of dataset compression\nmethods that generate a smaller, synthetic, dataset from a larger dataset. This\nsynthetic dataset retains the essential information of the original dataset,\nenabling models trained on it to achieve performance levels comparable to those\ntrained on the full dataset. Most current DC methods have mainly concerned with\nachieving high test performance with limited data budget, and have not directly\naddressed the question of adversarial robustness. In this work, we investigate\nthe impact of adversarial robustness on models trained with compressed\ndatasets. We show that the compressed datasets obtained from DC methods are not\neffective in transferring adversarial robustness to models. As a solution to\nimprove dataset compression efficiency and adversarial robustness\nsimultaneously, we propose a novel robustness-aware dataset compression method\nbased on finding the Minimal Finite Covering (MFC) of the dataset. The proposed\nmethod is (1) obtained by one-time computation and is applicable for any model,\n(2) more effective than DC methods when applying adversarial training over MFC,\n(3) provably robust by minimizing the generalized adversarial loss.\nAdditionally, empirical evaluation on three datasets shows that the proposed\nmethod is able to achieve better robustness and performance trade-off compared\nto DC methods such as distribution matching.",
    "comment": "20 pages, 14 figures, 3 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05675v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05675v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05675v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05674v1",
    "updated": "2024-02-08T13:52:35+00:00",
    "published": "2024-02-08T13:52:35+00:00",
    "title": "A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs",
    "authors": [
      {
        "name": "Kasimir Tanner"
      },
      {
        "name": "Matteo Vilucchio"
      },
      {
        "name": "Bruno Loureiro"
      },
      {
        "name": "Florent Krzakala"
      }
    ],
    "summary": "This work investigates adversarial training in the context of margin-based\nlinear classifiers in the high-dimensional regime where the dimension $d$ and\nthe number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. We\nintroduce a tractable mathematical model where the interplay between the data\nand adversarial attacker geometries can be studied, while capturing the core\nphenomenology observed in the adversarial robustness literature. Our main\ntheoretical contribution is an exact asymptotic description of the sufficient\nstatistics for the adversarial empirical risk minimiser, under generic convex\nand non-increasing losses. Our result allow us to precisely characterise which\ndirections in the data are associated with a higher generalisation/robustness\ntrade-off, as defined by a robustness and a usefulness metric. In particular,\nwe unveil the existence of directions which can be defended without penalising\naccuracy. Finally, we show the advantage of defending non-robust features\nduring training, identifying a uniform protection as an inherently effective\ndefence mechanism.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05674v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05674v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05674v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05668v1",
    "updated": "2024-02-08T13:42:50+00:00",
    "published": "2024-02-08T13:42:50+00:00",
    "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
    "authors": [
      {
        "name": "Junjie Chu"
      },
      {
        "name": "Yugeng Liu"
      },
      {
        "name": "Ziqing Yang"
      },
      {
        "name": "Xinyue Shen"
      },
      {
        "name": "Michael Backes"
      },
      {
        "name": "Yang Zhang"
      }
    ],
    "summary": "Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.",
    "comment": "18 pages, 12 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05668v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05668v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05668v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05667v1",
    "updated": "2024-02-08T13:38:23+00:00",
    "published": "2024-02-08T13:38:23+00:00",
    "title": "S$\u03a9$I: Score-based O-INFORMATION Estimation",
    "authors": [
      {
        "name": "Mustapha Bounoua"
      },
      {
        "name": "Giulio Franzese"
      },
      {
        "name": "Pietro Michiardi"
      }
    ],
    "summary": "The analysis of scientific data and complex multivariate systems requires\ninformation quantities that capture relationships among multiple random\nvariables. Recently, new information-theoretic measures have been developed to\novercome the shortcomings of classical ones, such as mutual information, that\nare restricted to considering pairwise interactions. Among them, the concept of\ninformation synergy and redundancy is crucial for understanding the high-order\ndependencies between variables. One of the most prominent and versatile\nmeasures based on this concept is O-information, which provides a clear and\nscalable way to quantify the synergy-redundancy balance in multivariate\nsystems. However, its practical application is limited to simplified cases. In\nthis work, we introduce S$\\Omega$I, which allows for the first time to compute\nO-information without restrictive assumptions about the system. Our experiments\nvalidate our approach on synthetic data, and demonstrate the effectiveness of\nS$\\Omega$I in the context of a real-world use case.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05667v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05667v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05667v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05663v1",
    "updated": "2024-02-08T13:27:10+00:00",
    "published": "2024-02-08T13:27:10+00:00",
    "title": "Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction",
    "authors": [
      {
        "name": "Raphael Chekroun"
      },
      {
        "name": "Han Wang"
      },
      {
        "name": "Jonathan Lee"
      },
      {
        "name": "Marin Toromanoff"
      },
      {
        "name": "Sascha Hornauer"
      },
      {
        "name": "Fabien Moutarde"
      },
      {
        "name": "Maria Laura Delle Monache"
      }
    ],
    "summary": "Accurate real-time traffic state forecasting plays a pivotal role in traffic\ncontrol research. In particular, the CIRCLES consortium project necessitates\npredictive techniques to mitigate the impact of data source delays. After the\nsuccess of the MegaVanderTest experiment, this paper aims at overcoming the\ncurrent system limitations and develop a more suited approach to improve the\nreal-time traffic state estimation for the next iterations of the experiment.\nIn this paper, we introduce the SA-LSTM, a deep forecasting method integrating\nSelf-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM)\nyielding state-of-the-art results in real-time mesoscale traffic forecasting.\nWe extend this approach to multi-step forecasting with the n-step SA-LSTM,\nwhich outperforms traditional multi-step forecasting methods in the trade-off\nbetween short-term and long-term predictions, all while operating in real-time.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05663v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05663v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05663v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05660v1",
    "updated": "2024-02-08T13:24:57+00:00",
    "published": "2024-02-08T13:24:57+00:00",
    "title": "Rethinking Propagation for Unsupervised Graph Domain Adaptation",
    "authors": [
      {
        "name": "Meihan Liu"
      },
      {
        "name": "Zeyu Fang"
      },
      {
        "name": "Zhen Zhang"
      },
      {
        "name": "Ming Gu"
      },
      {
        "name": "Sheng Zhou"
      },
      {
        "name": "Xin Wang"
      },
      {
        "name": "Jiajun Bu"
      }
    ],
    "summary": "Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a\nlabelled source graph to an unlabelled target graph in order to address the\ndistribution shifts between graph domains. Previous works have primarily\nfocused on aligning data from the source and target graph in the representation\nspace learned by graph neural networks (GNNs). However, the inherent\ngeneralization capability of GNNs has been largely overlooked. Motivated by our\nempirical analysis, we reevaluate the role of GNNs in graph domain adaptation\nand uncover the pivotal role of the propagation process in GNNs for adapting to\ndifferent graph domains. We provide a comprehensive theoretical analysis of\nUGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN\nLipschitz for k-layer GNNs, we show that the target risk bound can be tighter\nby removing propagation layers in source graph and stacking multiple\npropagation layers in target graph. Based on the empirical and theoretical\nanalysis mentioned above, we propose a simple yet effective approach called\nA2GNN for graph domain adaptation. Through extensive experiments on real-world\ndatasets, we demonstrate the effectiveness of our proposed A2GNN framework.",
    "comment": "Accepted by AAAI-24",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05660v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05660v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05660v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05650v2",
    "updated": "2024-02-09T07:20:18+00:00",
    "published": "2024-02-08T13:07:31+00:00",
    "title": "Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",
    "authors": [
      {
        "name": "Wei Wang"
      },
      {
        "name": "Huilong Ning"
      },
      {
        "name": "Gaowei Zhang"
      },
      {
        "name": "Libo Liu"
      },
      {
        "name": "Yi Wang"
      }
    ],
    "summary": "Recently, large language models (LLM) based generative AI has been gaining\nmomentum for their impressive high-quality performances in multiple domains,\nparticularly after the release of the ChatGPT. Many believe that they have the\npotential to perform general-purpose problem-solving in software development\nand replace human software developers. Nevertheless, there are in a lack of\nserious investigation into the capability of these LLM techniques in fulfilling\nsoftware development tasks. In a controlled 2 $\\times$ 2 between-subject\nexperiment with 109 participants, we examined whether and to what degree\nworking with ChatGPT was helpful in the coding task and typical software\ndevelopment task and how people work with ChatGPT. We found that while ChatGPT\nperformed well in solving simple coding problems, its performance in supporting\ntypical software development tasks was not that good. We also observed the\ninteractions between participants and ChatGPT and found the relations between\nthe interactions and the outcomes. Our study thus provides first-hand insights\ninto using ChatGPT to fulfill software engineering tasks with real-world\ndevelopers and motivates the need for novel interaction mechanisms that help\ndevelopers effectively work with large language models to achieve desired\noutcomes.",
    "comment": "I have decided to withdraw this article as I am in the process of\n  making further revisions and edits to improve its content. Thank you for your\n  understanding",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05650v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05650v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05650v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05982v1",
    "updated": "2024-02-08T13:02:05+00:00",
    "published": "2024-02-08T13:02:05+00:00",
    "title": "Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design",
    "authors": [
      {
        "name": "Nayoung Kim"
      },
      {
        "name": "Minsu Kim"
      },
      {
        "name": "Jinkyoo Park"
      }
    ],
    "summary": "Antibody design plays a pivotal role in advancing therapeutics. Although deep\nlearning has made rapid progress in this field, existing methods make limited\nuse of general protein knowledge and assume a graphical model (GM) that\nviolates empirical findings on proteins. To address these limitations, we\npresent Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained\nprotein language model (pLM) and encodes a seminal finding on proteins called\nAnfinsen's dogma. Our framework follows a two-step process of sequence\ngeneration with pLM and structure prediction with graph neural network (GNN).\nExperiments show that our approach outperforms state-of-the-art results on\nbenchmark experiments. We also address a critical limitation of\nnon-autoregressive models -- namely, that they tend to generate unrealistic\nsequences with overly repeating tokens. To resolve this, we introduce a\ncomposition-based regularization term to the cross-entropy objective that\nallows an efficient trade-off between high performance and low token\nrepetition. We demonstrate that our approach establishes a Pareto frontier over\nthe current state-of-the-art. Our code is available at\nhttps://github.com/lkny123/AGN.",
    "comment": "18 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.QM",
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05982v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05982v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05982v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05645v1",
    "updated": "2024-02-08T13:00:18+00:00",
    "published": "2024-02-08T13:00:18+00:00",
    "title": "Investigating Reproducibility in Deep Learning-Based Software Fault Prediction",
    "authors": [
      {
        "name": "Adil Mukhtar"
      },
      {
        "name": "Dietmar Jannach"
      },
      {
        "name": "Franz Wotawa"
      }
    ],
    "summary": "Over the past few years, deep learning methods have been applied for a wide\nrange of Software Engineering (SE) tasks, including in particular for the\nimportant task of automatically predicting and localizing faults in software.\nWith the rapid adoption of increasingly complex machine learning models, it\nhowever becomes more and more difficult for scholars to reproduce the results\nthat are reported in the literature. This is in particular the case when the\napplied deep learning models and the evaluation methodology are not properly\ndocumented and when code and data are not shared. Given some recent -- and very\nworrying -- findings regarding reproducibility and progress in other areas of\napplied machine learning, the goal of this work is to analyze to what extent\nthe field of software engineering, in particular in the area of software fault\nprediction, is plagued by similar problems. We have therefore conducted a\nsystematic review of the current literature and examined the level of\nreproducibility of 56 research articles that were published between 2019 and\n2022 in top-tier software engineering conferences. Our analysis revealed that\nscholars are apparently largely aware of the reproducibility problem, and about\ntwo thirds of the papers provide code for their proposed deep learning models.\nHowever, it turned out that in the vast majority of cases, crucial elements for\nreproducibility are missing, such as the code of the compared baselines, code\nfor data pre-processing or code for hyperparameter tuning. In these cases, it\ntherefore remains challenging to exactly reproduce the results in the current\nresearch literature. Overall, our meta-analysis therefore calls for improved\nresearch practices to ensure the reproducibility of machine-learning based\nresearch.",
    "comment": "12 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05645v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05645v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05645v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05643v2",
    "updated": "2024-02-13T15:38:11+00:00",
    "published": "2024-02-08T12:58:07+00:00",
    "title": "Improving Token-Based World Models with Parallel Observation Prediction",
    "authors": [
      {
        "name": "Lior Cohen"
      },
      {
        "name": "Kaixin Wang"
      },
      {
        "name": "Bingyi Kang"
      },
      {
        "name": "Shie Mannor"
      }
    ],
    "summary": "Motivated by the success of Transformers when applied to sequences of\ndiscrete symbols, token-based world models (TBWMs) were recently proposed as\nsample-efficient methods. In TBWMs, the world model consumes agent experience\nas a language-like sequence of tokens, where each observation constitutes a\nsub-sequence. However, during imagination, the sequential token-by-token\ngeneration of next observations results in a severe bottleneck, leading to long\ntraining times, poor GPU utilization, and limited representations. To resolve\nthis bottleneck, we devise a novel Parallel Observation Prediction (POP)\nmechanism. POP augments a Retentive Network (RetNet) with a novel forward mode\ntailored to our reinforcement learning setting. We incorporate POP in a novel\nTBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster\nimagination compared to prior TBWMs. REM attains superhuman performance on 12\nout of 26 games of the Atari 100K benchmark, while training in less than 12\nhours. Our code is available at \\url{https://github.com/leor-c/REM}.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05643v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05643v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05643v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05639v1",
    "updated": "2024-02-08T12:50:38+00:00",
    "published": "2024-02-08T12:50:38+00:00",
    "title": "Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients",
    "authors": [
      {
        "name": "Caio Peixoto"
      },
      {
        "name": "Yuri Saporito"
      },
      {
        "name": "Yuri Fonseca"
      }
    ],
    "summary": "This paper proposes SAGD-IV, a novel framework for conducting nonparametric\ninstrumental variable (NPIV) regression by employing stochastic approximate\ngradients to minimize the projected populational risk. Instrumental Variables\n(IVs) are widely used in econometrics to address estimation problems in the\npresence of unobservable confounders, and the Machine Learning community has\ndevoted significant effort to improving existing methods and devising new ones\nin the NPIV setting, which is known to be an ill-posed linear inverse problem.\nWe provide theoretical support for our algorithm and further exemplify its\ncompetitive performance through empirical experiments. Furthermore, we address,\nwith promising results, the case of binary outcomes, which has not received as\nmuch attention from the community as its continuous counterpart.",
    "comment": "22 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05639v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05639v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05639v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05636v1",
    "updated": "2024-02-08T12:47:57+00:00",
    "published": "2024-02-08T12:47:57+00:00",
    "title": "The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment",
    "authors": [
      {
        "name": "Sayan Chatterjee"
      },
      {
        "name": "Ching Louis Liu"
      },
      {
        "name": "Gareth Rowland"
      },
      {
        "name": "Tim Hogarth"
      }
    ],
    "summary": "The increasing popularity of AI, particularly Large Language Models (LLMs),\nhas significantly impacted various domains, including Software Engineering.\nThis study explores the integration of AI tools in software engineering\npractices within a large organization. We focus on ANZ Bank, which employs over\n5000 engineers covering all aspects of the software development life cycle.\nThis paper details an experiment conducted using GitHub Copilot, a notable AI\ntool, within a controlled environment to evaluate its effectiveness in\nreal-world engineering tasks. Additionally, this paper shares initial findings\non the productivity improvements observed after GitHub Copilot was adopted on a\nlarge scale, with about 1000 engineers using it. ANZ Bank's six-week experiment\nwith GitHub Copilot included two weeks of preparation and four weeks of active\ntesting. The study evaluated participant sentiment and the tool's impact on\nproductivity, code quality, and security. Initially, participants used GitHub\nCopilot for proposed use-cases, with their feedback gathered through regular\nsurveys. In the second phase, they were divided into Control and Copilot\ngroups, each tackling the same Python challenges, and their experiences were\nagain surveyed. Results showed a notable boost in productivity and code quality\nwith GitHub Copilot, though its impact on code security remained inconclusive.\nParticipant responses were overall positive, confirming GitHub Copilot's\neffectiveness in large-scale software engineering environments. Early data from\n1000 engineers also indicated a significant increase in productivity and job\nsatisfaction.",
    "comment": "16 pages, 4 figures. in proceeding for 10th International Conference\n  on Software Engineering (SEC 2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05636v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05636v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05636v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05628v1",
    "updated": "2024-02-08T12:35:41+00:00",
    "published": "2024-02-08T12:35:41+00:00",
    "title": "RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization",
    "authors": [
      {
        "name": "Zhikai Li"
      },
      {
        "name": "Xuewen Liu"
      },
      {
        "name": "Jing Zhang"
      },
      {
        "name": "Qingyi Gu"
      }
    ],
    "summary": "Large transformer models have demonstrated remarkable success. Post-training\nquantization (PTQ), which requires only a small dataset for calibration and\navoids end-to-end retraining, is a promising solution for compressing these\nlarge models. Regrettably, existing PTQ methods typically exhibit non-trivial\nperformance loss. We find that the performance bottleneck stems from\nover-consideration of hardware compatibility in the quantization process,\ncompelling them to reluctantly employ simple quantizers, albeit at the expense\nof accuracy. With the above insights, we propose RepQuant, a novel PTQ\nframework with quantization-inference decoupling paradigm to address the above\nissues. RepQuant employs complex quantizers in the quantization process and\nsimplified quantizers in the inference process, and performs mathematically\nequivalent transformations between the two through quantization scale\nreparameterization, thus ensuring both accurate quantization and efficient\ninference. More specifically, we focus on two components with extreme\ndistributions: LayerNorm activations and Softmax activations. Initially, we\napply channel-wise quantization and log$\\sqrt{2}$ quantization, respectively,\nwhich are tailored to their distributions. In particular, for the former, we\nintroduce a learnable per-channel dual clipping scheme, which is designed to\nefficiently identify outliers in the unbalanced activations with fine\ngranularity. Then, we reparameterize the scales to hardware-friendly layer-wise\nquantization and log2 quantization for inference. Moreover, quantized weight\nreconstruction is seamlessly integrated into the above procedure to further\npush the performance limits. Extensive experiments are performed on different\nlarge-scale transformer variants on multiple tasks, including vision, language,\nand multi-modal transformers, and RepQuant encouragingly demonstrates\nsignificant performance advantages.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05628v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05628v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05628v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05627v1",
    "updated": "2024-02-08T12:31:08+00:00",
    "published": "2024-02-08T12:31:08+00:00",
    "title": "Binding Dynamics in Rotating Features",
    "authors": [
      {
        "name": "Sindy L\u00f6we"
      },
      {
        "name": "Francesco Locatello"
      },
      {
        "name": "Max Welling"
      }
    ],
    "summary": "In human cognition, the binding problem describes the open question of how\nthe brain flexibly integrates diverse information into cohesive object\nrepresentations. Analogously, in machine learning, there is a pursuit for\nmodels capable of strong generalization and reasoning by learning\nobject-centric representations in an unsupervised manner. Drawing from\nneuroscientific theories, Rotating Features learn such representations by\nintroducing vector-valued features that encapsulate object characteristics in\ntheir magnitudes and object affiliation in their orientations. The\n\"$\\chi$-binding\" mechanism, embedded in every layer of the architecture, has\nbeen shown to be crucial, but remains poorly understood. In this paper, we\npropose an alternative \"cosine binding\" mechanism, which explicitly computes\nthe alignment between features and adjusts weights accordingly, and we show\nthat it achieves equivalent performance. This allows us to draw direct\nconnections to self-attention and biological neural processes, and to shed\nlight on the fundamental dynamics for object-centric representations to emerge\nin Rotating Features.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "q-bio.NC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05627v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05627v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05627v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05626v1",
    "updated": "2024-02-08T12:30:29+00:00",
    "published": "2024-02-08T12:30:29+00:00",
    "title": "The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding",
    "authors": [
      {
        "name": "Zhengqing Wu"
      },
      {
        "name": "Berfin Simsek"
      },
      {
        "name": "Francois Ged"
      }
    ],
    "summary": "In this paper, we investigate the loss landscape of one-hidden-layer neural\nnetworks with ReLU-like activation functions trained with the empirical squared\nloss. As the activation function is non-differentiable, it is so far unclear\nhow to completely characterize the stationary points. We propose the conditions\nfor stationarity that apply to both non-differentiable and differentiable\ncases. Additionally, we show that, if a stationary point does not contain\n\"escape neurons\", which are defined with first-order conditions, then it must\nbe a local minimum. Moreover, for the scalar-output case, the presence of an\nescape neuron guarantees that the stationary point is not a local minimum. Our\nresults refine the description of the saddle-to-saddle training process\nstarting from infinitesimally small (vanishing) initialization for shallow\nReLU-like networks, linking saddle escaping directly with the parameter changes\nof escape neurons. Moreover, we are also able to fully discuss how network\nembedding, which is to instantiate a narrower network within a wider network,\nreshapes the stationary points.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05626v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05626v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05626v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05624v1",
    "updated": "2024-02-08T12:28:18+00:00",
    "published": "2024-02-08T12:28:18+00:00",
    "title": "Efficient Models for the Detection of Hate, Abuse and Profanity",
    "authors": [
      {
        "name": "Christoph Tillmann"
      },
      {
        "name": "Aashka Trivedi"
      },
      {
        "name": "Bishwaranjan Bhattacharjee"
      }
    ],
    "summary": "Large Language Models (LLMs) are the cornerstone for many Natural Language\nProcessing (NLP) tasks like sentiment analysis, document classification, named\nentity recognition, question answering, summarization, etc. LLMs are often\ntrained on data which originates from the web. This data is prone to having\ncontent with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,\nplease refer to the Appendix. Due to the LLMs being exposed to HAP content\nduring training, the models learn it and may then generate hateful or profane\ncontent. For example, when the open-source RoBERTa model (specifically, the\nRoBERTA base model) from the HuggingFace (HF) Transformers library is prompted\nto replace the mask token in `I do not know that Persian people are that MASK`\nit returns the word `stupid` with the highest score. This is unacceptable in\ncivil discourse.The detection of Hate, Abuse and Profanity in text is a vital\ncomponent of creating civil and unbiased LLMs, which is needed not only for\nEnglish, but for all languages. In this article, we briefly describe the\ncreation of HAP detectors and various ways of using them to make models civil\nand acceptable in the output they generate.",
    "comment": "8 pages, 7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05624v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05624v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05624v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05616v1",
    "updated": "2024-02-08T12:19:32+00:00",
    "published": "2024-02-08T12:19:32+00:00",
    "title": "Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks",
    "authors": [
      {
        "name": "Ben Fauber"
      }
    ],
    "summary": "We propose that small pretrained foundational generative language models with\nmillions of parameters can be utilized as a general learning framework for\nsequence-based tasks. Our proposal overcomes the computational resource, skill\nset, and timeline challenges associated with training neural networks and\nlanguage models from scratch. Further, our approach focuses on creating small\nand highly specialized models that can accurately execute a challenging task of\nwhich the base model is incapable of performing. We demonstrate that 125M,\n350M, and 1.3B parameter pretrained foundational language models can be\ninstruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve\nnear state-of-the-art results on challenging cheminformatics tasks. We also\ndemonstrate the role of successive language model fine-tuning epochs on\nimproved outcomes, as well as the importance of both data formatting and\npretrained foundational language model selection for instruction fine-tuning\nsuccess.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05616v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05616v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05616v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05610v1",
    "updated": "2024-02-08T12:08:52+00:00",
    "published": "2024-02-08T12:08:52+00:00",
    "title": "Extending 6D Object Pose Estimators for Stereo Vision",
    "authors": [
      {
        "name": "Thomas P\u00f6llabauer"
      },
      {
        "name": "Jan Emrich"
      },
      {
        "name": "Volker Knauthe"
      },
      {
        "name": "Arjan Kuijper"
      }
    ],
    "summary": "Estimating the 6D pose of objects accurately, quickly, and robustly remains a\ndifficult task. However, recent methods for directly regressing poses from RGB\nimages using dense features have achieved state-of-the-art results. Stereo\nvision, which provides an additional perspective on the object, can help reduce\npose ambiguity and occlusion. Moreover, stereo can directly infer the distance\nof an object, while mono-vision requires internalized knowledge of the object's\nsize. To extend the state-of-the-art in 6D object pose estimation to stereo, we\ncreated a BOP compatible stereo version of the YCB-V dataset. Our method\noutperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo\nvision and can easily be adopted for other dense feature-based algorithms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05610v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05610v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05610v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05605v1",
    "updated": "2024-02-08T12:04:43+00:00",
    "published": "2024-02-08T12:04:43+00:00",
    "title": "Optimizing Delegation in Collaborative Human-AI Hybrid Teams",
    "authors": [
      {
        "name": "Andrew Fuchs"
      },
      {
        "name": "Andrea Passarella"
      },
      {
        "name": "Marco Conti"
      }
    ],
    "summary": "When humans and autonomous systems operate together as what we refer to as a\nhybrid team, we of course wish to ensure the team operates successfully and\neffectively. We refer to team members as agents. In our proposed framework, we\naddress the case of hybrid teams in which, at any time, only one team member\n(the control agent) is authorized to act as control for the team. To determine\nthe best selection of a control agent, we propose the addition of an AI manager\n(via Reinforcement Learning) which learns as an outside observer of the team.\nThe manager learns a model of behavior linking observations of agent\nperformance and the environment/world the team is operating in, and from these\nobservations makes the most desirable selection of a control agent. We restrict\nthe manager task by introducing a set of constraints. The manager constraints\nindicate acceptable team operation, so a violation occurs if the team enters a\ncondition which is unacceptable and requires manager intervention. To ensure\nminimal added complexity or potential inefficiency for the team, the manager\nshould attempt to minimize the number of times the team reaches a constraint\nviolation and requires subsequent manager intervention. Therefore our manager\nis optimizing its selection of authorized agents to boost overall team\nperformance while minimizing the frequency of manager intervention. We\ndemonstrate our manager performance in a simulated driving scenario\nrepresenting the case of a hybrid team of agents composed of a human driver and\nautonomous driving system. We perform experiments for our driving scenario with\ninterfering vehicles, indicating the need for collision avoidance and proper\nspeed control. Our results indicate a positive impact of our manager, with some\ncases resulting in increased team performance up to ~187% that of the best solo\nagent performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05605v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05605v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05605v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05602v1",
    "updated": "2024-02-08T12:01:24+00:00",
    "published": "2024-02-08T12:01:24+00:00",
    "title": "AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers",
    "authors": [
      {
        "name": "Reduan Achtibat"
      },
      {
        "name": "Sayed Mohammad Vakilzadeh Hatefi"
      },
      {
        "name": "Maximilian Dreyer"
      },
      {
        "name": "Aakriti Jain"
      },
      {
        "name": "Thomas Wiegand"
      },
      {
        "name": "Sebastian Lapuschkin"
      },
      {
        "name": "Wojciech Samek"
      }
    ],
    "summary": "Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a singular backward pass. Through extensive evaluations against existing\nmethods on Llama 2, Flan-T5 and the Vision Transformer architecture, we\ndemonstrate that our proposed approach surpasses alternative methods in terms\nof faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an open-source\nimplementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05602v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05602v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05602v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05593v1",
    "updated": "2024-02-08T11:46:26+00:00",
    "published": "2024-02-08T11:46:26+00:00",
    "title": "A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only",
    "authors": [
      {
        "name": "Thomas P\u00f6llabauer"
      },
      {
        "name": "Julius K\u00fchn"
      }
    ],
    "summary": "In medieval times, stuccoworkers used a red color, called sinopia, to first\ncreate a sketch of the to-be-made statue on the wall. Today, many of these\nstatues are destroyed, but using the original drawings, deriving from the red\ncolor also called sinopia, we can reconstruct how the final statue might have\nlooked.We propose a fully-automated approach to reconstruct a point cloud and\nshow preliminary results by generating a color-image, a depth-map, as well as\nsurface normals requiring only a single sketch, and without requiring a\ncollection of other, similar samples. Our proposed solution allows real-time\nreconstruction on-site, for instance, within an exhibition, or to generate a\nuseful starting point for an expert, trying to manually reconstruct the statue,\nall while using only synthetic data for training.",
    "comment": null,
    "journal_ref": "Eurographics Workshop on Graphics and Cultural Heritage 2022",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05593v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05593v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05593v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05591v1",
    "updated": "2024-02-08T11:44:25+00:00",
    "published": "2024-02-08T11:44:25+00:00",
    "title": "SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels",
    "authors": [
      {
        "name": "Juhwan Choi"
      },
      {
        "name": "Kyohoon Jin"
      },
      {
        "name": "Junho Lee"
      },
      {
        "name": "Sangmin Song"
      },
      {
        "name": "Youngbin Kim"
      }
    ],
    "summary": "Rule-based text data augmentation is widely used for NLP tasks due to its\nsimplicity. However, this method can potentially damage the original meaning of\nthe text, ultimately hurting the performance of the model. To overcome this\nlimitation, we propose a straightforward technique for applying soft labels to\naugmented data. We conducted experiments across seven different classification\ntasks and empirically demonstrated the effectiveness of our proposed approach.\nWe have publicly opened our source code for reproducibility.",
    "comment": "ICLR 2023 Tiny Papers",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05591v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05591v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05591v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05584v1",
    "updated": "2024-02-08T11:36:23+00:00",
    "published": "2024-02-08T11:36:23+00:00",
    "title": "AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes",
    "authors": [
      {
        "name": "Juhwan Choi"
      },
      {
        "name": "Kyohoon Jin"
      },
      {
        "name": "Junho Lee"
      },
      {
        "name": "Sangmin Song"
      },
      {
        "name": "Youngbin Kim"
      }
    ],
    "summary": "Text data augmentation is a complex problem due to the discrete nature of\nsentences. Although rule-based augmentation methods are widely adopted in\nreal-world applications because of their simplicity, they suffer from potential\nsemantic damage. Previous researchers have suggested easy data augmentation\nwith soft labels (softEDA), employing label smoothing to mitigate this problem.\nHowever, finding the best factor for each model and dataset is challenging;\ntherefore, using softEDA in real-world applications is still difficult. In this\npaper, we propose adapting AutoAugment to solve this problem. The experimental\nresults suggest that the proposed method can boost existing augmentation\nmethods and that rule-based methods can enhance cutting-edge pre-trained\nlanguage models. We offer the source code.",
    "comment": "EACL 2024 Student Research Workshop",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05584v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05584v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05584v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05576v1",
    "updated": "2024-02-08T11:23:11+00:00",
    "published": "2024-02-08T11:23:11+00:00",
    "title": "Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry",
    "authors": [
      {
        "name": "Anastasis Kratsios"
      },
      {
        "name": "A. Martina Neuman"
      },
      {
        "name": "Gudmund Pammer"
      }
    ],
    "summary": "Many of the foundations of machine learning rely on the idealized premise\nthat all input and output spaces are infinite, e.g.~$\\mathbb{R}^d$. This core\nassumption is systematically violated in practice due to digital computing\nlimitations from finite machine precision, rounding, and limited RAM. In short,\ndigital computers operate on finite grids in $\\mathbb{R}^d$. By exploiting\nthese discrete structures, we show the curse of dimensionality in statistical\nlearning is systematically broken when models are implemented on real\ncomputers. Consequentially, we obtain new generalization bounds with\ndimension-free rates for kernel and deep ReLU MLP regressors, which are\nimplemented on real-world machines.\n  Our results are derived using a new non-asymptotic concentration of measure\nresult between a probability measure over any finite metric space and its\nempirical version associated with $N$ i.i.d. samples when measured in the\n$1$-Wasserstein distance. Unlike standard concentration of measure results, the\nconcentration rates in our bounds do not hold uniformly for all sample sizes\n$N$; instead, our rates can adapt to any given $N$. This yields significantly\ntighter bounds for realistic sample sizes while achieving the optimal\nworst-case rate of $\\mathcal{O}(1/N^{1/2})$ for massive. Our results are built\non new techniques combining metric embedding theory with optimal transport",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05576v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05576v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05576v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05575v1",
    "updated": "2024-02-08T11:19:58+00:00",
    "published": "2024-02-08T11:19:58+00:00",
    "title": "Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits",
    "authors": [
      {
        "name": "Subham Pokhriyal"
      },
      {
        "name": "Shweta Jain"
      },
      {
        "name": "Ganesh Ghalme"
      },
      {
        "name": "Swapnil Dhamal"
      },
      {
        "name": "Sujit Gujar"
      }
    ],
    "summary": "Existing approaches to fairness in stochastic multi-armed bandits (MAB)\nprimarily focus on exposure guarantee to individual arms. When arms are\nnaturally grouped by certain attribute(s), we propose Bi-Level Fairness, which\nconsiders two levels of fairness. At the first level, Bi-Level Fairness\nguarantees a certain minimum exposure to each group. To address the unbalanced\nallocation of pulls to individual arms within a group, we consider meritocratic\nfairness at the second level, which ensures that each arm is pulled according\nto its merit within the group. Our work shows that we can adapt a UCB-based\nalgorithm to achieve a Bi-Level Fairness by providing (i) anytime Group\nExposure Fairness guarantees and (ii) ensuring individual-level Meritocratic\nFairness within each group. We first show that one can decompose regret bounds\ninto two components: (a) regret due to anytime group exposure fairness and (b)\nregret due to meritocratic fairness within each group. Our proposed algorithm\nBF-UCB balances these two regrets optimally to achieve the upper bound of\n$O(\\sqrt{T})$ on regret; $T$ being the stopping time. With the help of\nsimulated experiments, we further show that BF-UCB achieves sub-linear regret;\nprovides better group and individual exposure guarantees compared to existing\nalgorithms; and does not result in a significant drop in reward with respect to\nUCB algorithm, which does not impose any fairness constraint.",
    "comment": "Accepted in AAMAS 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05575v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05575v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05575v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05571v1",
    "updated": "2024-02-08T11:16:13+00:00",
    "published": "2024-02-08T11:16:13+00:00",
    "title": "Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study",
    "authors": [
      {
        "name": "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
      },
      {
        "name": "Jos\u00e9-Manuel Alija-P\u00e9rez"
      },
      {
        "name": "Maria-Esther Vidal"
      },
      {
        "name": "Rafael Pastor-Vargas"
      },
      {
        "name": "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s"
      }
    ],
    "summary": "Background: Eating disorders are increasingly prevalent, and social networks\noffer valuable information.\n  Objective: Our goal was to identify efficient machine learning models for\ncategorizing tweets related to eating disorders.\n  Methods: Over three months, we collected tweets about eating disorders. A\n2,000-tweet subset was labeled for: (1) being written by individuals with\neating disorders, (2) promoting eating disorders, (3) informativeness, and (4)\nscientific content. Both traditional machine learning and deep learning models\nwere employed for classification, assessing accuracy, F1 score, and\ncomputational time.\n  Results: From 1,058,957 collected tweets, transformer-based bidirectional\nencoder representations achieved the highest F1 scores (71.1%-86.4%) across all\nfour categories.\n  Conclusions: Transformer-based models outperform traditional techniques in\nclassifying eating disorder-related tweets, though they require more\ncomputational resources.",
    "comment": null,
    "journal_ref": "JMIR Medical Informatics, Volume 10, Issue 2, 2022, ID e34492",
    "doi": "10.2196/34492",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.2196/34492",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05571v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05571v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05571v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05569v1",
    "updated": "2024-02-08T11:10:39+00:00",
    "published": "2024-02-08T11:10:39+00:00",
    "title": "Hypergraph Node Classification With Graph Neural Networks",
    "authors": [
      {
        "name": "Bohan Tang"
      },
      {
        "name": "Zexi Liu"
      },
      {
        "name": "Keyue Jiang"
      },
      {
        "name": "Siheng Chen"
      },
      {
        "name": "Xiaowen Dong"
      }
    ],
    "summary": "Hypergraphs, with hyperedges connecting more than two nodes, are key for\nmodelling higher-order interactions in real-world data. The success of graph\nneural networks (GNNs) reveals the capability of neural networks to process\ndata with pairwise interactions. This inspires the usage of neural networks for\ndata with higher-order interactions, thereby leading to the development of\nhypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically\nconsidered distinct since they are designed for data on different geometric\ntopologies. However, in this paper, we theoretically demonstrate that, in the\ncontext of node classification, most HyperGNNs can be approximated using a GNN\nwith a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a\nsimple and efficient framework comprising a GNN and a weighted clique expansion\n(WCE), for hypergraph node classification. Experiments on nine real-world\nhypergraph node classification benchmarks showcase that WCE-GNN demonstrates\nnot only higher classification accuracy compared to state-of-the-art HyperGNNs,\nbut also superior memory and runtime efficiency.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05569v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05569v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05569v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05566v1",
    "updated": "2024-02-08T11:04:11+00:00",
    "published": "2024-02-08T11:04:11+00:00",
    "title": "Succint Interaction-Aware Explanations",
    "authors": [
      {
        "name": "Sascha Xu"
      },
      {
        "name": "Joscha C\u00fcppers"
      },
      {
        "name": "Jilles Vreeken"
      }
    ],
    "summary": "SHAP is a popular approach to explain black-box models by revealing the\nimportance of individual features. As it ignores feature interactions, SHAP\nexplanations can be confusing up to misleading. NSHAP, on the other hand,\nreports the additive importance for all subsets of features. While this does\ninclude all interacting sets of features, it also leads to an exponentially\nsized, difficult to interpret explanation. In this paper, we propose to combine\nthe best of these two worlds, by partitioning the features into parts that\nsignificantly interact, and use these parts to compose a succinct,\ninterpretable, additive explanation. We derive a criterion by which to measure\nthe representativeness of such a partition for a models behavior, traded off\nagainst the complexity of the resulting explanation. To efficiently find the\nbest partition out of super-exponentially many, we show how to prune\nsub-optimal solutions using a statistical test, which not only improves runtime\nbut also helps to detect spurious interactions. Experiments on synthetic and\nreal world data show that our explanations are both more accurate resp. more\neasily interpretable than those of SHAP and NSHAP.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05566v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05566v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05566v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05563v1",
    "updated": "2024-02-08T11:02:06+00:00",
    "published": "2024-02-08T11:02:06+00:00",
    "title": "Neural Multigrid Architectures",
    "authors": [
      {
        "name": "Vladimir Fanaskov"
      }
    ],
    "summary": "We propose a convenient matrix-free neural architecture for the multigrid\nmethod. The architecture is simple enough to be implemented in less than fifty\nlines of code, yet it encompasses a large number of distinct multigrid solvers.\nWe argue that a fixed neural network without dense layers can not realize an\nefficient iterative method. Because of that, standard training protocols do not\nlead to competitive solvers. To overcome this difficulty, we use parameter\nsharing and serialization of layers. The resulting network can be trained on\nlinear problems with thousands of unknowns and retains its efficiency on\nproblems with millions of unknowns. From the point of view of numerical linear\nalgebra network's training corresponds to finding optimal smoothers for the\ngeometric multigrid method. We demonstrate our approach on a few second-order\nelliptic equations. For tested linear systems, we obtain from two to five times\nsmaller spectral radius of the error propagation matrix compare to a basic\nlinear multigrid with Jacobi smoother.",
    "comment": null,
    "journal_ref": null,
    "doi": "10.1109/IJCNN52387.2021.9533736",
    "primary_category": "math.NA",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/IJCNN52387.2021.9533736",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05563v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05563v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05563v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06684v1",
    "updated": "2024-02-08T11:00:51+00:00",
    "published": "2024-02-08T11:00:51+00:00",
    "title": "Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal",
    "authors": [
      {
        "name": "Filip Sabo"
      },
      {
        "name": "Martin Claverie"
      },
      {
        "name": "Michele Meroni"
      },
      {
        "name": "Arthur Hrast Essenfelder"
      }
    ],
    "summary": "This paper investigated the potential of a multivariate Transformer model to\nforecast the temporal trajectory of the Fraction of Absorbed Photosynthetically\nActive Radiation (FAPAR) for short (1 month) and long horizon (more than 1\nmonth) periods at the regional level in Europe and North Africa. The input data\ncovers the period from 2002 to 2022 and includes remote sensing and weather\ndata for modelling FAPAR predictions. The model was evaluated using a leave one\nyear out cross-validation and compared with the climatological benchmark.\nResults show that the transformer model outperforms the benchmark model for one\nmonth forecasting horizon, after which the climatological benchmark is better.\nThe RMSE values of the transformer model ranged from 0.02 to 0.04 FAPAR units\nfor the first 2 months of predictions. Overall, the tested Transformer model is\na valid method for FAPAR forecasting, especially when combined with weather\ndata and used for short-term predictions.",
    "comment": null,
    "journal_ref": "Proceedings of the 2023 conference on Big Data from Space, Soille,\n  P., Lumnitz, S. and Albani, S. editor(s), Publications Office of the European\n  Union, Luxembourg, 2023, JRC135493",
    "doi": "10.2760/46796",
    "primary_category": "physics.ao-ph",
    "categories": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.2760/46796",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.06684v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06684v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06684v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05558v1",
    "updated": "2024-02-08T10:52:37+00:00",
    "published": "2024-02-08T10:52:37+00:00",
    "title": "Flashback: Understanding and Mitigating Forgetting in Federated Learning",
    "authors": [
      {
        "name": "Mohammed Aljahdali"
      },
      {
        "name": "Ahmed M. Abdelmoniem"
      },
      {
        "name": "Marco Canini"
      },
      {
        "name": "Samuel Horv\u00e1th"
      }
    ],
    "summary": "In Federated Learning (FL), forgetting, or the loss of knowledge across\nrounds, hampers algorithm convergence, particularly in the presence of severe\ndata heterogeneity among clients. This study explores the nuances of this\nissue, emphasizing the critical role of forgetting in FL's inefficient learning\nwithin heterogeneous data contexts. Knowledge loss occurs in both client-local\nupdates and server-side aggregation steps; addressing one without the other\nfails to mitigate forgetting. We introduce a metric to measure forgetting\ngranularly, ensuring distinct recognition amid new knowledge acquisition.\nLeveraging these insights, we propose Flashback, an FL algorithm with a dynamic\ndistillation approach that is used to regularize the local models, and\neffectively aggregate their knowledge. Across different benchmarks, Flashback\noutperforms other methods, mitigates forgetting, and achieves faster\nround-to-target-accuracy, by converging in 6 to 16 rounds.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05558v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05558v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05558v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05552v1",
    "updated": "2024-02-08T10:42:47+00:00",
    "published": "2024-02-08T10:42:47+00:00",
    "title": "Learning quantum Hamiltonians at any temperature in polynomial time with Chebyshev and bit complexity",
    "authors": [
      {
        "name": "Ales Wodecki"
      },
      {
        "name": "Jakub Marecek"
      }
    ],
    "summary": "We consider the problem of learning local quantum Hamiltonians given copies\nof their Gibbs state at a known inverse temperature, following Haah et al.\n[2108.04842] and Bakshi et al. [arXiv:2310.02243]. Our main technical\ncontribution is a new flat polynomial approximation of the exponential function\nbased on the Chebyshev expansion, which enables the formulation of learning\nquantum Hamiltonians as a polynomial optimization problem. This, in turn, can\nbenefit from the use of moment/SOS relaxations, whose polynomial bit complexity\nrequires careful analysis [O'Donnell, ITCS 2017]. Finally, we show that\nlearning a $k$-local Hamiltonian, whose dual interaction graph is of bounded\ndegree, runs in polynomial time under mild assumptions.",
    "comment": "16 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "quant-ph",
    "categories": [
      "quant-ph",
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05552v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05552v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05552v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05547v1",
    "updated": "2024-02-08T10:32:06+00:00",
    "published": "2024-02-08T10:32:06+00:00",
    "title": "Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset",
    "authors": [
      {
        "name": "Hengguan Huang"
      },
      {
        "name": "Songtao Wang"
      },
      {
        "name": "Hongfu Liu"
      },
      {
        "name": "Hao Wang"
      },
      {
        "name": "Ye Wang"
      }
    ],
    "summary": "Traditional applications of natural language processing (NLP) in healthcare\nhave predominantly focused on patient-centered services, enhancing patient\ninteractions and care delivery, such as through medical dialogue systems.\nHowever, the potential of NLP to benefit inexperienced doctors, particularly in\nareas such as communicative medical coaching, remains largely unexplored. We\nintroduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within\nthis framework, both a patient agent and a coaching agent collaboratively\nsupport medical learners in practicing their medical communication skills\nduring consultations. Unlike traditional dialogue systems, ChatCoach provides a\nsimulated environment where a human doctor can engage in medical dialogue with\na patient agent. Simultaneously, a coaching agent provides real-time feedback\nto the doctor. To construct the ChatCoach system, we developed a dataset and\nintegrated Large Language Models such as ChatGPT and Llama2, aiming to assess\ntheir effectiveness in communicative medical coaching tasks. Our comparative\nanalysis demonstrates that instruction-tuned Llama2 significantly outperforms\nChatGPT's prompting-based approaches.",
    "comment": "NA",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05547v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05547v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05547v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05546v1",
    "updated": "2024-02-08T10:29:46+00:00",
    "published": "2024-02-08T10:29:46+00:00",
    "title": "Offline Actor-Critic Reinforcement Learning Scales to Large Models",
    "authors": [
      {
        "name": "Jost Tobias Springenberg"
      },
      {
        "name": "Abbas Abdolmaleki"
      },
      {
        "name": "Jingwei Zhang"
      },
      {
        "name": "Oliver Groth"
      },
      {
        "name": "Michael Bloesch"
      },
      {
        "name": "Thomas Lampe"
      },
      {
        "name": "Philemon Brakel"
      },
      {
        "name": "Sarah Bechtle"
      },
      {
        "name": "Steven Kapturowski"
      },
      {
        "name": "Roland Hafner"
      },
      {
        "name": "Nicolas Heess"
      },
      {
        "name": "Martin Riedmiller"
      }
    ],
    "summary": "We show that offline actor-critic reinforcement learning can scale to large\nmodels - such as transformers - and follows similar scaling laws as supervised\nlearning. We find that offline actor-critic algorithms can outperform strong,\nsupervised, behavioral cloning baselines for multi-task training on a large\ndataset containing both sub-optimal and expert behavior on 132 continuous\ncontrol tasks. We introduce a Perceiver-based actor-critic model and elucidate\nthe key model features needed to make offline RL work with self- and\ncross-attention modules. Overall, we find that: i) simple offline actor critic\nalgorithms are a natural choice for gradually moving away from the currently\npredominant paradigm of behavioral cloning, and ii) via offline RL it is\npossible to learn multi-task policies that master many domains simultaneously,\nincluding real robotics tasks, from sub-optimal demonstrations or\nself-generated data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05546v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05546v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05546v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05543v1",
    "updated": "2024-02-08T10:22:45+00:00",
    "published": "2024-02-08T10:22:45+00:00",
    "title": "Machine learning applied to omics data",
    "authors": [
      {
        "name": "Aida Calvi\u00f1o"
      },
      {
        "name": "Almudena Moreno-Ribera"
      },
      {
        "name": "Silvia Pineda"
      }
    ],
    "summary": "In this chapter we illustrate the use of some Machine Learning techniques in\nthe context of omics data. More precisely, we review and evaluate the use of\nRandom Forest and Penalized Multinomial Logistic Regression for integrative\nanalysis of genomics and immunomics in pancreatic cancer. Furthermore, we\npropose the use of association rules with predictive purposes to overcome the\nlow predictive power of the previously mentioned models. Finally, we apply the\nreviewed methods to a real data set from TCGA made of 107 tumoral pancreatic\nsamples and 117,486 germline SNPs, showing the good performance of the proposed\nmethods to predict the immunological infiltration in pancreatic cancer.",
    "comment": "Part of the book \"Statistical Methods at the Forefront of Biomedical\n  Advances\" published by Springer Cham",
    "journal_ref": null,
    "doi": "10.1007/978-3-031-32729-2_2",
    "primary_category": "q-bio.GN",
    "categories": [
      "q-bio.GN",
      "cs.LG",
      "stat.AP"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1007/978-3-031-32729-2_2",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05543v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05543v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05543v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05541v1",
    "updated": "2024-02-08T10:22:12+00:00",
    "published": "2024-02-08T10:22:12+00:00",
    "title": "Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions",
    "authors": [
      {
        "name": "Jialuo He"
      },
      {
        "name": "Wei Chen"
      },
      {
        "name": "Xiaojin Zhang"
      }
    ],
    "summary": "Recent advancements in federated learning (FL) have produced models that\nretain user privacy by training across multiple decentralized devices or\nsystems holding local data samples. However, these strategies often neglect the\ninherent challenges of statistical heterogeneity and vulnerability to\nadversarial attacks, which can degrade model robustness and fairness.\nPersonalized FL strategies offer some respite by adjusting models to fit\nindividual client profiles, yet they tend to neglect server-side aggregation\nvulnerabilities. To address these issues, we propose Reinforcement Federated\nLearning (RFL), a novel framework that leverages deep reinforcement learning to\nadaptively optimize client contribution during aggregation, thereby enhancing\nboth model robustness against malicious clients and fairness across\nparticipants under non-identically distributed settings. To achieve this goal,\nwe propose a meticulous approach involving a Deep Deterministic Policy\nGradient-based algorithm for continuous control of aggregation weights, an\ninnovative client selection method based on model parameter distances, and a\nreward mechanism guided by validation set performance. Empirically, extensive\nexperiments demonstrate that, in terms of robustness, RFL outperforms the\nstate-of-the-art methods, while maintaining comparable levels of fairness,\noffering a promising solution to build resilient and fair federated systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05541v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05541v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05541v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05536v1",
    "updated": "2024-02-08T10:15:41+00:00",
    "published": "2024-02-08T10:15:41+00:00",
    "title": "Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts",
    "authors": [
      {
        "name": "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
      },
      {
        "name": "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "Mayra Russo"
      },
      {
        "name": "Ahmad Sakor"
      },
      {
        "name": "Luis Daniel Fernandes Rotger"
      },
      {
        "name": "Maria-Esther Vidal"
      }
    ],
    "summary": "Social networks are vital for information sharing, especially in the health\nsector for discussing diseases and treatments. These platforms, however, often\nfeature posts as brief texts, posing challenges for Artificial Intelligence\n(AI) in understanding context. We introduce a novel hybrid approach combining\ncommunity-maintained knowledge graphs (like Wikidata) with deep learning to\nenhance the categorization of social media posts. This method uses advanced\nentity recognizers and linkers (like Falcon 2.0) to connect short post entities\nto knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word\nembeddings (like BERT) are then employed to create rich, context-based\nrepresentations of these posts.\n  Our focus is on the health domain, particularly in identifying posts related\nto eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in\nearly diagnosis. We tested our approach on a dataset of 2,000 tweets about\neating disorders, finding that merging word embeddings with knowledge graph\ninformation enhances the predictive models' reliability. This methodology aims\nto assist health experts in spotting patterns indicative of mental disorders,\nthereby improving early detection and accurate diagnosis for personalized\nmedicine.",
    "comment": null,
    "journal_ref": "Semantic Web, Volume 4, Issue 5, pp. 873-892, 2023",
    "doi": "10.3233/SW-223269",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.3233/SW-223269",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05536v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05536v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05536v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05529v1",
    "updated": "2024-02-08T10:07:30+00:00",
    "published": "2024-02-08T10:07:30+00:00",
    "title": "Asynchronous Diffusion Learning with Agent Subsampling and Local Updates",
    "authors": [
      {
        "name": "Elsa Rizk"
      },
      {
        "name": "Kun Yuan"
      },
      {
        "name": "Ali H. Sayed"
      }
    ],
    "summary": "In this work, we examine a network of agents operating asynchronously, aiming\nto discover an ideal global model that suits individual local datasets. Our\nassumption is that each agent independently chooses when to participate\nthroughout the algorithm and the specific subset of its neighbourhood with\nwhich it will cooperate at any given moment. When an agent chooses to take\npart, it undergoes multiple local updates before conveying its outcomes to the\nsub-sampled neighbourhood. Under this setup, we prove that the resulting\nasynchronous diffusion strategy is stable in the mean-square error sense and\nprovide performance guarantees specifically for the federated learning setting.\nWe illustrate the findings with numerical simulations.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05529v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05529v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05529v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05526v1",
    "updated": "2024-02-08T10:05:28+00:00",
    "published": "2024-02-08T10:05:28+00:00",
    "title": "Buffer Overflow in Mixture of Experts",
    "authors": [
      {
        "name": "Jamie Hayes"
      },
      {
        "name": "Ilia Shumailov"
      },
      {
        "name": "Itay Yona"
      }
    ],
    "summary": "Mixture of Experts (MoE) has become a key ingredient for scaling large\nfoundation models while keeping inference costs steady. We show that expert\nrouting strategies that have cross-batch dependencies are vulnerable to\nattacks. Malicious queries can be sent to a model and can affect a model's\noutput on other benign queries if they are grouped in the same batch. We\ndemonstrate this via a proof-of-concept attack in a toy experimental setting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05526v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05526v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05526v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05525v1",
    "updated": "2024-02-08T10:05:11+00:00",
    "published": "2024-02-08T10:05:11+00:00",
    "title": "Differentially Private Model-Based Offline Reinforcement Learning",
    "authors": [
      {
        "name": "Alexandre Rio"
      },
      {
        "name": "Merwan Barlier"
      },
      {
        "name": "Igor Colin"
      },
      {
        "name": "Albert Thomas"
      }
    ],
    "summary": "We address offline reinforcement learning with privacy guarantees, where the\ngoal is to train a policy that is differentially private with respect to\nindividual trajectories in the dataset. To achieve this, we introduce DP-MORL,\nan MBRL algorithm coming with differential privacy guarantees. A private model\nof the environment is first learned from offline data using DP-FedAvg, a\ntraining method for neural networks that provides differential privacy\nguarantees at the trajectory level. Then, we use model-based policy\noptimization to derive a policy from the (penalized) private model, without any\nfurther interaction with the system or access to the input data. We empirically\nshow that DP-MORL enables the training of private RL agents from offline data\nand we furthermore outline the price of privacy in this setting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05525v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05525v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05525v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05521v1",
    "updated": "2024-02-08T10:01:29+00:00",
    "published": "2024-02-08T10:01:29+00:00",
    "title": "Linearizing Models for Efficient yet Robust Private Inference",
    "authors": [
      {
        "name": "Sreetama Sarkar"
      },
      {
        "name": "Souvik Kundu"
      },
      {
        "name": "Peter A. Beerel"
      }
    ],
    "summary": "The growing concern about data privacy has led to the development of private\ninference (PI) frameworks in client-server applications which protects both\ndata privacy and model IP. However, the cryptographic primitives required yield\nsignificant latency overhead which limits its wide-spread application. At the\nsame time, changing environments demand the PI service to be robust against\nvarious naturally occurring and gradient-based perturbations. Despite several\nworks focused on the development of latency-efficient models suitable for PI,\nthe impact of these models on robustness has remained unexplored. Towards this\ngoal, this paper presents RLNet, a class of robust linearized networks that can\nyield latency improvement via reduction of high-latency ReLU operations while\nimproving the model performance on both clean and corrupted images. In\nparticular, RLNet models provide a \"triple win ticket\" of improved\nclassification accuracy on clean, naturally perturbed, and gradient-based\nperturbed images using a shared-mask shared-weight architecture with over an\norder of magnitude fewer ReLUs than baseline models. To demonstrate the\nefficacy of RLNet, we perform extensive experiments with ResNet and WRN model\nvariants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Our experimental\nevaluations show that RLNet can yield models with up to 11.14x fewer ReLUs,\nwith accuracy close to the all-ReLU models, on clean, naturally perturbed, and\ngradient-based perturbed images. Compared with the SoTA non-robust linearized\nmodels at similar ReLU budgets, RLNet achieves an improvement in adversarial\naccuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while\nimproving clean image accuracy up to ~1.5%.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05521v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05521v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05521v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05519v1",
    "updated": "2024-02-08T10:00:40+00:00",
    "published": "2024-02-08T10:00:40+00:00",
    "title": "Can ChatGPT evaluate research quality?",
    "authors": [
      {
        "name": "Mike Thelwall"
      }
    ],
    "summary": "Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research\nevaluations on journal articles to automate this time-consuming task.\nDesign/methodology/approach: Test the extent to which ChatGPT-4 can assess the\nquality of journal articles using a case study of the published scoring\nguidelines of the UK Research Excellence Framework (REF) 2021 to create a\nresearch evaluation ChatGPT. This was applied to 51 of my own articles and\ncompared against my own quality judgements. Findings: ChatGPT-4 can produce\nplausible document summaries and quality evaluation rationales that match the\nREF criteria. Its overall scores have weak correlations with my self-evaluation\nscores of the same documents (averaging r=0.281 over 15 iterations, with 8\nbeing statistically significantly different from 0). In contrast, the average\nscores from the 15 iterations produced a statistically significant positive\ncorrelation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds\nseems more effective than individual scores. The positive correlation may be\ndue to ChatGPT being able to extract the author's significance, rigour, and\noriginality claims from inside each paper. If my weakest articles are removed,\nthen the correlation with average scores (r=0.200) falls below statistical\nsignificance, suggesting that ChatGPT struggles to make fine-grained\nevaluations. Research limitations: The data is self-evaluations of a\nconvenience sample of articles from one academic in one field. Practical\nimplications: Overall, ChatGPT does not yet seem to be accurate enough to be\ntrusted for any formal or informal research quality evaluation tasks. Research\nevaluators, including journal editors, should therefore take steps to control\nits use. Originality/value: This is the first published attempt at\npost-publication expert review accuracy testing for ChatGPT.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DL",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05519v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05519v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05519v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05515v1",
    "updated": "2024-02-08T09:48:02+00:00",
    "published": "2024-02-08T09:48:02+00:00",
    "title": "NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning",
    "authors": [
      {
        "name": "Yufeng Zhao"
      },
      {
        "name": "Yoshihiro Sakai"
      },
      {
        "name": "Naoya Inoue"
      }
    ],
    "summary": "In-Context Learning (ICL) is suffering from unsatisfactory performance and\nunder-calibration due to high prior bias and unfaithful confidence. Some\nprevious works fine-tuned language models for better ICL performance with\nenormous datasets and computing costs. In this paper, we propose NoisyICL,\nsimply perturbing the model parameters by random noises to strive for better\nperformance and calibration. Our experiments on 2 models and 12 downstream\ndatasets show that NoisyICL can help ICL produce more accurate predictions. Our\nfurther analysis indicates that NoisyICL enables the model to provide more fair\npredictions, and also with less unfaithful confidence. Therefore, we believe\nthat NoisyICL is an effective calibration of ICL. Our experimental code is\nuploaded to Github.",
    "comment": "19 pages, 28 figures, 7 tables (5 pages, 4 figures, 1 table in main\n  body)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05515v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05515v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05515v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05512v1",
    "updated": "2024-02-08T09:44:02+00:00",
    "published": "2024-02-08T09:44:02+00:00",
    "title": "GPTs Are Multilingual Annotators for Sequence Generation Tasks",
    "authors": [
      {
        "name": "Juhwan Choi"
      },
      {
        "name": "Eunju Lee"
      },
      {
        "name": "Kyohoon Jin"
      },
      {
        "name": "YoungBin Kim"
      }
    ],
    "summary": "Data annotation is an essential step for constructing new datasets. However,\nthe conventional approach of data annotation through crowdsourcing is both\ntime-consuming and expensive. In addition, the complexity of this process\nincreases when dealing with low-resource languages owing to the difference in\nthe language pool of crowdworkers. To address these issues, this study proposes\nan autonomous annotation method by utilizing large language models, which have\nbeen recently demonstrated to exhibit remarkable performance. Through our\nexperiments, we demonstrate that the proposed method is not just cost-efficient\nbut also applicable for low-resource language annotation. Additionally, we\nconstructed an image captioning dataset using our approach and are committed to\nopen this dataset for future study. We have opened our source code for further\nstudy and reproducibility.",
    "comment": "EACL 2024 Findings: Camera-ready version",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05512v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05512v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05512v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05501v1",
    "updated": "2024-02-08T09:19:26+00:00",
    "published": "2024-02-08T09:19:26+00:00",
    "title": "Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming",
    "authors": [
      {
        "name": "Lara Scavuzzo"
      },
      {
        "name": "Karen Aardal"
      },
      {
        "name": "Andrea Lodi"
      },
      {
        "name": "Neil Yorke-Smith"
      }
    ],
    "summary": "Mixed Integer Linear Programming (MILP) is a pillar of mathematical\noptimization that offers a powerful modeling language for a wide range of\napplications. During the past decades, enormous algorithmic progress has been\nmade in solving MILPs, and many commercial and academic software packages\nexist. Nevertheless, the availability of data, both from problem instances and\nfrom solvers, and the desire to solve new problems and larger (real-life)\ninstances, trigger the need for continuing algorithmic development. MILP\nsolvers use branch and bound as their main component. In recent years, there\nhas been an explosive development in the use of machine learning algorithms for\nenhancing all main tasks involved in the branch-and-bound algorithm, such as\nprimal heuristics, branching, cutting planes, node selection and solver\nconfiguration decisions. This paper presents a survey of such approaches,\naddressing the vision of integration of machine learning and mathematical\noptimization as complementary technologies, and how this integration can\nbenefit MILP solving. In particular, we give detailed attention to machine\nlearning algorithms that automatically optimize some metric of branch-and-bound\nefficiency. We also address how to represent MILPs in the context of applying\nlearning algorithms, MILP benchmarks and software.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05501v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05501v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05501v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05495v1",
    "updated": "2024-02-08T09:07:38+00:00",
    "published": "2024-02-08T09:07:38+00:00",
    "title": "Heart disease risk prediction using deep learning techniques with feature augmentation",
    "authors": [
      {
        "name": "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "Mart\u00edn Bay\u00f3n-Guti\u00e9rrez"
      },
      {
        "name": "Carmen Benavides"
      },
      {
        "name": "Jose Aveleira-Mata"
      },
      {
        "name": "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
      }
    ],
    "summary": "Cardiovascular diseases state as one of the greatest risks of death for the\ngeneral population. Late detection in heart diseases highly conditions the\nchances of survival for patients. Age, sex, cholesterol level, sugar level,\nheart rate, among other factors, are known to have an influence on\nlife-threatening heart problems, but, due to the high amount of variables, it\nis often difficult for an expert to evaluate each patient taking this\ninformation into account. In this manuscript, the authors propose using deep\nlearning methods, combined with feature augmentation techniques for evaluating\nwhether patients are at risk of suffering cardiovascular disease. The results\nof the proposed methods outperform other state of the art methods by 4.4%,\nleading to a precision of a 90%, which presents a significant improvement, even\nmore so when it comes to an affliction that affects a large population.",
    "comment": null,
    "journal_ref": "Multimedia Tools and Applications, Volume 82, pp. 31759 - 31773,\n  August 2024",
    "doi": "10.1007/s11042-023-14817-z",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1007/s11042-023-14817-z",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05495v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05495v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05495v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05493v1",
    "updated": "2024-02-08T09:03:17+00:00",
    "published": "2024-02-08T09:03:17+00:00",
    "title": "Investigating White-Box Attacks for On-Device Models",
    "authors": [
      {
        "name": "Mingyi Zhou"
      },
      {
        "name": "Xiang Gao"
      },
      {
        "name": "Jing Wu"
      },
      {
        "name": "Kui Liu"
      },
      {
        "name": "Hailong Sun"
      },
      {
        "name": "Li Li"
      }
    ],
    "summary": "Numerous mobile apps have leveraged deep learning capabilities. However,\non-device models are vulnerable to attacks as they can be easily extracted from\ntheir corresponding mobile apps. Existing on-device attacking approaches only\ngenerate black-box attacks, which are far less effective and efficient than\nwhite-box strategies. This is because mobile deep learning frameworks like\nTFLite do not support gradient computing, which is necessary for white-box\nattacking algorithms. Thus, we argue that existing findings may underestimate\nthe harmfulness of on-device attacks. To this end, we conduct a study to answer\nthis research question: Can on-device models be directly attacked via white-box\nstrategies? We first systematically analyze the difficulties of transforming\nthe on-device model to its debuggable version, and propose a Reverse\nEngineering framework for On-device Models (REOM), which automatically reverses\nthe compiled on-device TFLite model to the debuggable model. Specifically, REOM\nfirst transforms compiled on-device models into Open Neural Network Exchange\nformat, then removes the non-debuggable parts, and converts them to the\ndebuggable DL models format that allows attackers to exploit in a white-box\nsetting. Our experimental results show that our approach is effective in\nachieving automated transformation among 244 TFLite models. Compared with\nprevious attacks using surrogate models, REOM enables attackers to achieve\nhigher attack success rates with a hundred times smaller attack perturbations.\nIn addition, because the ONNX platform has plenty of tools for model format\nexchanging, the proposed method based on the ONNX platform can be adapted to\nother model formats. Our findings emphasize the need for developers to\ncarefully consider their model deployment strategies, and use white-box methods\nto evaluate the vulnerability of on-device models.",
    "comment": "The International Conference on Software Engineering 2024 (ICSE'24)",
    "journal_ref": null,
    "doi": "10.1145/3597503.3639144",
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3597503.3639144",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05493v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05493v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05493v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05491v1",
    "updated": "2024-02-08T08:55:34+00:00",
    "published": "2024-02-08T08:55:34+00:00",
    "title": "Determining the severity of Parkinson's disease in patients using a multi task neural network",
    "authors": [
      {
        "name": "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
      },
      {
        "name": "Jose Aveleira-Mata"
      },
      {
        "name": "Jos\u00e9-Manuel Alija-P\u00e9rez"
      },
      {
        "name": "Carmen Benavides"
      }
    ],
    "summary": "Parkinson's disease is easy to diagnose when it is advanced, but it is very\ndifficult to diagnose in its early stages. Early diagnosis is essential to be\nable to treat the symptoms. It impacts on daily activities and reduces the\nquality of life of both the patients and their families and it is also the\nsecond most prevalent neurodegenerative disorder after Alzheimer in people over\nthe age of 60. Most current studies on the prediction of Parkinson's severity\nare carried out in advanced stages of the disease. In this work, the study\nanalyzes a set of variables that can be easily extracted from voice analysis,\nmaking it a very non-intrusive technique. In this paper, a method based on\ndifferent deep learning techniques is proposed with two purposes. On the one\nhand, to find out if a person has severe or non-severe Parkinson's disease, and\non the other hand, to determine by means of regression techniques the degree of\nevolution of the disease in a given patient. The UPDRS (Unified Parkinson's\nDisease Rating Scale) has been used by taking into account both the motor and\ntotal labels, and the best results have been obtained using a mixed multi-layer\nperceptron (MLP) that classifies and regresses at the same time and the most\nimportant features of the data obtained are taken as input, using an\nautoencoder. A success rate of 99.15% has been achieved in the problem of\npredicting whether a person suffers from severe Parkinson's disease or\nnon-severe Parkinson's disease. In the degree of disease involvement prediction\nproblem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a\nfull deep learning pipeline for data preprocessing and classification has\nproven to be very promising in the field Parkinson's outperforming the\nstate-of-the-art proposals.",
    "comment": null,
    "journal_ref": "Multimedia Tools and Applications, Volume 83, pages 6077-6092,\n  2024",
    "doi": "10.1007/s11042-023-14932-x",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1007/s11042-023-14932-x",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05491v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05491v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05491v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05484v1",
    "updated": "2024-02-08T08:25:41+00:00",
    "published": "2024-02-08T08:25:41+00:00",
    "title": "Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive Study and Framework Proposal",
    "authors": [
      {
        "name": "Nhi Tran"
      },
      {
        "name": "Tan Tran"
      },
      {
        "name": "Nam Nguyen"
      }
    ],
    "summary": "This paper presents an extensive study on the application of AI techniques\nfor software effort estimation in the past five years from 2017 to 2023. By\novercoming the limitations of traditional methods, the study aims to improve\naccuracy and reliability. Through performance evaluation and comparison with\ndiverse Machine Learning models, including Artificial Neural Network (ANN),\nSupport Vector Machine (SVM), Linear Regression, Random Forest and other\ntechniques, the most effective method is identified. The proposed AI-based\nframework holds the potential to enhance project planning and resource\nallocation, contributing to the research area of software project effort\nestimation.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05484v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05484v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05484v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05482v1",
    "updated": "2024-02-08T08:23:33+00:00",
    "published": "2024-02-08T08:23:33+00:00",
    "title": "A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals",
    "authors": [
      {
        "name": "Cho-Yuan Lee"
      },
      {
        "name": "Kuan-Chen Wang"
      },
      {
        "name": "Kai-Chun Liu"
      },
      {
        "name": "Xugang Lu"
      },
      {
        "name": "Ping-Chen Yeh"
      },
      {
        "name": "Yu Tsao"
      }
    ],
    "summary": "In practical scenarios involving the measurement of surface electromyography\n(sEMG) in muscles, particularly those areas near the heart, one of the primary\nsources of contamination is the presence of electrocardiogram (ECG) signals. To\nassess the quality of real-world sEMG data more effectively, this study\nproposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG\nsignals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an\nend-to-end training strategy. Our experimental framework utilizes real-world\nsEMG and ECG data from two open-access databases, the Non-Invasive Adaptive\nProsthetics Database and the MIT-BIH Normal Sinus Rhythm Database,\nrespectively. The experimental results demonstrate the superiority of QASE-net\nover the previous assessment model, exhibiting significantly reduced prediction\nerrors and notably higher linear correlations with the ground truth. These\nfindings show the potential of QASE-net to substantially enhance the\nreliability and precision of sEMG quality assessment in practical applications.",
    "comment": "5 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.SP",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05482v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05482v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05482v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05476v1",
    "updated": "2024-02-08T08:08:23+00:00",
    "published": "2024-02-08T08:08:23+00:00",
    "title": "Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization",
    "authors": [
      {
        "name": "Talha Bozkus"
      },
      {
        "name": "Urbashi Mitra"
      }
    ],
    "summary": "Reinforcement learning (RL) is a classical tool to solve network control or\npolicy optimization problems in unknown environments. The original Q-learning\nsuffers from performance and complexity challenges across very large networks.\nHerein, a novel model-free ensemble reinforcement learning algorithm which\nadapts the classical Q-learning is proposed to handle these challenges for\nnetworks which admit Markov decision process (MDP) models. Multiple Q-learning\nalgorithms are run on multiple, distinct, synthetically created and\nstructurally related Markovian environments in parallel; the outputs are fused\nusing an adaptive weighting mechanism based on the Jensen-Shannon divergence\n(JSD) to obtain an approximately optimal policy with low complexity. The\ntheoretical justification of the algorithm, including the convergence of key\nstatistics and Q-functions are provided. Numerical results across several\nnetwork models show that the proposed algorithm can achieve up to 55% less\naverage policy error with up to 50% less runtime complexity than the\nstate-of-the-art Q-learning algorithms. Numerical results validate assumptions\nmade in the theoretical analysis.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05476v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05476v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05476v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05981v1",
    "updated": "2024-02-08T08:02:57+00:00",
    "published": "2024-02-08T08:02:57+00:00",
    "title": "Exploring the Impact of In-Browser Deep Learning Inference on Quality of User Experience and Performance",
    "authors": [
      {
        "name": "Qipeng Wang"
      },
      {
        "name": "Shiqi Jiang"
      },
      {
        "name": "Zhenpeng Chen"
      },
      {
        "name": "Xu Cao"
      },
      {
        "name": "Yuanchun Li"
      },
      {
        "name": "Aoyu Li"
      },
      {
        "name": "Ying Zhang"
      },
      {
        "name": "Yun Ma"
      },
      {
        "name": "Ting Cao"
      },
      {
        "name": "Xuanzhe Liu"
      }
    ],
    "summary": "Deep Learning (DL) is increasingly being integrated into Web applications\nthrough a method known as \"in-browser inference\", where the DL processes occur\ndirectly within Web browsers. However, the actual performance of this method\nand its effect on user experience quality (QoE) is not well-understood. This\ngap in knowledge necessitates new forms of QoE measurement, going beyond\ntraditional metrics such as page load time. To address this, we conducted the\nfirst extensive performance evaluation of in-browser inference. We introduced\nnew metrics for this purpose: responsiveness, smoothness, and inference\naccuracy.\n  Our thorough study included 9 widely-used DL models and tested them across 50\npopular PC Web browsers. The findings show a significant latency issue with\nin-browser inference: it's on average 16.9 times slower on CPU and 4.9 times\nslower on GPU than native inference methods. Several factors contribute to this\nlatency, including underused hardware instruction sets, inherent delays in the\nruntime environment, resource competition within the browser, and\ninefficiencies in software libraries and GPU abstractions.\n  Moreover, in-browser inference demands a lot of memory, sometimes up to 334.6\ntimes more than the size of the DL models themselves. This excessive memory\nusage is partly due to suboptimal memory management. Additionally, we noticed\nthat in-browser inference increases the time it takes for graphical user\ninterface (GUI) components to load in web browsers by a significant 67.2\\%,\nwhich severely impacts the overall QoE for users of web applications that\ndepend on this technology.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.PF"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05981v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05981v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05981v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05468v1",
    "updated": "2024-02-08T08:00:11+00:00",
    "published": "2024-02-08T08:00:11+00:00",
    "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
    "authors": [
      {
        "name": "Pierre Marion"
      },
      {
        "name": "Anna Korba"
      },
      {
        "name": "Peter Bartlett"
      },
      {
        "name": "Mathieu Blondel"
      },
      {
        "name": "Valentin De Bortoli"
      },
      {
        "name": "Arnaud Doucet"
      },
      {
        "name": "Felipe Llinares-L\u00f3pez"
      },
      {
        "name": "Courtney Paquette"
      },
      {
        "name": "Quentin Berthet"
      }
    ],
    "summary": "We present a new algorithm to optimize distributions defined implicitly by\nparameterized stochastic diffusions. Doing so allows us to modify the outcome\ndistribution of sampling processes by optimizing over their parameters. We\nintroduce a general framework for first-order optimization of these processes,\nthat performs jointly, in a single loop, optimization and sampling steps. This\napproach is inspired by recent advances in bilevel optimization and automatic\nimplicit differentiation, leveraging the point of view of sampling as\noptimization over the space of probability distributions. We provide\ntheoretical guarantees on the performance of our method, as well as\nexperimental results demonstrating its effectiveness in real-world settings.",
    "comment": "33 pages, 13 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05468v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05468v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05468v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05467v1",
    "updated": "2024-02-08T07:56:49+00:00",
    "published": "2024-02-08T07:56:49+00:00",
    "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia",
    "authors": [
      {
        "name": "Guangyu Shen"
      },
      {
        "name": "Siyuan Cheng"
      },
      {
        "name": "Kaiyuan Zhang"
      },
      {
        "name": "Guanhong Tao"
      },
      {
        "name": "Shengwei An"
      },
      {
        "name": "Lu Yan"
      },
      {
        "name": "Zhuo Zhang"
      },
      {
        "name": "Shiqing Ma"
      },
      {
        "name": "Xiangyu Zhang"
      }
    ],
    "summary": "Large Language Models (LLMs) have become prevalent across diverse sectors,\ntransforming human life with their extraordinary reasoning and comprehension\nabilities. As they find increased use in sensitive tasks, safety concerns have\ngained widespread attention. Extensive efforts have been dedicated to aligning\nLLMs with human moral principles to ensure their safe deployment. Despite their\npotential, recent research indicates aligned LLMs are prone to specialized\njailbreaking prompts that bypass safety measures to elicit violent and harmful\ncontent. The intrinsic discrete nature and substantial scale of contemporary\nLLMs pose significant challenges in automatically generating diverse,\nefficient, and potent jailbreaking prompts, representing a continuous obstacle.\nIn this paper, we introduce RIPPLE (Rapid Optimization via Subconscious\nExploitation and Echopraxia), a novel optimization-based method inspired by two\npsychological concepts: subconsciousness and echopraxia, which describe the\nprocesses of the mind that occur without conscious awareness and the\ninvoluntary mimicry of actions, respectively. Evaluations across 6 open-source\nLLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success\nRate of 91.5\\%, outperforming five current methods by up to 47.0\\% with an 8x\nreduction in overhead. Furthermore, it displays significant transferability and\nstealth, successfully evading established detection mechanisms. The code of our\nwork is available at\n\\url{https://github.com/SolidShen/RIPPLE_official/tree/official}",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05467v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05467v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05467v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06683v1",
    "updated": "2024-02-08T07:22:39+00:00",
    "published": "2024-02-08T07:22:39+00:00",
    "title": "Sound Source Separation Using Latent Variational Block-Wise Disentanglement",
    "authors": [
      {
        "name": "Karim Helwani"
      },
      {
        "name": "Masahito Togami"
      },
      {
        "name": "Paris Smaragdis"
      },
      {
        "name": "Michael M. Goodwin"
      }
    ],
    "summary": "While neural network approaches have made significant strides in resolving\nclassical signal processing problems, it is often the case that hybrid\napproaches that draw insight from both signal processing and neural networks\nproduce more complete solutions. In this paper, we present a hybrid classical\ndigital signal processing/deep neural network (DSP/DNN) approach to source\nseparation (SS) highlighting the theoretical link between variational\nautoencoder and classical approaches to SS. We propose a system that transforms\nthe single channel under-determined SS task to an equivalent multichannel\nover-determined SS problem in a properly designed latent space. The separation\ntask in the latent space is treated as finding a variational block-wise\ndisentangled representation of the mixture. We show empirically, that the\ndesign choices and the variational formulation of the task at hand motivated by\nthe classical signal processing theoretical results lead to robustness to\nunseen out-of-distribution data and reduction of the overfitting risk. To\naddress the resulting permutation issue we explicitly incorporate a novel\ndifferentiable permutation loss function and augment the model with a memory\nmechanism to keep track of the statistics of the individual sources.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06683v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06683v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06683v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05457v1",
    "updated": "2024-02-08T07:21:45+00:00",
    "published": "2024-02-08T07:21:45+00:00",
    "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
    "authors": [
      {
        "name": "Chen Chen"
      },
      {
        "name": "Ruizhe Li"
      },
      {
        "name": "Yuchen Hu"
      },
      {
        "name": "Sabato Marco Siniscalchi"
      },
      {
        "name": "Pin-Yu Chen"
      },
      {
        "name": "Ensiong Chng"
      },
      {
        "name": "Chao-Han Huck Yang"
      }
    ],
    "summary": "Recent studies have successfully shown that large language models (LLMs) can\nbe successfully used for generative error correction (GER) on top of the\nautomatic speech recognition (ASR) output. Specifically, an LLM is utilized to\ncarry out a direct mapping from the N-best hypotheses list generated by an ASR\nsystem to the predicted output transcription. However, despite its\neffectiveness, GER introduces extra data uncertainty since the LLM is trained\nwithout taking into account acoustic information available in the speech\nsignal. In this work, we aim to overcome such a limitation by infusing acoustic\ninformation before generating the predicted transcription through a novel late\nfusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a\nmultimodal fusion approach implemented into an auto-regressive decoding process\nand works in two stages: (i) It first analyzes and calibrates the token-level\nLLM decision, and (ii) it then dynamically assimilates the information from the\nacoustic modality. Experimental evidence collected from various ASR tasks shows\nthat UADF surpasses existing fusion mechanisms in several ways. It yields\nsignificant improvements in word error rate (WER) while mitigating data\nuncertainty issues in LLM and addressing the poor generalization relied with\nsole modality during fusion. We also demonstrate that UADF seamlessly adapts to\naudio-visual speech recognition.",
    "comment": "Accepted to ICLR 2024, 17 pages. This work will be open sourced under\n  MIT license",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05457v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05457v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05457v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06682v1",
    "updated": "2024-02-08T07:18:23+00:00",
    "published": "2024-02-08T07:18:23+00:00",
    "title": "Private Knowledge Sharing in Distributed Learning: A Survey",
    "authors": [
      {
        "name": "Yasas Supeksala"
      },
      {
        "name": "Dinh C. Nguyen"
      },
      {
        "name": "Ming Ding"
      },
      {
        "name": "Thilina Ranbaduge"
      },
      {
        "name": "Calson Chua"
      },
      {
        "name": "Jun Zhang"
      },
      {
        "name": "Jun Li"
      },
      {
        "name": "H. Vincent Poor"
      }
    ],
    "summary": "The rise of Artificial Intelligence (AI) has revolutionized numerous\nindustries and transformed the way society operates. Its widespread use has led\nto the distribution of AI and its underlying data across many intelligent\nsystems. In this light, it is crucial to utilize information in learning\nprocesses that are either distributed or owned by different entities. As a\nresult, modern data-driven services have been developed to integrate\ndistributed knowledge entities into their outcomes. In line with this goal, the\nlatest AI models are frequently trained in a decentralized manner. Distributed\nlearning involves multiple entities working together to make collective\npredictions and decisions. However, this collaboration can also bring about\nsecurity vulnerabilities and challenges. This paper provides an in-depth survey\non private knowledge sharing in distributed learning, examining various\nknowledge components utilized in leading distributed learning architectures.\nOur analysis sheds light on the most critical vulnerabilities that may arise\nwhen using these components in a distributed setting. We further identify and\nexamine defensive strategies for preserving the privacy of these knowledge\ncomponents and preventing malicious parties from manipulating or accessing the\nknowledge information. Finally, we highlight several key limitations of\nknowledge sharing in distributed learning and explore potential avenues for\nfuture research.",
    "comment": "Manuscript submitted to ACM",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06682v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06682v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06682v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05453v1",
    "updated": "2024-02-08T07:14:17+00:00",
    "published": "2024-02-08T07:14:17+00:00",
    "title": "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "authors": [
      {
        "name": "Zhenlong Liu"
      },
      {
        "name": "Lei Feng"
      },
      {
        "name": "Huiping Zhuang"
      },
      {
        "name": "Xiaofeng Cao"
      },
      {
        "name": "Hongxin Wei"
      }
    ],
    "summary": "Machine learning models are susceptible to membership inference attacks\n(MIAs), which aim to infer whether a sample is in the training set. Existing\nwork utilizes gradient ascent to enlarge the loss variance of training data,\nalleviating the privacy risk. However, optimizing toward a reverse direction\nmay cause the model parameters to oscillate near local minima, leading to\ninstability and suboptimal performance. In this work, we propose a novel method\n-- Convex-Concave Loss, which enables a high variance of training loss\ndistribution by gradient descent. Our method is motivated by the theoretical\nanalysis that convex losses tend to decrease the loss variance during training.\nThus, our key idea behind CCL is to reduce the convexity of loss functions with\na concave term. Trained with CCL, neural networks produce losses with high\nvariance for training data, reinforcing the defense against MIAs. Extensive\nexperiments demonstrate the superiority of CCL, achieving state-of-the-art\nbalance in the privacy-utility trade-off.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05453v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05453v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05453v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05448v1",
    "updated": "2024-02-08T07:01:00+00:00",
    "published": "2024-02-08T07:01:00+00:00",
    "title": "Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application",
    "authors": [
      {
        "name": "Bumsoo Kim"
      },
      {
        "name": "Sanghyun Byun"
      },
      {
        "name": "Yonghoon Jung"
      },
      {
        "name": "Wonseop Shin"
      },
      {
        "name": "Sareer UI Amin"
      },
      {
        "name": "Sanghyun Seo"
      }
    ],
    "summary": "In this paper, we first present the character texture generation system\n\\textit{Minecraft-ify}, specified to Minecraft video game toward in-game\napplication. Ours can generate face-focused image for texture mapping tailored\nto 3D virtual character having cube manifold. While existing projects or works\nonly generate texture, proposed system can inverse the user-provided real\nimage, or generate average/random appearance from learned distribution.\nMoreover, it can be manipulated with text-guidance using StyleGAN and\nStyleCLIP. These features provide a more extended user experience with enlarged\nfreedom as a user-friendly AI-tool. Project page can be found at\nhttps://gh-bumsookim.github.io/Minecraft-ify/",
    "comment": "2 pages, 2 figures. Accepted to NeurIPS 2023 Workshop on Machine\n  Learning for Creativity and Design",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05448v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05448v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05448v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05445v1",
    "updated": "2024-02-08T06:53:31+00:00",
    "published": "2024-02-08T06:53:31+00:00",
    "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
    "authors": [
      {
        "name": "Haotong Qin"
      },
      {
        "name": "Xudong Ma"
      },
      {
        "name": "Xingyu Zheng"
      },
      {
        "name": "Xiaoyang Li"
      },
      {
        "name": "Yang Zhang"
      },
      {
        "name": "Shouda Liu"
      },
      {
        "name": "Jie Luo"
      },
      {
        "name": "Xianglong Liu"
      },
      {
        "name": "Michele Magno"
      }
    ],
    "summary": "The LoRA-finetuning quantization of LLMs has been extensively studied to\nobtain accurate yet compact LLMs for deployment on resource-constrained\nhardware. However, existing methods cause the quantized LLM to severely degrade\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\nthrough information retention. The proposed IR-QLoRA mainly relies on two\ntechnologies derived from the perspective of unified information: (1)\nstatistics-based Information Calibration Quantization allows the quantized\nparameters of LLM to retain original information accurately; (2)\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\nrepresentation transformation with diverse information. Comprehensive\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\nimprovement on MMLU compared with the state-of-the-art methods. The significant\nperformance gain requires only a tiny 0.31% additional time consumption,\nrevealing the satisfactory efficiency of our IRQLoRA. We highlight that\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\nThe code is available at https://github.com/htqin/ir-qlora.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05445v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05445v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05445v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05980v1",
    "updated": "2024-02-08T06:48:01+00:00",
    "published": "2024-02-08T06:48:01+00:00",
    "title": "Do Large Code Models Understand Programming Concepts? A Black-box Approach",
    "authors": [
      {
        "name": "Ashish Hooda"
      },
      {
        "name": "Mihai Christodorescu"
      },
      {
        "name": "Miltos Allamanis"
      },
      {
        "name": "Aaron Wilson"
      },
      {
        "name": "Kassem Fawaz"
      },
      {
        "name": "Somesh Jha"
      }
    ],
    "summary": "Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05980v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05980v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05980v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05443v1",
    "updated": "2024-02-08T06:45:03+00:00",
    "published": "2024-02-08T06:45:03+00:00",
    "title": "Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport",
    "authors": [
      {
        "name": "Jaemoo Choi"
      },
      {
        "name": "Jaewoong Choi"
      },
      {
        "name": "Myungjoo Kang"
      }
    ],
    "summary": "Wasserstein Gradient Flow (WGF) describes the gradient dynamics of\nprobability density within the Wasserstein space. WGF provides a promising\napproach for conducting optimization over the probability distributions.\nNumerically approximating the continuous WGF requires the time discretization\nmethod. The most well-known method for this is the JKO scheme. In this regard,\nprevious WGF models employ the JKO scheme and parametrize transport map for\neach JKO step. However, this approach results in quadratic training complexity\n$O(K^2)$ with the number of JKO step $K$. This severely limits the scalability\nof WGF models. In this paper, we introduce a scalable WGF-based generative\nmodel, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form\nof the JKO step, derived from the equivalence between the JKO step and the\nUnbalanced Optimal Transport. Our approach reduces the training complexity to\n$O(K)$. We demonstrate that our model significantly outperforms existing\nWGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19\non CelebA-HQ-256, which are comparable to state-of-the-art image generative\nmodels.",
    "comment": "20 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05443v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05443v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05443v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05439v1",
    "updated": "2024-02-08T06:32:06+00:00",
    "published": "2024-02-08T06:32:06+00:00",
    "title": "Learning Uncertainty-Aware Temporally-Extended Actions",
    "authors": [
      {
        "name": "Joongkyu Lee"
      },
      {
        "name": "Seung Joon Park"
      },
      {
        "name": "Yunhao Tang"
      },
      {
        "name": "Min-hwan Oh"
      }
    ],
    "summary": "In reinforcement learning, temporal abstraction in the action space,\nexemplified by action repetition, is a technique to facilitate policy learning\nthrough extended actions. However, a primary limitation in previous studies of\naction repetition is its potential to degrade performance, particularly when\nsub-optimal actions are repeated. This issue often negates the advantages of\naction repetition. To address this, we propose a novel algorithm named\nUncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to\naccurately measure uncertainty during action extension. This feature allows\npolicies to strategically choose between emphasizing exploration or adopting an\nuncertainty-averse approach, tailored to their specific needs. We demonstrate\nthe effectiveness of UTE through experiments in Gridworld and Atari 2600\nenvironments. Our findings show that UTE outperforms existing action repetition\nalgorithms, effectively mitigating their inherent limitations and significantly\nenhancing policy learning efficiency.",
    "comment": "Accepted in AAAI 2024 (Main Technical Track)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05439v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05439v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05439v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05435v1",
    "updated": "2024-02-08T06:20:01+00:00",
    "published": "2024-02-08T06:20:01+00:00",
    "title": "GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study",
    "authors": [
      {
        "name": "Christopher J. Lynch"
      },
      {
        "name": "Erik Jensen"
      },
      {
        "name": "Madison H. Munro"
      },
      {
        "name": "Virginia Zamponi"
      },
      {
        "name": "Joseph Martinez"
      },
      {
        "name": "Kevin O'Brien"
      },
      {
        "name": "Brandon Feldhaus"
      },
      {
        "name": "Katherine Smith"
      },
      {
        "name": "Ann Marie Reinhold"
      },
      {
        "name": "Ross Gore"
      }
    ],
    "summary": "Large Language Models (LLMs) play a pivotal role in generating vast arrays of\nnarratives, facilitating a systematic exploration of their effectiveness for\ncommunicating life events in narrative form. In this study, we employ a\nzero-shot structured narrative prompt to generate 24,000 narratives using\nOpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and\nevaluate their validity in conveying birth, death, hiring, and firing events.\nRemarkably, 87.43% of the narratives sufficiently convey the intention of the\nstructured prompt. To automate the identification of valid and invalid\nnarratives, we train and validate nine Machine Learning models on the\nclassified datasets. Leveraging these models, we extend our analysis to predict\nthe classifications of the remaining 21,120 narratives. All the ML models\nexcelled at classifying valid narratives as valid, but experienced challenges\nat simultaneously classifying invalid narratives as invalid. Our findings not\nonly advance the study of LLM capabilities, limitations, and validity but also\noffer practical insights for narrative generation and natural language\nprocessing applications.",
    "comment": "29 pages, 24 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7; I.6.4"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05435v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05435v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05435v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05428v1",
    "updated": "2024-02-08T05:54:08+00:00",
    "published": "2024-02-08T05:54:08+00:00",
    "title": "Mixture Density Networks for Classification with an Application to Product Bundling",
    "authors": [
      {
        "name": "Narendhar Gugulothu"
      },
      {
        "name": "Sanjay P. Bhat"
      },
      {
        "name": "Tejas Bodas"
      }
    ],
    "summary": "While mixture density networks (MDNs) have been extensively used for\nregression tasks, they have not been used much for classification tasks. One\nreason for this is that the usability of MDNs for classification is not clear\nand straightforward. In this paper, we propose two MDN-based models for\nclassification tasks. Both models fit mixtures of Gaussians to the the data and\nuse the fitted distributions to classify a given sample by evaluating the\nlearnt cumulative distribution function for the given input features. While the\nproposed MDN-based models perform slightly better than, or on par with, five\nbaseline classification models on three publicly available datasets, the real\nutility of our models comes out through a real-world product bundling\napplication. Specifically, we use our MDN-based models to learn the\nwillingness-to-pay (WTP) distributions for two products from synthetic sales\ndata of the individual products. The Gaussian mixture representation of the\nlearnt WTP distributions is then exploited to obtain the WTP distribution of\nthe bundle consisting of both the products. The proposed MDN-based models are\nable to approximate the true WTP distributions of both products and the bundle\nwell.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05428v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05428v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05428v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05427v1",
    "updated": "2024-02-08T05:52:45+00:00",
    "published": "2024-02-08T05:52:45+00:00",
    "title": "A Sampling Theory Perspective on Activations for Implicit Neural Representations",
    "authors": [
      {
        "name": "Hemanth Saratchandran"
      },
      {
        "name": "Sameera Ramasinghe"
      },
      {
        "name": "Violetta Shevchenko"
      },
      {
        "name": "Alexander Long"
      },
      {
        "name": "Simon Lucey"
      }
    ],
    "summary": "Implicit Neural Representations (INRs) have gained popularity for encoding\nsignals as compact, differentiable entities. While commonly using techniques\nlike Fourier positional encodings or non-traditional activation functions\n(e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content,\ntheir properties lack exploration within a unified theoretical framework.\nAddressing this gap, we conduct a comprehensive analysis of these activations\nfrom a sampling theory perspective. Our investigation reveals that sinc\nactivations, previously unused in conjunction with INRs, are theoretically\noptimal for signal encoding. Additionally, we establish a connection between\ndynamical systems and INRs, leveraging sampling theory to bridge these two\nparadigms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05427v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05427v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05427v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05424v1",
    "updated": "2024-02-08T05:42:13+00:00",
    "published": "2024-02-08T05:42:13+00:00",
    "title": "Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures",
    "authors": [
      {
        "name": "Vincent Abbott"
      }
    ],
    "summary": "Diagrams matter. Unfortunately, the deep learning community has no standard\nmethod for diagramming architectures. The current combination of linear algebra\nnotation and ad-hoc diagrams fails to offer the necessary precision to\nunderstand architectures in all their detail. However, this detail is critical\nfor faithful implementation, mathematical analysis, further innovation, and\nethical assurances. I present neural circuit diagrams, a graphical language\ntailored to the needs of communicating deep learning architectures. Neural\ncircuit diagrams naturally keep track of the changing arrangement of data,\nprecisely show how operations are broadcast over axes, and display the critical\nparallel behavior of linear operations. A lingering issue with existing\ndiagramming methods is the inability to simultaneously express the detail of\naxes and the free arrangement of data, which neural circuit diagrams solve.\nTheir compositional structure is analogous to code, creating a close\ncorrespondence between diagrams and implementation.\n  In this work, I introduce neural circuit diagrams for an audience of machine\nlearning researchers. After introducing neural circuit diagrams, I cover a host\nof architectures to show their utility and breed familiarity. This includes the\ntransformer architecture, convolution (and its difficult-to-explain\nextensions), residual networks, the U-Net, and the vision transformer. I\ninclude a Jupyter notebook that provides evidence for the close correspondence\nbetween diagrams and code. Finally, I examine backpropagation using neural\ncircuit diagrams. I show their utility in providing mathematical insight and\nanalyzing algorithms' time and space complexities.",
    "comment": null,
    "journal_ref": "Transactions on Machine Learning Research (2024)",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05424v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05424v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05424v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05421v1",
    "updated": "2024-02-08T05:26:40+00:00",
    "published": "2024-02-08T05:26:40+00:00",
    "title": "DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning",
    "authors": [
      {
        "name": "Weikang Wan"
      },
      {
        "name": "Yufei Wang"
      },
      {
        "name": "Zackory Erickson"
      },
      {
        "name": "David Held"
      }
    ],
    "summary": "This paper introduces DiffTOP, which utilizes Differentiable Trajectory\nOPtimization as the policy representation to generate actions for deep\nreinforcement and imitation learning. Trajectory optimization is a powerful and\nwidely used algorithm in control, parameterized by a cost and a dynamics\nfunction. The key to our approach is to leverage the recent progress in\ndifferentiable trajectory optimization, which enables computing the gradients\nof the loss with respect to the parameters of trajectory optimization. As a\nresult, the cost and dynamics functions of trajectory optimization can be\nlearned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior\nmodel-based RL algorithms, as the dynamics model in DiffTOP is learned to\ndirectly maximize task performance by differentiating the policy gradient loss\nthrough the trajectory optimization process. We further benchmark DiffTOP for\nimitation learning on standard robotic manipulation task suites with\nhigh-dimensional sensory observations and compare our method to feed-forward\npolicy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15\nmodel-based RL tasks and 13 imitation learning tasks with high-dimensional\nimage and point cloud inputs, DiffTOP outperforms prior state-of-the-art\nmethods in both domains.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05421v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05421v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05421v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05417v1",
    "updated": "2024-02-08T05:18:11+00:00",
    "published": "2024-02-08T05:18:11+00:00",
    "title": "Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification",
    "authors": [
      {
        "name": "Vaibhav Khatavkar"
      },
      {
        "name": "Makarand Velankar"
      },
      {
        "name": "Sneha Petkar"
      }
    ],
    "summary": "Captcha are widely used to secure systems from automatic responses by\ndistinguishing computer responses from human responses. Text, audio, video,\npicture picture-based Optical Character Recognition (OCR) are used for creating\ncaptcha. Text-based OCR captcha are the most often used captcha which faces\nissues namely, complex and distorted contents. There are attempts to build\ncaptcha detection and classification-based systems using machine learning and\nneural networks, which need to be tuned for accuracy. The existing systems face\nchallenges in the recognition of distorted characters, handling variable-length\ncaptcha and finding sequential dependencies in captcha. In this work, we\npropose a segmentation-free OCR model for text captcha classification based on\nthe connectionist temporal classification loss technique. The proposed model is\ntrained and tested on a publicly available captcha dataset. The proposed model\ngives 99.80\\% character level accuracy, while 95\\% word level accuracy. The\naccuracy of the proposed model is compared with the state-of-the-art models and\nproves to be effective. The variable length complex captcha can be thus\nprocessed with the segmentation-free connectionist temporal classification loss\ntechnique with dependencies which will be massively used in securing the\nsoftware systems.",
    "comment": "17 pages, 5 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05417v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05417v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05417v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05407v1",
    "updated": "2024-02-08T04:48:51+00:00",
    "published": "2024-02-08T04:48:51+00:00",
    "title": "Version age-based client scheduling policy for federated learning",
    "authors": [
      {
        "name": "Xinyi Hu"
      },
      {
        "name": "Nikolaos Pappas"
      },
      {
        "name": "Howard H. Yang"
      }
    ],
    "summary": "Federated Learning (FL) has emerged as a privacy-preserving machine learning\nparadigm facilitating collaborative training across multiple clients without\nsharing local data. Despite advancements in edge device capabilities,\ncommunication bottlenecks present challenges in aggregating a large number of\nclients; only a portion of the clients can update their parameters upon each\nglobal aggregation. This phenomenon introduces the critical challenge of\nstragglers in FL and the profound impact of client scheduling policies on\nglobal model convergence and stability. Existing scheduling strategies address\nstaleness but predominantly focus on either timeliness or content. Motivated by\nthis, we introduce the novel concept of Version Age of Information (VAoI) to\nFL. Unlike traditional Age of Information metrics, VAoI considers both\ntimeliness and content staleness. Each client's version age is updated\ndiscretely, indicating the freshness of information. VAoI is incorporated into\nthe client scheduling policy to minimize the average VAoI, mitigating the\nimpact of outdated local updates and enhancing the stability of FL systems.",
    "comment": "5 pages, 4 figures, ICASSP 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05407v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05407v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05407v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05406v2",
    "updated": "2024-02-09T19:53:56+00:00",
    "published": "2024-02-08T04:48:26+00:00",
    "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
    "authors": [
      {
        "name": "Lucio Dery"
      },
      {
        "name": "Steven Kolawole"
      },
      {
        "name": "Jean-Fran\u00e7ois Kagy"
      },
      {
        "name": "Virginia Smith"
      },
      {
        "name": "Graham Neubig"
      },
      {
        "name": "Ameet Talwalkar"
      }
    ],
    "summary": "Given the generational gap in available hardware between lay practitioners\nand the most endowed institutions, LLMs are becoming increasingly inaccessible\nas they grow in size. Whilst many approaches have been proposed to compress\nLLMs to make their resource consumption manageable, these methods themselves\ntend to be resource intensive, putting them out of the reach of the very user\ngroups they target. In this work, we explore the problem of structured pruning\nof LLMs using only forward passes. We seek to empower practitioners to prune\nmodels so large that their available hardware has just enough memory to run\ninference. We develop Bonsai, a gradient-free, perturbative pruning method\ncapable of delivering small, fast, and accurate pruned models.\n  We observe that Bonsai outputs pruned models that (i) outperform those\ngenerated by more expensive gradient-based structured pruning methods, and (ii)\nare twice as fast (with comparable accuracy) as those generated by\nsemi-structured pruning methods requiring comparable resources as Bonsai. We\nalso leverage Bonsai to produce a new sub-2B model using a single A6000 that\nyields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM\nleaderboard.",
    "comment": "15 pages, 4 fiigures, 15 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05406v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05406v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05406v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06680v1",
    "updated": "2024-02-08T04:43:33+00:00",
    "published": "2024-02-08T04:43:33+00:00",
    "title": "Social Physics Informed Diffusion Model for Crowd Simulation",
    "authors": [
      {
        "name": "Hongyi Chen"
      },
      {
        "name": "Jingtao Ding"
      },
      {
        "name": "Yong Li"
      },
      {
        "name": "Yue Wang"
      },
      {
        "name": "Xiao-Ping Zhang"
      }
    ],
    "summary": "Crowd simulation holds crucial applications in various domains, such as urban\nplanning, architectural design, and traffic arrangement. In recent years,\nphysics-informed machine learning methods have achieved state-of-the-art\nperformance in crowd simulation but fail to model the heterogeneity and\nmulti-modality of human movement comprehensively. In this paper, we propose a\nsocial physics-informed diffusion model named SPDiff to mitigate the above gap.\nSPDiff takes both the interactive and historical information of crowds in the\ncurrent timeframe to reverse the diffusion process, thereby generating the\ndistribution of pedestrian movement in the subsequent timeframe. Inspired by\nthe well-known social physics model, i.e., Social Force, regarding crowd\ndynamics, we design a crowd interaction module to guide the denoising process\nand further enhance this module with the equivariant properties of crowd\ninteractions. To mitigate error accumulation in long-term simulations, we\npropose a multi-frame rollout training algorithm for diffusion modeling.\nExperiments conducted on two real-world datasets demonstrate the superior\nperformance of SPDiff in terms of macroscopic and microscopic evaluation\nmetrics. Code and appendix are available at\nhttps://github.com/tsinghua-fib-lab/SPDiff.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.soc-ph",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06680v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06680v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06680v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05403v2",
    "updated": "2024-02-09T19:09:52+00:00",
    "published": "2024-02-08T04:42:29+00:00",
    "title": "In-Context Principle Learning from Mistakes",
    "authors": [
      {
        "name": "Tianjun Zhang"
      },
      {
        "name": "Aman Madaan"
      },
      {
        "name": "Luyu Gao"
      },
      {
        "name": "Steven Zheng"
      },
      {
        "name": "Swaroop Mishra"
      },
      {
        "name": "Yiming Yang"
      },
      {
        "name": "Niket Tandon"
      },
      {
        "name": "Uri Alon"
      }
    ],
    "summary": "In-context learning (ICL, also known as few-shot prompting) has been the\nstandard method of adapting LLMs to downstream tasks, by learning from a few\ninput-output examples. Nonetheless, all ICL-based approaches only learn from\ncorrect input-output pairs. In this paper, we revisit this paradigm, by\nlearning more from the few given input-output examples. We introduce Learning\nPrinciples (LEAP): First, we intentionally induce the model to make mistakes on\nthese few examples; then we reflect on these mistakes, and learn explicit\ntask-specific \"principles\" from them, which help solve similar problems and\navoid common mistakes; finally, we prompt the model to answer unseen test\nquestions using the original few-shot examples and these learned general\nprinciples. We evaluate LEAP on a wide range of benchmarks, including multi-hop\nquestion answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning,\nand math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the\nstrongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and\nClaude-2.1. For example, LEAP improves over the standard few-shot prompting\nusing GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does\nnot require any more input or examples than the standard few-shot prompting\nsettings.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05403v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05403v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05403v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05401v1",
    "updated": "2024-02-08T04:35:09+00:00",
    "published": "2024-02-08T04:35:09+00:00",
    "title": "Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data",
    "authors": [
      {
        "name": "Farhad Pourkamali-Anaraki"
      },
      {
        "name": "Tahamina Nasrin"
      },
      {
        "name": "Robert E. Jensen"
      },
      {
        "name": "Amy M. Peterson"
      },
      {
        "name": "Christopher J. Hansen"
      }
    ],
    "summary": "A pivotal aspect in the design of neural networks lies in selecting\nactivation functions, crucial for introducing nonlinear structures that capture\nintricate input-output patterns. While the effectiveness of adaptive or\ntrainable activation functions has been studied in domains with ample data,\nlike image classification problems, significant gaps persist in understanding\ntheir influence on classification accuracy and predictive uncertainty in\nsettings characterized by limited data availability. This research aims to\naddress these gaps by investigating the use of two types of adaptive activation\nfunctions. These functions incorporate shared and individual trainable\nparameters per hidden layer and are examined in three testbeds derived from\nadditive manufacturing problems containing fewer than one hundred training\ninstances. Our investigation reveals that adaptive activation functions, such\nas Exponential Linear Unit (ELU) and Softplus, with individual trainable\nparameters, result in accurate and confident prediction models that outperform\nfixed-shape activation functions and the less flexible method of using\nidentical trainable activation functions in a hidden layer. Therefore, this\nwork presents an elegant way of facilitating the design of adaptive neural\nnetworks in scientific and engineering problems.",
    "comment": "7 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05401v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05401v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05401v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05400v1",
    "updated": "2024-02-08T04:31:21+00:00",
    "published": "2024-02-08T04:31:21+00:00",
    "title": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions",
    "authors": [
      {
        "name": "Kelsey Lieberman"
      },
      {
        "name": "Shuai Yuan"
      },
      {
        "name": "Swarna Kamlam Ravindran"
      },
      {
        "name": "Carlo Tomasi"
      }
    ],
    "summary": "Although binary classification is a well-studied problem in computer vision,\ntraining reliable classifiers under severe class imbalance remains a\nchallenging problem. Recent work has proposed techniques that mitigate the\neffects of training under imbalance by modifying the loss functions or\noptimization methods. While this work has led to significant improvements in\nthe overall accuracy in the multi-class case, we observe that slight changes in\nhyperparameter values of these methods can result in highly variable\nperformance in terms of Receiver Operating Characteristic (ROC) curves on\nbinary problems with severe imbalance. To reduce the sensitivity to\nhyperparameter choices and train more general models, we propose training over\na family of loss functions, instead of a single loss function. We develop a\nmethod for applying Loss Conditional Training (LCT) to an imbalanced\nclassification problem. Extensive experiment results, on both CIFAR and Kaggle\ncompetition datasets, show that our method improves model performance and is\nmore robust to hyperparameter choices. Code will be made available at:\nhttps://github.com/klieberman/roc_lct.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05400v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05400v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05400v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05399v1",
    "updated": "2024-02-08T04:27:14+00:00",
    "published": "2024-02-08T04:27:14+00:00",
    "title": "CURE: Simulation-Augmented Auto-Tuning in Robotics",
    "authors": [
      {
        "name": "Md Abir Hossen"
      },
      {
        "name": "Sonam Kharade"
      },
      {
        "name": "Jason M. O'Kane"
      },
      {
        "name": "Bradley Schmerl"
      },
      {
        "name": "David Garlan"
      },
      {
        "name": "Pooyan Jamshidi"
      }
    ],
    "summary": "Robotic systems are typically composed of various subsystems, such as\nlocalization and navigation, each encompassing numerous configurable components\n(e.g., selecting different planning algorithms). Once an algorithm has been\nselected for a component, its associated configuration options must be set to\nthe appropriate values. Configuration options across the system stack interact\nnon-trivially. Finding optimal configurations for highly configurable robots to\nachieve desired performance poses a significant challenge due to the\ninteractions between configuration options across software and hardware that\nresult in an exponentially large and complex configuration space. These\nchallenges are further compounded by the need for transferability between\ndifferent environments and robotic platforms. Data efficient optimization\nalgorithms (e.g., Bayesian optimization) have been increasingly employed to\nautomate the tuning of configurable parameters in cyber-physical systems.\nHowever, such optimization algorithms converge at later stages, often after\nexhausting the allocated budget (e.g., optimization steps, allotted time) and\nlacking transferability. This paper proposes CURE -- a method that identifies\ncausally relevant configuration options, enabling the optimization process to\noperate in a reduced search space, thereby enabling faster optimization of\nrobot performance. CURE abstracts the causal relationships between various\nconfiguration options and robot performance objectives by learning a causal\nmodel in the source (a low-cost environment such as the Gazebo simulator) and\napplying the learned knowledge to perform optimization in the target (e.g.,\nTurtlebot 3 physical robot). We demonstrate the effectiveness and\ntransferability of CURE by conducting experiments that involve varying degrees\nof deployment changes in both physical robots and simulation.",
    "comment": "Submitted in IEEE Transactions on Robotics (T-RO), 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05399v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05399v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05399v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05396v1",
    "updated": "2024-02-08T04:16:35+00:00",
    "published": "2024-02-08T04:16:35+00:00",
    "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning",
    "authors": [
      {
        "name": "Gangda Deng"
      },
      {
        "name": "Hongkuan Zhou"
      },
      {
        "name": "Hanqing Zeng"
      },
      {
        "name": "Yinglong Xia"
      },
      {
        "name": "Christopher Leung"
      },
      {
        "name": "Jianbo Li"
      },
      {
        "name": "Rajgopal Kannan"
      },
      {
        "name": "Viktor Prasanna"
      }
    ],
    "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05396v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05396v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05396v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05391v2",
    "updated": "2024-02-09T09:00:46+00:00",
    "published": "2024-02-08T04:04:36+00:00",
    "title": "Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey",
    "authors": [
      {
        "name": "Zhuo Chen"
      },
      {
        "name": "Yichi Zhang"
      },
      {
        "name": "Yin Fang"
      },
      {
        "name": "Yuxia Geng"
      },
      {
        "name": "Lingbing Guo"
      },
      {
        "name": "Xiang Chen"
      },
      {
        "name": "Qian Li"
      },
      {
        "name": "Wen Zhang"
      },
      {
        "name": "Jiaoyan Chen"
      },
      {
        "name": "Yushan Zhu"
      },
      {
        "name": "Jiaqi Li"
      },
      {
        "name": "Xiaoze Liu"
      },
      {
        "name": "Jeff Z. Pan"
      },
      {
        "name": "Ningyu Zhang"
      },
      {
        "name": "Huajun Chen"
      }
    ],
    "summary": "Knowledge Graphs (KGs) play a pivotal role in advancing various AI\napplications, with the semantic web community's exploration into multi-modal\ndimensions unlocking new avenues for innovation. In this survey, we carefully\nreview over 300 articles, focusing on KG-aware research in two principal\naspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal\ntasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into\nthe MMKG realm. We begin by defining KGs and MMKGs, then explore their\nconstruction progress. Our review includes two primary task categories:\nKG-aware multi-modal learning tasks, such as Image Classification and Visual\nQuestion Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph\nCompletion and Entity Alignment, highlighting specific research trajectories.\nFor most of these tasks, we provide definitions, evaluation benchmarks, and\nadditionally outline essential insights for conducting relevant research.\nFinally, we discuss current challenges and identify emerging trends, such as\nprogress in Large Language Modeling and Multi-modal Pre-training strategies.\nThis survey aims to serve as a comprehensive reference for researchers already\ninvolved in or considering delving into KG and multi-modal learning research,\noffering insights into the evolving landscape of MMKG research and supporting\nfuture work.",
    "comment": "Ongoing work; 55 pages, 11 Tables, 13 Figures, 619 citations; Paper\n  list is available at https://github.com/zjukg/KG-MM-Survey",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05391v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05391v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05391v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05382v1",
    "updated": "2024-02-08T03:46:32+00:00",
    "published": "2024-02-08T03:46:32+00:00",
    "title": "Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts",
    "authors": [
      {
        "name": "Zhili Liu"
      },
      {
        "name": "Kai Chen"
      },
      {
        "name": "Jianhua Han"
      },
      {
        "name": "Lanqing Hong"
      },
      {
        "name": "Hang Xu"
      },
      {
        "name": "Zhenguo Li"
      },
      {
        "name": "James T. Kwok"
      }
    ],
    "summary": "Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that\nachieves promising results in model pre-training. However, when the various\ndownstream tasks have data distributions different from the pre-training data,\nthe semantically irrelevant pre-training information might result in negative\ntransfer, impeding MAE's scalability. To address this issue, we propose a novel\nMAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE),\nwhich can be trained once but provides customized pre-training models for\ndiverse downstream tasks. Different from the mixture of experts (MoE), our MoCE\ntrains each expert only with semantically relevant images by using\ncluster-conditional gates. Thus, each downstream task can be allocated to its\ncustomized model pre-trained with data most similar to the downstream data.\nExperiments on a collection of 11 downstream tasks show that MoCE outperforms\nthe vanilla MAE by 2.45\\% on average. It also obtains new state-of-the-art\nself-supervised learning results on detection and segmentation.",
    "comment": "Accepted by ICLR 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05382v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05382v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05382v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05379v1",
    "updated": "2024-02-08T03:29:10+00:00",
    "published": "2024-02-08T03:29:10+00:00",
    "title": "Tradeoffs of Diagonal Fisher Information Matrix Estimators",
    "authors": [
      {
        "name": "Alexander Soen"
      },
      {
        "name": "Ke Sun"
      }
    ],
    "summary": "The Fisher information matrix characterizes the local geometry in the\nparameter space of neural networks. It elucidates insightful theories and\nuseful tools to understand and optimize neural networks. Given its high\ncomputational cost, practitioners often use random estimators and evaluate only\nthe diagonal entries. We examine two such estimators, whose accuracy and sample\ncomplexity depend on their associated variances. We derive bounds of the\nvariances and instantiate them in regression and classification networks. We\nnavigate trade-offs of both estimators based on analytical and numerical\nstudies. We find that the variance quantities depend on the non-linearity with\nrespect to different parameter groups and should not be neglected when\nestimating the Fisher information.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05379v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05379v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05379v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05378v1",
    "updated": "2024-02-08T03:22:12+00:00",
    "published": "2024-02-08T03:22:12+00:00",
    "title": "Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks",
    "authors": [
      {
        "name": "Tharaka Perera"
      },
      {
        "name": "Saman Atapattu"
      },
      {
        "name": "Yuting Fang"
      },
      {
        "name": "Jamie Evans"
      }
    ],
    "summary": "This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD)\nnetworks, considering scenarios involving eavesdroppers. Our investigation\nrevolves around the intricacies of the sum secrecy rate maximization problem,\nparticularly when faced with coordinated and distributed eavesdroppers\nemploying a Minimum Mean Square Error (MMSE) receiver. Our contributions\ninclude an iterative classical optimization solution and an unsupervised\nlearning strategy based on Graph Neural Networks (GNNs). To the best of our\nknowledge, this work marks the initial exploration of GNNs for PLS\napplications. Additionally, we extend the GNN approach to address the absence\nof eavesdroppers' channel knowledge. Extensive numerical simulations highlight\nFlexD's superiority over Half-Duplex (HD) communications and the GNN approach's\nsuperiority over the classical method in both performance and time complexity.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.SP",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05378v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05378v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05378v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05374v1",
    "updated": "2024-02-08T03:12:25+00:00",
    "published": "2024-02-08T03:12:25+00:00",
    "title": "CIC: A framework for Culturally-aware Image Captioning",
    "authors": [
      {
        "name": "Youngsik Yun"
      },
      {
        "name": "Jihie Kim"
      }
    ],
    "summary": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)},\nthat generates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs. Our code\nand dataset will be made publicly available upon acceptance.",
    "comment": "14 pages, 10 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05374v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05374v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05374v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05372v1",
    "updated": "2024-02-08T03:02:59+00:00",
    "published": "2024-02-08T03:02:59+00:00",
    "title": "Reduced-order modeling of unsteady fluid flow using neural network ensembles",
    "authors": [
      {
        "name": "Rakesh Halder"
      },
      {
        "name": "Mohammadmehdi Ataei"
      },
      {
        "name": "Hesam Salehipour"
      },
      {
        "name": "Krzysztof Fidkowski"
      },
      {
        "name": "Kevin Maki"
      }
    ],
    "summary": "The use of deep learning has become increasingly popular in reduced-order\nmodels (ROMs) to obtain low-dimensional representations of full-order models.\nConvolutional autoencoders (CAEs) are often used to this end as they are adept\nat handling data that are spatially distributed, including solutions to partial\ndifferential equations. When applied to unsteady physics problems, ROMs also\nrequire a model for time-series prediction of the low-dimensional latent\nvariables. Long short-term memory (LSTM) networks, a type of recurrent neural\nnetwork useful for modeling sequential data, are frequently employed in\ndata-driven ROMs for autoregressive time-series prediction. When making\npredictions at unseen design points over long time horizons, error propagation\nis a frequently encountered issue, where errors made early on can compound over\ntime and lead to large inaccuracies. In this work, we propose using bagging, a\ncommonly used ensemble learning technique, to develop a fully data-driven ROM\nframework referred to as the CAE-eLSTM ROM that uses CAEs for spatial\nreconstruction of the full-order model and LSTM ensembles for time-series\nprediction. When applied to two unsteady fluid dynamics problems, our results\nshow that the presented framework effectively reduces error propagation and\nleads to more accurate time-series prediction of latent variables at unseen\npoints.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.flu-dyn",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05372v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05372v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05372v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05370v1",
    "updated": "2024-02-08T03:00:50+00:00",
    "published": "2024-02-08T03:00:50+00:00",
    "title": "Attention as Robust Representation for Time Series Forecasting",
    "authors": [
      {
        "name": "PeiSong Niu"
      },
      {
        "name": "Tian Zhou"
      },
      {
        "name": "Xue Wang"
      },
      {
        "name": "Liang Sun"
      },
      {
        "name": "Rong Jin"
      }
    ],
    "summary": "Time series forecasting is essential for many practical applications, with\nthe adoption of transformer-based models on the rise due to their impressive\nperformance in NLP and CV. Transformers' key feature, the attention mechanism,\ndynamically fusing embeddings to enhance data representation, often relegating\nattention weights to a byproduct role. Yet, time series data, characterized by\nnoise and non-stationarity, poses significant forecasting challenges. Our\napproach elevates attention weights as the primary representation for time\nseries, capitalizing on the temporal relationships among data points to improve\nforecasting accuracy. Our study shows that an attention map, structured using\nglobal landmarks and local windows, acts as a robust kernel representation for\ndata points, withstanding noise and shifts in distribution. Our method\noutperforms state-of-the-art models, reducing mean squared error (MSE) in\nmultivariate time series forecasting by a notable 3.6% without altering the\ncore neural network architecture. It serves as a versatile component that can\nreadily replace recent patching based embedding schemes in transformer-based\nmodels, boosting their performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05370v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05370v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05370v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05369v1",
    "updated": "2024-02-08T02:58:47+00:00",
    "published": "2024-02-08T02:58:47+00:00",
    "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards",
    "authors": [
      {
        "name": "Huayu Chen"
      },
      {
        "name": "Guande He"
      },
      {
        "name": "Hang Su"
      },
      {
        "name": "Jun Zhu"
      }
    ],
    "summary": "User intentions are typically formalized as evaluation rewards to be\nmaximized when fine-tuning language models (LMs). Existing alignment methods,\nsuch as Direct Preference Optimization (DPO), are mainly tailored for pairwise\npreference data where rewards are implicitly defined rather than explicitly\ngiven. In this paper, we introduce a general framework for LM alignment,\nleveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling\nreward datasets explicitly annotated with scalar evaluations. Our framework\ncomprises two parallel algorithms, NCA and InfoNCA, both enabling the direct\nextraction of an LM policy from reward data as well as preference data.\nNotably, we show that the DPO loss is a special case of our proposed InfoNCA\nobjective under pairwise preference settings, thereby integrating and extending\ncurrent alignment theories. By contrasting NCA and InfoNCA, we show that\nInfoNCA and DPO adjust relative likelihood across different responses to a\nsingle instruction, while NCA optimizes absolute likelihood for each response.\nWe apply our methods to align a 7B language model with a GPT-4 annotated reward\ndataset. Experimental results suggest that InfoNCA surpasses the DPO baseline\nin GPT-4 evaluations, while NCA enjoys better training stability with\ncompetitive performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05369v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05369v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05369v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05367v1",
    "updated": "2024-02-08T02:57:47+00:00",
    "published": "2024-02-08T02:57:47+00:00",
    "title": "Principled Preferential Bayesian Optimization",
    "authors": [
      {
        "name": "Wenjie Xu"
      },
      {
        "name": "Wenbin Wang"
      },
      {
        "name": "Yuning Jiang"
      },
      {
        "name": "Bratislav Svetozarevic"
      },
      {
        "name": "Colin N. Jones"
      }
    ],
    "summary": "We study the problem of preferential Bayesian optimization (BO), where we aim\nto optimize a black-box function with only preference feedback over a pair of\ncandidate solutions. Inspired by the likelihood ratio idea, we construct a\nconfidence set of the black-box function using only the preference feedback. An\noptimistic algorithm with an efficient computational method is then developed\nto solve the problem, which enjoys an information-theoretic bound on the\ncumulative regret, a first-of-its-kind for preferential BO. This bound further\nallows us to design a scheme to report an estimated best solution, with a\nguaranteed convergence rate. Experimental results on sampled instances from\nGaussian processes, standard test functions, and a thermal comfort optimization\nproblem all show that our method stably achieves better or competitive\nperformance as compared to the existing state-of-the-art heuristics, which,\nhowever, do not have theoretical guarantees on regret bounds or convergence.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05367v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05367v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05367v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05359v1",
    "updated": "2024-02-08T02:37:30+00:00",
    "published": "2024-02-08T02:37:30+00:00",
    "title": "Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving",
    "authors": [
      {
        "name": "Yizhou Zhang"
      },
      {
        "name": "Lun Du"
      },
      {
        "name": "Defu Cao"
      },
      {
        "name": "Qiang Fu"
      },
      {
        "name": "Yan Liu"
      }
    ],
    "summary": "Foundation models, such as Large language Models (LLMs), have attracted\nsignificant amount of interest due to their large number of applications.\nExisting works show that appropriate prompt design, such as Chain-of-Thoughts,\ncan unlock LLM's powerful capacity in diverse areas. However, when handling\ntasks involving repetitive sub-tasks and/or deceptive contents, such as\narithmetic calculation and article-level fake news detection, existing\nprompting strategies either suffers from insufficient expressive power or\nintermediate errors triggered by hallucination. To make LLM more discerning to\nsuch intermediate errors, we propose to guide LLM with a Divide-and-Conquer\nprogram that simultaneously ensures superior expressive power and disentangles\ntask decomposition, sub-task resolution, and resolution assembly process.\nTheoretic analysis reveals that our strategy can guide LLM to extend the\nexpressive power of fixed-depth Transformer. Experiments indicate that our\nproposed method can achieve better performance than typical prompting\nstrategies in tasks bothered by intermediate errors and deceptive contents,\nsuch as large integer multiplication, hallucination detection and\nmisinformation detection.",
    "comment": "Preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05359v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05359v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05359v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05356v1",
    "updated": "2024-02-08T02:29:33+00:00",
    "published": "2024-02-08T02:29:33+00:00",
    "title": "Exploring Learning Complexity for Downstream Data Pruning",
    "authors": [
      {
        "name": "Wenyu Jiang"
      },
      {
        "name": "Zhenlong Liu"
      },
      {
        "name": "Zejian Xie"
      },
      {
        "name": "Songxin Zhang"
      },
      {
        "name": "Bingyi Jing"
      },
      {
        "name": "Hongxin Wei"
      }
    ],
    "summary": "The over-parameterized pre-trained models pose a great challenge to\nfine-tuning with limited computation resources. An intuitive solution is to\nprune the less informative samples from the fine-tuning dataset. A series of\ntraining-based scoring functions are proposed to quantify the informativeness\nof the data subset but the pruning cost becomes non-negligible due to the heavy\nparameter updating. For efficient pruning, it is viable to adapt the similarity\nscoring function of geometric-based methods from training-based to\ntraining-free. However, we empirically show that such adaption distorts the\noriginal pruning and results in inferior performance on the downstream tasks.\nIn this paper, we propose to treat the learning complexity (LC) as the scoring\nfunction for classification and regression tasks. Specifically, the learning\ncomplexity is defined as the average predicted confidence of subnets with\ndifferent capacities, which encapsulates data processing within a converged\nmodel. Then we preserve the diverse and easy samples for fine-tuning. Extensive\nexperiments with vision datasets demonstrate the effectiveness and efficiency\nof the proposed scoring function for classification tasks. For the instruction\nfine-tuning of large language models, our method achieves state-of-the-art\nperformance with stable convergence, outperforming the full training with only\n10\\% of the instruction dataset.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05356v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05356v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05356v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05355v1",
    "updated": "2024-02-08T02:27:13+00:00",
    "published": "2024-02-08T02:27:13+00:00",
    "title": "A Survey on Safe Multi-Modal Learning System",
    "authors": [
      {
        "name": "Tianyi Zhao"
      },
      {
        "name": "Liangliang Zhang"
      },
      {
        "name": "Yao Ma"
      },
      {
        "name": "Lu Cheng"
      }
    ],
    "summary": "With the wide deployment of multimodal learning systems (MMLS) in real-world\nscenarios, safety concerns have become increasingly prominent. The absence of\nsystematic research into their safety is a significant barrier to progress in\nthis field. To bridge the gap, we present the first taxonomy for MMLS safety,\nidentifying four essential pillars of these concerns. Leveraging this taxonomy,\nwe conduct in-depth reviews for each pillar, highlighting key limitations based\non the current state of development. Finally, we pinpoint unique challenges in\nMMLS safety and provide potential directions for future research.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CY",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05355v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05355v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05355v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05353v1",
    "updated": "2024-02-08T02:21:33+00:00",
    "published": "2024-02-08T02:21:33+00:00",
    "title": "Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels",
    "authors": [
      {
        "name": "Taehyeon Kim"
      },
      {
        "name": "Donggyu Kim"
      },
      {
        "name": "Se-Young Yun"
      }
    ],
    "summary": "In the evolving landscape of federated learning (FL), addressing label noise\npresents unique challenges due to the decentralized and diverse nature of data\ncollection across clients. Traditional centralized learning approaches to\nmitigate label noise are constrained in FL by privacy concerns and the\nheterogeneity of client data. This paper revisits early-learning\nregularization, introducing an innovative strategy, Federated Label-mixture\nRegularization (FLR). FLR adeptly adapts to FL's complexities by generating new\npseudo labels, blending local and global model predictions. This method not\nonly enhances the accuracy of the global model in both i.i.d. and non-i.i.d.\nsettings but also effectively counters the memorization of noisy labels.\nDemonstrating compatibility with existing label noise and FL techniques, FLR\npaves the way for improved generalization in FL environments fraught with label\ninaccuracies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05353v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05353v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05353v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05346v1",
    "updated": "2024-02-08T01:41:28+00:00",
    "published": "2024-02-08T01:41:28+00:00",
    "title": "KIX: A Metacognitive Generalization Framework",
    "authors": [
      {
        "name": "Arun Kumar"
      },
      {
        "name": "Paul Schrater"
      }
    ],
    "summary": "Humans and other animals aptly exhibit general intelligence behaviors in\nsolving a variety of tasks with flexibility and ability to adapt to novel\nsituations by reusing and applying high level knowledge acquired over time. But\nartificial agents are more of a specialist, lacking such generalist behaviors.\nArtificial agents will require understanding and exploiting critical structured\nknowledge representations. We present a metacognitive generalization framework,\nKnowledge-Interaction-eXecution (KIX), and argue that interactions with objects\nleveraging type space facilitate the learning of transferable interaction\nconcepts and generalization. It is a natural way of integrating knowledge into\nreinforcement learning and promising to act as an enabler for autonomous and\ngeneralist behaviors in artificial intelligence systems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05346v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05346v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05346v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05330v1",
    "updated": "2024-02-08T00:12:18+00:00",
    "published": "2024-02-08T00:12:18+00:00",
    "title": "Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference",
    "authors": [
      {
        "name": "Luca Masserano"
      },
      {
        "name": "Alex Shen"
      },
      {
        "name": "Michele Doro"
      },
      {
        "name": "Tommaso Dorigo"
      },
      {
        "name": "Rafael Izbicki"
      },
      {
        "name": "Ann B. Lee"
      }
    ],
    "summary": "An open scientific challenge is how to classify events with reliable measures\nof uncertainty, when we have a mechanistic model of the data-generating process\nbut the distribution over both labels and latent nuisance parameters is\ndifferent between train and target data. We refer to this type of\ndistributional shift as generalized label shift (GLS). Direct classification\nusing observed data $\\mathbf{X}$ as covariates leads to biased predictions and\ninvalid uncertainty estimates of labels $Y$. We overcome these biases by\nproposing a new method for robust uncertainty quantification that casts\nclassification as a hypothesis testing problem under nuisance parameters. The\nkey idea is to estimate the classifier's receiver operating characteristic\n(ROC) across the entire nuisance parameter space, which allows us to devise\ncutoffs that are invariant under GLS. Our method effectively endows a\npre-trained classifier with domain adaptation capabilities and returns valid\nprediction sets while maintaining high power. We demonstrate its performance on\ntwo challenging scientific problems in biology and astroparticle physics with\ndata from realistic mechanistic models.",
    "comment": "25 pages, 18 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05330v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05330v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05330v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05322v1",
    "updated": "2024-02-07T23:50:00+00:00",
    "published": "2024-02-07T23:50:00+00:00",
    "title": "Learning on Multimodal Graphs: A Survey",
    "authors": [
      {
        "name": "Ciyuan Peng"
      },
      {
        "name": "Jiayuan He"
      },
      {
        "name": "Feng Xia"
      }
    ],
    "summary": "Multimodal data pervades various domains, including healthcare, social media,\nand transportation, where multimodal graphs play a pivotal role. Machine\nlearning on multimodal graphs, referred to as multimodal graph learning (MGL),\nis essential for successful artificial intelligence (AI) applications. The\nburgeoning research in this field encompasses diverse graph data types and\nmodalities, learning techniques, and application scenarios. This survey paper\nconducts a comparative analysis of existing works in multimodal graph learning,\nelucidating how multimodal learning is achieved across different graph types\nand exploring the characteristics of prevalent learning techniques.\nAdditionally, we delineate significant applications of multimodal graph\nlearning and offer insights into future directions in this domain.\nConsequently, this paper serves as a foundational resource for researchers\nseeking to comprehend existing MGL techniques and their applicability across\ndiverse scenarios.",
    "comment": "9 pages, 1 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR",
      "cs.SI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05322v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05322v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05322v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05318v1",
    "updated": "2024-02-07T23:39:40+00:00",
    "published": "2024-02-07T23:39:40+00:00",
    "title": "Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs",
    "authors": [
      {
        "name": "Dipankar Sarkar"
      }
    ],
    "summary": "Information retrieval is a rapidly evolving field of information retrieval,\nwhich is characterized by a continuous refinement of techniques and\ntechnologies, from basic hyperlink-based navigation to sophisticated\nalgorithm-driven search engines. This paper aims to provide a comprehensive\noverview of the evolution of Information Retrieval Technology, with a\nparticular focus on the role of Large Language Models (LLMs) in bridging the\ngap between traditional search methods and the emerging paradigm of answer\nretrieval. The integration of LLMs in the realms of response retrieval and\nindexing signifies a paradigm shift in how users interact with information\nsystems. This paradigm shift is driven by the integration of large language\nmodels (LLMs) like GPT-4, which are capable of understanding and generating\nhuman-like text, thus enabling them to provide more direct and contextually\nrelevant answers to user queries. Through this exploration, we seek to\nilluminate the technological milestones that have shaped this journey and the\npotential future directions in this rapidly changing field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05318v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05318v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05318v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05309v1",
    "updated": "2024-02-07T23:02:53+00:00",
    "published": "2024-02-07T23:02:53+00:00",
    "title": "Investigating Generalization Behaviours of Generative Flow Networks",
    "authors": [
      {
        "name": "Lazar Atanackovic"
      },
      {
        "name": "Emmanuel Bengio"
      }
    ],
    "summary": "Generative Flow Networks (GFlowNets, GFNs) are a generative framework for\nlearning unnormalized probability mass functions over discrete spaces. Since\ntheir inception, GFlowNets have proven to be useful for learning generative\nmodels in applications where the majority of the discrete space is unvisited\nduring training. This has inspired some to hypothesize that GFlowNets, when\npaired with deep neural networks (DNNs), have favourable generalization\nproperties. In this work, we empirically verify some of the hypothesized\nmechanisms of generalization of GFlowNets. In particular, we find that the\nfunctions that GFlowNets learn to approximate have an implicit underlying\nstructure which facilitate generalization. We also find that GFlowNets are\nsensitive to being trained offline and off-policy; however, the reward\nimplicitly learned by GFlowNets is robust to changes in the training\ndistribution.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05309v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05309v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05309v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05307v1",
    "updated": "2024-02-07T23:00:24+00:00",
    "published": "2024-02-07T23:00:24+00:00",
    "title": "Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks",
    "authors": [
      {
        "name": "Peter Graf"
      },
      {
        "name": "Patrick Emami"
      }
    ],
    "summary": "Neurosymbolic AI combines the interpretability, parsimony, and explicit\nreasoning of classical symbolic approaches with the statistical learning of\ndata-driven neural approaches. Models and policies that are simultaneously\ndifferentiable and interpretable may be key enablers of this marriage. This\npaper demonstrates three pathways to implementing such models and policies in a\nreal-world reinforcement learning setting. Specifically, we study a broad class\nof neural networks that build interpretable semantics directly into their\narchitecture. We reveal and highlight both the potential and the essential\ndifficulties of combining logic, simulation, and learning. One lesson is that\nlearning benefits from continuity and differentiability, but classical logic is\ndiscrete and non-differentiable. The relaxation to real-valued, differentiable\nrepresentations presents a trade-off; the more learnable, the less\ninterpretable. Another lesson is that using logic in the context of a numerical\nsimulation involves a non-trivial mapping from raw (e.g., real-valued time\nseries) simulation data to logical predicates. Some open questions this note\nexposes include: What are the limits of rule-based controllers, and how\nlearnable are they? Do the differentiable interpretable approaches discussed\nhere scale to large, complex, uncertain systems? Can we truly achieve\ninterpretability? We highlight these and other themes across the three\napproaches.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05307v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05307v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05307v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05306v1",
    "updated": "2024-02-07T22:53:54+00:00",
    "published": "2024-02-07T22:53:54+00:00",
    "title": "Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making",
    "authors": [
      {
        "name": "Yuan Tian"
      },
      {
        "name": "Wenqi Zhou"
      },
      {
        "name": "Hao Dong"
      },
      {
        "name": "David S. Kammer"
      },
      {
        "name": "Olga Fink"
      }
    ],
    "summary": "Symbolic regression holds great potential for uncovering underlying\nmathematical and physical relationships from empirical data. While existing\ntransformer-based models have recently achieved significant success in this\ndomain, they face challenges in terms of generalizability and adaptability.\nTypically, in cases where the output expressions do not adequately fit\nexperimental data, the models lack efficient mechanisms to adapt or modify the\nexpression. This inflexibility hinders their application in real-world\nscenarios, particularly in discovering unknown physical or biological\nrelationships. Inspired by how human experts refine and adapt expressions, we\nintroduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based\nmodel that redefines symbolic regression as a sequential decision-making task.\nSym-Q leverages supervised demonstrations and refines expressions based on\nreward signals indicating the quality of fitting precision. Its distinctive\nability to manage the complexity of expression trees and perform precise\nstep-wise updates significantly enhances flexibility and efficiency. Our\nresults demonstrate that Sym-Q excels not only in recovering underlying\nmathematical structures but also uniquely learns to efficiently refine the\noutput expression based on reward signals, thereby discovering underlying\nexpressions. Sym-Q paves the way for more intuitive and impactful discoveries\nin physical science, marking a substantial advancement in the field of symbolic\nregression.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05306v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05306v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05306v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05301v2",
    "updated": "2024-02-09T21:26:40+00:00",
    "published": "2024-02-07T22:37:16+00:00",
    "title": "BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs",
    "authors": [
      {
        "name": "Lyle Regenwetter"
      },
      {
        "name": "Yazan Abu Obaideh"
      },
      {
        "name": "Amin Heyrani Nobari"
      },
      {
        "name": "Faez Ahmed"
      }
    ],
    "summary": "This paper introduces a public dataset of 1.4 million procedurally-generated\nbicycle designs represented parametrically, as JSON files, and as rasterized\nimages. The dataset is created through the use of a rendering engine which\nharnesses the BikeCAD software to generate vector graphics from parametric\ndesigns. This rendering engine is discussed in the paper and also released\npublicly alongside the dataset. Though this dataset has numerous applications,\na principal motivation is the need to train cross-modal predictive models\nbetween parametric and image-based design representations. For example, we\ndemonstrate that a predictive model can be trained to accurately estimate\nContrastive Language-Image Pretraining (CLIP) embeddings from a parametric\nrepresentation directly. This allows similarity relations to be established\nbetween parametric bicycle designs and text strings or reference images.\nTrained predictive models are also made public. The dataset joins the BIKED\ndataset family which includes thousands of mixed-representation human-designed\nbicycle models and several datasets quantifying design performance. The code\nand dataset can be found at:\nhttps://github.com/Lyleregenwetter/BIKED_multimodal/tree/main",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05301v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05301v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05301v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05979v1",
    "updated": "2024-02-07T22:29:42+00:00",
    "published": "2024-02-07T22:29:42+00:00",
    "title": "On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI",
    "authors": [
      {
        "name": "Daniel McDuff"
      },
      {
        "name": "Tim Korjakow"
      },
      {
        "name": "Scott Cambo"
      },
      {
        "name": "Jesse Josua Benjamin"
      },
      {
        "name": "Jenny Lee"
      },
      {
        "name": "Yacine Jernite"
      },
      {
        "name": "Carlos Mu\u00f1oz Ferrandis"
      },
      {
        "name": "Aaron Gokaslan"
      },
      {
        "name": "Alek Tarkowski"
      },
      {
        "name": "Joseph Lindley"
      },
      {
        "name": "A. Feder Cooper"
      },
      {
        "name": "Danish Contractor"
      }
    ],
    "summary": "Growing concerns over negligent or malicious uses of AI have increased the\nappetite for tools that help manage the risks of the technology. In 2018,\nlicenses with behaviorial-use clauses (commonly referred to as Responsible AI\nLicenses) were proposed to give developers a framework for releasing AI assets\nwhile specifying their users to mitigate negative applications. As of the end\nof 2023, on the order of 40,000 software and model repositories have adopted\nresponsible AI licenses licenses. Notable models licensed with behavioral use\nclauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion\n(image), and GRID (robotics). This paper explores why and how these licenses\nhave been adopted, and why and how they have been adapted to fit particular use\ncases. We use a mixed-methods methodology of qualitative interviews, clustering\nof license clauses, and quantitative analysis of license adoption. Based on\nthis evidence we take the position that responsible AI licenses need\nstandardization to avoid confusing users or diluting their impact. At the same\ntime, customization of behavioral restrictions is also appropriate in some\ncontexts (e.g., medical domains). We advocate for ``standardized\ncustomization'' that can meet users' needs and can be supported via tooling.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05979v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05979v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05979v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05978v1",
    "updated": "2024-02-07T22:27:16+00:00",
    "published": "2024-02-07T22:27:16+00:00",
    "title": "Combining shape and contour features to improve tool wear monitoring in milling processes",
    "authors": [
      {
        "name": "M. T. Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "E. Alegre-Guti\u00e9rrez"
      },
      {
        "name": "V. Gonz\u00e1lez-Castro"
      },
      {
        "name": "R. Alaiz-Rodr\u00edguez"
      }
    ],
    "summary": "In this paper, a new system based on combinations of a shape descriptor and a\ncontour descriptor has been proposed for classifying inserts in milling\nprocesses according to their wear level following a computer vision based\napproach. To describe the wear region shape we have proposed a new descriptor\ncalled ShapeFeat and its contour has been characterized using the method\nBORCHIZ that, to the best of our knowledge, achieves the best performance for\ntool wear monitoring following a computer vision-based approach. Results show\nthat the combination of BORCHIZ with ShapeFeat using a late fusion method\nimproves the classification performance significantly, obtaining an accuracy of\n91.44% in the binary classification (i.e. the classification of the wear as\nhigh or low) and 82.90% using three target classes (i.e. classification of the\nwear as high, medium or low). These results outperform the ones obtained by\nboth descriptors used on their own, which achieve accuracies of 88.70 and\n80.67% for two and three classes, respectively, using ShapeFeat and 87.06 and\n80.24% with B-ORCHIZ. This study yielded encouraging results for the\nmanufacturing community in order to classify automatically the inserts in terms\nof their wear for milling processes.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05978v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05978v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05978v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05977v1",
    "updated": "2024-02-07T22:25:54+00:00",
    "published": "2024-02-07T22:25:54+00:00",
    "title": "Tool wear monitoring using an online, automatic and low cost system based on local texture",
    "authors": [
      {
        "name": "M. T. Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "E. Alegre-Guti\u00e9rrez"
      },
      {
        "name": "R. Alaiz-Rodr\u00edguez"
      },
      {
        "name": "V. Gonz\u00e1lez-Castro"
      }
    ],
    "summary": "In this work we propose a new online, low cost and fast approach based on\ncomputer vision and machine learning to determine whether cutting tools used in\nedge profile milling processes are serviceable or disposable based on their\nwear level. We created a new dataset of 254 images of edge profile cutting\nheads which is, to the best of our knowledge, the first publicly available\ndataset with enough quality for this purpose. All the inserts were segmented\nand their cutting edges were cropped, obtaining 577 images of cutting edges:\n301 functional and 276 disposable. The proposed method is based on (1) dividing\nthe cutting edge image in different regions, called Wear Patches (WP), (2)\ncharacterising each one as worn or serviceable using texture descriptors based\non different variants of Local Binary Patterns (LBP) and (3) determine, based\non the state of these WP, if the cutting edge (and, therefore, the tool) is\nserviceable or disposable. We proposed and assessed five different patch\ndivision configurations. The individual WP were classified by a Support Vector\nMachine (SVM) with an intersection kernel. The best patch division\nconfiguration and texture descriptor for the WP achieves an accuracy of 90.26%\nin the detection of the disposable cutting edges. These results show a very\npromising opportunity for automatic wear monitoring in edge profile milling\nprocesses.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05977v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05977v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05977v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05976v1",
    "updated": "2024-02-07T22:24:09+00:00",
    "published": "2024-02-07T22:24:09+00:00",
    "title": "RankSum An unsupervised extractive text summarization based on rank fusion",
    "authors": [
      {
        "name": "A. Joshi"
      },
      {
        "name": "E. Fidalgo"
      },
      {
        "name": "E. Alegre"
      },
      {
        "name": "R. Alaiz-Rodriguez"
      }
    ],
    "summary": "In this paper, we propose Ranksum, an approach for extractive text\nsummarization of single documents based on the rank fusion of four\nmulti-dimensional sentence features extracted for each sentence: topic\ninformation, semantic content, significant keywords, and position. The Ranksum\nobtains the sentence saliency rankings corresponding to each feature in an\nunsupervised way followed by the weighted fusion of the four scores to rank the\nsentences according to their significance. The scores are generated in\ncompletely unsupervised way, and a labeled document set is required to learn\nthe fusion weights. Since we found that the fusion weights can generalize to\nother datasets, we consider the Ranksum as an unsupervised approach. To\ndetermine topic rank, we employ probabilistic topic models whereas semantic\ninformation is captured using sentence embeddings. To derive rankings using\nsentence embeddings, we utilize Siamese networks to produce abstractive\nsentence representation and then we formulate a novel strategy to arrange them\nin their order of importance. A graph-based strategy is applied to find the\nsignificant keywords and related sentence rankings in the document. We also\nformulate a sentence novelty measure based on bigrams, trigrams, and sentence\nembeddings to eliminate redundant sentences from the summary. The ranks of all\nthe sentences computed for each feature are finally fused to get the final\nscore for each sentence in the document. We evaluate our approach on publicly\navailable summarization datasets CNN/DailyMail and DUC 2002. Experimental\nresults show that our approach outperforms other existing state-of-the-art\nsummarization methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05976v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05976v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05976v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05296v1",
    "updated": "2024-02-07T22:19:08+00:00",
    "published": "2024-02-07T22:19:08+00:00",
    "title": "Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach",
    "authors": [
      {
        "name": "F. Janez-Martino"
      },
      {
        "name": "R. Alaiz-Rodriguez"
      },
      {
        "name": "V. Gonzalez-Castro"
      },
      {
        "name": "E. Fidalgo"
      },
      {
        "name": "E. Alegre"
      }
    ],
    "summary": "Spam emails are unsolicited, annoying and sometimes harmful messages which\nmay contain malware, phishing or hoaxes. Unlike most studies that address the\ndesign of efficient anti-spam filters, we approach the spam email problem from\na different and novel perspective. Focusing on the needs of cybersecurity\nunits, we follow a topic-based approach for addressing the classification of\nspam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S,\ntwo novel datasets with approximately 15K emails each in English and Spanish,\nrespectively, and we label them using agglomerative hierarchical clustering\ninto 11 classes. We evaluate 16 pipelines, combining four text representation\ntechniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words,\nWord2Vec and BERT- and four classifiers: Support Vector Machine, N\\\"aive Bayes,\nRandom Forest and Logistic Regression. Experimental results show that the\nhighest performance is achieved with TF-IDF and LR for the English dataset,\nwith a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish\ndataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5% accuracy.\nRegarding the processing time, TF-IDF with LR leads to the fastest\nclassification, processing an English and Spanish spam email in and on average,\nrespectively.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05296v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05296v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05296v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05295v1",
    "updated": "2024-02-07T22:17:37+00:00",
    "published": "2024-02-07T22:17:37+00:00",
    "title": "An information theoretic approach to quantify the stability of feature selection and ranking algorithms",
    "authors": [
      {
        "name": "Alaiz-Rodriguez"
      },
      {
        "name": "R."
      },
      {
        "name": "Parnell"
      },
      {
        "name": "A. C"
      }
    ],
    "summary": "Feature selection is a key step when dealing with high dimensional data. In\nparticular, these techniques simplify the process of knowledge discovery from\nthe data by selecting the most relevant features out of the noisy, redundant\nand irrelevant features. A problem that arises in many of these practical\napplications is that the outcome of the feature selection algorithm is not\nstable. Thus, small variations in the data may yield very different feature\nrankings. Assessing the stability of these methods becomes an important issue\nin the previously mentioned situations. We propose an information theoretic\napproach based on the Jensen Shannon divergence to quantify this robustness.\nUnlike other stability measures, this metric is suitable for different\nalgorithm outcomes: full ranked lists, feature subsets as well as the lesser\nstudied partial ranked lists. This generalized metric quantifies the difference\namong a whole set of lists with the same size, following a probabilistic\napproach and being able to give more importance to the disagreements that\nappear at the top of the list. Moreover, it possesses desirable properties\nincluding correction for change, upper lower bounds and conditions for a\ndeterministic selection. We illustrate the use of this stability metric with\ndata generated in a fully controlled way and compare it with popular metrics\nincluding the Spearmans rank correlation and the Kunchevas index on feature\nranking and selection outcomes, respectively. Additionally, experimental\nvalidation of the proposed approach is carried out on a real-world problem of\nfood quality assessment showing its potential to quantify stability from\ndifferent perspectives.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05295v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05295v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05295v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05294v1",
    "updated": "2024-02-07T22:16:53+00:00",
    "published": "2024-02-07T22:16:53+00:00",
    "title": "Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection",
    "authors": [
      {
        "name": "Pramit Saha"
      },
      {
        "name": "Divyanshu Mishra"
      },
      {
        "name": "Felix Wagner"
      },
      {
        "name": "Konstantinos Kamnitsas"
      },
      {
        "name": "J. Alison Noble"
      }
    ],
    "summary": "Multimodal Federated Learning (MMFL) utilizes multiple modalities in each\nclient to build a more powerful Federated Learning (FL) model than its unimodal\ncounterpart. However, the impact of missing modality in different clients, also\ncalled modality incongruity, has been greatly overlooked. This paper, for the\nfirst time, analyses the impact of modality incongruity and reveals its\nconnection with data heterogeneity across participating clients. We\nparticularly inspect whether incongruent MMFL with unimodal and multimodal\nclients is more beneficial than unimodal FL. Furthermore, we examine three\npotential routes of addressing this issue. Firstly, we study the effectiveness\nof various self-attention mechanisms towards incongruity-agnostic information\nfusion in MMFL. Secondly, we introduce a modality imputation network (MIN)\npre-trained in a multimodal client for modality translation in unimodal clients\nand investigate its potential towards mitigating the missing modality problem.\nThirdly, we assess the capability of client-level and server-level\nregularization techniques towards mitigating modality incongruity effects.\nExperiments are conducted under several MMFL settings on two publicly available\nreal-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology\nreports.",
    "comment": "42 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05294v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05294v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05294v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05293v1",
    "updated": "2024-02-07T22:14:14+00:00",
    "published": "2024-02-07T22:14:14+00:00",
    "title": "A comparative study on feature selection for a risk prediction model for colorectal cancer",
    "authors": [
      {
        "name": "N. Cueto-L\u00f3pez"
      },
      {
        "name": "M. T. Garc\u00eda-Ord\u00e1s"
      },
      {
        "name": "V. D\u00e1vila-Batista"
      },
      {
        "name": "V. Moreno"
      },
      {
        "name": "N. Aragon\u00e9s"
      },
      {
        "name": "R. Alaiz-Rodr\u00edguez"
      }
    ],
    "summary": "Background and objective\n  Risk prediction models aim at identifying people at higher risk of developing\na target disease. Feature selection is particularly important to improve the\nprediction model performance avoiding overfitting and to identify the leading\ncancer risk (and protective) factors. Assessing the stability of feature\nselection/ranking algorithms becomes an important issue when the aim is to\nanalyze the features with more prediction power. Methods\n  This work is focused on colorectal cancer, assessing several feature ranking\nalgorithms in terms of performance for a set of risk prediction models (Neural\nNetworks, Support Vector Machines (SVM), Logistic Regression, k-Nearest\nNeighbors and Boosted Trees). Additionally, their robustness is evaluated\nfollowing a conventional approach with scalar stability metrics and a visual\napproach proposed in this work to study both similarity among feature ranking\ntechniques as well as their individual stability. A comparative analysis is\ncarried out between the most relevant features found out in this study and\nfeatures provided by the experts according to the state-of-the-art knowledge.\nResults\n  The two best performance results in terms of Area Under the ROC Curve (AUC)\nare achieved with a SVM classifier using the top-41 features selected by the\nSVM wrapper approach (AUC=0.693) and Logistic Regression with the top-40\nfeatures selected by the Pearson (AUC=0.689). Experiments showed that\nperforming feature selection contributes to classification performance with a\n3.9% and 1.9% improvement in AUC for the SVM and Logistic Regression\nclassifier, respectively, with respect to the results using the full feature\nset. The visual approach proposed in this work allows to see that the Neural\nNetwork-based wrapper ranking is the most unstable while the Random Forest is\nthe most stable.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05293v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05293v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05293v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05291v1",
    "updated": "2024-02-07T22:10:36+00:00",
    "published": "2024-02-07T22:10:36+00:00",
    "title": "Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling",
    "authors": [
      {
        "name": "Maryam Rahnemoonfar"
      },
      {
        "name": "Younghyun Koo"
      }
    ],
    "summary": "Although the finite element approach of the Ice-sheet and Sea-level System\nModel (ISSM) solves ice dynamics problems governed by Stokes equations quickly\nand accurately, such numerical modeling requires intensive computation on\ncentral processing units (CPU). In this study, we develop graph neural networks\n(GNN) as fast surrogate models to preserve the finite element structure of\nISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG),\nwe train and test three GNNs: graph convolutional network (GCN), graph\nattention network (GAT), and equivariant graph convolutional network (EGCN).\nThese GNNs reproduce ice thickness and velocity with better accuracy than the\nclassic convolutional neural network (CNN) and multi-layer perception (MLP). In\nparticular, GNNs successfully capture the ice mass loss and acceleration\ninduced by higher basal melting rates in the PIG. When our GNN emulators are\nimplemented on graphic processing units (GPUs), they show up to 50 times faster\ncomputational time than the CPU-based ISSM simulation.",
    "comment": "12 pages, 7 figures, 3 tables, Submitted to Nature Communications on\n  Feb 7, 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05291v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05291v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05291v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05290v2",
    "updated": "2024-02-11T00:50:25+00:00",
    "published": "2024-02-07T22:09:46+00:00",
    "title": "Do Transformer World Models Give Better Policy Gradients?",
    "authors": [
      {
        "name": "Michel Ma"
      },
      {
        "name": "Tianwei Ni"
      },
      {
        "name": "Clement Gehring"
      },
      {
        "name": "Pierluca D'Oro"
      },
      {
        "name": "Pierre-Luc Bacon"
      }
    ],
    "summary": "A natural approach for reinforcement learning is to predict future rewards by\nunrolling a neural network world model, and to backpropagate through the\nresulting computational graph to learn a policy. However, this method often\nbecomes impractical for long horizons since typical world models induce\nhard-to-optimize loss landscapes. Transformers are known to efficiently\npropagate gradients over long horizons: could they be the solution to this\nproblem? Surprisingly, we show that commonly-used transformer world models\nproduce circuitous gradient paths, which can be detrimental to long-range\npolicy gradients. To tackle this challenge, we propose a class of world models\ncalled Actions World Models (AWMs), designed to provide more direct routes for\ngradient propagation. We integrate such AWMs into a policy gradient framework\nthat underscores the relationship between network architectures and the policy\ngradient updates they inherently represent. We demonstrate that AWMs can\ngenerate optimization landscapes that are easier to navigate even when compared\nto those from the simulator itself. This property allows transformer AWMs to\nproduce better policies than competitive baselines in realistic long-horizon\ntasks.",
    "comment": "Michel Ma and Pierluca D'Oro contributed equally",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05290v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05290v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05290v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05284v1",
    "updated": "2024-02-07T21:58:40+00:00",
    "published": "2024-02-07T21:58:40+00:00",
    "title": "Analyzing Adversarial Inputs in Deep Reinforcement Learning",
    "authors": [
      {
        "name": "Davide Corsi"
      },
      {
        "name": "Guy Amir"
      },
      {
        "name": "Guy Katz"
      },
      {
        "name": "Alessandro Farinelli"
      }
    ],
    "summary": "In recent years, Deep Reinforcement Learning (DRL) has become a popular\nparadigm in machine learning due to its successful applications to real-world\nand complex systems. However, even the state-of-the-art DRL models have been\nshown to suffer from reliability concerns -- for example, their susceptibility\nto adversarial inputs, i.e., small and abundant input perturbations that can\nfool the models into making unpredictable and potentially dangerous decisions.\nThis drawback limits the deployment of DRL systems in safety-critical contexts,\nwhere even a small error cannot be tolerated. In this work, we present a\ncomprehensive analysis of the characterization of adversarial inputs, through\nthe lens of formal verification. Specifically, we introduce a novel metric, the\nAdversarial Rate, to classify models based on their susceptibility to such\nperturbations, and present a set of tools and algorithms for its computation.\nOur analysis empirically demonstrates how adversarial inputs can affect the\nsafety of a given DRL system with respect to such perturbations. Moreover, we\nanalyze the behavior of these configurations to suggest several useful\npractices and guidelines to help mitigate the vulnerability of trained DRL\nnetworks.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05284v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05284v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05284v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05280v1",
    "updated": "2024-02-07T21:53:01+00:00",
    "published": "2024-02-07T21:53:01+00:00",
    "title": "No Dimensional Sampling Coresets for Classification",
    "authors": [
      {
        "name": "Meysam Alishahi"
      },
      {
        "name": "Jeff M. Phillips"
      }
    ],
    "summary": "We refine and generalize what is known about coresets for classification\nproblems via the sensitivity sampling framework. Such coresets seek the\nsmallest possible subsets of input data, so one can optimize a loss function on\nthe coreset and ensure approximation guarantees with respect to the original\ndata. Our analysis provides the first no dimensional coresets, so the size does\nnot depend on the dimension. Moreover, our results are general, apply for\ndistributional input and can use iid samples, so provide sample complexity\nbounds, and work for a variety of loss functions. A key tool we develop is a\nRadamacher complexity version of the main sensitivity sampling approach, which\ncan be of independent interest.",
    "comment": "47 Pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05280v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05280v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05280v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05279v1",
    "updated": "2024-02-07T21:49:51+00:00",
    "published": "2024-02-07T21:49:51+00:00",
    "title": "Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes",
    "authors": [
      {
        "name": "Will Lavanakul"
      },
      {
        "name": "Jason J. Choi"
      },
      {
        "name": "Koushil Sreenath"
      },
      {
        "name": "Claire J. Tomlin"
      }
    ],
    "summary": "Learning-based approaches are emerging as an effective approach for safety\nfilters for black-box dynamical systems. Existing methods have relied on\ncertificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi\n(HJ) reachability value functions. The primary motivation for our work is the\nrecognition that ultimately, enforcing the safety constraint as a control input\nconstraint at each state is what matters. By focusing on this constraint, we\ncan eliminate dependence on any specific certificate function-based design. To\nachieve this, we define a discriminating hyperplane that shapes the half-space\nconstraint on control input at each state, serving as a sufficient condition\nfor safety. This concept not only generalizes over traditional safety methods\nbut also simplifies safety filter design by eliminating dependence on specific\ncertificate functions. We present two strategies to learn the discriminating\nhyperplane: (a) a supervised learning approach, using pre-verified control\ninvariant sets for labeling, and (b) a reinforcement learning (RL) approach,\nwhich does not require such labels. The main advantage of our method, unlike\nconventional safe RL approaches, is the separation of performance and safety.\nThis offers a reusable safety filter for learning new tasks, avoiding the need\nto retrain from scratch. As such, we believe that the new notion of the\ndiscriminating hyperplane offers a more generalizable direction towards\ndesigning safety filters, encompassing and extending existing\ncertificate-function-based or safe RL methodologies.",
    "comment": "* indicate co-first authors. This is an extended version of the paper\n  submitted to L4DC 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05279v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05279v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05279v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05275v1",
    "updated": "2024-02-07T21:46:26+00:00",
    "published": "2024-02-07T21:46:26+00:00",
    "title": "Exploring Hierarchical Classification Performance for Time Series Data: Dissimilarity Measures and Classifier Comparisons",
    "authors": [
      {
        "name": "Celal Alagoz"
      }
    ],
    "summary": "The comparative performance of hierarchical classification (HC) and flat\nclassification (FC) methodologies in the realm of time series data analysis is\ninvestigated in this study. Dissimilarity measures, including Jensen-Shannon\nDistance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance\n(CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF,\nand SVM. A subset of datasets from the UCR archive, focusing on multi-class\ncases comprising more than two classes, is employed for analysis. A significant\ntrend is observed wherein HC demonstrates significant superiority over FC when\npaired with MINIROCKET utilizing TSD, diverging from conventional\nunderstandings. Conversely, FC exhibits consistent dominance across all\nconfigurations when employing alternative classifiers such as STSF and SVM.\nMoreover, TSD is found to consistently outperform both CBD and JSD across\nnearly all scenarios, except in instances involving the STSF classifier where\nCBD showcases superior performance. This discrepancy underscores the nuanced\nnature of dissimilarity measures and emphasizes the importance of their\ntailored selection based on the dataset and classifier employed. Valuable\ninsights into the dynamic interplay between classification methodologies and\ndissimilarity measures in the realm of time series data analysis are provided\nby these findings. By elucidating the performance variations across different\nconfigurations, a foundation is laid for refining classification methodologies\nand dissimilarity measures to optimize performance in diverse analytical\nscenarios. Furthermore, the need for continued research aimed at elucidating\nthe underlying mechanisms driving classification performance in time series\ndata analysis is underscored, with implications for enhancing predictive\nmodeling and decision-making in various domains.",
    "comment": "9 pages, 2 figures, 5th International Mediterranean Congress 1,\n  1367-1376",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "62H30",
      "I.5.2; I.5.3"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05275v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05275v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05275v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05274v1",
    "updated": "2024-02-07T21:43:57+00:00",
    "published": "2024-02-07T21:43:57+00:00",
    "title": "Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes",
    "authors": [
      {
        "name": "Isaac Grosof"
      },
      {
        "name": "Siva Theja Maguluri"
      },
      {
        "name": "R. Srikant"
      }
    ],
    "summary": "Infinite-state Markov Decision Processes (MDPs) are essential in modeling and\noptimizing a wide variety of engineering problems. In the reinforcement\nlearning (RL) context, a variety of algorithms have been developed to learn and\noptimize these MDPs. At the heart of many popular policy-gradient based\nlearning algorithms, such as natural actor-critic, TRPO, and PPO, lies the\nNatural Policy Gradient (NPG) algorithm. Convergence results for these RL\nalgorithms rest on convergence results for the NPG algorithm. However, all\nexisting results on the convergence of the NPG algorithm are limited to\nfinite-state settings.\n  We prove the first convergence rate bound for the NPG algorithm for\ninfinite-state average-reward MDPs, proving a $O(1/\\sqrt{T})$ convergence rate,\nif the NPG algorithm is initialized with a good initial policy. Moreover, we\nshow that in the context of a large class of queueing MDPs, the MaxWeight\npolicy suffices to satisfy our initial-policy requirement and achieve a\n$O(1/\\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds\non the relative value function achieved by the iterate policies of the NPG\nalgorithm.",
    "comment": "26 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05274v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05274v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05274v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05271v1",
    "updated": "2024-02-07T21:31:53+00:00",
    "published": "2024-02-07T21:31:53+00:00",
    "title": "Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks",
    "authors": [
      {
        "name": "Daniel Beaglehole"
      },
      {
        "name": "Ioannis Mitliagkas"
      },
      {
        "name": "Atish Agarwala"
      }
    ],
    "summary": "Understanding the mechanisms through which neural networks extract statistics\nfrom input-label pairs is one of the most important unsolved problems in\nsupervised learning. Prior works have identified that the gram matrices of the\nweights in trained neural networks of general architectures are proportional to\nthe average gradient outer product of the model, in a statement known as the\nNeural Feature Ansatz (NFA). However, the reason these quantities become\ncorrelated during training is poorly understood. In this work, we explain the\nemergence of this correlation. We identify that the NFA is equivalent to\nalignment between the left singular structure of the weight matrices and a\nsignificant component of the empirical neural tangent kernels associated with\nthose weights. We establish that the NFA introduced in prior works is driven by\na centered NFA that isolates this alignment. We show that the speed of NFA\ndevelopment can be predicted analytically at early training times in terms of\nsimple statistics of the inputs and labels. Finally, we introduce a simple\nintervention to increase NFA correlation at any given layer, which dramatically\nimproves the quality of features learned.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05271v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05271v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05271v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05264v1",
    "updated": "2024-02-07T21:19:05+00:00",
    "published": "2024-02-07T21:19:05+00:00",
    "title": "AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size",
    "authors": [
      {
        "name": "Petr Ostroukhov"
      },
      {
        "name": "Aigerim Zhumabayeva"
      },
      {
        "name": "Chulu Xiang"
      },
      {
        "name": "Alexander Gasnikov"
      },
      {
        "name": "Martin Tak\u00e1\u010d"
      },
      {
        "name": "Dmitry Kamzolov"
      }
    ],
    "summary": "This paper presents a novel adaptation of the Stochastic Gradient Descent\n(SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive\nstep size with an adjustable batch size. An increase in batch size and a\ndecrease in step size are well-known techniques to tighten the area of\nconvergence of SGD and decrease its variance. A range of studies by R. Byrd and\nJ. Nocedal introduced various testing techniques to assess the quality of\nmini-batch gradient approximations and choose the appropriate batch sizes at\nevery step. Methods that utilized exact tests were observed to converge within\n$O(LR^2/\\varepsilon)$ iterations. Conversely, inexact test implementations\nsometimes resulted in non-convergence and erratic performance. To address these\nchallenges, AdaBatchGrad incorporates both adaptive batch and step sizes,\nenhancing the method's robustness and stability. For exact tests, our approach\nconverges in $O(LR^2/\\varepsilon)$ iterations, analogous to standard gradient\ndescent. For inexact tests, it achieves convergence in $O(\\max\\lbrace\nLR^2/\\varepsilon, \\sigma^2 R^2/\\varepsilon^2 \\rbrace )$ iterations. This makes\nAdaBatchGrad markedly more robust and computationally efficient relative to\nprevailing methods. To substantiate the efficacy of our method, we\nexperimentally show, how the introduction of adaptive step size and adaptive\nbatch size gradually improves the performance of regular SGD. The results imply\nthat AdaBatchGrad surpasses alternative methods, especially when applied to\ninexact tests.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05264v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05264v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05264v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.07938v1",
    "updated": "2024-02-07T21:08:49+00:00",
    "published": "2024-02-07T21:08:49+00:00",
    "title": "Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs",
    "authors": [
      {
        "name": "Syed Mekael Wasti"
      },
      {
        "name": "Ken Q. Pu"
      },
      {
        "name": "Ali Neshati"
      }
    ],
    "summary": "The recent meteoric advancements in large language models have showcased a\nremarkable capacity for logical reasoning and comprehension. These newfound\ncapabilities have opened the door to a new generation of software, as has been\nmade obvious through the innumerable ways they are being applied in the\nindustry. This research focuses on harnessing and guiding the upgraded power of\nLLMs to construct a framework that can serve as an intermediary between a user\nand their user interface. By comprehending a user's needs through a thorough\nanalysis of natural textual inputs, an effectively crafted LLM engine can\nclassify the most likely available application, identify the desired UI\ncomponent and subsequently execute the user's expected actions. This\nintegration can evolve static UI systems into highly dynamic and adaptable\nsolutions, introducing a new frontier of intelligent and responsive user\nexperiences. Such a framework can fundamentally shift how users accomplish\ndaily tasks, skyrocket efficiency, and greatly reduce cognitive load.",
    "comment": "Submitted to peer review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7; I.2.1"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.07938v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.07938v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.07938v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05252v1",
    "updated": "2024-02-07T20:53:53+00:00",
    "published": "2024-02-07T20:53:53+00:00",
    "title": "Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages",
    "authors": [
      {
        "name": "My H. Dinh"
      },
      {
        "name": "James Kotary"
      },
      {
        "name": "Ferdinando Fioretto"
      }
    ],
    "summary": "Learning to Rank (LTR) is one of the most widely used machine learning\napplications. It is a key component in platforms with profound societal\nimpacts, including job search, healthcare information retrieval, and social\nmedia content feeds. Conventional LTR models have been shown to produce biases\nresults, stimulating a discourse on how to address the disparities introduced\nby ranking systems that solely prioritize user relevance. However, while\nseveral models of fair learning to rank have been proposed, they suffer from\ndeficiencies either in accuracy or efficiency, thus limiting their\napplicability to real-world ranking platforms. This paper shows how\nefficiently-solvable fair ranking models, based on the optimization of Ordered\nWeighted Average (OWA) functions, can be integrated into the training loop of\nan LTR model to achieve favorable balances between fairness, user utility, and\nruntime efficiency. In particular, this paper is the first to show how to\nbackpropagate through constrained optimizations of OWA objectives, enabling\ntheir use in integrated prediction and decision models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05252v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05252v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05252v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05234v1",
    "updated": "2024-02-07T20:14:22+00:00",
    "published": "2024-02-07T20:14:22+00:00",
    "title": "QGFN: Controllable Greediness with Action Values",
    "authors": [
      {
        "name": "Elaine Lau"
      },
      {
        "name": "Stephen Zhewen Lu"
      },
      {
        "name": "Ling Pan"
      },
      {
        "name": "Doina Precup"
      },
      {
        "name": "Emmanuel Bengio"
      }
    ],
    "summary": "Generative Flow Networks (GFlowNets; GFNs) are a family of\nreward/energy-based generative methods for combinatorial objects, capable of\ngenerating diverse and high-utility samples. However, biasing GFNs towards\nproducing high-utility samples is non-trivial. In this work, we leverage\nconnections between GFNs and reinforcement learning (RL) and propose to combine\nthe GFN policy with an action-value estimate, $Q$, to create greedier sampling\npolicies which can be controlled by a mixing parameter. We show that several\nvariants of the proposed method, QGFN, are able to improve on the number of\nhigh-reward samples generated in a variety of tasks without sacrificing\ndiversity.",
    "comment": "Under review",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05234v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05234v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05234v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05232v1",
    "updated": "2024-02-07T20:12:27+00:00",
    "published": "2024-02-07T20:12:27+00:00",
    "title": "Universal Neural Functionals",
    "authors": [
      {
        "name": "Allan Zhou"
      },
      {
        "name": "Chelsea Finn"
      },
      {
        "name": "James Harrison"
      }
    ],
    "summary": "A challenging problem in many modern machine learning tasks is to process\nweight-space features, i.e., to transform or extract information from the\nweights and gradients of a neural network. Recent works have developed\npromising weight-space models that are equivariant to the permutation\nsymmetries of simple feedforward networks. However, they are not applicable to\ngeneral architectures, since the permutation symmetries of a weight space can\nbe complicated by recurrence or residual connections. This work proposes an\nalgorithm that automatically constructs permutation equivariant models, which\nwe refer to as universal neural functionals (UNFs), for any weight space. Among\nother applications, we demonstrate how UNFs can be substituted into existing\nlearned optimizer designs, and find promising improvements over prior methods\nwhen optimizing small image classifiers and language models. Our results\nsuggest that learned optimizers can benefit from considering the (symmetry)\nstructure of the weight space they optimize. We open-source our library for\nconstructing UNFs at\nhttps://github.com/AllanYangZhou/universal_neural_functional.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05232v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05232v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05232v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05224v1",
    "updated": "2024-02-07T20:02:09+00:00",
    "published": "2024-02-07T20:02:09+00:00",
    "title": "VerAs: Verify then Assess STEM Lab Reports",
    "authors": [
      {
        "name": "Berk Atil"
      },
      {
        "name": "Mahsa Sheikhi Karizaki"
      },
      {
        "name": "Rebecca J. Passonneau"
      }
    ],
    "summary": "With an increasing focus in STEM education on critical thinking skills,\nscience writing plays an ever more important role in curricula that stress\ninquiry skills. A recently published dataset of two sets of college level lab\nreports from an inquiry-based physics curriculum relies on analytic assessment\nrubrics that utilize multiple dimensions, specifying subject matter knowledge\nand general components of good explanations. Each analytic dimension is\nassessed on a 6-point scale, to provide detailed feedback to students that can\nhelp them improve their science writing skills. Manual assessment can be slow,\nand difficult to calibrate for consistency across all students in large\nclasses. While much work exists on automated assessment of open-ended questions\nin STEM subjects, there has been far less work on long-form writing such as lab\nreports. We present an end-to-end neural architecture that has separate\nverifier and assessment modules, inspired by approaches to Open Domain Question\nAnswering (OpenQA). VerAs first verifies whether a report contains any content\nrelevant to a given rubric dimension, and if so, assesses the relevant\nsentences. On the lab reports, VerAs outperforms multiple baselines based on\nOpenQA systems or Automated Essay Scoring (AES). VerAs also performs well on an\nanalytic rubric for middle school physics essays.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05224v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05224v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05224v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06678v1",
    "updated": "2024-02-07T19:56:35+00:00",
    "published": "2024-02-07T19:56:35+00:00",
    "title": "Can machine learning predict citizen-reported angler behavior?",
    "authors": [
      {
        "name": "Julia S. Schmid"
      },
      {
        "name": "Sean Simmons"
      },
      {
        "name": "Mark A. Lewis"
      },
      {
        "name": "Mark S. Poesch"
      },
      {
        "name": "Pouria Ramazi"
      }
    ],
    "summary": "Prediction of angler behaviors, such as catch rates and angler pressure, is\nessential to maintaining fish populations and ensuring angler satisfaction.\nAngler behavior can partly be tracked by online platforms and mobile phone\napplications that provide fishing activities reported by recreational anglers.\nMoreover, angler behavior is known to be driven by local site attributes. Here,\nthe prediction of citizen-reported angler behavior was investigated by\nmachine-learning methods using auxiliary data on the environment,\nsocioeconomics, fisheries management objectives, and events at a freshwater\nbody. The goal was to determine whether auxiliary data alone could predict the\nreported behavior. Different spatial and temporal extents and temporal\nresolutions were considered. Accuracy scores averaged 88% for monthly\npredictions at single water bodies and 86% for spatial predictions on a day in\na specific region across Canada. At other resolutions and scales, the models\nonly achieved low prediction accuracy of around 60%. The study represents a\nfirst attempt at predicting angler behavior in time and space at a large scale\nand establishes a foundation for potential future expansions in various\ndirections.",
    "comment": "36 pages, 10 figures, 4 tables (including supplementary information)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.soc-ph",
    "categories": [
      "physics.soc-ph",
      "cs.LG",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06678v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06678v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06678v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05220v1",
    "updated": "2024-02-07T19:52:35+00:00",
    "published": "2024-02-07T19:52:35+00:00",
    "title": "On Parameter Estimation in Deviated Gaussian Mixture of Experts",
    "authors": [
      {
        "name": "Huy Nguyen"
      },
      {
        "name": "Khai Nguyen"
      },
      {
        "name": "Nhat Ho"
      }
    ],
    "summary": "We consider the parameter estimation problem in the deviated Gaussian mixture\nof experts in which the data are generated from $(1 - \\lambda^{\\ast}) g_0(Y|\nX)+ \\lambda^{\\ast} \\sum_{i = 1}^{k_{\\ast}} p_{i}^{\\ast}\nf(Y|(a_{i}^{\\ast})^{\\top}X+b_i^{\\ast},\\sigma_{i}^{\\ast})$, where $X, Y$ are\nrespectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a\nknown function, $\\lambda^{\\ast} \\in [0, 1]$ is true but unknown mixing\nproportion, and $(p_{i}^{\\ast}, a_{i}^{\\ast}, b_{i}^{\\ast}, \\sigma_{i}^{\\ast})$\nfor $1 \\leq i \\leq k^{\\ast}$ are unknown parameters of the Gaussian mixture of\nexperts. This problem arises from the goodness-of-fit test when we would like\nto test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or\nthey are generated from the whole mixture (alternative hypothesis). Based on\nthe algebraic structure of the expert functions and the distinguishability\nbetween $g_0$ and the mixture part, we construct novel Voronoi-based loss\nfunctions to capture the convergence rates of maximum likelihood estimation\n(MLE) for our models. We further demonstrate that our proposed loss functions\ncharacterize the local convergence rates of parameter estimation more\naccurately than the generalized Wasserstein, a loss function being commonly\nused for estimating parameters in the Gaussian mixture of experts.",
    "comment": "34 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05220v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05220v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05220v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05218v1",
    "updated": "2024-02-07T19:51:13+00:00",
    "published": "2024-02-07T19:51:13+00:00",
    "title": "Self-calibrated convolution towards glioma segmentation",
    "authors": [
      {
        "name": "Felipe C. R. Salvagnini"
      },
      {
        "name": "Gerson O. Barbosa"
      },
      {
        "name": "Alexandre X. Falcao"
      },
      {
        "name": "Cid A. N. Santos"
      }
    ],
    "summary": "Accurate brain tumor segmentation in the early stages of the disease is\ncrucial for the treatment's effectiveness, avoiding exhaustive visual\ninspection of a qualified specialist on 3D MR brain images of multiple\nprotocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma\nsegmentation, being nnU-Net one of the best. In this work, we evaluate\nself-calibrated convolutions in different parts of the nnU-Net network to\ndemonstrate that self-calibrated modules in skip connections can significantly\nimprove the enhanced-tumor and tumor-core segmentation accuracy while\npreserving the wholetumor segmentation accuracy.",
    "comment": null,
    "journal_ref": null,
    "doi": "10.1109/SIPAIM56729.2023.10373517",
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1109/SIPAIM56729.2023.10373517",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05218v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05218v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05218v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05210v2",
    "updated": "2024-02-09T21:40:29+00:00",
    "published": "2024-02-07T19:35:09+00:00",
    "title": "Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models",
    "authors": [
      {
        "name": "Nicholas Konz"
      },
      {
        "name": "Yuwen Chen"
      },
      {
        "name": "Haoyu Dong"
      },
      {
        "name": "Maciej A. Mazurowski"
      }
    ],
    "summary": "Diffusion models have enabled remarkably high-quality medical image\ngeneration, which can help mitigate the expenses of acquiring and annotating\nnew images by supplementing small or imbalanced datasets, along with other\napplications. However, these are hampered by the challenge of enforcing global\nanatomical realism in generated images. To this end, we propose a diffusion\nmodel for anatomically-controlled medical image generation. Our model follows a\nmulti-class anatomical segmentation mask at each sampling step and incorporates\na \\textit{random mask ablation} training algorithm, to enable conditioning on a\nselected combination of anatomical constraints while allowing flexibility in\nother anatomical areas. This also improves the network's learning of anatomical\nrealism for the completely unconditional (unconstrained generation) case.\nComparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets\ndemonstrates superior anatomical realism and input mask faithfulness over\nstate-of-the-art models. We also offer an accessible codebase and release a\ndataset of generated paired breast MRIs. Our approach facilitates diverse\napplications, including pre-registered image generation, counterfactual\nscenarios, and others.",
    "comment": "Code and synthetic dataset:\n  https://github.com/mazurowski-lab/segmentation-guided-diffusion",
    "journal_ref": null,
    "doi": null,
    "primary_category": "eess.IV",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05210v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05210v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05210v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05203v2",
    "updated": "2024-02-09T16:47:02+00:00",
    "published": "2024-02-07T19:15:33+00:00",
    "title": "Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series",
    "authors": [
      {
        "name": "Zitong Yang"
      },
      {
        "name": "Emmanuel Cand\u00e8s"
      },
      {
        "name": "Lihua Lei"
      }
    ],
    "summary": "We introduce Bellman Conformal Inference (BCI), a framework that wraps around\nany time series forecasting models and provides approximately calibrated\nprediction intervals. Unlike existing methods, BCI is able to leverage\nmulti-step ahead forecasts and explicitly optimize the average interval lengths\nby solving a one-dimensional stochastic control problem (SCP) at each time\nstep. In particular, we use the dynamic programming algorithm to find the\noptimal policy for the SCP. We prove that BCI achieves long-term coverage under\narbitrary distribution shifts and temporal dependence, even with poor\nmulti-step ahead forecasts. We find empirically that BCI avoids uninformative\nintervals that have infinite lengths and generates substantially shorter\nprediction intervals in multiple applications when compared with existing\nmethods.",
    "comment": "17 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05203v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05203v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05203v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05201v1",
    "updated": "2024-02-07T19:11:23+00:00",
    "published": "2024-02-07T19:11:23+00:00",
    "title": "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
    "authors": [
      {
        "name": "Matthew Renze"
      },
      {
        "name": "Erhan Guven"
      }
    ],
    "summary": "In this research study, we empirically investigate the effect of sampling\ntemperature on the performance of Large Language Models (LLMs) on various\nproblem-solving tasks. We created a multiple-choice question-and-answer (MCQA)\nexam by randomly sampling problems from standard LLM benchmarks. Then, we used\nfour popular LLMs with five prompt-engineering techniques to solve the MCQA\nproblems while increasing the sampling temperature from 0.0 to 1.0. Despite\nanecdotal reports to the contrary, our empirical results indicate that changes\nin temperature in the range 0.0 to 1.0 do not have a statistically significant\nimpact on LLM performance for problem-solving tasks. In addition, these results\nappear to hold regardless of the LLM, the prompt-engineering technique, or the\nproblem domain. All code, data, and supplemental materials are available on\nGitHub at: https://github.com/matthewrenze/jhu-llm-temperature.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05201v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05201v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05201v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05200v1",
    "updated": "2024-02-07T19:10:36+00:00",
    "published": "2024-02-07T19:10:36+00:00",
    "title": "Are LLMs Ready for Real-World Materials Discovery?",
    "authors": [
      {
        "name": "Santiago Miret"
      },
      {
        "name": "N M Anoop Krishnan"
      }
    ],
    "summary": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cond-mat.mtrl-sci",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05200v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05200v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05200v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05193v1",
    "updated": "2024-02-07T19:05:27+00:00",
    "published": "2024-02-07T19:05:27+00:00",
    "title": "JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows",
    "authors": [
      {
        "name": "Deniz A. Bezgin"
      },
      {
        "name": "Aaron B. Buhendwa"
      },
      {
        "name": "Nikolaus A. Adams"
      }
    ],
    "summary": "In our effort to facilitate machine learning-assisted computational fluid\ndynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is\na Python-based fully-differentiable CFD solver designed for compressible\nsingle- and two-phase flows. In this work, the first version is extended to\nincorporate high-performance computing (HPC) capabilities. We introduce a\nparallelization strategy utilizing JAX primitive operations that scales\nefficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024\nTPU v3 cores) HPC systems. We further demonstrate the stable parallel\ncomputation of automatic differentiation gradients across extended integration\ntrajectories. The new code version offers enhanced two-phase flow modeling\ncapabilities. In particular, a five-equation diffuse-interface model is\nincorporated which complements the level-set sharp-interface model. Additional\nalgorithmic improvements include positivity-preserving limiters for increased\nrobustness, support for stretched Cartesian meshes, refactored I/O handling,\ncomprehensive post-processing routines, and an updated list of state-of-the-art\nhigh-order numerical discretization schemes. We verify newly added numerical\nmodels by showcasing simulation results for single- and two-phase flows,\nincluding turbulent boundary layer and channel flows, air-helium shock bubble\ninteractions, and air-water shock drop interactions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.flu-dyn",
    "categories": [
      "physics.flu-dyn",
      "cs.CE",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05193v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05193v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05193v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05188v1",
    "updated": "2024-02-07T19:01:11+00:00",
    "published": "2024-02-07T19:01:11+00:00",
    "title": "InCoRo: In-Context Learning for Robotics Control with Feedback Loops",
    "authors": [
      {
        "name": "Jiaqiang Ye Zhu"
      },
      {
        "name": "Carla Gomez Cano"
      },
      {
        "name": "David Vazquez Bermudez"
      },
      {
        "name": "Michal Drozdzal"
      }
    ],
    "summary": "One of the challenges in robotics is to enable robotic units with the\nreasoning capability that would be robust enough to execute complex tasks in\ndynamic environments. Recent advances in LLMs have positioned them as go-to\ntools for simple reasoning tasks, motivating the pioneering work of Liang et\nal. [35] that uses an LLM to translate natural language commands into low-level\nstatic execution plans for robotic units. Using LLMs inside robotics systems\nbrings their generalization to a new level, enabling zero-shot generalization\nto new tasks. This paper extends this prior work to dynamic environments. We\npropose InCoRo, a system that uses a classical robotic feedback loop composed\nof an LLM controller, a scene understanding unit, and a robot. Our system\ncontinuously analyzes the state of the environment and provides adapted\nexecution commands, enabling the robot to adjust to changing environmental\nconditions and correcting for controller errors. Our system does not require\nany iterative optimization to learn to accomplish a task as it leverages\nin-context learning with an off-the-shelf LLM model. Through an extensive\nvalidation process involving two standardized industrial robotic units -- SCARA\nand DELTA types -- we contribute knowledge about these robots, not popular in\nthe community, thereby enriching it. We highlight the generalization\ncapabilities of our system and show that (1) in-context learning in combination\nwith the current state-of-the-art LLMs is an effective way to implement a\nrobotic controller; (2) in static environments, InCoRo surpasses the prior art\nin terms of the success rate; (3) in dynamic environments, we establish new\nstate-of-the-art for the SCARA and DELTA units, respectively. This research\npaves the way towards building reliable, efficient, intelligent autonomous\nsystems that adapt to dynamic environments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05188v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05188v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05188v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05187v1",
    "updated": "2024-02-07T19:01:06+00:00",
    "published": "2024-02-07T19:01:06+00:00",
    "title": "Meta-learning the mirror map in policy mirror descent",
    "authors": [
      {
        "name": "Carlo Alfano"
      },
      {
        "name": "Sebastian Towers"
      },
      {
        "name": "Silvia Sapora"
      },
      {
        "name": "Chris Lu"
      },
      {
        "name": "Patrick Rebeschini"
      }
    ],
    "summary": "Policy Mirror Descent (PMD) is a popular framework in reinforcement learning,\nserving as a unifying perspective that encompasses numerous algorithms. These\nalgorithms are derived through the selection of a mirror map and enjoy\nfinite-time convergence guarantees. Despite its popularity, the exploration of\nPMD's full potential is limited, with the majority of research focusing on a\nparticular mirror map -- namely, the negative entropy -- which gives rise to\nthe renowned Natural Policy Gradient (NPG) method. It remains uncertain from\nexisting theoretical studies whether the choice of mirror map significantly\ninfluences PMD's efficacy. In our work, we conduct empirical investigations to\nshow that the conventional mirror map choice (NPG) often yields\nless-than-optimal outcomes across several standard benchmark environments. By\napplying a meta-learning approach, we identify more efficient mirror maps that\nenhance performance, both on average and in terms of best performance achieved\nalong the training trajectory. We analyze the characteristics of these learned\nmirror maps and reveal shared traits among certain settings. Our results\nsuggest that mirror maps have the potential to be adaptable across various\nenvironments, raising questions about how to best match a mirror map to an\nenvironment's structure and characteristics.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05187v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05187v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05187v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05176v1",
    "updated": "2024-02-07T19:00:02+00:00",
    "published": "2024-02-07T19:00:02+00:00",
    "title": "cecilia: A Machine Learning-Based Pipeline for Measuring Metal Abundances of Helium-rich Polluted White Dwarfs",
    "authors": [
      {
        "name": "M. Badenas-Agusti"
      },
      {
        "name": "J. Via\u00f1a"
      },
      {
        "name": "A. Vanderburg"
      },
      {
        "name": "S. Blouin"
      },
      {
        "name": "P. Dufour"
      },
      {
        "name": "S. Xu"
      },
      {
        "name": "L. Sha"
      }
    ],
    "summary": "Over the past several decades, conventional spectral analysis techniques of\npolluted white dwarfs have become powerful tools to learn about the geology and\nchemistry of extrasolar bodies. Despite their proven capabilities and extensive\nlegacy of scientific discoveries, these techniques are however still limited by\ntheir manual, time-intensive, and iterative nature. As a result, they are\nsusceptible to human errors and are difficult to scale up to population-wide\nstudies of metal pollution. This paper seeks to address this problem by\npresenting cecilia, the first Machine Learning (ML)-powered spectral modeling\ncode designed to measure the metal abundances of intermediate-temperature\n(10,000$\\leq T_{\\rm eff} \\leq$20,000 K), Helium-rich polluted white dwarfs.\nTrained with more than 22,000 randomly drawn atmosphere models and stellar\nparameters, our pipeline aims to overcome the limitations of classical methods\nby replacing the generation of synthetic spectra from computationally expensive\ncodes and uniformly spaced model grids, with a fast, automated, and efficient\nneural-network-based interpolator. More specifically, cecilia combines\nstate-of-the-art atmosphere models, powerful artificial intelligence tools, and\nrobust statistical techniques to rapidly generate synthetic spectra of polluted\nwhite dwarfs in high-dimensional space, and enable accurate ($\\lesssim$0.1 dex)\nand simultaneous measurements of 14 stellar parameters -- including 11\nelemental abundances -- from real spectroscopic observations. As massively\nmultiplexed astronomical surveys begin scientific operations, cecilia's\nperformance has the potential to unlock large-scale studies of extrasolar\ngeochemistry and propel the field of white dwarf science into the era of Big\nData. In doing so, we aspire to uncover new statistical insights that were\npreviously impractical with traditional white dwarf characterisation\ntechniques.",
    "comment": "28 pages, 16 figures, 5 tables. Accepted for publication in MNRAS",
    "journal_ref": null,
    "doi": null,
    "primary_category": "astro-ph.IM",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "astro-ph.SR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05176v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05176v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05176v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05173v1",
    "updated": "2024-02-07T19:00:01+00:00",
    "published": "2024-02-07T19:00:01+00:00",
    "title": "Towards Understanding Inductive Bias in Transformers: A View From Infinity",
    "authors": [
      {
        "name": "Itay Lavie"
      },
      {
        "name": "Guy Gur-Ari"
      },
      {
        "name": "Zohar Ringel"
      }
    ],
    "summary": "We study inductive bias in Transformers in the infinitely over-parameterized\nGaussian process limit and argue transformers tend to be biased towards more\npermutation symmetric functions in sequence space. We show that the\nrepresentation theory of the symmetric group can be used to give quantitative\nanalytical predictions when the dataset is symmetric to permutations between\ntokens. We present a simplified transformer block and solve the model at the\nlimit, including accurate predictions for the learning curves and network\noutputs. We show that in common setups, one can derive tight bounds in the form\nof a scaling law for the learnability as a function of the context length.\nFinally, we argue WikiText dataset, does indeed possess a degree of permutation\nsymmetry.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05173v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05173v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05173v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05111v1",
    "updated": "2024-02-07T18:59:31+00:00",
    "published": "2024-02-07T18:59:31+00:00",
    "title": "Edu-ConvoKit: An Open-Source Library for Education Conversation Data",
    "authors": [
      {
        "name": "Rose E. Wang"
      },
      {
        "name": "Dorottya Demszky"
      }
    ],
    "summary": "We introduce Edu-ConvoKit, an open-source library designed to handle\npre-processing, annotation and analysis of conversation data in education.\nResources for analyzing education conversation data are scarce, making the\nresearch challenging to perform and therefore hard to access. We address these\nchallenges with Edu-ConvoKit. Edu-ConvoKit is open-source\n(https://github.com/stanfordnlp/edu-convokit ), pip-installable\n(https://pypi.org/project/edu-convokit/ ), with comprehensive documentation\n(https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available\nat: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional\nresources, such as Colab applications of Edu-ConvoKit to three diverse\neducation datasets and a repository of Edu-ConvoKit related papers, that can be\nfound in our GitHub repository.",
    "comment": "https://github.com/stanfordnlp/edu-convokit\n  https://edu-convokit.readthedocs.io/en/latest/",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05111v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05111v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05111v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05110v1",
    "updated": "2024-02-07T18:59:12+00:00",
    "published": "2024-02-07T18:59:12+00:00",
    "title": "Opening the AI black box: program synthesis via mechanistic interpretability",
    "authors": [
      {
        "name": "Eric J. Michaud"
      },
      {
        "name": "Isaac Liao"
      },
      {
        "name": "Vedang Lad"
      },
      {
        "name": "Ziming Liu"
      },
      {
        "name": "Anish Mudide"
      },
      {
        "name": "Chloe Loughridge"
      },
      {
        "name": "Zifan Carl Guo"
      },
      {
        "name": "Tara Rezaei Kheirkhah"
      },
      {
        "name": "Mateja Vukeli\u0107"
      },
      {
        "name": "Max Tegmark"
      }
    ],
    "summary": "We present MIPS, a novel method for program synthesis based on automated\nmechanistic interpretability of neural networks trained to perform the desired\ntask, auto-distilling the learned algorithm into Python code. We test MIPS on a\nbenchmark of 62 algorithmic tasks that can be learned by an RNN and find it\nhighly complementary to GPT-4: MIPS solves 32 of them, including 13 that are\nnot solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to\nconvert the RNN into a finite state machine, then applies Boolean or integer\nsymbolic regression to capture the learned algorithm. As opposed to large\nlanguage models, this program synthesis technique makes no use of (and is\ntherefore not limited by) human training data such as algorithms and code from\nGitHub. We discuss opportunities and challenges for scaling up this approach to\nmake machine-learned models more interpretable and trustworthy.",
    "comment": "24 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05110v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05110v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05110v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05109v1",
    "updated": "2024-02-07T18:58:50+00:00",
    "published": "2024-02-07T18:58:50+00:00",
    "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
    "authors": [
      {
        "name": "Zachary Ankner"
      },
      {
        "name": "Rishab Parthasarathy"
      },
      {
        "name": "Aniruddha Nrusimha"
      },
      {
        "name": "Christopher Rinard"
      },
      {
        "name": "Jonathan Ragan-Kelley"
      },
      {
        "name": "William Brandon"
      }
    ],
    "summary": "To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding framework. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence, that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of light-weight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads, a sequentially dependent,\ndrop-in replacement for standard draft heads that significantly improves\nspeculation accuracy. Decoding with Hydra heads improves throughput compared to\nMedusa decoding with standard draft heads. We further explore the design space\nof Hydra head training objectives and architectures, and propose a\ncarefully-tuned Hydra head recipe, which we call Hydra++, that improves\ndecoding throughput by 1.31x and 2.71x compared to Medusa decoding and\nautoregressive decoding, respectively. Overall, Hydra heads are a simple\nintervention on standard draft heads that significantly improve the end-to-end\nspeed of draft head based speculative decoding.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05109v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05109v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05109v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05164v1",
    "updated": "2024-02-07T18:58:18+00:00",
    "published": "2024-02-07T18:58:18+00:00",
    "title": "A Resource Model For Neural Scaling Law",
    "authors": [
      {
        "name": "Jinyeop Song"
      },
      {
        "name": "Ziming Liu"
      },
      {
        "name": "Max Tegmark"
      },
      {
        "name": "Jeff Gore"
      }
    ],
    "summary": "Neural scaling laws characterize how model performance improves as the model\nsize scales up. Inspired by empirical observations, we introduce a resource\nmodel of neural scaling. A task is usually composite hence can be decomposed\ninto many subtasks, which compete for resources (measured by the number of\nneurons allocated to subtasks). On toy problems, we empirically find that: (1)\nThe loss of a subtask is inversely proportional to its allocated neurons. (2)\nWhen multiple subtasks are present in a composite task, the resources acquired\nby each subtask uniformly grow as models get larger, keeping the ratios of\nacquired resources constants. We hypothesize these findings to be generally\ntrue and build a model to predict neural scaling laws for general composite\ntasks, which successfully replicates the neural scaling law of Chinchilla\nmodels reported in arXiv:2203.15556. We believe that the notion of resource\nused in this paper will be a useful tool for characterizing and diagnosing\nneural networks.",
    "comment": "10 pages, 8 figures, Under review as a workshop paper at ICLR 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05164v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05164v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05164v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05106v1",
    "updated": "2024-02-07T18:57:37+00:00",
    "published": "2024-02-07T18:57:37+00:00",
    "title": "Image captioning for Brazilian Portuguese using GRIT model",
    "authors": [
      {
        "name": "Rafael Silva de Alencar"
      },
      {
        "name": "William Alberto Cruz Casta\u00f1eda"
      },
      {
        "name": "Marcellus Amadeus"
      }
    ],
    "summary": "This work presents the early development of a model of image captioning for\nthe Brazilian Portuguese language. We used the GRIT (Grid - and Region-based\nImage captioning Transformer) model to accomplish this work. GRIT is a\nTransformer-only neural architecture that effectively utilizes two visual\nfeatures to generate better captions. The GRIT method emerged as a proposal to\nbe a more efficient way to generate image captioning. In this work, we adapt\nthe GRIT model to be trained in a Brazilian Portuguese dataset to have an image\ncaptioning method for the Brazilian Portuguese Language.",
    "comment": "arXiv admin note: text overlap with arXiv:2207.09666 by other authors",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05106v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05106v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05106v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05101v1",
    "updated": "2024-02-07T18:55:22+00:00",
    "published": "2024-02-07T18:55:22+00:00",
    "title": "Tighter Generalisation Bounds via Interpolation",
    "authors": [
      {
        "name": "Paul Viallard"
      },
      {
        "name": "Maxime Haddouche"
      },
      {
        "name": "Umut \u015eim\u015fekli"
      },
      {
        "name": "Benjamin Guedj"
      }
    ],
    "summary": "This paper contains a recipe for deriving new PAC-Bayes generalisation bounds\nbased on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes\ngeneralisation bounds where we interpolate between a series of probability\ndivergences (including but not limited to KL, Wasserstein, and total\nvariation), making the best out of many worlds depending on the posterior\ndistributions properties. We explore the tightness of these bounds and connect\nthem to earlier results from statistical learning, which are specific cases. We\nalso instantiate our bounds as training objectives, yielding non-trivial\nguarantees and practical performances.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05101v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05101v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05101v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05099v1",
    "updated": "2024-02-07T18:53:01+00:00",
    "published": "2024-02-07T18:53:01+00:00",
    "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
    "authors": [
      {
        "name": "Jordan Juravsky"
      },
      {
        "name": "Bradley Brown"
      },
      {
        "name": "Ryan Ehrlich"
      },
      {
        "name": "Daniel Y. Fu"
      },
      {
        "name": "Christopher R\u00e9"
      },
      {
        "name": "Azalia Mirhoseini"
      }
    ],
    "summary": "Transformer-based large language models (LLMs) are now deployed to hundreds\nof millions of users. LLM inference is commonly performed on batches of\nsequences that share a prefix, such as few-shot examples or a chatbot system\nprompt. Decoding in this large-batch setting can be bottlenecked by the\nattention operation, which reads large key-value (KV) caches from memory and\ncomputes inefficient matrix-vector products for every sequence in the batch. In\nthis work, we introduce Hydragen, a hardware-aware exact implementation of\nattention with shared prefixes. Hydragen computes attention over the shared\nprefix and unique suffixes separately. This decomposition enables efficient\nprefix attention by batching queries together across sequences, reducing\nredundant memory reads and enabling the use of hardware-friendly matrix\nmultiplications. Our method can improve end-to-end LLM throughput by up to 32x\nagainst competitive baselines, with speedup growing with the batch size and\nshared prefix length. Hydragen also enables the use of very long shared\ncontexts: with a high batch size, increasing the prefix length from 1K to 16K\ntokens decreases Hydragen throughput by less than 15%, while the throughput of\nbaselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix\ndecomposition and can be applied to tree-based prompt sharing patterns,\nallowing us to further reduce inference time on competitive programming\nproblems by 55%.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05099v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05099v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05099v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05098v2",
    "updated": "2024-02-13T16:32:09+00:00",
    "published": "2024-02-07T18:51:49+00:00",
    "title": "On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling",
    "authors": [
      {
        "name": "Marcin Sendera"
      },
      {
        "name": "Minsu Kim"
      },
      {
        "name": "Sarthak Mittal"
      },
      {
        "name": "Pablo Lemos"
      },
      {
        "name": "Luca Scimeca"
      },
      {
        "name": "Jarrid Rector-Brooks"
      },
      {
        "name": "Alexandre Adam"
      },
      {
        "name": "Yoshua Bengio"
      },
      {
        "name": "Nikolay Malkin"
      }
    ],
    "summary": "We study the problem of training diffusion models to sample from a\ndistribution with a given unnormalized density or energy function. We benchmark\nseveral diffusion-structured inference methods, including simulation-based\nvariational approaches and off-policy methods (continuous generative flow\nnetworks). Our results shed light on the relative advantages of existing\nalgorithms while bringing into question some claims from past work. We also\npropose a novel exploration strategy for off-policy methods, based on local\nsearch in the target space with the use of a replay buffer, and show that it\nimproves the quality of samples on a variety of target distributions. Our code\nfor the sampling methods and benchmarks studied is made public at\nhttps://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion\nmodels for amortized inference.",
    "comment": "21 pages; code: https://github.com/GFNOrg/gfn-diffusion",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05098v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05098v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05098v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05162v1",
    "updated": "2024-02-07T18:34:38+00:00",
    "published": "2024-02-07T18:34:38+00:00",
    "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
    "authors": [
      {
        "name": "Boyi Wei"
      },
      {
        "name": "Kaixuan Huang"
      },
      {
        "name": "Yangsibo Huang"
      },
      {
        "name": "Tinghao Xie"
      },
      {
        "name": "Xiangyu Qi"
      },
      {
        "name": "Mengzhou Xia"
      },
      {
        "name": "Prateek Mittal"
      },
      {
        "name": "Mengdi Wang"
      },
      {
        "name": "Peter Henderson"
      }
    ],
    "summary": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.",
    "comment": "22 pages, 9 figures. Project page is available at\n  https://boyiwei.com/alignment-attribution/",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05162v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05162v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05162v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05073v1",
    "updated": "2024-02-07T18:27:29+00:00",
    "published": "2024-02-07T18:27:29+00:00",
    "title": "NITO: Neural Implicit Fields for Resolution-free Topology Optimization",
    "authors": [
      {
        "name": "Amin Heyrani Nobari"
      },
      {
        "name": "Giorgio Giannone"
      },
      {
        "name": "Lyle Regenwetter"
      },
      {
        "name": "Faez Ahmed"
      }
    ],
    "summary": "Topology optimization is a critical task in engineering design, where the\ngoal is to optimally distribute material in a given space for maximum\nperformance. We introduce Neural Implicit Topology Optimization (NITO), a novel\napproach to accelerate topology optimization problems using deep learning. NITO\nstands out as one of the first frameworks to offer a resolution-free and\ndomain-agnostic solution in deep learning-based topology optimization. NITO\nsynthesizes structures with up to seven times better structural efficiency\ncompared to SOTA diffusion models and does so in a tenth of the time. In the\nNITO framework, we introduce a novel method, the Boundary Point Order-Invariant\nMLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic\nmanner, moving away from expensive simulation-based approaches. Crucially, NITO\ncircumvents the domain and resolution limitations that restrict Convolutional\nNeural Network (CNN) models to a structured domain of fixed size -- limitations\nthat hinder the widespread adoption of CNNs in engineering applications. This\ngeneralizability allows a single NITO model to train and generate solutions in\ncountless domains, eliminating the need for numerous domain-specific CNNs and\ntheir extensive datasets. Despite its generalizability, NITO outperforms SOTA\nmodels even in specialized tasks, is an order of magnitude smaller, and is\npractically trainable at high resolutions that would be restrictive for CNNs.\nThis combination of versatility, efficiency, and performance underlines NITO's\npotential to transform the landscape of engineering design optimization\nproblems through implicit fields.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05073v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05073v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05073v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05071v1",
    "updated": "2024-02-07T18:22:41+00:00",
    "published": "2024-02-07T18:22:41+00:00",
    "title": "Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity",
    "authors": [
      {
        "name": "Ahmet Alacaoglu"
      },
      {
        "name": "Donghwan Kim"
      },
      {
        "name": "Stephen J. Wright"
      }
    ],
    "summary": "We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems\neither satisfying $\\rho$-cohypomonotonicity or admitting a solution to the\n$\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the\nparameter $\\rho>0$ correspond to a greater degree of nonconvexity. These\nproblem classes include examples in two player reinforcement learning,\ninteraction dominant min-max problems, and certain synthetic test problems on\nwhich classical min-max algorithms fail. It has been conjectured that\nfirst-order methods can tolerate value of $\\rho$ no larger than $\\frac{1}{L}$,\nbut existing results in the literature have stagnated at the tighter\nrequirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or\nbest-known complexity guarantees with cohypomonotonicity or weak MVI conditions\nfor $\\rho < \\frac{1}{L}$. The algorithms we analyze are inexact variants of\nHalpern and Krasnosel'ski\\u{\\i}-Mann (KM) iterations. We also provide\nalgorithms and complexity guarantees in the stochastic case with the same range\non $\\rho$. Our main insight for the improvements in the convergence analyses is\nto harness the recently proposed \"conic nonexpansiveness\" property of\noperators. As byproducts, we provide a refined analysis for inexact Halpern\niteration and propose a stochastic KM iteration with a multilevel Monte Carlo\nestimator.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05071v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05071v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05071v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05070v1",
    "updated": "2024-02-07T18:21:17+00:00",
    "published": "2024-02-07T18:21:17+00:00",
    "title": "A Roadmap to Pluralistic Alignment",
    "authors": [
      {
        "name": "Taylor Sorensen"
      },
      {
        "name": "Jared Moore"
      },
      {
        "name": "Jillian Fisher"
      },
      {
        "name": "Mitchell Gordon"
      },
      {
        "name": "Niloofar Mireshghallah"
      },
      {
        "name": "Christopher Michael Rytting"
      },
      {
        "name": "Andre Ye"
      },
      {
        "name": "Liwei Jiang"
      },
      {
        "name": "Ximing Lu"
      },
      {
        "name": "Nouha Dziri"
      },
      {
        "name": "Tim Althoff"
      },
      {
        "name": "Yejin Choi"
      }
    ],
    "summary": "With increased power and prevalence of AI systems, it is ever more critical\nthat AI systems are designed to serve all, i.e., people with diverse values and\nperspectives. However, aligning models to serve pluralistic human values\nremains an open research question. In this piece, we propose a roadmap to\npluralistic alignment, specifically using language models as a test bed. We\nidentify and formalize three possible ways to define and operationalize\npluralism in AI systems: 1) Overton pluralistic models that present a spectrum\nof reasonable responses; 2) Steerably pluralistic models that can steer to\nreflect certain perspectives; and 3) Distributionally pluralistic models that\nare well-calibrated to a given population in distribution. We also propose and\nformalize three possible classes of pluralistic benchmarks: 1) Multi-objective\nbenchmarks, 2) Trade-off steerable benchmarks, which incentivize models to\nsteer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which\nexplicitly model diverse human ratings. We use this framework to argue that\ncurrent alignment techniques may be fundamentally limited for pluralistic AI;\nindeed, we highlight empirical evidence, both from our own experiments and from\nother work, that standard alignment procedures might reduce distributional\npluralism in models, motivating the need for further research on pluralistic\nalignment.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05070v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05070v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05070v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05067v2",
    "updated": "2024-02-08T07:37:50+00:00",
    "published": "2024-02-07T18:19:51+00:00",
    "title": "Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems",
    "authors": [
      {
        "name": "Jing Wang"
      },
      {
        "name": "Zheng Li"
      },
      {
        "name": "Pengyu Lai"
      },
      {
        "name": "Rui Wang"
      },
      {
        "name": "Di Yang"
      },
      {
        "name": "Dewu Yang"
      },
      {
        "name": "Hui Xu"
      }
    ],
    "summary": "Multiscale phenomena manifest across various scientific domains, presenting a\nubiquitous challenge in accurately and effectively predicting multiscale\ndynamics in complex systems. In this paper, a novel decoupling solving mode is\nproposed through modelling large-scale dynamics independently and treating\nsmall-scale dynamics as a slaved system. A Spectral Physics-informed Neural\nNetwork (PINN) is developed to characterize the small-scale system in an\nefficient and accurate way. The effectiveness of the method is demonstrated\nthrough extensive numerical experiments, including one-dimensional\nKuramot-Sivashinsky equation, two- and three-dimensional Navier-Stokes\nequations, showcasing its versatility in addressing problems of fluid dynamics.\nFurthermore, we also delve into the application of the proposed approach to\nmore complex problems, including non-uniform meshes, complex geometries,\nlarge-scale data with noise, and high-dimensional small-scale dynamics. The\ndiscussions about these scenarios contribute to a comprehensive understanding\nof the method's capabilities and limitations. This paper presents a valuable\nand promising approach to enhance the computational simulations of multiscale\nspatiotemporal systems, which enables the acquisition of large-scale data with\nminimal computational demands, followed by Spectral PINN to capture small-scale\ndynamics with improved efficiency and accuracy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "physics.flu-dyn",
    "categories": [
      "physics.flu-dyn",
      "cs.LG",
      "physics.comp-ph"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05067v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05067v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05067v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05160v1",
    "updated": "2024-02-07T18:04:32+00:00",
    "published": "2024-02-07T18:04:32+00:00",
    "title": "What's documented in AI? Systematic Analysis of 32K AI Model Cards",
    "authors": [
      {
        "name": "Weixin Liang"
      },
      {
        "name": "Nazneen Rajani"
      },
      {
        "name": "Xinyu Yang"
      },
      {
        "name": "Ezinwanne Ozoani"
      },
      {
        "name": "Eric Wu"
      },
      {
        "name": "Yiqun Chen"
      },
      {
        "name": "Daniel Scott Smith"
      },
      {
        "name": "James Zou"
      }
    ],
    "summary": "The rapid proliferation of AI models has underscored the importance of\nthorough documentation, as it enables users to understand, trust, and\neffectively utilize these models in various applications. Although developers\nare encouraged to produce model cards, it's not clear how much information or\nwhat information these cards contain. In this study, we conduct a comprehensive\nanalysis of 32,111 AI model documentations on Hugging Face, a leading platform\nfor distributing and deploying AI models. Our investigation sheds light on the\nprevailing model card documentation practices. Most of the AI models with\nsubstantial downloads provide model cards, though the cards have uneven\ninformativeness. We find that sections addressing environmental impact,\nlimitations, and evaluation exhibit the lowest filled-out rates, while the\ntraining section is the most consistently filled-out. We analyze the content of\neach section to characterize practitioners' priorities. Interestingly, there\nare substantial discussions of data, sometimes with equal or even greater\nemphasis than the model itself. To evaluate the impact of model cards, we\nconducted an intervention study by adding detailed model cards to 42 popular\nmodels which had no or sparse model cards previously. We find that adding model\ncards is moderately correlated with an increase weekly download rates. Our\nstudy opens up a new perspective for analyzing community norms and practices\nfor model documentation through large-scale data science and linguistics\nanalysis.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05160v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05160v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05160v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05158v1",
    "updated": "2024-02-07T18:02:33+00:00",
    "published": "2024-02-07T18:02:33+00:00",
    "title": "Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types",
    "authors": [
      {
        "name": "AKM Shahariar Azad Rabby"
      },
      {
        "name": "Hasmot Ali"
      },
      {
        "name": "Md. Majedul Islam"
      },
      {
        "name": "Sheikh Abujar"
      },
      {
        "name": "Fuad Rahman"
      }
    ],
    "summary": "This research paper presents a unique Bengali OCR system with some\ncapabilities. The system excels in reconstructing document layouts while\npreserving structure, alignment, and images. It incorporates advanced image and\nsignature detection for accurate extraction. Specialized models for word\nsegmentation cater to diverse document types, including computer-composed,\nletterpress, typewriter, and handwritten documents. The system handles static\nand dynamic handwritten inputs, recognizing various writing styles.\nFurthermore, it has the ability to recognize compound characters in Bengali.\nExtensive data collection efforts provide a diverse corpus, while advanced\ntechnical components optimize character and word recognition. Additional\ncontributions include image, logo, signature and table recognition, perspective\ncorrection, layout reconstruction, and a queuing module for efficient and\nscalable processing. The system demonstrates outstanding performance in\nefficient and accurate text extraction and analysis.",
    "comment": "8 pages, 7 figures, 4 table Link of the paper\n  https://openaccess.thecvf.com/content/WACV2024W/WVLL/html/Rabby_Enhancement_of_Bengali_OCR_by_Specialized_Models_and_Advanced_Techniques_WACVW_2024_paper.html",
    "journal_ref": "Proceedings of the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV) Workshops, 2024, pp. 1102-1109",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05158v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05158v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05158v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05052v1",
    "updated": "2024-02-07T17:51:38+00:00",
    "published": "2024-02-07T17:51:38+00:00",
    "title": "Causal Representation Learning from Multiple Distributions: A General Setting",
    "authors": [
      {
        "name": "Kun Zhang"
      },
      {
        "name": "Shaoan Xie"
      },
      {
        "name": "Ignavier Ng"
      },
      {
        "name": "Yujia Zheng"
      }
    ],
    "summary": "In many problems, the measured variables (e.g., image pixels) are just\nmathematical functions of the hidden causal variables (e.g., the underlying\nconcepts or objects). For the purpose of making predictions in changing\nenvironments or making proper changes to the system, it is helpful to recover\nthe hidden causal variables $Z_i$ and their causal relations represented by\ngraph $\\mathcal{G}_Z$. This problem has recently been known as causal\nrepresentation learning. This paper is concerned with a general, completely\nnonparametric setting of causal representation learning from multiple\ndistributions (arising from heterogeneous data or nonstationary time series),\nwithout assuming hard interventions behind distribution changes. We aim to\ndevelop general solutions in this fundamental case; as a by product, this helps\nsee the unique benefit offered by other assumptions such as parametric causal\nmodels or hard interventions. We show that under the sparsity constraint on the\nrecovered graph over the latent variables and suitable sufficient change\nconditions on the causal influences, interestingly, one can recover the\nmoralized graph of the underlying directed acyclic graph, and the recovered\nlatent variables and their relations are related to the underlying causal model\nin a specific, nontrivial way. In some cases, each latent variable can even be\nrecovered up to component-wise transformations. Experimental results verify our\ntheoretical claims.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05052v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05052v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05052v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05050v3",
    "updated": "2024-02-14T11:22:57+00:00",
    "published": "2024-02-07T17:46:37+00:00",
    "title": "Federated Learning Can Find Friends That Are Beneficial",
    "authors": [
      {
        "name": "Nazarii Tupitsa"
      },
      {
        "name": "Samuel Horv\u00e1th"
      },
      {
        "name": "Martin Tak\u00e1\u010d"
      },
      {
        "name": "Eduard Gorbunov"
      }
    ],
    "summary": "In Federated Learning (FL), the distributed nature and heterogeneity of\nclient data present both opportunities and challenges. While collaboration\namong clients can significantly enhance the learning process, not all\ncollaborations are beneficial; some may even be detrimental. In this study, we\nintroduce a novel algorithm that assigns adaptive aggregation weights to\nclients participating in FL training, identifying those with data distributions\nmost conducive to a specific learning objective. We demonstrate that our\naggregation method converges no worse than the method that aggregates only the\nupdates received from clients with the same data distribution. Furthermore,\nempirical evaluations consistently reveal that collaborations guided by our\nalgorithm outperform traditional FL approaches. This underscores the critical\nrole of judicious client selection and lays the foundation for more streamlined\nand effective FL implementations in the coming years.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05050v3",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05050v3",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05050v3"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05048v2",
    "updated": "2024-02-14T12:02:45+00:00",
    "published": "2024-02-07T17:41:15+00:00",
    "title": "How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation",
    "authors": [
      {
        "name": "Leonardo C. T. Bezerra"
      },
      {
        "name": "Alexander E. I. Brownlee"
      },
      {
        "name": "Luana Ferraz Alvarenga"
      },
      {
        "name": "Renan Cipriano Moioli"
      },
      {
        "name": "Thais Vasconcelos Batista"
      }
    ],
    "summary": "Artificial intelligence (AI) has driven many information and communication\ntechnology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has\nexpanded far beyond AI since the Turing test proposal. Critically, recent AI\nregulation proposals adopt AI definitions affecting ICT techniques, approaches,\nand systems that are not AI. In some cases, even works from mathematics,\nstatistics, and engineering would be affected. Worryingly, AI misdefinitions\nare observed from Western societies to the Global South. In this paper, we\npropose a framework to score how validated as appropriately-defined for\nregulation (VADER) an AI definition is. Our online, publicly-available VADER\nframework scores the coverage of premises that should underlie AI definitions\nfor regulation, which aim to (i) reproduce principles observed in other\nsuccessful technology regulations, and (ii) include all AI techniques and\napproaches while excluding non-AI works. Regarding the latter, our score is\nbased on a dataset of representative AI, non-AI ICT, and non-ICT examples. We\ndemonstrate our contribution by reviewing the AI regulation proposals of key\nplayers, namely the United States, United Kingdom, European Union, and Brazil.\nImportantly, none of the proposals assessed achieve the appropriateness score,\nranging from a revision need to a concrete risk to ICT systems and works from\nother fields.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05048v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05048v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05048v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05044v2",
    "updated": "2024-02-08T02:50:22+00:00",
    "published": "2024-02-07T17:33:54+00:00",
    "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "authors": [
      {
        "name": "Lijun Li"
      },
      {
        "name": "Bowen Dong"
      },
      {
        "name": "Ruohui Wang"
      },
      {
        "name": "Xuhao Hu"
      },
      {
        "name": "Wangmeng Zuo"
      },
      {
        "name": "Dahua Lin"
      },
      {
        "name": "Yu Qiao"
      },
      {
        "name": "Jing Shao"
      }
    ],
    "summary": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring\nrobust safety measures is paramount. To meet this crucial need, we propose\n\\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating\nLLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench\ntranscends conventional benchmarks through its large scale, rich diversity,\nintricate taxonomy spanning three levels, and versatile\nfunctionalities.SALAD-Bench is crafted with a meticulous array of questions,\nfrom standard queries to complex ones enriched with attack, defense\nmodifications and multiple-choice. To effectively manage the inherent\ncomplexity, we introduce an innovative evaluators: the LLM-based MD-Judge for\nQA pairs with a particular focus on attack-enhanced queries, ensuring a\nseamless, and reliable evaluation. Above components extend SALAD-Bench from\nstandard LLM safety evaluation to both LLM attack and defense methods\nevaluation, ensuring the joint-purpose utility. Our extensive experiments shed\nlight on the resilience of LLMs against emerging threats and the efficacy of\ncontemporary defense tactics. Data and evaluator are released under\nhttps://github.com/OpenSafetyLab/SALAD-BENCH.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05044v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05044v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05044v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05039v1",
    "updated": "2024-02-07T17:23:15+00:00",
    "published": "2024-02-07T17:23:15+00:00",
    "title": "PAC Learnability under Explanation-Preserving Graph Perturbations",
    "authors": [
      {
        "name": "Xu Zheng"
      },
      {
        "name": "Farhad Shirani"
      },
      {
        "name": "Tianchun Wang"
      },
      {
        "name": "Shouwei Gao"
      },
      {
        "name": "Wenqian Dong"
      },
      {
        "name": "Wei Cheng"
      },
      {
        "name": "Dongsheng Luo"
      }
    ],
    "summary": "Graphical models capture relations between entities in a wide range of\napplications including social networks, biology, and natural language\nprocessing, among others. Graph neural networks (GNN) are neural models that\noperate over graphs, enabling the model to leverage the complex relationships\nand dependencies in graph-structured data. A graph explanation is a subgraph\nwhich is an `almost sufficient' statistic of the input graph with respect to\nits classification label. Consequently, the classification label is invariant,\nwith high probability, to perturbations of graph edges not belonging to its\nexplanation subgraph. This work considers two methods for leveraging such\nperturbation invariances in the design and training of GNNs. First,\nexplanation-assisted learning rules are considered. It is shown that the sample\ncomplexity of explanation-assisted learning can be arbitrarily smaller than\nexplanation-agnostic learning. Next, explanation-assisted data augmentation is\nconsidered, where the training set is enlarged by artificially producing new\ntraining samples via perturbation of the non-explanation edges in the original\ntraining set. It is shown that such data augmentation methods may improve\nperformance if the augmented data is in-distribution, however, it may also lead\nto worse sample complexity compared to explanation-agnostic learning rules if\nthe augmented data is out-of-distribution. Extensive empirical evaluations are\nprovided to verify the theoretical analysis.",
    "comment": "21 pages, 6 figures, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05039v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05039v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05039v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05033v1",
    "updated": "2024-02-07T17:07:41+00:00",
    "published": "2024-02-07T17:07:41+00:00",
    "title": "Simulated Overparameterization",
    "authors": [
      {
        "name": "Hanna Mazzawi"
      },
      {
        "name": "Pranjal Awasthi"
      },
      {
        "name": "Xavi Gonzalvo"
      },
      {
        "name": "Srikumar Ramalingam"
      }
    ],
    "summary": "In this work, we introduce a novel paradigm called Simulated\nOverparametrization (SOP). SOP merges the computational efficiency of compact\nmodels with the advanced learning proficiencies of overparameterized models.\nSOP proposes a unique approach to model training and inference, where a model\nwith a significantly larger number of parameters is trained in such a way that\na smaller, efficient subset of these parameters is used for the actual\ncomputation during inference. Building upon this framework, we present a novel,\narchitecture agnostic algorithm called \"majority kernels\", which seamlessly\nintegrates with predominant architectures, including Transformer models.\nMajority kernels enables the simulated training of overparameterized models,\nresulting in performance gains across architectures and tasks. Furthermore, our\napproach adds minimal overhead to the cost incurred (wall clock time) at\ntraining time. The proposed approach shows strong performance on a wide variety\nof datasets and models, even outperforming strong baselines such as\ncombinatorial optimization methods based on submodular optimization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05033v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05033v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05033v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05027v2",
    "updated": "2024-02-09T16:36:48+00:00",
    "published": "2024-02-07T16:53:09+00:00",
    "title": "Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing",
    "authors": [
      {
        "name": "Jannis Weil"
      },
      {
        "name": "Zhenghua Bao"
      },
      {
        "name": "Osama Abboud"
      },
      {
        "name": "Tobias Meuser"
      }
    ],
    "summary": "Graph-based environments pose unique challenges to multi-agent reinforcement\nlearning. In decentralized approaches, agents operate within a given graph and\nmake decisions based on partial or outdated observations. The size of the\nobserved neighborhood limits the generalizability to different graphs and\naffects the reactivity of agents, the quality of the selected actions, and the\ncommunication overhead. This work focuses on generalizability and resolves the\ntrade-off in observed neighborhood size with a continuous information flow in\nthe whole graph. We propose a recurrent message-passing model that iterates\nwith the environment's steps and allows nodes to create a global representation\nof the graph by exchanging messages with their neighbors. Agents receive the\nresulting learned graph observations based on their location in the graph. Our\napproach can be used in a decentralized manner at runtime and in combination\nwith a reinforcement learning algorithm of choice. We evaluate our method\nacross 1000 diverse graphs in the context of routing in communication networks\nand find that it enables agents to generalize and adapt to changes in the\ngraph.",
    "comment": "Accepted at AAMAS 2024, version with appendix; revised sections 1 and\n  7, corrected table 1, final results unchanged",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.MA",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05027v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05027v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05027v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05025v1",
    "updated": "2024-02-07T16:47:07+00:00",
    "published": "2024-02-07T16:47:07+00:00",
    "title": "Strong convexity-guided hyper-parameter optimization for flatter losses",
    "authors": [
      {
        "name": "Rahul Yedida"
      },
      {
        "name": "Snehanshu Saha"
      }
    ],
    "summary": "We propose a novel white-box approach to hyper-parameter optimization.\nMotivated by recent work establishing a relationship between flat minima and\ngeneralization, we first establish a relationship between the strong convexity\nof the loss and its flatness. Based on this, we seek to find hyper-parameter\nconfigurations that improve flatness by minimizing the strong convexity of the\nloss. By using the structure of the underlying neural network, we derive\nclosed-form equations to approximate the strong convexity parameter, and\nattempt to find hyper-parameters that minimize it in a randomized fashion.\nThrough experiments on 14 classification datasets, we show that our method\nachieves strong performance at a fraction of the runtime.",
    "comment": "v1",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05025v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05025v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05025v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05015v1",
    "updated": "2024-02-07T16:32:58+00:00",
    "published": "2024-02-07T16:32:58+00:00",
    "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
    "authors": [
      {
        "name": "Agustinus Kristiadi"
      },
      {
        "name": "Felix Strieth-Kalthoff"
      },
      {
        "name": "Marta Skreta"
      },
      {
        "name": "Pascal Poupart"
      },
      {
        "name": "Al\u00e1n Aspuru-Guzik"
      },
      {
        "name": "Geoff Pleiss"
      }
    ],
    "summary": "Automation is one of the cornerstones of contemporary material discovery.\nBayesian optimization (BO) is an essential part of such workflows, enabling\nscientists to leverage prior domain knowledge into efficient exploration of a\nlarge molecular space. While such prior knowledge can take many forms, there\nhas been significant fanfare around the ancillary scientific knowledge\nencapsulated in large language models (LLMs). However, existing work thus far\nhas only explored LLMs for heuristic materials searches. Indeed, recent work\nobtains the uncertainty estimate -- an integral part of BO -- from\npoint-estimated, non-Bayesian LLMs. In this work, we study the question of\nwhether LLMs are actually useful to accelerate principled Bayesian optimization\nin the molecular space. We take a sober, dispassionate stance in answering this\nquestion. This is done by carefully (i) viewing LLMs as fixed feature\nextractors for standard but principled BO surrogate models and by (ii)\nleveraging parameter-efficient finetuning methods and Bayesian neural networks\nto obtain the posterior of the LLM surrogate. Our extensive experiments with\nreal-world chemistry problems show that LLMs can be useful for BO over\nmolecules, but only if they have been pretrained or finetuned with\ndomain-specific data.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05015v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05015v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05015v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05013v1",
    "updated": "2024-02-07T16:32:29+00:00",
    "published": "2024-02-07T16:32:29+00:00",
    "title": "Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth",
    "authors": [
      {
        "name": "Kevin K\u00f6gler"
      },
      {
        "name": "Alexander Shevchenko"
      },
      {
        "name": "Hamed Hassani"
      },
      {
        "name": "Marco Mondelli"
      }
    ],
    "summary": "Autoencoders are a prominent model in many empirical branches of machine\nlearning and lossy data compression. However, basic theoretical questions\nremain unanswered even in a shallow two-layer setting. In particular, to what\ndegree does a shallow autoencoder capture the structure of the underlying data\ndistribution? For the prototypical case of the 1-bit compression of sparse\nGaussian data, we prove that gradient descent converges to a solution that\ncompletely disregards the sparse structure of the input. Namely, the\nperformance of the algorithm is the same as if it was compressing a Gaussian\nsource - with no sparsity. For general data distributions, we give evidence of\na phase transition phenomenon in the shape of the gradient descent minimizer,\nas a function of the data sparsity: below the critical sparsity level, the\nminimizer is a rotation taken uniformly at random (just like in the compression\nof non-sparse data); above the critical sparsity, the minimizer is the identity\n(up to a permutation). Finally, by exploiting a connection with approximate\nmessage passing algorithms, we show how to improve upon Gaussian performance\nfor the compression of sparse data: adding a denoising function to a shallow\narchitecture already reduces the loss provably, and a suitable multi-layer\ndecoder leads to a further improvement. We validate our findings on image\ndatasets, such as CIFAR-10 and MNIST.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05013v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05013v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05013v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05011v1",
    "updated": "2024-02-07T16:32:02+00:00",
    "published": "2024-02-07T16:32:02+00:00",
    "title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching",
    "authors": [
      {
        "name": "Yuchen Zhang"
      },
      {
        "name": "Tianle Zhang"
      },
      {
        "name": "Kai Wang"
      },
      {
        "name": "Ziyao Guo"
      },
      {
        "name": "Yuxuan Liang"
      },
      {
        "name": "Xavier Bresson"
      },
      {
        "name": "Wei Jin"
      },
      {
        "name": "Yang You"
      }
    ],
    "summary": "Graph condensation aims to reduce the size of a large-scale graph dataset by\nsynthesizing a compact counterpart without sacrificing the performance of Graph\nNeural Networks (GNNs) trained on it, which has shed light on reducing the\ncomputational cost for training GNNs. Nevertheless, existing methods often fall\nshort of accurately replicating the original graph for certain datasets,\nthereby failing to achieve the objective of lossless condensation. To\nunderstand this phenomenon, we investigate the potential reasons and reveal\nthat the previous state-of-the-art trajectory matching method provides biased\nand restricted supervision signals from the original graph when optimizing the\ncondensed one. This significantly limits both the scale and efficacy of the\ncondensed graph. In this paper, we make the first attempt toward\n\\textit{lossless graph condensation} by bridging the previously neglected\nsupervision signals. Specifically, we employ a curriculum learning strategy to\ntrain expert trajectories with more diverse supervision signals from the\noriginal graph, and then effectively transfer the information into the\ncondensed graph with expanding window matching. Moreover, we design a loss\nfunction to further extract knowledge from the expert trajectories. Theoretical\nanalysis justifies the design of our method and extensive experiments verify\nits superiority across different datasets. Code is released at\nhttps://github.com/NUS-HPC-AI-Lab/GEOM.",
    "comment": "Lossless graph condensation method",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05011v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05011v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05011v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05156v1",
    "updated": "2024-02-07T16:31:58+00:00",
    "published": "2024-02-07T16:31:58+00:00",
    "title": "What About the Data? A Mapping Study on Data Engineering for AI Systems",
    "authors": [
      {
        "name": "Petra Heck"
      }
    ],
    "summary": "AI systems cannot exist without data. Now that AI models (data science and\nAI) have matured and are readily available to apply in practice, most\norganizations struggle with the data infrastructure to do so. There is a\ngrowing need for data engineers that know how to prepare data for AI systems or\nthat can setup enterprise-wide data architectures for analytical projects. But\nuntil now, the data engineering part of AI engineering has not been getting\nmuch attention, in favor of discussing the modeling part. In this paper we aim\nto change this by perform a mapping study on data engineering for AI systems,\ni.e., AI data engineering. We found 25 relevant papers between January 2019 and\nJune 2023, explaining AI data engineering activities. We identify which life\ncycle phases are covered, which technical solutions or architectures are\nproposed and which lessons learned are presented. We end by an overall\ndiscussion of the papers with implications for practitioners and researchers.\nThis paper creates an overview of the body of knowledge on data engineering for\nAI. This overview is useful for practitioners to identify solutions and best\npractices as well as for researchers to identify gaps.",
    "comment": "Preprint, accepted for CAIN24",
    "journal_ref": null,
    "doi": "10.1145/3644815.3644954",
    "primary_category": "cs.DL",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.DB"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3644815.3644954",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05156v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05156v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05156v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05008v1",
    "updated": "2024-02-07T16:28:36+00:00",
    "published": "2024-02-07T16:28:36+00:00",
    "title": "EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss",
    "authors": [
      {
        "name": "Zhuoyang Zhang"
      },
      {
        "name": "Han Cai"
      },
      {
        "name": "Song Han"
      }
    ],
    "summary": "We present EfficientViT-SAM, a new family of accelerated segment anything\nmodels. We retain SAM's lightweight prompt encoder and mask decoder while\nreplacing the heavy image encoder with EfficientViT. For the training, we begin\nwith the knowledge distillation from the SAM-ViT-H image encoder to\nEfficientViT. Subsequently, we conduct end-to-end training on the SA-1B\ndataset. Benefiting from EfficientViT's efficiency and capacity,\nEfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over\nSAM-ViT-H without sacrificing performance. Our code and pre-trained models are\nreleased at https://github.com/mit-han-lab/efficientvit.",
    "comment": "tech report",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05008v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05008v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05008v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05007v1",
    "updated": "2024-02-07T16:28:04+00:00",
    "published": "2024-02-07T16:28:04+00:00",
    "title": "Example-based Explanations for Random Forests using Machine Unlearning",
    "authors": [
      {
        "name": "Tanmay Surve"
      },
      {
        "name": "Romila Pradhan"
      }
    ],
    "summary": "Tree-based machine learning models, such as decision trees and random\nforests, have been hugely successful in classification tasks primarily because\nof their predictive power in supervised learning tasks and ease of\ninterpretation. Despite their popularity and power, these models have been\nfound to produce unexpected or discriminatory outcomes. Given their\noverwhelming success for most tasks, it is of interest to identify sources of\ntheir unexpected and discriminatory behavior. However, there has not been much\nwork on understanding and debugging tree-based classifiers in the context of\nfairness.\n  We introduce FairDebugger, a system that utilizes recent advances in machine\nunlearning research to identify training data subsets responsible for instances\nof fairness violations in the outcomes of a random forest classifier.\nFairDebugger generates top-$k$ explanations (in the form of coherent training\ndata subsets) for model unfairness. Toward this goal, FairDebugger first\nutilizes machine unlearning to estimate the change in the tree structures of\nthe random forest when parts of the underlying training data are removed, and\nthen leverages the Apriori algorithm from frequent itemset mining to reduce the\nsubset search space. We empirically evaluate our approach on three real-world\ndatasets, and demonstrate that the explanations generated by FairDebugger are\nconsistent with insights from prior studies on these datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05007v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05007v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05007v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05002v1",
    "updated": "2024-02-07T16:18:59+00:00",
    "published": "2024-02-07T16:18:59+00:00",
    "title": "Randomized Confidence Bounds for Stochastic Partial Monitoring",
    "authors": [
      {
        "name": "Maxime Heuillet"
      },
      {
        "name": "Ola Ahmad"
      },
      {
        "name": "Audrey Durand"
      }
    ],
    "summary": "The partial monitoring (PM) framework provides a theoretical formulation of\nsequential learning problems with incomplete feedback. On each round, a\nlearning agent plays an action while the environment simultaneously chooses an\noutcome. The agent then observes a feedback signal that is only partially\ninformative about the (unobserved) outcome. The agent leverages the received\nfeedback signals to select actions that minimize the (unobserved) cumulative\nloss. In contextual PM, the outcomes depend on some side information that is\nobservable by the agent before selecting the action on each round. In this\npaper, we consider the contextual and non-contextual PM settings with\nstochastic outcomes. We introduce a new class of strategies based on the\nrandomization of deterministic confidence bounds, that extend regret guarantees\nto settings where existing stochastic strategies are not applicable. Our\nexperiments show that the proposed RandCBP and RandCBPside* strategies improve\nstate-of-the-art baselines in PM games. To encourage the adoption of the PM\nframework, we design a use case on the real-world problem of monitoring the\nerror rate of any deployed classification system.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05002v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05002v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05002v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04997v1",
    "updated": "2024-02-07T16:15:36+00:00",
    "published": "2024-02-07T16:15:36+00:00",
    "title": "Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design",
    "authors": [
      {
        "name": "Andrew Campbell"
      },
      {
        "name": "Jason Yim"
      },
      {
        "name": "Regina Barzilay"
      },
      {
        "name": "Tom Rainforth"
      },
      {
        "name": "Tommi Jaakkola"
      }
    ],
    "summary": "Combining discrete and continuous data is an important capability for\ngenerative models. We present Discrete Flow Models (DFMs), a new flow-based\nmodel of discrete data that provides the missing link in enabling flow-based\ngenerative models to be applied to multimodal continuous and discrete data\nproblems. Our key insight is that the discrete equivalent of continuous space\nflow matching can be realized using Continuous Time Markov Chains. DFMs benefit\nfrom a simple derivation that includes discrete diffusion models as a specific\ninstance while allowing improved performance over existing diffusion-based\napproaches. We utilize our DFMs method to build a multimodal flow-based\nmodeling framework. We apply this capability to the task of protein co-design,\nwherein we learn a model for jointly generating protein structure and sequence.\nOur approach achieves state-of-the-art co-design performance while allowing the\nsame multimodal model to be used for flexible generation of the sequence or\nstructure.",
    "comment": "52 pages, 11 figures, 5 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-bio.QM"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04997v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04997v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04997v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05155v1",
    "updated": "2024-02-07T16:14:04+00:00",
    "published": "2024-02-07T16:14:04+00:00",
    "title": "Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks",
    "authors": [
      {
        "name": "Arnulf Jentzen"
      },
      {
        "name": "Adrian Riekert"
      }
    ],
    "summary": "Stochastic gradient descent (SGD) optimization methods such as the plain\nvanilla SGD method and the popular Adam optimizer are nowadays the method of\nchoice in the training of artificial neural networks (ANNs). Despite the\nremarkable success of SGD methods in the ANN training in numerical simulations,\nit remains in essentially all practical relevant scenarios an open problem to\nrigorously explain why SGD methods seem to succeed to train ANNs. In\nparticular, in most practically relevant supervised learning problems, it seems\nthat SGD methods do with high probability not converge to global minimizers in\nthe optimization landscape of the ANN training problem. Nevertheless, it\nremains an open problem of research to disprove the convergence of SGD methods\nto global minimizers. In this work we solve this research problem in the\nsituation of shallow ANNs with the rectified linear unit (ReLU) and related\nactivations with the standard mean square error loss by disproving in the\ntraining of such ANNs that SGD methods (such as the plain vanilla SGD, the\nmomentum SGD, the AdaGrad, the RMSprop, and the Adam optimizers) can find a\nglobal minimizer with high probability. Even stronger, we reveal in the\ntraining of such ANNs that SGD methods do with high probability fail to\nconverge to global minimizers in the optimization landscape. The findings of\nthis work do, however, not disprove that SGD methods succeed to train ANNs\nsince they do not exclude the possibility that SGD methods find good local\nminimizers whose risk values are close to the risk values of the global\nminimizers. In this context, another key contribution of this work is to\nestablish the existence of a hierarchical structure of local minimizers with\ndistinct risk values in the optimization landscape of ANN training problems\nwith ReLU and related activations.",
    "comment": "36 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05155v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05155v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05155v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04987v1",
    "updated": "2024-02-07T16:06:20+00:00",
    "published": "2024-02-07T16:06:20+00:00",
    "title": "PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses",
    "authors": [
      {
        "name": "Adel Javanmard"
      },
      {
        "name": "Matthew Fahrbach"
      },
      {
        "name": "Vahab Mirrokni"
      }
    ],
    "summary": "This work studies algorithms for learning from aggregate responses. We focus\non the construction of aggregation sets (called bags in the literature) for\nevent-level loss functions. We prove for linear regression and generalized\nlinear models (GLMs) that the optimal bagging problem reduces to\none-dimensional size-constrained $k$-means clustering. Further, we\ntheoretically quantify the advantage of using curated bags over random bags. We\nthen propose the PriorBoost algorithm, which adaptively forms bags of samples\nthat are increasingly homogeneous with respect to (unobserved) individual\nresponses to improve model quality. We study label differential privacy for\naggregate learning, and we also provide extensive experiments showing that\nPriorBoost regularly achieves optimal model quality for event-level\npredictions, in stark contrast to non-adaptive algorithms.",
    "comment": "29 pages, 4 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04987v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04987v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04987v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04982v1",
    "updated": "2024-02-07T15:58:51+00:00",
    "published": "2024-02-07T15:58:51+00:00",
    "title": "Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for Energy Consumption Prediction",
    "authors": [
      {
        "name": "Tobias Clement"
      },
      {
        "name": "Hung Truong Thanh Nguyen"
      },
      {
        "name": "Nils Kemmerzell"
      },
      {
        "name": "Mohamed Abdelaal"
      },
      {
        "name": "Davor Stjelja"
      }
    ],
    "summary": "This paper presents an approach integrating explainable artificial\nintelligence (XAI) techniques with adaptive learning to enhance energy\nconsumption prediction models, with a focus on handling data distribution\nshifts. Leveraging SHAP clustering, our method provides interpretable\nexplanations for model predictions and uses these insights to adaptively refine\nthe model, balancing model complexity with predictive performance. We introduce\na three-stage process: (1) obtaining SHAP values to explain model predictions,\n(2) clustering SHAP values to identify distinct patterns and outliers, and (3)\nrefining the model based on the derived SHAP clustering characteristics. Our\napproach mitigates overfitting and ensures robustness in handling data\ndistribution shifts. We evaluate our method on a comprehensive dataset\ncomprising energy consumption records of buildings, as well as two additional\ndatasets to assess the transferability of our approach to other domains,\nregression, and classification problems. Our experiments demonstrate the\neffectiveness of our approach in both task types, resulting in improved\npredictive performance and interpretable model explanations.",
    "comment": "A short version of this paper was published at the Australasian Joint\n  Conference on Artificial Intelligence in 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04982v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04982v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04982v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04980v1",
    "updated": "2024-02-07T15:57:30+00:00",
    "published": "2024-02-07T15:57:30+00:00",
    "title": "Asymptotics of feature learning in two-layer networks after one gradient-step",
    "authors": [
      {
        "name": "Hugo Cui"
      },
      {
        "name": "Luca Pesce"
      },
      {
        "name": "Yatin Dandi"
      },
      {
        "name": "Florent Krzakala"
      },
      {
        "name": "Yue M. Lu"
      },
      {
        "name": "Lenka Zdeborov\u00e1"
      },
      {
        "name": "Bruno Loureiro"
      }
    ],
    "summary": "In this manuscript we investigate the problem of how two-layer neural\nnetworks learn features from data, and improve over the kernel regime, after\nbeing trained with a single gradient descent step. Leveraging a connection from\n(Ba et al., 2022) with a non-linear spiked matrix model and recent progress on\nGaussian universality (Dandi et al., 2023), we provide an exact asymptotic\ndescription of the generalization error in the high-dimensional limit where the\nnumber of samples $n$, the width $p$ and the input dimension $d$ grow at a\nproportional rate. We characterize exactly how adapting to the data is crucial\nfor the network to efficiently learn non-linear functions in the direction of\nthe gradient -- where at initialization it can only express linear functions in\nthis regime. To our knowledge, our results provides the first tight description\nof the impact of feature learning in the generalization of two-layer neural\nnetworks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond\nperturbative finite width corrections of the conjugate and neural tangent\nkernels.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04980v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04980v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04980v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04979v1",
    "updated": "2024-02-07T15:57:28+00:00",
    "published": "2024-02-07T15:57:28+00:00",
    "title": "Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training",
    "authors": [
      {
        "name": "Thomas P\u00f6llabauer"
      },
      {
        "name": "Fabian R\u00fccker"
      },
      {
        "name": "Andreas Franek"
      },
      {
        "name": "Felix Gorschl\u00fcter"
      }
    ],
    "summary": "Current state-of-the-art 6d pose estimation is too compute intensive to be\ndeployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both\nused for an increasing number of augmented reality applications. The quality of\nAR is greatly dependent on its capabilities to detect and overlay geometry\nwithin the scene. We propose a synthetically trained client-server-based\naugmented reality application, demonstrating state-of-the-art object pose\nestimation of metallic and texture-less industry objects on edge devices.\nSynthetic data enables training without real photographs, i.e. for\nyet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted\nsorting task, and quantitative evaluation on both renderings, as well as\nreal-world data recorded on HoloLens 2, sheds light on its real-world\napplicability.",
    "comment": "Scandinavian Conference on Image Analysis 2023",
    "journal_ref": "In Scandinavian Conference on Image Analysis 2023 (pp. 569-585).\n  Cham: Springer Nature Switzerland",
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04979v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04979v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04979v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04978v1",
    "updated": "2024-02-07T15:56:17+00:00",
    "published": "2024-02-07T15:56:17+00:00",
    "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
    "authors": [
      {
        "name": "Yihao Li"
      },
      {
        "name": "Ru Zhang"
      },
      {
        "name": "Jianyi Liu"
      },
      {
        "name": "Gongshen Liu"
      }
    ],
    "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in a\nmultitude of Natural Language Processing (NLP) tasks, they encounter challenges\nin practical applications, including issues with hallucinations, inadequate\nknowledge updating, and limited transparency in the reasoning process. To\novercome these limitations, this study innovatively proposes a collaborative\ntraining-free reasoning scheme involving tight cooperation between Knowledge\nGraph (KG) and LLMs. This scheme first involves using LLMs to iteratively\nexplore KG, selectively retrieving a task-relevant knowledge subgraph to\nsupport reasoning. The LLMs are then guided to further combine inherent\nimplicit knowledge to reason on the subgraph while explicitly elucidating the\nreasoning process. Through such a cooperative approach, our scheme achieves\nmore reliable knowledge-based reasoning and facilitates the tracing of the\nreasoning results. Experimental results show that our scheme significantly\nprogressed across multiple datasets, notably achieving over a 10% improvement\non the QALD10 dataset compared to the best baseline and the fine-tuned\nstate-of-the-art (SOTA) work. Building on this success, this study hopes to\noffer a valuable reference for future research in the fusion of KG and LLMs,\nthereby enhancing LLMs' proficiency in solving complex issues.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04978v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04978v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04978v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04975v1",
    "updated": "2024-02-07T15:55:51+00:00",
    "published": "2024-02-07T15:55:51+00:00",
    "title": "ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12",
    "authors": [
      {
        "name": "Liuqing Chen"
      },
      {
        "name": "Shuhong Xiao"
      },
      {
        "name": "Yunnong Chen"
      },
      {
        "name": "Ruoyu Wu"
      },
      {
        "name": "Yaxuan Song"
      },
      {
        "name": "Lingyun Sun"
      }
    ],
    "summary": "As Computational Thinking (CT) continues to permeate younger age groups in\nK-12 education, established CT platforms such as Scratch face challenges in\ncatering to these younger learners, particularly those in the elementary school\n(ages 6-12). Through formative investigation with Scratch experts, we uncover\nthree key obstacles to children's autonomous Scratch learning: artist's block\nin project planning, bounded creativity in asset creation, and inadequate\ncoding guidance during implementation. To address these barriers, we introduce\nChatScratch, an AI-augmented system to facilitate autonomous programming\nlearning for young children. ChatScratch employs structured interactive\nstoryboards and visual cues to overcome artist's block, integrates digital\ndrawing and advanced image generation technologies to elevate creativity, and\nleverages Scratch-specialized Large Language Models (LLMs) for professional\ncoding guidance. Our study shows that, compared to Scratch, ChatScratch\nefficiently fosters autonomous programming learning, and contributes to the\ncreation of high-quality, personally meaningful Scratch projects for children.",
    "comment": "29 pages, 7 figures, accepted by CHI 2024",
    "journal_ref": null,
    "doi": "10.1145/3613904.3642229",
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.PL"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3613904.3642229",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.04975v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04975v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04975v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04971v2",
    "updated": "2024-02-08T02:10:26+00:00",
    "published": "2024-02-07T15:50:20+00:00",
    "title": "Multi-Sender Persuasion -- A Computational Perspective",
    "authors": [
      {
        "name": "Safwan Hossain"
      },
      {
        "name": "Tonghan Wang"
      },
      {
        "name": "Tao Lin"
      },
      {
        "name": "Yiling Chen"
      },
      {
        "name": "David C. Parkes"
      },
      {
        "name": "Haifeng Xu"
      }
    ],
    "summary": "We consider multiple senders with informational advantage signaling to\nconvince a single self-interested actor towards certain actions. Generalizing\nthe seminal Bayesian Persuasion framework, such settings are ubiquitous in\ncomputational economics, multi-agent learning, and machine learning with\nmultiple objectives. The core solution concept here is the Nash equilibrium of\nsenders' signaling policies. Theoretically, we prove that finding an\nequilibrium in general is PPAD-Hard; in fact, even computing a sender's best\nresponse is NP-Hard. Given these intrinsic difficulties, we turn to finding\nlocal Nash equilibria. We propose a novel differentiable neural network to\napproximate this game's non-linear and discontinuous utilities. Complementing\nthis with the extra-gradient algorithm, we discover local equilibria that\nPareto dominates full-revelation equilibria and those found by existing neural\nnetworks. Broadly, our theoretical and empirical contributions are of interest\nto a large class of economic problems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04971v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04971v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04971v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04967v1",
    "updated": "2024-02-07T15:44:55+00:00",
    "published": "2024-02-07T15:44:55+00:00",
    "title": "Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?",
    "authors": [
      {
        "name": "Piush Aggarwal"
      },
      {
        "name": "Jawar Mehrabanian"
      },
      {
        "name": "Weigang Huang"
      },
      {
        "name": "\u00d6zge Alacam"
      },
      {
        "name": "Torsten Zesch"
      }
    ],
    "summary": "This paper delves into the formidable challenge of cross-domain\ngeneralization in multimodal hate meme detection, presenting compelling\nfindings. We provide enough pieces of evidence supporting the hypothesis that\nonly the textual component of hateful memes enables the existing multimodal\nclassifier to generalize across different domains, while the image component\nproves highly sensitive to a specific training dataset. The evidence includes\ndemonstrations showing that hate-text classifiers perform similarly to\nhate-meme classifiers in a zero-shot setting. Simultaneously, the introduction\nof captions generated from images of memes to the hate-meme classifier worsens\nperformance by an average F1 of 0.02. Through blackbox explanations, we\nidentify a substantial contribution of the text modality (average of 83%),\nwhich diminishes with the introduction of meme's image captions (52%).\nAdditionally, our evaluation on a newly created confounder dataset reveals\nhigher performance on text confounders as compared to image confounders with an\naverage $\\Delta$F1 of 0.18.",
    "comment": "Accepted at EACL'2024 Findings",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04967v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04967v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04967v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04955v1",
    "updated": "2024-02-07T15:39:07+00:00",
    "published": "2024-02-07T15:39:07+00:00",
    "title": "Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems",
    "authors": [
      {
        "name": "Samuel Kernan Freire"
      },
      {
        "name": "Chaofan Wang"
      },
      {
        "name": "Evangelos Niforatos"
      }
    ],
    "summary": "Cognitive assistants (CA) are chatbots that provide context-aware support to\nhuman workers in knowledge-intensive tasks. Traditionally, cognitive assistants\nrespond in specific ways to predefined user intents and conversation patterns.\nHowever, this rigidness does not handle the diversity of natural language well.\nRecent advances in natural language processing (NLP), powering large language\nmodels (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in\na more flexible, human-like manner. However, the additional degrees of freedom\nmay have unforeseen consequences, especially in knowledge-intensive contexts\nwhere accuracy is crucial. As a preliminary step to assessing the potential of\nusing LLMs in these contexts, we conducted a user study comparing an LLM-based\nCA to an intent-based system regarding interaction efficiency, user experience,\nworkload, and usability. This revealed that LLM-based CAs exhibited better user\nexperience, task completion rate, usability, and perceived performance than\nintent-based systems, suggesting that switching NLP techniques should be\ninvestigated further.",
    "comment": "10 pages, 7 figures, under review at an ACM venue",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04955v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04955v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04955v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06675v1",
    "updated": "2024-02-07T15:38:29+00:00",
    "published": "2024-02-07T15:38:29+00:00",
    "title": "A Masked language model for multi-source EHR trajectories contextual representation learning",
    "authors": [
      {
        "name": "Ali Amirahmadi"
      },
      {
        "name": "Mattias Ohlsson"
      },
      {
        "name": "Kobra Etminani"
      },
      {
        "name": "Olle Melander"
      },
      {
        "name": "Jonas Bj\u00f6rk"
      }
    ],
    "summary": "Using electronic health records data and machine learning to guide future\ndecisions needs to address challenges, including 1) long/short-term\ndependencies and 2) interactions between diseases and interventions.\nBidirectional transformers have effectively addressed the first challenge. Here\nwe tackled the latter challenge by masking one source (e.g., ICD10 codes) and\ntraining the transformer to predict it using other sources (e.g., ATC codes).",
    "comment": "Presented at Proceedings of MIE 2023",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06675v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06675v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06675v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05154v1",
    "updated": "2024-02-07T15:21:18+00:00",
    "published": "2024-02-07T15:21:18+00:00",
    "title": "Adaptive Hypergraph Network for Trust Prediction",
    "authors": [
      {
        "name": "Rongwei Xu"
      },
      {
        "name": "Guanfeng Liu"
      },
      {
        "name": "Yan Wang"
      },
      {
        "name": "Xuyun Zhang"
      },
      {
        "name": "Kai Zheng"
      },
      {
        "name": "Xiaofang Zhou"
      }
    ],
    "summary": "Trust plays an essential role in an individual's decision-making. Traditional\ntrust prediction models rely on pairwise correlations to infer potential\nrelationships between users. However, in the real world, interactions between\nusers are usually complicated rather than pairwise only. Hypergraphs offer a\nflexible approach to modeling these complex high-order correlations (not just\npairwise connections), since hypergraphs can leverage hyperedeges to link more\nthan two nodes. However, most hypergraph-based methods are generic and cannot\nbe well applied to the trust prediction task. In this paper, we propose an\nAdaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that\nimproves trust prediction accuracy by using higher-order correlations. AHNTP\nutilizes Motif-based PageRank to capture high-order social influence\ninformation. In addition, it constructs hypergroups from both node-level and\nstructure-level attributes to incorporate complex correlation information.\nFurthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network\n(GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user\nembeddings, facilitating trust relationship prediction. To enhance model\ngeneralization and robustness, we introduce a novel supervised contrastive\nlearning loss for optimization. Extensive experiments demonstrate the\nsuperiority of our model over the state-of-the-art approaches in terms of trust\nprediction accuracy. The source code of this work can be accessed via\nhttps://github.com/Sherry-XU1995/AHNTP.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SI",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05154v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05154v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05154v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04938v1",
    "updated": "2024-02-07T15:16:21+00:00",
    "published": "2024-02-07T15:16:21+00:00",
    "title": "An approach to automated videogame beta testing",
    "authors": [
      {
        "name": "Jennifer Hern\u00e1ndez-B\u00e9cares"
      },
      {
        "name": "Luis Costero"
      },
      {
        "name": "Pedro Pablo G\u00f3mez-Mart\u00edn"
      }
    ],
    "summary": "Videogames developed in the 1970s and 1980s were modest programs created in a\ncouple of months by a single person, who played the roles of designer, artist\nand programmer. Since then, videogames have evolved to become a multi-million\ndollar industry. Today, AAA game development involves hundreds of people\nworking together over several years. Management and engineering requirements\nhave changed at the same pace. Although many of the processes have been adapted\nover time, this is not quite true for quality assurance tasks, which are still\ndone mainly manually by human beta testers due to the specific peculiarities of\nvideogames. This paper presents an approach to automate this beta testing.",
    "comment": null,
    "journal_ref": "Entertainment Computing, Elsevier. 18. pp 79 to 92. (2017)",
    "doi": "10.1016/j.entcom.2016.08.002",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1016/j.entcom.2016.08.002",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.04938v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04938v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04938v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04933v1",
    "updated": "2024-02-07T15:11:37+00:00",
    "published": "2024-02-07T15:11:37+00:00",
    "title": "A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health",
    "authors": [
      {
        "name": "Biyonka Liang"
      },
      {
        "name": "Lily Xu"
      },
      {
        "name": "Aparna Taneja"
      },
      {
        "name": "Milind Tambe"
      },
      {
        "name": "Lucas Janson"
      }
    ],
    "summary": "Restless multi-armed bandits (RMABs) are used to model sequential resource\nallocation in public health intervention programs. In these settings, the\nunderlying transition dynamics are often unknown a priori, requiring online\nreinforcement learning (RL). However, existing methods in online RL for RMABs\ncannot incorporate properties often present in real-world public health\napplications, such as contextual information and non-stationarity. We present\nBayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs\nthat novelly combines techniques in Bayesian modeling with Thompson sampling to\nflexibly model a wide range of complex RMAB settings, such as contextual and\nnon-stationary RMABs. A key contribution of our approach is its ability to\nleverage shared information within and between arms to learn unknown RMAB\ntransition dynamics quickly in budget-constrained settings with relatively\nshort time horizons. Empirically, we show that BCoR achieves substantially\nhigher finite-sample performance than existing approaches over a range of\nexperimental settings, including one constructed from a real-world public\nhealth campaign in India.",
    "comment": "26 pages, 18 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04933v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04933v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04933v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04930v1",
    "updated": "2024-02-07T14:59:25+00:00",
    "published": "2024-02-07T14:59:25+00:00",
    "title": "Blue noise for diffusion models",
    "authors": [
      {
        "name": "Xingchang Huang"
      },
      {
        "name": "Corentin Sala\u00fcn"
      },
      {
        "name": "Cristina Vasconcelos"
      },
      {
        "name": "Christian Theobalt"
      },
      {
        "name": "Cengiz \u00d6ztireli"
      },
      {
        "name": "Gurprit Singh"
      }
    ],
    "summary": "Most of the existing diffusion models use Gaussian noise for training and\nsampling across all time steps, which may not optimally account for the\nfrequency contents reconstructed by the denoising network. Despite the diverse\napplications of correlated noise in computer graphics, its potential for\nimproving the training process has been underexplored. In this paper, we\nintroduce a novel and general class of diffusion models taking correlated noise\nwithin and across images into account. More specifically, we propose a\ntime-varying noise model to incorporate correlated noise into the training\nprocess, as well as a method for fast generation of correlated noise mask. Our\nmodel is built upon deterministic diffusion models and utilizes blue noise to\nhelp improve the generation quality compared to using Gaussian white (random)\nnoise only. Further, our framework allows introducing correlation across images\nwithin a single mini-batch to improve gradient flow. We perform both\nqualitative and quantitative evaluations on a variety of datasets using our\nmethod, achieving improvements on different tasks over existing deterministic\ndiffusion models in terms of FID metric.",
    "comment": "10 pages, 12 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04930v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04930v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04930v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04929v1",
    "updated": "2024-02-07T14:56:13+00:00",
    "published": "2024-02-07T14:56:13+00:00",
    "title": "Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation",
    "authors": [
      {
        "name": "Shivang Chopra"
      },
      {
        "name": "Suraj Kothawade"
      },
      {
        "name": "Houda Aynaou"
      },
      {
        "name": "Aman Chadha"
      }
    ],
    "summary": "This paper introduces a novel approach to leverage the generalizability\ncapability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our\nproposed DM-SFDA method involves fine-tuning a pre-trained text-to-image\ndiffusion model to generate source domain images using features from the target\nimages to guide the diffusion process. Specifically, the pre-trained diffusion\nmodel is fine-tuned to generate source samples that minimize entropy and\nmaximize confidence for the pre-trained source model. We then apply established\nunsupervised domain adaptation techniques to align the generated source images\nwith target domain data. We validate our approach through comprehensive\nexperiments across a range of datasets, including Office-31, Office-Home, and\nVisDA. The results highlight significant improvements in SFDA performance,\nshowcasing the potential of diffusion models in generating contextually\nrelevant, domain-specific images.",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2310.01701",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04929v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04929v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04929v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04924v2",
    "updated": "2024-02-08T02:38:14+00:00",
    "published": "2024-02-07T14:49:10+00:00",
    "title": "Two Trades is not Baffled: Condensing Graph via Crafting Rational Gradient Matching",
    "authors": [
      {
        "name": "Tianle Zhang"
      },
      {
        "name": "Yuchen Zhang"
      },
      {
        "name": "Kun Wang"
      },
      {
        "name": "Kai Wang"
      },
      {
        "name": "Beining Yang"
      },
      {
        "name": "Kaipeng Zhang"
      },
      {
        "name": "Wenqi Shao"
      },
      {
        "name": "Ping Liu"
      },
      {
        "name": "Joey Tianyi Zhou"
      },
      {
        "name": "Yang You"
      }
    ],
    "summary": "Training on large-scale graphs has achieved remarkable results in graph\nrepresentation learning, but its cost and storage have raised growing concerns.\nAs one of the most promising directions, graph condensation methods address\nthese issues by employing gradient matching, aiming to condense the full graph\ninto a more concise yet information-rich synthetic set. Though encouraging,\nthese strategies primarily emphasize matching directions of the gradients,\nwhich leads to deviations in the training trajectories. Such deviations are\nfurther magnified by the differences between the condensation and evaluation\nphases, culminating in accumulated errors, which detrimentally affect the\nperformance of the condensed graphs. In light of this, we propose a novel graph\ncondensation method named \\textbf{C}raf\\textbf{T}ing \\textbf{R}ationa\\textbf{L}\ntrajectory (\\textbf{CTRL}), which offers an optimized starting point closer to\nthe original dataset's feature distribution and a more refined strategy for\ngradient matching. Theoretically, CTRL can effectively neutralize the impact of\naccumulated errors on the performance of condensed graphs. We provide extensive\nexperiments on various graph datasets and downstream tasks to support the\neffectiveness of CTRL. Code is released at\nhttps://github.com/NUS-HPC-AI-Lab/CTRL.",
    "comment": "An effective method for graph condensation",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04924v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04924v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04924v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04922v1",
    "updated": "2024-02-07T14:47:13+00:00",
    "published": "2024-02-07T14:47:13+00:00",
    "title": "Voronoi Candidates for Bayesian Optimization",
    "authors": [
      {
        "name": "Nathan Wycoff"
      },
      {
        "name": "John W. Smith"
      },
      {
        "name": "Annie S. Booth"
      },
      {
        "name": "Robert B. Gramacy"
      }
    ],
    "summary": "Bayesian optimization (BO) offers an elegant approach for efficiently\noptimizing black-box functions. However, acquisition criteria demand their own\nchallenging inner-optimization, which can induce significant overhead. Many\npractical BO methods, particularly in high dimension, eschew a formal,\ncontinuous optimization of the acquisition function and instead search\ndiscretely over a finite set of space-filling candidates. Here, we propose to\nuse candidates which lie on the boundary of the Voronoi tessellation of the\ncurrent design points, so they are equidistant to two or more of them. We\ndiscuss strategies for efficient implementation by directly sampling the\nVoronoi boundary without explicitly generating the tessellation, thus\naccommodating large designs in high dimension. On a battery of test problems\noptimized via Gaussian processes with expected improvement, our proposed\napproach significantly improves the execution time of a multi-start continuous\nsearch without a loss in accuracy.",
    "comment": "comments very welcome",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04922v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04922v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04922v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04918v1",
    "updated": "2024-02-07T14:44:42+00:00",
    "published": "2024-02-07T14:44:42+00:00",
    "title": "Prompting Implicit Discourse Relation Annotation",
    "authors": [
      {
        "name": "Frances Yung"
      },
      {
        "name": "Mansoor Ahmad"
      },
      {
        "name": "Merel Scholman"
      },
      {
        "name": "Vera Demberg"
      }
    ],
    "summary": "Pre-trained large language models, such as ChatGPT, archive outstanding\nperformance in various reasoning tasks without supervised training and were\nfound to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's\nperformance in the task of implicit discourse relation classification, prompted\nby a standard multiple-choice question, is still far from satisfactory and\nconsiderably inferior to state-of-the-art supervised approaches. This work\ninvestigates several proven prompting techniques to improve ChatGPT's\nrecognition of discourse relations. In particular, we experimented with\nbreaking down the classification task that involves numerous abstract labels\ninto smaller subtasks. Nonetheless, experiment results show that the inference\naccuracy hardly changes even with sophisticated prompt engineering, suggesting\nthat implicit discourse relation classification is not yet resolvable under\nzero-shot or few-shot settings.",
    "comment": "To appear at the Linguistic Annotation Workshop 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04918v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04918v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04918v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04915v2",
    "updated": "2024-02-09T15:12:42+00:00",
    "published": "2024-02-07T14:41:17+00:00",
    "title": "Moco: A Learnable Meta Optimizer for Combinatorial Optimization",
    "authors": [
      {
        "name": "Tim Dernedde"
      },
      {
        "name": "Daniela Thyssens"
      },
      {
        "name": "S\u00f6ren Dittrich"
      },
      {
        "name": "Maximilian Stubbemann"
      },
      {
        "name": "Lars Schmidt-Thieme"
      }
    ],
    "summary": "Relevant combinatorial optimization problems (COPs) are often NP-hard. While\nthey have been tackled mainly via handcrafted heuristics in the past, advances\nin neural networks have motivated the development of general methods to learn\nheuristics from data. Many approaches utilize a neural network to directly\nconstruct a solution, but are limited in further improving based on already\nconstructed solutions at inference time. Our approach, Moco, learns a graph\nneural network that updates the solution construction procedure based on\nfeatures extracted from the current search state. This meta training procedure\ntargets the overall best solution found during the search procedure given\ninformation such as the search budget. This allows Moco to adapt to varying\ncircumstances such as different computational budgets. Moco is a fully\nlearnable meta optimizer that does not utilize any problem specific local\nsearch or decomposition. We test Moco on the Traveling Salesman Problem (TSP)\nand Maximum Independent Set (MIS) and show that it outperforms other approaches\non MIS and is overall competitive on the TSP, especially outperforming related\napproaches, partially even if they use additional local search.",
    "comment": "13 pages, 3 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04915v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04915v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04915v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04912v1",
    "updated": "2024-02-07T14:39:11+00:00",
    "published": "2024-02-07T14:39:11+00:00",
    "title": "Towards Biologically Plausible and Private Gene Expression Data Generation",
    "authors": [
      {
        "name": "Dingfan Chen"
      },
      {
        "name": "Marie Oestreich"
      },
      {
        "name": "Tejumade Afonja"
      },
      {
        "name": "Raouf Kerkouche"
      },
      {
        "name": "Matthias Becker"
      },
      {
        "name": "Mario Fritz"
      }
    ],
    "summary": "Generative models trained with Differential Privacy (DP) are becoming\nincreasingly prominent in the creation of synthetic data for downstream\napplications. Existing literature, however, primarily focuses on basic\nbenchmarking datasets and tends to report promising results only for elementary\nmetrics and relatively simple data distributions. In this paper, we initiate a\nsystematic analysis of how DP generative models perform in their natural\napplication scenarios, specifically focusing on real-world gene expression\ndata. We conduct a comprehensive analysis of five representative DP generation\nmethods, examining them from various angles, such as downstream utility,\nstatistical properties, and biological plausibility. Our extensive evaluation\nilluminates the unique characteristics of each DP generation method, offering\ncritical insights into the strengths and weaknesses of each approach, and\nuncovering intriguing possibilities for future developments. Perhaps\nsurprisingly, our analysis reveals that most methods are capable of achieving\nseemingly reasonable downstream utility, according to the standard evaluation\nmetrics considered in existing literature. Nevertheless, we find that none of\nthe DP methods are able to accurately capture the biological characteristics of\nthe real dataset. This observation suggests a potential over-optimistic\nassessment of current methodologies in this field and underscores a pressing\nneed for future enhancements in model design.",
    "comment": null,
    "journal_ref": "Proceedings on Privacy Enhancing Technologies (PoPETs 2024)",
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04912v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04912v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04912v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04907v2",
    "updated": "2024-02-09T07:15:02+00:00",
    "published": "2024-02-07T14:37:37+00:00",
    "title": "On a Combinatorial Problem Arising in Machine Teaching",
    "authors": [
      {
        "name": "Brigt H\u00e5vardstun"
      },
      {
        "name": "Jan Kratochv\u00edl"
      },
      {
        "name": "Joakim Sunde"
      },
      {
        "name": "Jan Arne Telle"
      }
    ],
    "summary": "We study a model of machine teaching where the teacher mapping is constructed\nfrom a size function on both concepts and examples. The main question in\nmachine teaching is the minimum number of examples needed for any concept, the\nso-called teaching dimension. A recent paper [7] conjectured that the worst\ncase for this model, as a function of the size of the concept class, occurs\nwhen the consistency matrix contains the binary representations of numbers from\nzero and up. In this paper we prove their conjecture. The result can be seen as\na generalization of a theorem resolving the edge isoperimetry problem for\nhypercubes [12], and our proof is based on a lemma of [10].",
    "comment": "14 pages, 1 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.CO",
    "categories": [
      "math.CO",
      "cs.LG",
      "G.2.1"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04907v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04907v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04907v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04906v1",
    "updated": "2024-02-07T14:35:25+00:00",
    "published": "2024-02-07T14:35:25+00:00",
    "title": "Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects",
    "authors": [
      {
        "name": "Jef Jonkers"
      },
      {
        "name": "Jarne Verhaeghe"
      },
      {
        "name": "Glenn Van Wallendael"
      },
      {
        "name": "Luc Duchateau"
      },
      {
        "name": "Sofie Van Hoecke"
      }
    ],
    "summary": "Knowledge of the effect of interventions, called the treatment effect, is\nparamount for decision-making. Approaches to estimating this treatment effect,\ne.g. by using Conditional Average Treatment Effect (CATE) estimators, often\nonly provide a point estimate of this treatment effect, while additional\nuncertainty quantification is frequently desired instead. Therefore, we present\na novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging\nconformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to\ninstead produce a predictive distribution usable in individualized\ndecision-making. Furthermore, we show how specific assumptions on the noise\ndistribution of the outcome heavily affect these uncertainty predictions.\nNonetheless, the CMC framework shows strong experimental coverage while\nretaining small interval widths to provide estimates of the true individual\ntreatment effect.",
    "comment": "21 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04906v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04906v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04906v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04902v1",
    "updated": "2024-02-07T14:35:05+00:00",
    "published": "2024-02-07T14:35:05+00:00",
    "title": "L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ",
    "authors": [
      {
        "name": "Hyesung Jeon"
      },
      {
        "name": "Yulhwa Kim"
      },
      {
        "name": "Jae-joon Kim"
      }
    ],
    "summary": "Post-training quantization (PTQ) and quantization-aware training (QAT)\nmethods are gaining popularity in mitigating the high memory and computational\ncosts associated with Large Language Models (LLMs). In resource-constrained\nscenarios, PTQ, with its reduced training overhead, is often preferred over\nQAT, despite the latter's potential for higher accuracy. Meanwhile,\nparameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA)\nhave been introduced, and recent efforts have explored quantization-aware PEFT\ntechniques. However, these approaches may lack generality due to their reliance\non the pre-quantized model's configuration. Their effectiveness may be\ncompromised by non-linearly quantized or mixed-precision weights, and the\nretraining of specific quantization parameters might impede optimal\nperformance. To address these challenges, we propose L4Q, an algorithm for\nparameter-efficient quantization-aware training. L4Q leverages LoRA-wise\nlearned quantization step size for LLMs, aiming to enhance generality. The\nsimultaneous quantization-and-fine-tuning process of L4Q is applicable to\nhigh-precision models, yielding linearly quantized weights with superior\naccuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families\nusing an instructional dataset, showcase L4Q's capabilities in language\ncomprehension and few-shot in-context learning, achieving sub-4-bit precision\nwhile maintaining comparable training times to applying PEFT on a quantized\nmodel.",
    "comment": "8 pages, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04902v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04902v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04902v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04898v1",
    "updated": "2024-02-07T14:28:04+00:00",
    "published": "2024-02-07T14:28:04+00:00",
    "title": "The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer",
    "authors": [
      {
        "name": "Gregory Everett"
      },
      {
        "name": "Ryan Beal"
      },
      {
        "name": "Tim Matthews"
      },
      {
        "name": "Timothy J. Norman"
      },
      {
        "name": "Sarvapali D. Ramchurn"
      }
    ],
    "summary": "In this paper, we present a novel sequential team selection model in soccer.\nSpecifically, we model the stochastic process of player injury and\nunavailability using player-specific information learned from real-world soccer\ndata. Monte-Carlo Tree Search is used to select teams for games that optimise\nlong-term team performance across a soccer season by reasoning over player\ninjury probability. We validate our approach compared to benchmark solutions\nfor the 2018/19 English Premier League season. Our model achieves similar\nseason expected points to the benchmark whilst reducing first-team injuries by\n~13% and the money inefficiently spent on injured players by ~11% -\ndemonstrating the potential to reduce costs and improve player welfare in\nreal-world soccer teams.",
    "comment": "19 pages (16 main, 2 references, 1 appendix), 10 figures (9 main, 1\n  appendix). Accepted at the MIT Sloan Sports Analytics Conference 2024\n  Research Paper Competition",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04898v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04898v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04898v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04894v1",
    "updated": "2024-02-07T14:24:41+00:00",
    "published": "2024-02-07T14:24:41+00:00",
    "title": "Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning",
    "authors": [
      {
        "name": "Apoorva Vashisth"
      },
      {
        "name": "Julius R\u00fcckin"
      },
      {
        "name": "Federico Magistri"
      },
      {
        "name": "Cyrill Stachniss"
      },
      {
        "name": "Marija Popovi\u0107"
      }
    ],
    "summary": "Autonomous robots are often employed for data collection due to their\nefficiency and low labour costs. A key task in robotic data acquisition is\nplanning paths through an initially unknown environment to collect observations\ngiven platform-specific resource constraints, such as limited battery life.\nAdaptive online path planning in 3D environments is challenging due to the\nlarge set of valid actions and the presence of unknown occlusions. To address\nthese issues, we propose a novel deep reinforcement learning approach for\nadaptively replanning robot paths to map targets of interest in unknown 3D\nenvironments. A key aspect of our approach is a dynamically constructed graph\nthat restricts planning actions local to the robot, allowing us to quickly\nreact to newly discovered obstacles and targets of interest. For replanning, we\npropose a new reward function that balances between exploring the unknown\nenvironment and exploiting online-collected data about the targets of interest.\nOur experiments show that our method enables more efficient target detection\ncompared to state-of-the-art learning and non-learning baselines. We also show\nthe applicability of our approach for orchard monitoring using an unmanned\naerial vehicle in a photorealistic simulator.",
    "comment": "8 pages, 6 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04894v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04894v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04894v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04892v1",
    "updated": "2024-02-07T14:24:04+00:00",
    "published": "2024-02-07T14:24:04+00:00",
    "title": "A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration",
    "authors": [
      {
        "name": "Paolo Morettin"
      },
      {
        "name": "Andrea Passerini"
      },
      {
        "name": "Roberto Sebastiani"
      }
    ],
    "summary": "The probabilistic formal verification (PFV) of AI systems is in its infancy.\nSo far, approaches have been limited to ad-hoc algorithms for specific classes\nof models and/or properties.\n  We propose a unifying framework for the PFV of AI systems based onWeighted\nModel Integration (WMI), which allows to frame the problem in very general\nterms.\n  Crucially, this reduction enables the verification of many properties of\ninterest, like fairness, robustness or monotonicity, over a wide range of\nmachine learning models, without making strong distributional assumptions.\n  We support the generality of the approach by solving multiple verification\ntasks with a single, off-the-shelf WMI solver, then discuss the scalability\nchallenges and research directions related to this promising framework.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04892v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04892v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04892v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06674v1",
    "updated": "2024-02-07T14:23:01+00:00",
    "published": "2024-02-07T14:23:01+00:00",
    "title": "Understanding Practical Membership Privacy of Deep Learning",
    "authors": [
      {
        "name": "Marlon Tobaben"
      },
      {
        "name": "Gauri Pradhan"
      },
      {
        "name": "Yuan He"
      },
      {
        "name": "Joonas J\u00e4lk\u00f6"
      },
      {
        "name": "Antti Honkela"
      }
    ],
    "summary": "We apply a state-of-the-art membership inference attack (MIA) to\nsystematically test the practical privacy vulnerability of fine-tuning large\nimage classification models.We focus on understanding the properties of data\nsets and samples that make them vulnerable to membership inference. In terms of\ndata set properties, we find a strong power law dependence between the number\nof examples per class in the data and the MIA vulnerability, as measured by\ntrue positive rate of the attack at a low false positive rate. For an\nindividual sample, large gradients at the end of training are strongly\ncorrelated with MIA vulnerability.",
    "comment": "21 pages, 8 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06674v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06674v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06674v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04875v1",
    "updated": "2024-02-07T14:16:28+00:00",
    "published": "2024-02-07T14:16:28+00:00",
    "title": "On Provable Length and Compositional Generalization",
    "authors": [
      {
        "name": "Kartik Ahuja"
      },
      {
        "name": "Amin Mansouri"
      }
    ],
    "summary": "Length generalization -- the ability to generalize to longer sequences than\nones seen during training, and compositional generalization -- the ability to\ngeneralize to token combinations not seen during training, are crucial forms of\nout-of-distribution generalization in sequence-to-sequence models. In this\nwork, we take the first steps towards provable length and compositional\ngeneralization for a range of architectures, including deep sets, transformers,\nstate space models, and simple recurrent neural nets. Depending on the\narchitecture, we prove different degrees of representation identification,\ne.g., a linear or a permutation relation with ground truth representation, is\nnecessary for length and compositional generalization.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04875v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04875v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04875v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04869v1",
    "updated": "2024-02-07T14:09:34+00:00",
    "published": "2024-02-07T14:09:34+00:00",
    "title": "Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy",
    "authors": [
      {
        "name": "Ruichu Cai"
      },
      {
        "name": "Siyang Huang"
      },
      {
        "name": "Jie Qiao"
      },
      {
        "name": "Wei Chen"
      },
      {
        "name": "Yan Zeng"
      },
      {
        "name": "Keli Zhang"
      },
      {
        "name": "Fuchun Sun"
      },
      {
        "name": "Yang Yu"
      },
      {
        "name": "Zhifeng Hao"
      }
    ],
    "summary": "As a key component to intuitive cognition and reasoning solutions in human\nintelligence, causal knowledge provides great potential for reinforcement\nlearning (RL) agents' interpretability towards decision-making by helping\nreduce the searching space. However, there is still a considerable gap in\ndiscovering and incorporating causality into RL, which hinders the rapid\ndevelopment of causal RL. In this paper, we consider explicitly modeling the\ngeneration process of states with the causal graphical model, based on which we\naugment the policy. We formulate the causal structure updating into the RL\ninteraction process with active intervention learning of the environment. To\noptimize the derived objective, we propose a framework with theoretical\nperformance guarantees that alternates between two steps: using interventions\nfor causal structure learning during exploration and using the learned causal\nstructure for policy guidance during exploitation. Due to the lack of public\nbenchmarks that allow direct intervention in the state space, we design the\nroot cause localization task in our simulated fault alarm environment and then\nempirically show the effectiveness and robustness of the proposed method\nagainst state-of-the-art baselines. Theoretical analysis shows that our\nperformance improvement attributes to the virtuous cycle of causal-guided\npolicy learning and causal structure learning, which aligns with our\nexperimental results.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04869v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04869v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04869v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.06673v1",
    "updated": "2024-02-07T14:09:11+00:00",
    "published": "2024-02-07T14:09:11+00:00",
    "title": "Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain",
    "authors": [
      {
        "name": "Yongchen Zhou"
      },
      {
        "name": "Richard Jiang"
      }
    ],
    "summary": "The intersection of Artificial Intelligence (AI) and neuroscience in\nExplainable AI (XAI) is pivotal for enhancing transparency and interpretability\nin complex decision-making processes. This paper explores the evolution of XAI\nmethodologies, ranging from feature-based to human-centric approaches, and\ndelves into their applications in diverse domains, including healthcare and\nfinance. The challenges in achieving explainability in generative models,\nensuring responsible AI practices, and addressing ethical implications are\ndiscussed. The paper further investigates the potential convergence of XAI with\ncognitive sciences, the development of emotionally intelligent AI, and the\nquest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards\nArtificial General Intelligence (AGI), considerations of consciousness, ethics,\nand societal impact become paramount. The ongoing pursuit of deciphering the\nmysteries of the brain with AI and the quest for HLI represent transformative\nendeavors, bridging technical advancements with multidisciplinary explorations\nof human cognition.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.06673v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.06673v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.06673v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04858v1",
    "updated": "2024-02-07T13:55:27+00:00",
    "published": "2024-02-07T13:55:27+00:00",
    "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay",
    "authors": [
      {
        "name": "Natasha Butt"
      },
      {
        "name": "Blazej Manczak"
      },
      {
        "name": "Auke Wiggers"
      },
      {
        "name": "Corrado Rainone"
      },
      {
        "name": "David Zhang"
      },
      {
        "name": "Micha\u00ebl Defferrard"
      },
      {
        "name": "Taco Cohen"
      }
    ],
    "summary": "Large language models are increasingly solving tasks that are commonly\nbelieved to require human-level reasoning ability. However, these models still\nperform very poorly on benchmarks of general intelligence such as the\nAbstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a\nprogramming-by-examples problem, and introduce a novel and scalable method for\nlanguage model self-improvement called Code Iteration (CodeIt). Our method\niterates between 1) program sampling and hindsight relabeling, and 2) learning\nfrom prioritized experience replay. By relabeling the goal of an episode (i.e.,\nthe target program output given input) to the realized output produced by the\nsampled program, our method effectively deals with the extreme sparsity of\nrewards in program synthesis. Applying CodeIt to the ARC dataset, we\ndemonstrate that prioritized hindsight replay, along with pre-training and\ndata-augmentation, leads to successful inter-task generalization. CodeIt is the\nfirst neuro-symbolic approach that scales to the full ARC evaluation dataset.\nOur method solves 15% of ARC evaluation tasks, achieving state-of-the-art\nperformance and outperforming existing neural and symbolic baselines.",
    "comment": "8 pages, 11 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04858v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04858v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04858v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04856v1",
    "updated": "2024-02-07T13:54:38+00:00",
    "published": "2024-02-07T13:54:38+00:00",
    "title": "Explaining Learned Reward Functions with Counterfactual Trajectories",
    "authors": [
      {
        "name": "Jan Wehner"
      },
      {
        "name": "Frans Oliehoek"
      },
      {
        "name": "Luciano Cavalcante Siebert"
      }
    ],
    "summary": "Learning rewards from human behaviour or feedback is a promising approach to\naligning AI systems with human values but fails to consistently extract correct\nreward functions. Interpretability tools could enable users to understand and\nevaluate possible flaws in learned reward functions. We propose Counterfactual\nTrajectory Explanations (CTEs) to interpret reward functions in reinforcement\nlearning by contrasting an original with a counterfactual partial trajectory\nand the rewards they each receive. We derive six quality criteria for CTEs and\npropose a novel Monte-Carlo-based algorithm for generating CTEs that optimises\nthese quality criteria. Finally, we measure how informative the generated\nexplanations are to a proxy-human model by training it on CTEs. CTEs are\ndemonstrably informative for the proxy-human model, increasing the similarity\nbetween its predictions and the reward function on unseen trajectories.\nFurther, it learns to accurately judge differences in rewards between\ntrajectories and generalises to out-of-distribution examples. Although CTEs do\nnot lead to a perfect understanding of the reward, our method, and more\ngenerally the adaptation of XAI methods, are presented as a fruitful approach\nfor interpreting learned reward functions.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04856v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04856v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04856v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04854v1",
    "updated": "2024-02-07T13:54:06+00:00",
    "published": "2024-02-07T13:54:06+00:00",
    "title": "Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey",
    "authors": [
      {
        "name": "Jinghong Li"
      },
      {
        "name": "Huy Phan"
      },
      {
        "name": "Wen Gu"
      },
      {
        "name": "Koichi Ota"
      },
      {
        "name": "Shinobu Hasegawa"
      }
    ],
    "summary": "Research surveys have always posed a challenge for beginner researchers who\nlack of research training. These researchers struggle to understand the\ndirections within their research topic, and the discovery of new research\nfindings within a short time. One way to provide intuitive assistance to\nbeginner researchers is by offering relevant knowledge graphs(KG) and\nrecommending related academic papers. However, existing navigation knowledge\ngraphs primarily rely on keywords in the research field and often fail to\npresent the logical hierarchy among multiple related papers clearly. Moreover,\nmost recommendation systems for academic papers simply rely on high text\nsimilarity, which can leave researchers confused as to why a particular article\nis being recommended. They may lack of grasp important information about the\ninsight connection between \"Issue resolved\" and \"Issue finding\" that they hope\nto obtain. To address these issues, this study aims to support research insight\nsurveys for beginner researchers by establishing a hierarchical tree-structured\nknowledge graph that reflects the inheritance insight of research topics and\nthe relevance insight among the academic papers.",
    "comment": "This paper will submit to '27th International Symposium on\n  Methodologies for Intelligent Systems'(ISMIS 2024)",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.DL",
    "categories": [
      "cs.DL",
      "cs.CL",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04854v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04854v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04854v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05153v1",
    "updated": "2024-02-07T13:51:33+00:00",
    "published": "2024-02-07T13:51:33+00:00",
    "title": "Estimating On-road Transportation Carbon Emissions from Open Data of Road Network and Origin-destination Flow Data",
    "authors": [
      {
        "name": "Jinwei Zeng"
      },
      {
        "name": "Yu Liu"
      },
      {
        "name": "Jingtao Ding"
      },
      {
        "name": "Jian Yuan"
      },
      {
        "name": "Yong Li"
      }
    ],
    "summary": "Accounting for over 20% of the total carbon emissions, the precise estimation\nof on-road transportation carbon emissions is crucial for carbon emission\nmonitoring and efficient mitigation policy formulation. However, existing\nestimation methods typically depend on hard-to-collect individual statistics of\nvehicle miles traveled to calculate emissions, thereby suffering from high data\ncollection difficulty. To relieve this issue by utilizing the strong pattern\nrecognition of artificial intelligence, we incorporate two sources of open data\nrepresentative of the transportation demand and capacity factors, the\norigin-destination (OD) flow data and the road network data, to build a\nhierarchical heterogeneous graph learning method for on-road carbon emission\nestimation (HENCE). Specifically, a hierarchical graph consisting of the road\nnetwork level, community level, and region level is constructed to model the\nmulti-scale road network-based connectivity and travel connection between\nspatial areas. Heterogeneous graphs consisting of OD links and spatial links\nare further built at both the community level and region level to capture the\nintrinsic interactions between travel demand and road network accessibility.\nExtensive experiments on two large-scale real-world datasets demonstrate\nHENCE's effectiveness and superiority with R-squared exceeding 0.75 and\noutperforming baselines by 9.60% on average, validating its success in\npioneering the use of artificial intelligence to empower carbon emission\nmanagement and sustainability development. The implementation codes are\navailable at this link: https://github.com/tsinghua-fib-lab/HENCE.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05153v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05153v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05153v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04852v1",
    "updated": "2024-02-07T13:51:26+00:00",
    "published": "2024-02-07T13:51:26+00:00",
    "title": "Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning",
    "authors": [
      {
        "name": "Yuxuan Bian"
      },
      {
        "name": "Xuan Ju"
      },
      {
        "name": "Jiangtong Li"
      },
      {
        "name": "Zhijian Xu"
      },
      {
        "name": "Dawei Cheng"
      },
      {
        "name": "Qiang Xu"
      }
    ],
    "summary": "In this study, we present aLLM4TS, an innovative framework that adapts Large\nLanguage Models (LLMs) for time-series representation learning. Central to our\napproach is that we reconceive time-series forecasting as a self-supervised,\nmulti-patch prediction task, which, compared to traditional\nmask-and-reconstruction methods, captures temporal dynamics in patch\nrepresentations more effectively. Our strategy encompasses two-stage training:\n(i). a causal continual pre-training phase on various time-series datasets,\nanchored on next patch prediction, effectively syncing LLM capabilities with\nthe intricacies of time-series data; (ii). fine-tuning for multi-patch\nprediction in the targeted time-series context. A distinctive element of our\nframework is the patch-wise decoding layer, which departs from previous methods\nreliant on sequence-level decoding. Such a design directly transposes\nindividual patches into temporal sequences, thereby significantly bolstering\nthe model's proficiency in mastering temporal patch-based representations.\naLLM4TS demonstrates superior performance in several downstream tasks, proving\nits effectiveness in deriving temporal representations with enhanced\ntransferability and marking a pivotal advancement in the adaptation of LLMs for\ntime-series analysis.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04852v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04852v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04852v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04845v1",
    "updated": "2024-02-07T13:44:47+00:00",
    "published": "2024-02-07T13:44:47+00:00",
    "title": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
    "authors": [
      {
        "name": "Bowen Jing"
      },
      {
        "name": "Bonnie Berger"
      },
      {
        "name": "Tommi Jaakkola"
      }
    ],
    "summary": "The biological functions of proteins often depend on dynamic structural\nensembles. In this work, we develop a flow-based generative modeling approach\nfor learning and sampling the conformational landscapes of proteins. We\nrepurpose highly accurate single-state predictors such as AlphaFold and ESMFold\nand fine-tune them under a custom flow matching framework to obtain\nsequence-conditoned generative models of protein structure called AlphaFlow and\nESMFlow. When trained and evaluated on the PDB, our method provides a superior\ncombination of precision and diversity compared to AlphaFold with MSA\nsubsampling. When further trained on ensembles from all-atom MD, our method\naccurately captures conformational flexibility, positional distributions, and\nhigher-order ensemble observables for unseen proteins. Moreover, our method can\ndiversify a static PDB structure with faster wall-clock convergence to certain\nequilibrium properties than replicate MD trajectories, demonstrating its\npotential as a proxy for expensive physics-based simulations. Code is available\nat https://github.com/bjing2016/alphaflow.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.BM",
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04845v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04845v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04845v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04838v3",
    "updated": "2024-02-14T12:51:56+00:00",
    "published": "2024-02-07T13:39:38+00:00",
    "title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition",
    "authors": [
      {
        "name": "Jinghui Lu"
      },
      {
        "name": "Ziwei Yang"
      },
      {
        "name": "Yanjie Wang"
      },
      {
        "name": "Xuejing Liu"
      },
      {
        "name": "Can Huang"
      }
    ],
    "summary": "In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04838v3",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04838v3",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04838v3"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04836v1",
    "updated": "2024-02-07T13:32:53+00:00",
    "published": "2024-02-07T13:32:53+00:00",
    "title": "On the Completeness of Invariant Geometric Deep Learning Models",
    "authors": [
      {
        "name": "Zian Li"
      },
      {
        "name": "Xiyuan Wang"
      },
      {
        "name": "Shijia Kang"
      },
      {
        "name": "Muhan Zhang"
      }
    ],
    "summary": "Invariant models, one important class of geometric deep learning models, are\ncapable of generating meaningful geometric representations by leveraging\ninformative geometric features. These models are characterized by their\nsimplicity, good experimental results and computational efficiency. However,\ntheir theoretical expressive power still remains unclear, restricting a deeper\nunderstanding of the potential of such models. In this work, we concentrate on\ncharacterizing the theoretical expressiveness of invariant models. We first\nrigorously bound the expressiveness of the most classical invariant model,\nVanilla DisGNN (message passing neural networks incorporating distance),\nrestricting its unidentifiable cases to be only those highly symmetric\ngeometric graphs. To break these corner cases' symmetry, we introduce a simple\nyet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN.\nLeveraging GeoNGNN as a theoretical tool, we for the first time prove the\nE(3)-completeness of three well-established geometric models: DimeNet, GemNet\nand SphereNet. Our results fill the gap in the theoretical power of invariant\nmodels, contributing to a rigorous and comprehensive understanding of their\ncapabilities. Experimentally, GeoNGNN exhibits good inductive bias in capturing\nlocal environments, and achieves competitive results w.r.t. complicated models\nrelying on high-order invariant/equivariant representations while exhibiting\nsignificantly faster computational speed.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04836v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04836v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04836v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04835v1",
    "updated": "2024-02-07T13:32:47+00:00",
    "published": "2024-02-07T13:32:47+00:00",
    "title": "SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning",
    "authors": [
      {
        "name": "Darshana Saravanan"
      },
      {
        "name": "Naresh Manwani"
      },
      {
        "name": "Vineet Gandhi"
      }
    ],
    "summary": "Partial label learning (PLL) is a weakly-supervised learning paradigm where\neach training instance is paired with a set of candidate labels (partial\nlabel), one of which is the true label. Noisy PLL (NPLL) relaxes this\nconstraint by allowing some partial labels to not contain the true label,\nenhancing the practicality of the problem. Our work centers on NPLL and\npresents a minimalistic framework called SARI that initially assigns\npseudo-labels to images by exploiting the noisy partial labels through a\nweighted nearest neighbour algorithm. These pseudo-label and image pairs are\nthen used to train a deep neural network classifier with label smoothing and\nstandard regularization techniques. The classifier's features and predictions\nare subsequently employed to refine and enhance the accuracy of pseudo-labels.\nSARI combines the strengths of Average Based Strategies (in pseudo labelling)\nand Identification Based Strategies (in classifier training) from the\nliterature. We perform thorough experiments on seven datasets and compare SARI\nagainst nine NPLL and PLL methods from the prior art. SARI achieves\nstate-of-the-art results in almost all studied settings, obtaining substantial\ngains in fine-grained classification and extreme noise settings.",
    "comment": "13 pages, 6 tables, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04835v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04835v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04835v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04832v1",
    "updated": "2024-02-07T13:31:59+00:00",
    "published": "2024-02-07T13:31:59+00:00",
    "title": "Structured d-DNNF Is Not Closed Under Negation",
    "authors": [
      {
        "name": "Harry Vinall-Smeeth"
      }
    ],
    "summary": "Both structured d-DNNF and SDD can be exponentially more succinct than OBDD.\nMoreover, SDD is essentially as tractable as OBDD. But this has left two\nimportant open questions. Firstly, does OBDD support more tractable\ntransformations than structured d-DNNF? And secondly, is structured d-DNNF more\nsuccinct than SDD? In this paper, we answer both questions in the affirmative.\nFor the first question we show that, unlike OBDD, structured d-DNNF does not\nsupport polytime negation, disjunction, or existential quantification\noperations. As a corollary, we deduce that there are functions with an\nequivalent polynomial-sized structured d-DNNF but with no such representation\nas an SDD, thus answering the second question. We also lift this second result\nto arithmetic circuits (AC) to show a succinctness gap between PSDD and the\nmonotone AC analogue to structured d-DNNF.",
    "comment": "9 pages, 2 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04832v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04832v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04832v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04830v1",
    "updated": "2024-02-07T13:26:10+00:00",
    "published": "2024-02-07T13:26:10+00:00",
    "title": "Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming",
    "authors": [
      {
        "name": "Giacomo Acciarini"
      },
      {
        "name": "At\u0131l\u0131m G\u00fcne\u015f Baydin"
      },
      {
        "name": "Dario Izzo"
      }
    ],
    "summary": "The Simplified General Perturbations 4 (SGP4) orbital propagation method is\nwidely used for predicting the positions and velocities of Earth-orbiting\nobjects rapidly and reliably. Despite continuous refinement, SGP models still\nlack the precision of numerical propagators, which offer significantly smaller\nerrors. This study presents dSGP4, a novel differentiable version of SGP4\nimplemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates\nvarious space-related applications, including spacecraft orbit determination,\nstate conversion, covariance transformation, state transition matrix\ncomputation, and covariance propagation. Additionally, dSGP4's PyTorch\nimplementation allows for embarrassingly parallel orbital propagation across\nbatches of Two-Line Element Sets (TLEs), leveraging the computational power of\nCPUs, GPUs, and advanced hardware for distributed prediction of satellite\npositions at future times. Furthermore, dSGP4's differentiability enables\nintegration with modern machine learning techniques. Thus, we propose a novel\norbital propagation paradigm, ML-dSGP4, where neural networks are integrated\ninto the orbital propagator. Through stochastic gradient descent, this combined\nmodel's inputs, outputs, and parameters can be iteratively refined, surpassing\nSGP4's precision. Neural networks act as identity operators by default,\nadhering to SGP4's behavior. However, dSGP4's differentiability allows\nfine-tuning with ephemeris data, enhancing precision while maintaining\ncomputational speed. This empowers satellite operators and researchers to train\nthe model using specific ephemeris or high-precision numerical propagation\ndata, significantly advancing orbital prediction capabilities.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "astro-ph.EP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04830v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04830v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04830v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04825v2",
    "updated": "2024-02-08T06:26:54+00:00",
    "published": "2024-02-07T13:23:25+00:00",
    "title": "Fast Timing-Conditioned Latent Audio Diffusion",
    "authors": [
      {
        "name": "Zach Evans"
      },
      {
        "name": "CJ Carr"
      },
      {
        "name": "Josiah Taylor"
      },
      {
        "name": "Scott H. Hawley"
      },
      {
        "name": "Jordi Pons"
      }
    ],
    "summary": "Generating long-form 44.1kHz stereo audio from text prompts can be\ncomputationally demanding. Further, most previous works do not tackle that\nmusic and sound effects naturally vary in their duration. Our research focuses\non the efficient generation of long-form, variable-length stereo music and\nsounds at 44.1kHz using text prompts with a generative model. Stable Audio is\nbased on latent diffusion, with its latent defined by a fully-convolutional\nvariational autoencoder. It is conditioned on text prompts as well as timing\nembeddings, allowing for fine control over both the content and length of the\ngenerated music and sounds. Stable Audio is capable of rendering stereo signals\nof up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute\nefficiency and fast inference, it is one of the best in two public\ntext-to-music and -audio benchmarks and, differently from state-of-the-art\nmodels, can generate music with structure and stereo sounds.",
    "comment": "Code: https://github.com/Stability-AI/stable-audio-tools. Metrics:\n  https://github.com/Stability-AI/stable-audio-metrics. Demo:\n  https://stability-ai.github.io/stable-audio-demo",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SD",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04825v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04825v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04825v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04823v1",
    "updated": "2024-02-07T13:22:05+00:00",
    "published": "2024-02-07T13:22:05+00:00",
    "title": "How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data",
    "authors": [
      {
        "name": "Mihaela C\u0103t\u0103lina Stoian"
      },
      {
        "name": "Salijona Dyrmishi"
      },
      {
        "name": "Maxime Cordy"
      },
      {
        "name": "Thomas Lukasiewicz"
      },
      {
        "name": "Eleonora Giunchiglia"
      }
    ],
    "summary": "Deep Generative Models (DGMs) have been shown to be powerful tools for\ngenerating tabular data, as they have been increasingly able to capture the\ncomplex distributions that characterize them. However, to generate realistic\nsynthetic data, it is often not enough to have a good approximation of their\ndistribution, as it also requires compliance with constraints that encode\nessential background knowledge on the problem at hand. In this paper, we\naddress this limitation and show how DGMs for tabular data can be transformed\ninto Constrained Deep Generative Models (C-DGMs), whose generated samples are\nguaranteed to be compliant with the given constraints. This is achieved by\nautomatically parsing the constraints and transforming them into a Constraint\nLayer (CL) seamlessly integrated with the DGM. Our extensive experimental\nanalysis with various DGMs and tasks reveals that standard DGMs often violate\nconstraints, some exceeding $95\\%$ non-compliance, while their corresponding\nC-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at\ntraining time, C-DGMs are able to exploit the background knowledge expressed by\nthe constraints to outperform their standard counterparts with up to $6.5\\%$\nimprovement in utility and detection. Further, we show how our CL does not\nnecessarily need to be integrated at training time, as it can be also used as a\nguardrail at inference time, still producing some improvements in the overall\nperformance of the models. Finally, we show that our CL does not hinder the\nsample generation time of the models.",
    "comment": "Accepted at ICLR 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04823v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04823v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04823v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04821v1",
    "updated": "2024-02-07T13:21:41+00:00",
    "published": "2024-02-07T13:21:41+00:00",
    "title": "E(3)-Equivariant Mesh Neural Networks",
    "authors": [
      {
        "name": "Thuan Trang"
      },
      {
        "name": "Nhat Khang Ngo"
      },
      {
        "name": "Daniel Levy"
      },
      {
        "name": "Thieu N. Vo"
      },
      {
        "name": "Siamak Ravanbakhsh"
      },
      {
        "name": "Truong Son Hy"
      }
    ],
    "summary": "Triangular meshes are widely used to represent three-dimensional objects. As\na result, many recent works have address the need for geometric deep learning\non 3D mesh. However, we observe that the complexities in many of these\narchitectures does not translate to practical performance, and simple deep\nmodels for geometric graphs are competitive in practice. Motivated by this\nobservation, we minimally extend the update equations of E(n)-Equivariant Graph\nNeural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face\ninformation, and further improve it to account for long-range interactions\nthrough hierarchy. The resulting architecture, Equivariant Mesh Neural Network\n(EMNN), outperforms other, more complicated equivariant methods on mesh tasks,\nwith a fast run-time and no expensive pre-processing.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04821v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04821v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04821v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05151v1",
    "updated": "2024-02-07T13:09:23+00:00",
    "published": "2024-02-07T13:09:23+00:00",
    "title": "CrashFormer: A Multimodal Architecture to Predict the Risk of Crash",
    "authors": [
      {
        "name": "Amin Karimi Monsefi"
      },
      {
        "name": "Pouya Shiri"
      },
      {
        "name": "Ahmad Mohammadshirazi"
      },
      {
        "name": "Nastaran Karimi Monsefi"
      },
      {
        "name": "Ron Davies"
      },
      {
        "name": "Sobhan Moosavi"
      },
      {
        "name": "Rajiv Ramnath"
      }
    ],
    "summary": "Reducing traffic accidents is a crucial global public safety concern.\nAccident prediction is key to improving traffic safety, enabling proactive\nmeasures to be taken before a crash occurs, and informing safety policies,\nregulations, and targeted interventions. Despite numerous studies on accident\nprediction over the past decades, many have limitations in terms of\ngeneralizability, reproducibility, or feasibility for practical use due to\ninput data or problem formulation. To address existing shortcomings, we propose\nCrashFormer, a multi-modal architecture that utilizes comprehensive (but\nrelatively easy to obtain) inputs such as the history of accidents, weather\ninformation, map images, and demographic information. The model predicts the\nfuture risk of accidents on a reasonably acceptable cadence (i.e., every six\nhours) for a geographical location of 5.161 square kilometers. CrashFormer is\ncomposed of five components: a sequential encoder to utilize historical\naccidents and weather data, an image encoder to use map imagery data, a raw\ndata encoder to utilize demographic information, a feature fusion module for\naggregating the encoded features, and a classifier that accepts the aggregated\ndata and makes predictions accordingly. Results from extensive real-world\nexperiments in 10 major US cities show that CrashFormer outperforms\nstate-of-the-art sequential and non-sequential models by 1.8% in F1-score on\naverage when using ``sparse'' input data.",
    "comment": "The paper is accepted In 1st ACM SIGSPATIAL International Workshop on\n  Advances in Urban-AI (UrbanAI 23), November 13, 2023, Hamburg, Germany",
    "journal_ref": null,
    "doi": "10.1145/3615900.3628769",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1145/3615900.3628769",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.05151v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05151v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05151v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04814v1",
    "updated": "2024-02-07T13:04:35+00:00",
    "published": "2024-02-07T13:04:35+00:00",
    "title": "BOWLL: A Deceptively Simple Open World Lifelong Learner",
    "authors": [
      {
        "name": "Roshni Kamath"
      },
      {
        "name": "Rupert Mitchell"
      },
      {
        "name": "Subarnaduti Paul"
      },
      {
        "name": "Kristian Kersting"
      },
      {
        "name": "Martin Mundt"
      }
    ],
    "summary": "The quest to improve scalar performance numbers on predetermined benchmarks\nseems to be deeply engraved in deep learning. However, the real world is seldom\ncarefully curated and applications are seldom limited to excelling on test\nsets. A practical system is generally required to recognize novel concepts,\nrefrain from actively including uninformative data, and retain previously\nacquired knowledge throughout its lifetime. Despite these key elements being\nrigorously researched individually, the study of their conjunction, open world\nlifelong learning, is only a recent trend. To accelerate this multifaceted\nfield's exploration, we introduce its first monolithic and much-needed\nbaseline. Leveraging the ubiquitous use of batch normalization across deep\nneural networks, we propose a deceptively simple yet highly effective way to\nrepurpose standard models for open world lifelong learning. Through extensive\nempirical evaluation, we highlight why our approach should serve as a future\nstandard for models that are able to effectively maintain their knowledge,\nselectively focus on informative data, and accelerate future learning.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04814v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04814v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04814v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05150v1",
    "updated": "2024-02-07T12:54:15+00:00",
    "published": "2024-02-07T12:54:15+00:00",
    "title": "Designing deep neural networks for driver intention recognition",
    "authors": [
      {
        "name": "Koen Vellenga"
      },
      {
        "name": "H. Joe Steinhauer"
      },
      {
        "name": "Alexander Karlsson"
      },
      {
        "name": "G\u00f6ran Falkman"
      },
      {
        "name": "Asli Rhodin"
      },
      {
        "name": "Ashok Koppisetty"
      }
    ],
    "summary": "Driver intention recognition studies increasingly rely on deep neural\nnetworks. Deep neural networks have achieved top performance for many different\ntasks, but it is not a common practice to explicitly analyse the complexity and\nperformance of the network's architecture. Therefore, this paper applies neural\narchitecture search to investigate the effects of the deep neural network\narchitecture on a real-world safety critical application with limited\ncomputational capabilities. We explore a pre-defined search space for three\ndeep neural network layer types that are capable to handle sequential data (a\nlong-short term memory, temporal convolution, and a time-series transformer\nlayer), and the influence of different data fusion strategies on the driver\nintention recognition performance. A set of eight search strategies are\nevaluated for two driver intention recognition datasets. For the two datasets,\nwe observed that there is no search strategy clearly sampling better deep\nneural network architectures. However, performing an architecture search does\nimprove the model performance compared to the original manually designed\nnetworks. Furthermore, we observe no relation between increased model\ncomplexity and higher driver intention recognition performance. The result\nindicate that multiple architectures yield similar performance, regardless of\nthe deep neural network layer type or fusion strategy.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05150v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05150v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05150v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04794v1",
    "updated": "2024-02-07T12:35:31+00:00",
    "published": "2024-02-07T12:35:31+00:00",
    "title": "Scalable Multi-view Clustering via Explicit Kernel Features Maps",
    "authors": [
      {
        "name": "Chakib Fettal"
      },
      {
        "name": "Lazhar Labiod"
      },
      {
        "name": "Mohamed Nadif"
      }
    ],
    "summary": "A growing awareness of multi-view learning as an important component in data\nscience and machine learning is a consequence of the increasing prevalence of\nmultiple views in real-world applications, especially in the context of\nnetworks. In this paper we introduce a new scalability framework for multi-view\nsubspace clustering. An efficient optimization strategy is proposed, leveraging\nkernel feature maps to reduce the computational burden while maintaining good\nclustering performance. The scalability of the algorithm means that it can be\napplied to large-scale datasets, including those with millions of data points,\nusing a standard machine, in a few minutes. We conduct extensive experiments on\nreal-world benchmark networks of various sizes in order to evaluate the\nperformance of our algorithm against state-of-the-art multi-view subspace\nclustering methods and attributed-network multi-view approaches.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04794v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04794v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04794v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04792v1",
    "updated": "2024-02-07T12:31:13+00:00",
    "published": "2024-02-07T12:31:13+00:00",
    "title": "Direct Language Model Alignment from Online AI Feedback",
    "authors": [
      {
        "name": "Shangmin Guo"
      },
      {
        "name": "Biao Zhang"
      },
      {
        "name": "Tianlin Liu"
      },
      {
        "name": "Tianqi Liu"
      },
      {
        "name": "Misha Khalman"
      },
      {
        "name": "Felipe Llinares"
      },
      {
        "name": "Alexandre Rame"
      },
      {
        "name": "Thomas Mesnard"
      },
      {
        "name": "Yao Zhao"
      },
      {
        "name": "Bilal Piot"
      },
      {
        "name": "Johan Ferret"
      },
      {
        "name": "Mathieu Blondel"
      }
    ],
    "summary": "Direct alignment from preferences (DAP) methods, such as DPO, have recently\nemerged as efficient alternatives to reinforcement learning from human feedback\n(RLHF), that do not require a separate reward model. However, the preference\ndatasets used in DAP methods are usually collected ahead of training and never\nupdated, thus the feedback is purely offline. Moreover, responses in these\ndatasets are often sampled from a language model distinct from the one being\naligned, and since the model evolves over training, the alignment phase is\ninevitably off-policy. In this study, we posit that online feedback is key and\nimproves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as\nannotator: on each training iteration, we sample two responses from the current\nmodel and prompt the LLM annotator to choose which one is preferred, thus\nproviding online feedback. Despite its simplicity, we demonstrate via human\nevaluation in several tasks that OAIF outperforms both offline DAP and RLHF\nmethods. We further show that the feedback leveraged in OAIF is easily\ncontrollable, via instruction prompts to the LLM annotator.",
    "comment": "18 pages, 8 figures, 4 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04792v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04792v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04792v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04788v1",
    "updated": "2024-02-07T12:28:32+00:00",
    "published": "2024-02-07T12:28:32+00:00",
    "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
    "authors": [
      {
        "name": "Dongping Chen"
      },
      {
        "name": "Ruoxi Chen"
      },
      {
        "name": "Shilin Zhang"
      },
      {
        "name": "Yinuo Liu"
      },
      {
        "name": "Yaochen Wang"
      },
      {
        "name": "Huichi Zhou"
      },
      {
        "name": "Qihui Zhang"
      },
      {
        "name": "Pan Zhou"
      },
      {
        "name": "Yao Wan"
      },
      {
        "name": "Lichao Sun"
      }
    ],
    "summary": "Multimodal Large Language Models (MLLMs) have gained significant attention\nrecently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges,\nprimarily due to the absence multimodal benchmarks that align with human\npreferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel\nbenchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting\njudges including three distinct tasks: Scoring Evaluation, Pair Comparison, and\nBatch Ranking. Our study reveals that, while MLLMs demonstrate remarkable\nhuman-like discernment in Pair Comparisons, there is a significant divergence\nfrom human preferences in Scoring Evaluation and Batch Ranking tasks.\nFurthermore, MLLMs still face challenges in judgment, including diverse biases,\nhallucinatory responses, and inconsistencies, even for advanced models such as\nGPT-4V. These findings emphasize the pressing need for enhancements and further\nresearch efforts regarding MLLMs as fully reliable evaluators. Code and dataset\nare available at https://github.com/Dongping-Chen/MLLM-as-a-Judge.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04788v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04788v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04788v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05973v1",
    "updated": "2024-02-07T12:26:56+00:00",
    "published": "2024-02-07T12:26:56+00:00",
    "title": "Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) Framework in UAV Networks",
    "authors": [
      {
        "name": "Sana Hafeez"
      },
      {
        "name": "Lina Mohjazi"
      },
      {
        "name": "Muhammad Ali Imran"
      },
      {
        "name": "Yao Sun"
      }
    ],
    "summary": "Privacy, scalability, and reliability are significant challenges in unmanned\naerial vehicle (UAV) networks as distributed systems, especially when employing\nmachine learning (ML) technologies with substantial data exchange. Recently,\nthe application of federated learning (FL) to UAV networks has improved\ncollaboration, privacy, resilience, and adaptability, making it a promising\nframework for UAV applications. However, implementing FL for UAV networks\nintroduces drawbacks such as communication overhead, synchronization issues,\nscalability limitations, and resource constraints. To address these challenges,\nthis paper presents the Blockchain-enabled Clustered and Scalable Federated\nLearning (BCS-FL) framework for UAV networks. This improves the\ndecentralization, coordination, scalability, and efficiency of FL in\nlarge-scale UAV networks. The framework partitions UAV networks into separate\nclusters, coordinated by cluster head UAVs (CHs), to establish a connected\ngraph. Clustering enables efficient coordination of updates to the ML model.\nAdditionally, hybrid inter-cluster and intra-cluster model aggregation schemes\ngenerate the global model after each training round, improving collaboration\nand knowledge sharing among clusters. The numerical findings illustrate the\nachievement of convergence while also emphasizing the trade-offs between the\neffectiveness of training and communication efficiency.",
    "comment": "6 pages, 7 figures, 2023 IEEE International Workshop on Computer\n  Aided Modeling and Design of Communication Links and Networks (IEEE CAMAD),\n  Edinburgh UK",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.NI",
      "eess.SP"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05973v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05973v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05973v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04785v1",
    "updated": "2024-02-07T12:15:56+00:00",
    "published": "2024-02-07T12:15:56+00:00",
    "title": "Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity",
    "authors": [
      {
        "name": "Alexander Tyurin"
      },
      {
        "name": "Marta Pozzi"
      },
      {
        "name": "Ivan Ilin"
      },
      {
        "name": "Peter Richt\u00e1rik"
      }
    ],
    "summary": "We consider nonconvex stochastic optimization problems in the asynchronous\ncentralized distributed setup where the communication times from workers to a\nserver can not be ignored, and the computation and communication times are\npotentially different for all workers. Using an unbiassed compression\ntechnique, we develop a new method-Shadowheart SGD-that provably improves the\ntime complexities of all previous centralized methods. Moreover, we show that\nthe time complexity of Shadowheart SGD is optimal in the family of centralized\nmethods with compressed communication. We also consider the bidirectional\nsetup, where broadcasting from the server to the workers is non-negligible, and\ndevelop a corresponding method.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "math.OC",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04785v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04785v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04785v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04783v1",
    "updated": "2024-02-07T12:06:52+00:00",
    "published": "2024-02-07T12:06:52+00:00",
    "title": "Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks",
    "authors": [
      {
        "name": "Hemanth Saratchandran"
      },
      {
        "name": "Shin-Fang Chng"
      },
      {
        "name": "Simon Lucey"
      }
    ],
    "summary": "Recently, neural networks utilizing periodic activation functions have been\nproven to demonstrate superior performance in vision tasks compared to\ntraditional ReLU-activated networks. However, there is still a limited\nunderstanding of the underlying reasons for this improved performance. In this\npaper, we aim to address this gap by providing a theoretical understanding of\nperiodically activated networks through an analysis of their Neural Tangent\nKernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the\nfinite width setting, using a fairly general network architecture which\nrequires only one wide layer that grows at least linearly with the number of\ndata samples. Our findings indicate that periodically activated networks are\n\\textit{notably more well-behaved}, from the NTK perspective, than ReLU\nactivated networks. Additionally, we give an application to the memorization\ncapacity of such networks and verify our theoretical predictions empirically.\nOur study offers a deeper understanding of the properties of periodically\nactivated neural networks and their potential in the field of deep learning.",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2402.02711",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04783v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04783v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04783v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04779v1",
    "updated": "2024-02-07T12:01:02+00:00",
    "published": "2024-02-07T12:01:02+00:00",
    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
    "authors": [
      {
        "name": "Qingyu Yin"
      },
      {
        "name": "Xuzheng He"
      },
      {
        "name": "Xiang Zhuang"
      },
      {
        "name": "Yu Zhao"
      },
      {
        "name": "Jianhua Yao"
      },
      {
        "name": "Xiaoyu Shen"
      },
      {
        "name": "Qiang Zhang"
      }
    ],
    "summary": "The decoder-only Transformer architecture with causal masking and relative\nposition encoding (RPE) has become the de facto choice in language modeling.\nDespite its exceptional performance across various tasks, we have identified\ntwo limitations: First, it requires all attention scores to be non-zero and sum\nup to 1, even if the current embedding has sufficient self-contained\ninformation. This compels the model to assign disproportional excessive\nattention to specific tokens. Second, RPE-based Transformers are not universal\napproximators due to their limited capacity at encoding absolute positional\ninformation, which limits their application in position-critical tasks. In this\nwork, we propose StableMask: a parameter-free method to address both\nlimitations by refining the causal mask. It introduces pseudo-attention values\nto balance attention distributions and encodes absolute positional information\nvia a progressively decreasing mask ratio. StableMask's effectiveness is\nvalidated both theoretically and empirically, showing significant enhancements\nin language models with parameter sizes ranging from 71M to 1.4B across diverse\ndatasets and encoding methods. We further show that it naturally supports (1)\nefficient extrapolation without special tricks such as StreamingLLM and (2)\neasy integration with existing attention optimization techniques.",
    "comment": "Preprint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04779v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04779v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04779v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04777v1",
    "updated": "2024-02-07T11:56:34+00:00",
    "published": "2024-02-07T11:56:34+00:00",
    "title": "A fast score-based search algorithm for maximal ancestral graphs using entropy",
    "authors": [
      {
        "name": "Zhongyi Hu"
      },
      {
        "name": "Robin Evans"
      }
    ],
    "summary": "\\emph{Maximal ancestral graph} (MAGs) is a class of graphical model that\nextend the famous \\emph{directed acyclic graph} in the presence of latent\nconfounders. Most score-based approaches to learn the unknown MAG from\nempirical data rely on BIC score which suffers from instability and heavy\ncomputations. We propose to use the framework of imsets\n\\citep{studeny2006probabilistic} to score MAGs using empirical entropy\nestimation and the newly proposed \\emph{refined Markov property}\n\\citep{hu2023towards}. Our graphical search procedure is similar to\n\\citet{claassen2022greedy} but improved from our theoretical results. We show\nthat our search algorithm is polynomial in number of nodes by restricting\ndegree, maximal head size and number of discriminating paths. In simulated\nexperiment, our algorithm shows superior performance compared to other state of\nart MAG learning algorithms.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04777v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04777v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04777v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04764v1",
    "updated": "2024-02-07T11:27:45+00:00",
    "published": "2024-02-07T11:27:45+00:00",
    "title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
    "authors": [
      {
        "name": "David Venuto"
      },
      {
        "name": "Sami Nur Islam"
      },
      {
        "name": "Martin Klissarov"
      },
      {
        "name": "Doina Precup"
      },
      {
        "name": "Sherry Yang"
      },
      {
        "name": "Ankit Anand"
      }
    ],
    "summary": "Pre-trained Vision-Language Models (VLMs) are able to understand visual\nconcepts, describe and decompose complex tasks into sub-tasks, and provide\nfeedback on task completion. In this paper, we aim to leverage these\ncapabilities to support the training of reinforcement learning (RL) agents. In\nprinciple, VLMs are well suited for this purpose, as they can naturally analyze\nimage-based observations and provide feedback (reward) on learning progress.\nHowever, inference in VLMs is computationally expensive, so querying them\nfrequently to compute rewards would significantly slowdown the training of an\nRL agent. To address this challenge, we propose a framework named Code as\nReward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through\ncode generation, thereby significantly reducing the computational burden of\nquerying the VLM directly. We show that the dense rewards generated through our\napproach are very accurate across a diverse set of discrete and continuous\nenvironments, and can be more effective in training RL policies than the\noriginal sparse environment rewards.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04764v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04764v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04764v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04763v1",
    "updated": "2024-02-07T11:26:53+00:00",
    "published": "2024-02-07T11:26:53+00:00",
    "title": "Emergence of specialized Collective Behaviors in Evolving Heterogeneous Swarms",
    "authors": [
      {
        "name": "Fuda van Diggelen"
      },
      {
        "name": "Matteo De Carlo"
      },
      {
        "name": "Nicolas Cambier"
      },
      {
        "name": "Eliseo Ferrante"
      },
      {
        "name": "A. E. Eiben"
      }
    ],
    "summary": "Natural groups of animals, such as swarms of social insects, exhibit\nastonishing degrees of task specialization, useful to address complex tasks and\nto survive. This is supported by phenotypic plasticity: individuals sharing the\nsame genotype that is expressed differently for different classes of\nindividuals, each specializing in one task. In this work, we evolve a swarm of\nsimulated robots with phenotypic plasticity to study the emergence of\nspecialized collective behavior during an emergent perception task. Phenotypic\nplasticity is realized in the form of heterogeneity of behavior by dividing the\ngenotype into two components, with one different neural network controller\nassociated to each component. The whole genotype, expressing the behavior of\nthe whole group through the two components, is subject to evolution with a\nsingle fitness function. We analyse the obtained behaviors and use the insights\nprovided by these results to design an online regulatory mechanism. Our\nexperiments show three main findings: 1) The sub-groups evolve distinct\nemergent behaviors. 2) The effectiveness of the whole swarm depends on the\ninteraction between the two sub-groups, leading to a more robust performance\nthan with singular sub-group behavior. 3) The online regulatory mechanism\nenhances overall performance and scalability.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04763v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04763v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04763v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04762v1",
    "updated": "2024-02-07T11:26:00+00:00",
    "published": "2024-02-07T11:26:00+00:00",
    "title": "Color Recognition in Challenging Lighting Environments: CNN Approach",
    "authors": [
      {
        "name": "Nizamuddin Maitlo"
      },
      {
        "name": "Nooruddin Noonari"
      },
      {
        "name": "Sajid Ahmed Ghanghro"
      },
      {
        "name": "Sathishkumar Duraisamy"
      },
      {
        "name": "Fayaz Ahmed"
      }
    ],
    "summary": "Light plays a vital role in vision either human or machine vision, the\nperceived color is always based on the lighting conditions of the surroundings.\nResearchers are working to enhance the color detection techniques for the\napplication of computer vision. They have implemented proposed several methods\nusing different color detection approaches but still, there is a gap that can\nbe filled. To address this issue, a color detection method, which is based on a\nConvolutional Neural Network (CNN), is proposed. Firstly, image segmentation is\nperformed using the edge detection segmentation technique to specify the object\nand then the segmented object is fed to the Convolutional Neural Network\ntrained to detect the color of an object in different lighting conditions. It\nis experimentally verified that our method can substantially enhance the\nrobustness of color detection in different lighting conditions, and our method\nperformed better results than existing methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04762v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04762v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04762v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04754v1",
    "updated": "2024-02-07T11:12:41+00:00",
    "published": "2024-02-07T11:12:41+00:00",
    "title": "Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints",
    "authors": [
      {
        "name": "Jian Chen"
      },
      {
        "name": "Ruiyi Zhang"
      },
      {
        "name": "Yufan Zhou"
      },
      {
        "name": "Changyou Chen"
      }
    ],
    "summary": "Controllable layout generation refers to the process of creating a plausible\nvisual arrangement of elements within a graphic design (e.g., document and web\ndesigns) with constraints representing design intentions. Although recent\ndiffusion-based models have achieved state-of-the-art FID scores, they tend to\nexhibit more pronounced misalignment compared to earlier transformer-based\nmodels. In this work, we propose the $\\textbf{LA}$yout $\\textbf{C}$onstraint\ndiffusion mod$\\textbf{E}$l (LACE), a unified model to handle a broad range of\nlayout generation tasks, such as arranging elements with specified attributes\nand refining or completing a coarse layout design. The model is based on\ncontinuous diffusion models. Compared with existing methods that use discrete\ndiffusion models, continuous state-space design can enable the incorporation of\ndifferentiable aesthetic constraint functions in training. For conditional\ngeneration, we introduce conditions via masked input. Extensive experiment\nresults show that LACE produces high-quality layouts and outperforms existing\nstate-of-the-art baselines.",
    "comment": "Accepted by ICLR 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04754v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04754v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04754v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05149v1",
    "updated": "2024-02-07T11:11:46+00:00",
    "published": "2024-02-07T11:11:46+00:00",
    "title": "FlowPG: Action-constrained Policy Gradient with Normalizing Flows",
    "authors": [
      {
        "name": "Janaka Chathuranga Brahmanage"
      },
      {
        "name": "Jiajing Ling"
      },
      {
        "name": "Akshat Kumar"
      }
    ],
    "summary": "Action-constrained reinforcement learning (ACRL) is a popular approach for\nsolving safety-critical and resource-allocation related decision making\nproblems. A major challenge in ACRL is to ensure agent taking a valid action\nsatisfying constraints in each RL step. Commonly used approach of using a\nprojection layer on top of the policy network requires solving an optimization\nprogram which can result in longer training time, slow convergence, and zero\ngradient problem. To address this, first we use a normalizing flow model to\nlearn an invertible, differentiable mapping between the feasible action space\nand the support of a simple distribution on a latent variable, such as\nGaussian. Second, learning the flow model requires sampling from the feasible\naction space, which is also challenging. We develop multiple methods, based on\nHamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such\naction sampling for convex and non-convex constraints. Third, we integrate the\nlearned normalizing flow with the DDPG algorithm. By design, a well-trained\nnormalizing flow will transform policy output into a valid action without\nrequiring an optimization solver. Empirically, our approach results in\nsignificantly fewer constraint violations (upto an order-of-magnitude for\nseveral instances) and is multiple times faster on a variety of continuous\ncontrol tasks.",
    "comment": null,
    "journal_ref": "Thirty-seventh Conference on Neural Information Processing\n  Systems. 2023",
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05149v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05149v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05149v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04744v1",
    "updated": "2024-02-07T10:55:59+00:00",
    "published": "2024-02-07T10:55:59+00:00",
    "title": "Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers",
    "authors": [
      {
        "name": "Abhimanyu Rajeshkumar Bambhaniya"
      },
      {
        "name": "Amir Yazdanbakhsh"
      },
      {
        "name": "Suvinay Subramanian"
      },
      {
        "name": "Sheng-Chun Kao"
      },
      {
        "name": "Shivani Agrawal"
      },
      {
        "name": "Utku Evci"
      },
      {
        "name": "Tushar Krishna"
      }
    ],
    "summary": "N:M Structured sparsity has garnered significant interest as a result of\nrelatively modest overhead and improved efficiency. Additionally, this form of\nsparsity holds considerable appeal for reducing the memory footprint owing to\ntheir modest representation overhead. There have been efforts to develop\ntraining recipes for N:M structured sparsity, they primarily focus on\nlow-sparsity regions ($\\sim$50\\%). Nonetheless, performance of models trained\nusing these approaches tends to decline when confronted with high-sparsity\nregions ($>$80\\%). In this work, we study the effectiveness of existing sparse\ntraining recipes at \\textit{high-sparsity regions} and argue that these methods\nfail to sustain the model quality on par with low-sparsity regions. We\ndemonstrate that the significant factor contributing to this disparity is the\npresence of elevated levels of induced noise in the gradient magnitudes. To\nmitigate this undesirable effect, we employ decay mechanisms to progressively\nrestrict the flow of gradients towards pruned elements. Our approach improves\nthe model quality by up to 2$\\%$ and 5$\\%$ in vision and language models at\nhigh sparsity regime, respectively. We also evaluate the trade-off between\nmodel accuracy and training compute cost in terms of FLOPs. At iso-training\nFLOPs, our method yields better performance compared to conventional sparse\ntraining recipes, exhibiting an accuracy improvement of up to 2$\\%$. The source\ncode is available at\nhttps://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity.",
    "comment": "18 pages, 8 figures, 17 tables. Code is available at\n  https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04744v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04744v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04744v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04740v1",
    "updated": "2024-02-07T10:51:11+00:00",
    "published": "2024-02-07T10:51:11+00:00",
    "title": "Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes",
    "authors": [
      {
        "name": "Sobin Joseph"
      },
      {
        "name": "Shashi Jain"
      }
    ],
    "summary": "An extension of the Hawkes process, the Marked Hawkes process distinguishes\nitself by featuring variable jump size across each event, in contrast to the\nconstant jump size observed in a Hawkes process without marks. While extensive\nliterature has been dedicated to the non-parametric estimation of both the\nlinear and non-linear Hawkes process, there remains a significant gap in the\nliterature regarding the marked Hawkes process. In response to this, we propose\na methodology for estimating the conditional intensity of the marked Hawkes\nprocess. We introduce two distinct models: \\textit{Shallow Neural Hawkes with\nmarks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network\nfor Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these\napproaches take the past arrival times and their corresponding marks as the\ninput to obtain the arrival intensity. This approach is entirely\nnon-parametric, preserving the interpretability associated with the marked\nHawkes process. To validate the efficacy of our method, we subject the method\nto synthetic datasets with known ground truth. Additionally, we apply our\nmethod to model cryptocurrency order book data, demonstrating its applicability\nto real-world scenarios.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-fin.CP",
      "q-fin.ST"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04740v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04740v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04740v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04732v1",
    "updated": "2024-02-07T10:33:09+00:00",
    "published": "2024-02-07T10:33:09+00:00",
    "title": "Graph Cuts with Arbitrary Size Constraints Through Optimal Transport",
    "authors": [
      {
        "name": "Chakib Fettal"
      },
      {
        "name": "Lazhar Labiod"
      },
      {
        "name": "Mohamed Nadif"
      }
    ],
    "summary": "A common way of partitioning graphs is through minimum cuts. One drawback of\nclassical minimum cut methods is that they tend to produce small groups, which\nis why more balanced variants such as normalized and ratio cuts have seen more\nsuccess. However, we believe that with these variants, the balance constraints\ncan be too restrictive for some applications like for clustering of imbalanced\ndatasets, while not being restrictive enough for when searching for perfectly\nbalanced partitions. Here, we propose a new graph cut algorithm for\npartitioning graphs under arbitrary size constraints. We formulate the graph\ncut problem as a regularized Gromov-Wasserstein problem. We then propose to\nsolve it using accelerated proximal GD algorithm which has global convergence\nguarantees, results in sparse solutions and only incurs an additional ratio of\n$\\mathcal{O}(\\log(n))$ compared to the classical spectral clustering algorithm\nbut was seen to be more efficient.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04732v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04732v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04732v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04713v1",
    "updated": "2024-02-07T10:05:42+00:00",
    "published": "2024-02-07T10:05:42+00:00",
    "title": "Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search",
    "authors": [
      {
        "name": "Yutaro Oguri"
      },
      {
        "name": "Yusuke Matsui"
      }
    ],
    "summary": "We present a theoretical and empirical analysis of the adaptive entry point\nselection for graph-based approximate nearest neighbor search (ANNS). We\nintroduce novel concepts: $b\\textit{-monotonic path}$ and $B\\textit{-MSNET}$,\nwhich better capture an actual graph in practical algorithms than existing\nconcepts like MSNET. We prove that adaptive entry point selection offers better\nperformance upper bound than the fixed central entry point under more general\nconditions than previous work. Empirically, we validate the method's\neffectiveness in accuracy, speed, and memory usage across various datasets,\nespecially in challenging scenarios with out-of-distribution data and hard\ninstances. Our comprehensive study provides deeper insights into optimizing\nentry points for graph-based ANNS for real-world high-dimensional data\napplications.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.DB",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04713v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04713v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04713v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04710v1",
    "updated": "2024-02-07T09:57:39+00:00",
    "published": "2024-02-07T09:57:39+00:00",
    "title": "Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks",
    "authors": [
      {
        "name": "Jiahua Rao"
      },
      {
        "name": "Jiancong Xie"
      },
      {
        "name": "Hanjing Lin"
      },
      {
        "name": "Shuangjia Zheng"
      },
      {
        "name": "Zhen Wang"
      },
      {
        "name": "Yuedong Yang"
      }
    ],
    "summary": "Graph Neural Networks (GNNs) have gained considerable traction for their\ncapability to effectively process topological data, yet their interpretability\nremains a critical concern. Current interpretation methods are dominated by\npost-hoc explanations to provide a transparent and intuitive understanding of\nGNNs. However, they have limited performance in interpreting complicated\nsubgraphs and can't utilize the explanation to advance GNN predictions. On the\nother hand, transparent GNN models are proposed to capture critical subgraphs.\nWhile such methods could improve GNN predictions, they usually don't perform\nwell on explanations. Thus, it is desired for a new strategy to better couple\nGNN explanation and prediction. In this study, we have developed a novel\ninterpretable causal GNN framework that incorporates retrieval-based causal\nlearning with Graph Information Bottleneck (GIB) theory. The framework could\nsemi-parametrically retrieve crucial subgraphs detected by GIB and compress the\nexplanatory subgraphs via a causal module. The framework was demonstrated to\nconsistently outperform state-of-the-art methods, and to achieve 32.71\\% higher\nprecision on real-world explanation scenarios with diverse explanation types.\nMore importantly, the learned explanations were shown able to also improve GNN\nprediction performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04710v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04710v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04710v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05148v1",
    "updated": "2024-02-07T09:41:39+00:00",
    "published": "2024-02-07T09:41:39+00:00",
    "title": "Cost Optimized Scheduling in Modular Electrolysis Plants",
    "authors": [
      {
        "name": "Vincent Henkel"
      },
      {
        "name": "Maximilian Kilthau"
      },
      {
        "name": "Felix Gehlhoff"
      },
      {
        "name": "Lukas Wagner"
      },
      {
        "name": "Alexander Fay"
      }
    ],
    "summary": "In response to the global shift towards renewable energy resources, the\nproduction of green hydrogen through electrolysis is emerging as a promising\nsolution. Modular electrolysis plants, designed for flexibility and\nscalability, offer a dynamic response to the increasing demand for hydrogen\nwhile accommodating the fluctuations inherent in renewable energy sources.\nHowever, optimizing their operation is challenging, especially when a large\nnumber of electrolysis modules needs to be coordinated, each with potentially\ndifferent characteristics.\n  To address these challenges, this paper presents a decentralized scheduling\nmodel to optimize the operation of modular electrolysis plants using the\nAlternating Direction Method of Multipliers. The model aims to balance hydrogen\nproduction with fluctuating demand, to minimize the marginal Levelized Cost of\nHydrogen (mLCOH), and to ensure adaptability to operational disturbances. A\ncase study validates the accuracy of the model in calculating mLCOH values\nunder nominal load conditions and demonstrates its responsiveness to dynamic\nchanges, such as electrolyzer module malfunctions and scale-up scenarios.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.SY",
    "categories": [
      "cs.SY",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05148v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05148v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05148v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04699v1",
    "updated": "2024-02-07T09:39:29+00:00",
    "published": "2024-02-07T09:39:29+00:00",
    "title": "EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions",
    "authors": [
      {
        "name": "Shashank Kotyan"
      },
      {
        "name": "PoYuan Mao"
      },
      {
        "name": "Danilo Vasconcellos Vargas"
      }
    ],
    "summary": "Deep neural networks are exploited using natural adversarial samples, which\nhave no impact on human perception but are misclassified. Current approaches\noften rely on the white-box nature of deep neural networks to generate these\nadversarial samples or alter the distribution of adversarial samples compared\nto training distribution. To alleviate the limitations of current approaches,\nwe propose EvoSeed, a novel evolutionary strategy-based search algorithmic\nframework to generate natural adversarial samples. Our EvoSeed framework uses\nauxiliary Diffusion and Classifier models to operate in a model-agnostic\nblack-box setting. We employ CMA-ES to optimize the search for an adversarial\nseed vector, which, when processed by the Conditional Diffusion Model, results\nin an unrestricted natural adversarial sample misclassified by the Classifier\nModel. Experiments show that generated adversarial images are of high image\nquality and are transferable to different classifiers. Our approach\ndemonstrates promise in enhancing the quality of adversarial samples using\nevolutionary algorithms. We hope our research opens new avenues to enhance the\nrobustness of deep neural networks in real-world scenarios. Project Website can\nbe accessed at \\url{https://shashankkotyan.github.io/EvoSeed}.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04699v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04699v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04699v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05147v2",
    "updated": "2024-02-12T15:09:39+00:00",
    "published": "2024-02-07T09:36:54+00:00",
    "title": "ApiQ: Finetuning of 2-Bit Quantized Large Language Model",
    "authors": [
      {
        "name": "Baohao Liao"
      },
      {
        "name": "Christof Monz"
      }
    ],
    "summary": "Memory-efficient finetuning of large language models (LLMs) has recently\nattracted huge attention with the increasing size of LLMs, primarily due to the\nconstraints posed by GPU memory limitations and the comparable results of these\nmethods with full finetuning. Despite the advancements, current strategies for\nmemory-efficient finetuning, such as QLoRA, exhibit inconsistent performance\nacross diverse bit-width quantizations and multifaceted tasks. This\ninconsistency largely stems from the detrimental impact of the quantization\nprocess on preserved knowledge, leading to catastrophic forgetting and\nundermining the utilization of pretrained models for finetuning purposes. In\nthis work, we introduce a novel quantization framework named ApiQ, designed to\nrestore the lost information from quantization by concurrently initializing\nLoRA components and quantizing the weights of LLMs. This approach ensures the\nmaintenance of the original LLM's activation precision while mitigating the\nerror propagation from shallower into deeper layers. Through comprehensive\nevaluations conducted on a spectrum of language tasks with various models, ApiQ\ndemonstrably minimizes activation error during quantization. Consequently, it\nconsistently achieves superior finetuning outcomes across various bit-widths of\nquantization.",
    "comment": "compared to v0: new histogram formats for better reading",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05147v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05147v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05147v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05856v1",
    "updated": "2024-02-07T09:32:35+00:00",
    "published": "2024-02-07T09:32:35+00:00",
    "title": "Structure-Informed Protein Language Model",
    "authors": [
      {
        "name": "Zuobai Zhang"
      },
      {
        "name": "Jiarui Lu"
      },
      {
        "name": "Vijil Chenthamarakshan"
      },
      {
        "name": "Aur\u00e9lie Lozano"
      },
      {
        "name": "Payel Das"
      },
      {
        "name": "Jian Tang"
      }
    ],
    "summary": "Protein language models are a powerful tool for learning protein\nrepresentations through pre-training on vast protein sequence datasets.\nHowever, traditional protein language models lack explicit structural\nsupervision, despite its relevance to protein function. To address this issue,\nwe introduce the integration of remote homology detection to distill structural\ninformation into protein language models without requiring explicit protein\nstructures as input. We evaluate the impact of this structure-informed training\non downstream protein function prediction tasks. Experimental results reveal\nconsistent improvements in function annotation accuracy for EC number and GO\nterm prediction. Performance on mutant datasets, however, varies based on the\nrelationship between targeted properties and protein structures. This\nunderscores the importance of considering this relationship when applying\nstructure-aware training to protein function prediction tasks. Code and model\nweights are available at https://github.com/DeepGraphLearning/esm-s.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "q-bio.BM",
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05856v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05856v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05856v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04692v1",
    "updated": "2024-02-07T09:32:32+00:00",
    "published": "2024-02-07T09:32:32+00:00",
    "title": "From explained variance of correlated components to PCA without orthogonality constraints",
    "authors": [
      {
        "name": "Marie Chavent"
      },
      {
        "name": "Guy Chavent"
      }
    ],
    "summary": "Block Principal Component Analysis (Block PCA) of a data matrix A, where\nloadings Z are determined by maximization of AZ 2 over unit norm orthogonal\nloadings, is difficult to use for the design of sparse PCA by 1 regularization,\ndue to the difficulty of taking care of both the orthogonality constraint on\nloadings and the non differentiable 1 penalty. Our objective in this paper is\nto relax the orthogonality constraint on loadings by introducing new objective\nfunctions expvar(Y) which measure the part of the variance of the data matrix A\nexplained by correlated components Y = AZ. So we propose first a comprehensive\nstudy of mathematical and numerical properties of expvar(Y) for two existing\ndefinitions Zou et al. [2006], Shen and Huang [2008] and four new definitions.\nThen we show that only two of these explained variance are fit to use as\nobjective function in block PCA formulations for A rid of orthogonality\nconstraints.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04692v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04692v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04692v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04691v2",
    "updated": "2024-02-13T08:06:44+00:00",
    "published": "2024-02-07T09:31:01+00:00",
    "title": "Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces",
    "authors": [
      {
        "name": "Lei Shi"
      },
      {
        "name": "Jia-Qi Yang"
      }
    ],
    "summary": "This study investigates leveraging stochastic gradient descent (SGD) to learn\noperators between general Hilbert spaces. We propose weak and strong regularity\nconditions for the target operator to depict its intrinsic structure and\ncomplexity. Under these conditions, we establish upper bounds for convergence\nrates of the SGD algorithm and conduct a minimax lower bound analysis, further\nillustrating that our convergence analysis and regularity conditions\nquantitatively characterize the tractability of solving operator learning\nproblems using the SGD algorithm. It is crucial to highlight that our\nconvergence analysis is still valid for nonlinear operator learning. We show\nthat the SGD estimator will converge to the best linear approximation of the\nnonlinear target operator. Moreover, applying our analysis to operator learning\nproblems based on vector-valued and real-valued reproducing kernel Hilbert\nspaces yields new convergence results, thereby refining the conclusions of\nexisting literature.",
    "comment": "56 pages",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.FA",
      "math.ST",
      "stat.TH"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04691v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04691v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04691v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04678v1",
    "updated": "2024-02-07T09:09:14+00:00",
    "published": "2024-02-07T09:09:14+00:00",
    "title": "Large Language Models As Faithful Explainers",
    "authors": [
      {
        "name": "Yu-Neng Chuang"
      },
      {
        "name": "Guanchu Wang"
      },
      {
        "name": "Chia-Yuan Chang"
      },
      {
        "name": "Ruixiang Tang"
      },
      {
        "name": "Fan Yang"
      },
      {
        "name": "Mengnan Du"
      },
      {
        "name": "Xuanting Cai"
      },
      {
        "name": "Xia Hu"
      }
    ],
    "summary": "Large Language Models (LLMs) have recently become proficient in addressing\ncomplex tasks by utilizing their rich internal knowledge and reasoning ability.\nConsequently, this complexity hinders traditional input-focused explanation\nalgorithms for explaining the complex decision-making processes of LLMs. Recent\nadvancements have thus emerged for self-explaining their predictions through a\nsingle feed-forward inference in a natural language format. However, natural\nlanguage explanations are often criticized for lack of faithfulness since these\nexplanations may not accurately reflect the decision-making behaviors of the\nLLMs. In this work, we introduce a generative explanation framework, xLLM, to\nimprove the faithfulness of the explanations provided in natural language\nformats for LLMs. Specifically, we propose an evaluator to quantify the\nfaithfulness of natural language explanation and enhance the faithfulness by an\niterative optimization process of xLLM, with the goal of maximizing the\nfaithfulness scores. Experiments conducted on three NLU datasets demonstrate\nthat xLLM can significantly improve the faithfulness of generated explanations,\nwhich are in alignment with the behaviors of LLMs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04678v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04678v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04678v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05972v1",
    "updated": "2024-02-07T09:03:26+00:00",
    "published": "2024-02-07T09:03:26+00:00",
    "title": "Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra",
    "authors": [
      {
        "name": "Patrick Egenlauf"
      },
      {
        "name": "Patric Rommel"
      },
      {
        "name": "J\u00f6rg Main"
      }
    ],
    "summary": "Resonances in open quantum systems depending on at least two controllable\nparameters can show the phenomenon of exceptional points (EPs), where not only\nthe eigenvalues but also the eigenvectors of two or more resonances coalesce.\nTheir exact localization in the parameter space is challenging, in particular\nin systems, where the computation of the quantum spectra and resonances is\nnumerically very expensive. We introduce an efficient machine learning\nalgorithm to find exceptional points based on Gaussian process regression\n(GPR). The GPR-model is trained with an initial set of eigenvalue pairs\nbelonging to an EP and used for a first estimation of the EP position via a\nnumerically cheap root search. The estimate is then improved iteratively by\nadding selected exact eigenvalue pairs as training points to the GPR-model. The\nGPR-based method is developed and tested on a simple low-dimensional matrix\nmodel and then applied to a challenging real physical system, viz., the\nlocalization of EPs in the resonance spectra of excitons in cuprous oxide in\nexternal electric and magnetic fields. The precise computation of EPs, by\ntaking into account the complete valence band structure and central-cell\ncorrections of the crystal, can be the basis for the experimental observation\nof EPs in this system.",
    "comment": "28 pages, 10 figures, submitted to Machine Learning: Science and\n  Technology",
    "journal_ref": null,
    "doi": null,
    "primary_category": "quant-ph",
    "categories": [
      "quant-ph",
      "cond-mat.mes-hall",
      "cond-mat.str-el",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05972v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05972v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05972v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04676v1",
    "updated": "2024-02-07T09:03:04+00:00",
    "published": "2024-02-07T09:03:04+00:00",
    "title": "Group Distributionally Robust Dataset Distillation with Risk Minimization",
    "authors": [
      {
        "name": "Saeed Vahidian"
      },
      {
        "name": "Mingyu Wang"
      },
      {
        "name": "Jianyang Gu"
      },
      {
        "name": "Vyacheslav Kungurtsev"
      },
      {
        "name": "Wei Jiang"
      },
      {
        "name": "Yiran Chen"
      }
    ],
    "summary": "Dataset distillation (DD) has emerged as a widely adopted technique for\ncrafting a synthetic dataset that captures the essential information of a\ntraining dataset, facilitating the training of accurate neural models. Its\napplications span various domains, including transfer learning, federated\nlearning, and neural architecture search. The most popular methods for\nconstructing the synthetic data rely on matching the convergence properties of\ntraining the model with the synthetic dataset and the training dataset.\nHowever, targeting the training dataset must be thought of as auxiliary in the\nsame sense that the training set is an approximate substitute for the\npopulation distribution, and the latter is the data of interest. Yet despite\nits popularity, an aspect that remains unexplored is the relationship of DD to\nits generalization, particularly across uncommon subgroups. That is, how can we\nensure that a model trained on the synthetic dataset performs well when faced\nwith samples from regions with low population density? Here, the\nrepresentativeness and coverage of the dataset become salient over the\nguaranteed training error at inference. Drawing inspiration from\ndistributionally robust optimization, we introduce an algorithm that combines\nclustering with the minimization of a risk measure on the loss to conduct DD.\nWe provide a theoretical rationale for our approach and demonstrate its\neffective generalization and robustness across subgroups through numerical\nexperiments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04676v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04676v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04676v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05146v1",
    "updated": "2024-02-07T09:00:30+00:00",
    "published": "2024-02-07T09:00:30+00:00",
    "title": "Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving",
    "authors": [
      {
        "name": "Wensheng Su"
      },
      {
        "name": "Zhenni Li"
      },
      {
        "name": "Minrui Xu"
      },
      {
        "name": "Jiawen Kang"
      },
      {
        "name": "Dusit Niyato"
      },
      {
        "name": "Shengli Xie"
      }
    ],
    "summary": "Deep reinforcement learning (DRL) has shown remarkable success in complex\nautonomous driving scenarios. However, DRL models inevitably bring high memory\nconsumption and computation, which hinders their wide deployment in\nresource-limited autonomous driving devices. Structured Pruning has been\nrecognized as a useful method to compress and accelerate DRL models, but it is\nstill challenging to estimate the contribution of a parameter (i.e., neuron) to\nDRL models. In this paper, we introduce a novel dynamic structured pruning\napproach that gradually removes a DRL model's unimportant neurons during the\ntraining stage. Our method consists of two steps, i.e. training DRL models with\na group sparse regularizer and removing unimportant neurons with a dynamic\npruning threshold. To efficiently train the DRL model with a small number of\nimportant neurons, we employ a neuron-importance group sparse regularizer. In\ncontrast to conventional regularizers, this regularizer imposes a penalty on\nredundant groups of neurons that do not significantly influence the output of\nthe DRL model. Furthermore, we design a novel structured pruning strategy to\ndynamically determine the pruning threshold and gradually remove unimportant\nneurons with a binary mask. Therefore, our method can remove not only redundant\ngroups of neurons of the DRL model but also achieve high and robust\nperformance. Experimental results show that the proposed method is competitive\nwith existing DRL pruning methods on discrete control environments (i.e.,\nCartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e.,\nHopper-v3 and Walker2D-v3). Specifically, our method effectively compresses\n$93\\%$ neurons and $96\\%$ weights of the DRL model in four challenging DRL\nenvironments with slight accuracy degradation.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05146v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05146v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05146v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04668v1",
    "updated": "2024-02-07T08:53:46+00:00",
    "published": "2024-02-07T08:53:46+00:00",
    "title": "A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data",
    "authors": [
      {
        "name": "Ghadeer O. Ghosheh"
      },
      {
        "name": "Moritz G\u00f6gl"
      },
      {
        "name": "Tingting Zhu"
      }
    ],
    "summary": "The burden of diseases is rising worldwide, with unequal treatment efficacy\nfor patient populations that are underrepresented in clinical trials.\nHealthcare, however, is driven by the average population effect of medical\ntreatments and, therefore, operates in a \"one-size-fits-all\" approach, not\nnecessarily what best fits each patient. These facts suggest a pressing need\nfor methodologies to study individualized treatment effects (ITE) to drive\npersonalized treatment. Despite the increased interest in\nmachine-learning-driven ITE estimation models, the vast majority focus on\ntabular data with limited review and understanding of methodologies proposed\nfor time-series electronic health records (EHRs). To this end, this work\nprovides an overview of ITE works for time-series data and insights into future\nresearch. The work summarizes the latest work in the literature and reviews it\nin light of theoretical assumptions, types of treatment settings, and\ncomputational frameworks. Furthermore, this work discusses challenges and\nfuture research directions for ITEs in a time-series setting. We hope this work\nopens new directions and serves as a resource for understanding one of the\nexciting yet under-studied research areas.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04668v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04668v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04668v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04660v1",
    "updated": "2024-02-07T08:49:33+00:00",
    "published": "2024-02-07T08:49:33+00:00",
    "title": "Adversarial Robustness Through Artifact Design",
    "authors": [
      {
        "name": "Tsufit Shua"
      },
      {
        "name": "Mahmood Sharif"
      }
    ],
    "summary": "Adversarial examples arose as a challenge for machine learning. To hinder\nthem, most defenses alter how models are trained (e.g., adversarial training)\nor inference is made (e.g., randomized smoothing). Still, while these\napproaches markedly improve models' adversarial robustness, models remain\nhighly susceptible to adversarial examples. Identifying that, in certain\ndomains such as traffic-sign recognition, objects are implemented per standards\nspecifying how artifacts (e.g., signs) should be designed, we propose a novel\napproach for improving adversarial robustness. Specifically, we offer a method\nto redefine standards, making minor changes to existing ones, to defend against\nadversarial examples. We formulate the problem of artifact design as a robust\noptimization problem, and propose gradient-based and greedy search methods to\nsolve it. We evaluated our approach in the domain of traffic-sign recognition,\nallowing it to alter traffic-sign pictograms (i.e., symbols within the signs)\nand their colors. We found that, combined with adversarial training, our\napproach led to up to 25.18\\% higher robust accuracy compared to\nstate-of-the-art methods against two adversary types, while further increasing\naccuracy on benign inputs.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04660v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04660v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04660v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04655v1",
    "updated": "2024-02-07T08:42:48+00:00",
    "published": "2024-02-07T08:42:48+00:00",
    "title": "Open-Vocabulary Calibration for Vision-Language Models",
    "authors": [
      {
        "name": "Shuoyuan Wang"
      },
      {
        "name": "Jindong Wang"
      },
      {
        "name": "Guoqing Wang"
      },
      {
        "name": "Bob Zhang"
      },
      {
        "name": "Kaiyang Zhou"
      },
      {
        "name": "Hongxin Wei"
      }
    ],
    "summary": "Vision-language models (VLMs) have emerged as formidable tools, showing their\nstrong capability in handling various open-vocabulary tasks in image\nrecognition, text-driven visual content generation, and visual chatbots, to\nname a few. In recent years, considerable efforts and resources have been\ndevoted to adaptation methods for improving downstream performance of VLMs,\nparticularly on parameter-efficient fine-tuning methods like prompt learning.\nHowever, a crucial aspect that has been largely overlooked is the confidence\ncalibration problem in fine-tuned VLMs, which could greatly reduce reliability\nwhen deploying such models in the real world. This paper bridges the gap by\nsystematically investigating the confidence calibration problem in the context\nof prompt learning and reveals that existing calibration methods are\ninsufficient to address the problem, especially in the open-vocabulary setting.\nTo solve the problem, we present a simple and effective approach called\nDistance-Aware Calibration (DAC), which is based on scaling the temperature\nusing as guidance the distance between predicted text labels and base classes.\nThe experiments with 7 distinct prompt learning methods applied across 11\ndiverse downstream datasets demonstrate the effectiveness of DAC, which\nachieves high efficacy without sacrificing the inference speed.",
    "comment": "Preprrint",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04655v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04655v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04655v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04653v1",
    "updated": "2024-02-07T08:38:12+00:00",
    "published": "2024-02-07T08:38:12+00:00",
    "title": "An Over Complete Deep Learning Method for Inverse Problems",
    "authors": [
      {
        "name": "Moshe Eliasof"
      },
      {
        "name": "Eldad Haber"
      },
      {
        "name": "Eran Treister"
      }
    ],
    "summary": "Obtaining meaningful solutions for inverse problems has been a major\nchallenge with many applications in science and engineering. Recent machine\nlearning techniques based on proximal and diffusion-based methods have shown\npromising results. However, as we show in this work, they can also face\nchallenges when applied to some exemplary problems. We show that similar to\nprevious works on over-complete dictionaries, it is possible to overcome these\nshortcomings by embedding the solution into higher dimensions. The novelty of\nthe work proposed is that we jointly design and learn the embedding and the\nregularizer for the embedding vector. We demonstrate the merit of this approach\non several exemplary and common inverse problems.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04653v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04653v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04653v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04647v1",
    "updated": "2024-02-07T08:18:09+00:00",
    "published": "2024-02-07T08:18:09+00:00",
    "title": "Latent Plan Transformer: Planning as Latent Variable Inference",
    "authors": [
      {
        "name": "Deqian Kong"
      },
      {
        "name": "Dehong Xu"
      },
      {
        "name": "Minglu Zhao"
      },
      {
        "name": "Bo Pang"
      },
      {
        "name": "Jianwen Xie"
      },
      {
        "name": "Andrew Lizarraga"
      },
      {
        "name": "Yuhao Huang"
      },
      {
        "name": "Sirui Xie"
      },
      {
        "name": "Ying Nian Wu"
      }
    ],
    "summary": "In tasks aiming for long-term returns, planning becomes necessary. We study\ngenerative modeling for planning with datasets repurposed from offline\nreinforcement learning. Specifically, we identify temporal consistency in the\nabsence of step-wise rewards as one key technical challenge. We introduce the\nLatent Plan Transformer (LPT), a novel model that leverages a latent space to\nconnect a Transformer-based trajectory generator and the final return. LPT can\nbe learned with maximum likelihood estimation on trajectory-return pairs. In\nlearning, posterior sampling of the latent variable naturally gathers\nsub-trajectories to form a consistent abstraction despite the finite context.\nDuring test time, the latent variable is inferred from an expected return\nbefore policy execution, realizing the idea of planning as inference. It then\nguides the autoregressive policy throughout the episode, functioning as a plan.\nOur experiments demonstrate that LPT can discover improved decisions from\nsuboptimal trajectories. It achieves competitive performance across several\nbenchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting\ncapabilities of nuanced credit assignments, trajectory stitching, and\nadaptation to environmental contingencies. These results validate that latent\nvariable inference can be a strong alternative to step-wise reward prompting.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04647v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04647v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04647v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04646v1",
    "updated": "2024-02-07T08:18:06+00:00",
    "published": "2024-02-07T08:18:06+00:00",
    "title": "Learning with Diversification from Block Sparse Signal",
    "authors": [
      {
        "name": "Yanhao Zhang"
      },
      {
        "name": "Zhihan Zhu"
      },
      {
        "name": "Yong Xia"
      }
    ],
    "summary": "This paper introduces a novel prior called Diversified Block Sparse Prior to\ncharacterize the widespread block sparsity phenomenon in real-world data. By\nallowing diversification on variance and correlation matrix, we effectively\naddress the sensitivity issue of existing block sparse learning methods to\npre-defined block information, which enables adaptive block estimation while\nmitigating the risk of overfitting. Based on this, a diversified block sparse\nBayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual\nascent method for hyperparameter estimation. Moreover, we establish the global\nand local optimality theory of our model. Experiments validate the advantages\nof DivSBL over existing algorithms.",
    "comment": "12 pages, 12 figures, 3 tables",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04646v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04646v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04646v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04644v1",
    "updated": "2024-02-07T08:16:40+00:00",
    "published": "2024-02-07T08:16:40+00:00",
    "title": "LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views",
    "authors": [
      {
        "name": "Yuji Roh"
      },
      {
        "name": "Qingyun Liu"
      },
      {
        "name": "Huan Gui"
      },
      {
        "name": "Zhe Yuan"
      },
      {
        "name": "Yujin Tang"
      },
      {
        "name": "Steven Euijong Whang"
      },
      {
        "name": "Liang Liu"
      },
      {
        "name": "Shuchao Bi"
      },
      {
        "name": "Lichan Hong"
      },
      {
        "name": "Ed H. Chi"
      },
      {
        "name": "Zhe Zhao"
      }
    ],
    "summary": "Fine-tuning is becoming widely used for leveraging the power of pre-trained\nfoundation models in new downstream tasks. While there are many successes of\nfine-tuning on various tasks, recent studies have observed challenges in the\ngeneralization of fine-tuned models to unseen distributions (i.e.,\nout-of-distribution; OOD). To improve OOD generalization, some previous studies\nidentify the limitations of fine-tuning data and regulate fine-tuning to\npreserve the general representation learned from pre-training data. However,\npotential limitations in the pre-training data and models are often ignored. In\nthis paper, we contend that overly relying on the pre-trained representation\nmay hinder fine-tuning from learning essential representations for downstream\ntasks and thus hurt its OOD generalization. It can be especially catastrophic\nwhen new tasks are from different (sub)domains compared to pre-training data.\nTo address the issues in both pre-training and fine-tuning data, we propose a\nnovel generalizable fine-tuning method LEVI, where the pre-trained model is\nadaptively ensembled layer-wise with a small task-specific model, while\npreserving training and inference efficiencies. By combining two complementing\nmodels, LEVI effectively suppresses problematic features in both the\nfine-tuning data and pre-trained model and preserves useful features for new\ntasks. Broad experiments with large language and vision models show that LEVI\ngreatly improves fine-tuning generalization via emphasizing different views\nfrom fine-tuning data and pre-trained features.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04644v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04644v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04644v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05145v1",
    "updated": "2024-02-07T08:15:30+00:00",
    "published": "2024-02-07T08:15:30+00:00",
    "title": "Online Learning Approach for Survival Analysis",
    "authors": [
      {
        "name": "Camila Fernandez"
      },
      {
        "name": "Pierre Gaillard"
      },
      {
        "name": "Joseph de Vilmarest"
      },
      {
        "name": "Olivier Wintenberger"
      }
    ],
    "summary": "We introduce an online mathematical framework for survival analysis, allowing\nreal time adaptation to dynamic environments and censored data. This framework\nenables the estimation of event time distributions through an optimal second\norder online convex optimization algorithm-Online Newton Step (ONS). This\napproach, previously unexplored, presents substantial advantages, including\nexplicit algorithms with non-asymptotic convergence guarantees. Moreover, we\nanalyze the selection of ONS hyperparameters, which depends on the\nexp-concavity property and has a significant influence on the regret bound. We\npropose a stochastic approach that guarantees logarithmic stochastic regret for\nONS. Additionally, we introduce an adaptive aggregation method that ensures\nrobustness in hyperparameter selection while maintaining fast regret bounds.\nThe findings of this paper can extend beyond the survival analysis field, and\nare relevant for any case characterized by poor exp-concavity and unstable ONS.\nFinally, these assertions are illustrated by simulation experiments.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "physics.data-an",
      "stat.ML"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05145v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05145v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05145v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.05144v1",
    "updated": "2024-02-07T08:01:45+00:00",
    "published": "2024-02-07T08:01:45+00:00",
    "title": "A Bandit Approach with Evolutionary Operators for Model Selection",
    "authors": [
      {
        "name": "Margaux Br\u00e9g\u00e8re"
      },
      {
        "name": "Julie Keisler"
      }
    ],
    "summary": "This paper formulates model selection as an infinite-armed bandit problem.\nThe models are arms, and picking an arm corresponds to a partial training of\nthe model (resource allocation). The reward is the accuracy of the selected\nmodel after its partial training. In this best arm identification problem,\nregret is the gap between the expected accuracy of the optimal model and that\nof the model finally chosen. We first consider a straightforward generalization\nof UCB-E to the stochastic infinite-armed bandit problem and show that, under\nbasic assumptions, the expected regret order is $T^{-\\alpha}$ for some $\\alpha\n\\in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla\nalgorithm, we introduce the algorithm Mutant-UCB that incorporates operators\nfrom evolutionary algorithms. Tests carried out on three open source image\nclassification data sets attest to the relevance of this novel combining\napproach, which outperforms the state-of-the-art for a fixed budget.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.NE",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.05144v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.05144v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.05144v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04640v1",
    "updated": "2024-02-07T07:57:43+00:00",
    "published": "2024-02-07T07:57:43+00:00",
    "title": "Domain Bridge: Generative model-based domain forensic for black-box models",
    "authors": [
      {
        "name": "Jiyi Zhang"
      },
      {
        "name": "Han Fang"
      },
      {
        "name": "Ee-Chien Chang"
      }
    ],
    "summary": "In forensic investigations of machine learning models, techniques that\ndetermine a model's data domain play an essential role, with prior work relying\non large-scale corpora like ImageNet to approximate the target model's domain.\nAlthough such methods are effective in finding broad domains, they often\nstruggle in identifying finer-grained classes within those domains. In this\npaper, we introduce an enhanced approach to determine not just the general data\ndomain (e.g., human face) but also its specific attributes (e.g., wearing\nglasses). Our approach uses an image embedding model as the encoder and a\ngenerative model as the decoder. Beginning with a coarse-grained description,\nthe decoder generates a set of images, which are then presented to the unknown\ntarget model. Successful classifications by the model guide the encoder to\nrefine the description, which in turn, are used to produce a more specific set\nof images in the subsequent iteration. This iterative refinement narrows down\nthe exact class of interest. A key strength of our approach lies in leveraging\nthe expansive dataset, LAION-5B, on which the generative model Stable Diffusion\nis trained. This enlarges our search space beyond traditional corpora, such as\nImageNet. Empirical results showcase our method's performance in identifying\nspecific attributes of a model's input domain, paving the way for more detailed\nforensic analyses of deep learning models.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04640v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04640v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04640v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04627v1",
    "updated": "2024-02-07T07:24:01+00:00",
    "published": "2024-02-07T07:24:01+00:00",
    "title": "SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph",
    "authors": [
      {
        "name": "Julio C. Rangel"
      },
      {
        "name": "Tarcisio Mendes de Farias"
      },
      {
        "name": "Ana Claudia Sima"
      },
      {
        "name": "Norio Kobayashi"
      }
    ],
    "summary": "The recent success of Large Language Models (LLM) in a wide range of Natural\nLanguage Processing applications opens the path towards novel Question\nAnswering Systems over Knowledge Graphs leveraging LLMs. However, one of the\nmain obstacles preventing their implementation is the scarcity of training data\nfor the task of translating questions into corresponding SPARQL queries,\nparticularly in the case of domain-specific KGs. To overcome this challenge, in\nthis study, we evaluate several strategies for fine-tuning the OpenLlama LLM\nfor question answering over life science knowledge graphs. In particular, we\npropose an end-to-end data augmentation approach for extending a set of\nexisting queries over a given knowledge graph towards a larger dataset of\nsemantically enriched question-to-SPARQL query pairs, enabling fine-tuning even\nfor datasets where these pairs are scarce. In this context, we also investigate\nthe role of semantic \"clues\" in the queries, such as meaningful variable names\nand inline comments. Finally, we evaluate our approach over the real-world Bgee\ngene expression knowledge graph and we show that semantic clues can improve\nmodel performance by up to 33% compared to a baseline with random variable\nnames and no comments included.",
    "comment": "To appear in Proceedings of SWAT4HCLS 2024: Semantic Web Tools and\n  Applications for Healthcare and Life Sciences",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.IR"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04627v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04627v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04627v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04621v1",
    "updated": "2024-02-07T07:09:15+00:00",
    "published": "2024-02-07T07:09:15+00:00",
    "title": "Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective",
    "authors": [
      {
        "name": "Soo Yong Lee"
      },
      {
        "name": "Sunwoo Kim"
      },
      {
        "name": "Fanchen Bu"
      },
      {
        "name": "Jaemin Yoo"
      },
      {
        "name": "Jiliang Tang"
      },
      {
        "name": "Kijung Shin"
      }
    ],
    "summary": "How would randomly shuffling feature vectors among nodes from the same class\naffect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs\nthe dependence between graph topology and features (A-X dependence) for GNNs to\nlearn from. Surprisingly, we observe a consistent and significant improvement\nin GNN performance following the feature shuffle. Having overlooked the impact\nof A-X dependence on GNNs, the prior literature does not provide a satisfactory\nunderstanding of the phenomenon. Thus, we raise two research questions. First,\nhow should A-X dependence be measured, while controlling for potential\nconfounds? Second, how does A-X dependence affect GNNs? In response, we (i)\npropose a principled measure for A-X dependence, (ii) design a random graph\nmodel that controls A-X dependence, (iii) establish a theory on how A-X\ndependence relates to graph convolution, and (iv) present empirical analysis on\nreal-world graphs that aligns with the theory. We conclude that A-X dependence\nmediates the effect of graph convolution, such that smaller dependence improves\nGNN-based node classification.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04621v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04621v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04621v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04620v1",
    "updated": "2024-02-07T07:07:02+00:00",
    "published": "2024-02-07T07:07:02+00:00",
    "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients",
    "authors": [
      {
        "name": "Pragnya Ramjee"
      },
      {
        "name": "Bhuvan Sachdeva"
      },
      {
        "name": "Satvik Golechha"
      },
      {
        "name": "Shreyas Kulkarni"
      },
      {
        "name": "Geeta Fulari"
      },
      {
        "name": "Kaushik Murali"
      },
      {
        "name": "Mohit Jain"
      }
    ],
    "summary": "The healthcare landscape is evolving, with patients seeking more reliable\ninformation about their health conditions, treatment options, and potential\nrisks. Despite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\ndoctors and hospital staff, highlighting the need for expert-endorsed health\ninformation. However, the pressure on experts has led to reduced communication\ntime, impacting information sharing. To address this gap, we propose\nCataractBot, an experts-in-the-loop chatbot powered by large language models\n(LLMs). Developed in collaboration with a tertiary eye hospital in India,\nCataractBot answers cataract surgery related questions instantly by querying a\ncurated knowledge base, and provides expert-verified responses asynchronously.\nCataractBot features multimodal support and multilingual capabilities. In an\nin-the-wild deployment study with 49 participants, CataractBot proved valuable,\nproviding anytime accessibility, saving time, and accommodating diverse\nliteracy levels. Trust was established through expert verification. Broadly,\nour results could inform future work on designing expert-mediated LLM bots.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.HC",
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04620v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04620v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04620v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04617v1",
    "updated": "2024-02-07T06:50:42+00:00",
    "published": "2024-02-07T06:50:42+00:00",
    "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
    "authors": [
      {
        "name": "Chaojun Xiao"
      },
      {
        "name": "Pengle Zhang"
      },
      {
        "name": "Xu Han"
      },
      {
        "name": "Guangxuan Xiao"
      },
      {
        "name": "Yankai Lin"
      },
      {
        "name": "Zhengyan Zhang"
      },
      {
        "name": "Zhiyuan Liu"
      },
      {
        "name": "Song Han"
      },
      {
        "name": "Maosong Sun"
      }
    ],
    "summary": "Large language models (LLMs) have emerged as a cornerstone in real-world\napplications with lengthy streaming inputs, such as LLM-driven agents. However,\nexisting LLMs, pre-trained on sequences with restricted maximum length, cannot\ngeneralize to longer sequences due to the out-of-domain and distraction issues.\nTo alleviate these issues, existing efforts employ sliding attention windows\nand discard distant tokens to achieve the processing of extremely long\nsequences. Unfortunately, these approaches inevitably fail to capture\nlong-distance dependencies within sequences to deeply understand semantics.\nThis paper introduces a training-free memory-based method, InfLLM, to unveil\nthe intrinsic ability of LLMs to process streaming long sequences.\nSpecifically, InfLLM stores distant contexts into additional memory units and\nemploys an efficient mechanism to lookup token-relevant units for attention\ncomputation. Thereby, InfLLM allows LLMs to efficiently process long sequences\nwhile maintaining the ability to capture long-distance dependencies. Without\nany training, InfLLM enables LLMs pre-trained on sequences of a few thousand\ntokens to achieve superior performance than competitive baselines continually\ntraining these LLMs on long sequences. Even when the sequence length is scaled\nto $1,024$K, InfLLM still effectively captures long-distance dependencies.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04617v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04617v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04617v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04616v1",
    "updated": "2024-02-07T06:48:24+00:00",
    "published": "2024-02-07T06:48:24+00:00",
    "title": "TinyLLM: Learning a Small Student from Multiple Large Language Models",
    "authors": [
      {
        "name": "Yijun Tian"
      },
      {
        "name": "Yikun Han"
      },
      {
        "name": "Xiusi Chen"
      },
      {
        "name": "Wei Wang"
      },
      {
        "name": "Nitesh V. Chawla"
      }
    ],
    "summary": "Transferring the reasoning capability from stronger large language models\n(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more\nflexible to deploy with less expense. Among the existing solutions, knowledge\ndistillation stands out due to its outstanding efficiency and generalization.\nHowever, existing methods suffer from several drawbacks, including limited\nknowledge diversity and the lack of rich contextual information. To solve the\nproblems and facilitate the learning of compact language models, we propose\nTinyLLM, a novel knowledge distillation paradigm to learn a small student LLM\nfrom multiple large teacher LLMs. In particular, we encourage the student LLM\nto not only generate the correct answers but also understand the rationales\nbehind these answers. Given that different LLMs possess diverse reasoning\nskills, we guide the student model to assimilate knowledge from various teacher\nLLMs. We further introduce an in-context example generator and a\nteacher-forcing Chain-of-Thought strategy to ensure that the rationales are\naccurate and grounded in contextually appropriate scenarios. Extensive\nexperiments on six datasets across two reasoning tasks demonstrate the\nsuperiority of our method. Results show that TinyLLM can outperform large\nteacher LLMs significantly, despite having a considerably smaller model size.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04616v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04616v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04616v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04615v1",
    "updated": "2024-02-07T06:42:33+00:00",
    "published": "2024-02-07T06:42:33+00:00",
    "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
    "authors": [
      {
        "name": "Gilles Baechler"
      },
      {
        "name": "Srinivas Sunkara"
      },
      {
        "name": "Maria Wang"
      },
      {
        "name": "Fedir Zubach"
      },
      {
        "name": "Hassan Mansoor"
      },
      {
        "name": "Vincent Etter"
      },
      {
        "name": "Victor C\u0103rbune"
      },
      {
        "name": "Jason Lin"
      },
      {
        "name": "Jindong Chen"
      },
      {
        "name": "Abhanshu Sharma"
      }
    ],
    "summary": "Screen user interfaces (UIs) and infographics, sharing similar visual\nlanguage and design principles, play important roles in human communication and\nhuman-machine interaction. We introduce ScreenAI, a vision-language model that\nspecializes in UI and infographics understanding. Our model improves upon the\nPaLI architecture with the flexible patching strategy of pix2struct and is\ntrained on a unique mixture of datasets. At the heart of this mixture is a\nnovel screen annotation task in which the model has to identify the type and\nlocation of UI elements. We use these text annotations to describe screens to\nLarge Language Models and automatically generate question-answering (QA), UI\nnavigation, and summarization training datasets at scale. We run ablation\nstudies to demonstrate the impact of these design choices. At only 5B\nparameters, ScreenAI achieves new state-of-the-artresults on UI- and\ninfographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget\nCaptioning), and new best-in-class performance on others (Chart QA, DocVQA, and\nInfographicVQA) compared to models of similar size. Finally, we release three\nnew datasets: one focused on the screen annotation task and two others focused\non question answering.",
    "comment": "7 pages main tex with 5 figures, 2 page bib, 6 pages appendix",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04615v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04615v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04615v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04613v1",
    "updated": "2024-02-07T06:30:39+00:00",
    "published": "2024-02-07T06:30:39+00:00",
    "title": "Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces",
    "authors": [
      {
        "name": "Sebastian Neumayer"
      },
      {
        "name": "Viktor Stein"
      },
      {
        "name": "Gabriele Steidl"
      }
    ],
    "summary": "Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler\ndivergence, are subject to limitations regarding the support of the involved\nmeasures. A remedy consists of regularizing the $f$-divergence by a squared\nmaximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In\nthis paper, we use the so-called kernel mean embedding to show that the\ncorresponding regularization can be rewritten as the Moreau envelope of some\nfunction in the reproducing kernel Hilbert space associated with $K$. Then, we\nexploit well-known results on Moreau envelopes in Hilbert spaces to prove\nproperties of the MMD-regularized $f$-divergences and, in particular, their\ngradients. Subsequently, we use our findings to analyze Wasserstein gradient\nflows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein\ngradient flows starting from empirical measures and provide\nproof-of-the-concept numerical examples with Tsallis-$\\alpha$ divergences.",
    "comment": "42 pages, 13 figures",
    "journal_ref": null,
    "doi": null,
    "primary_category": "stat.ML",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.FA",
      "math.OC",
      "46N10 (Primary) 46E22, 94A15 (Secondary)"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04613v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04613v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04613v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04601v1",
    "updated": "2024-02-07T05:56:54+00:00",
    "published": "2024-02-07T05:56:54+00:00",
    "title": "Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector",
    "authors": [
      {
        "name": "Haihui Yang"
      },
      {
        "name": "Xiaojun Quan"
      }
    ],
    "summary": "Chinese grammatical error correction (CGEC) faces serious overcorrection\nchallenges when employing autoregressive generative models such as\nsequence-to-sequence (Seq2Seq) models and decoder-only large language models\n(LLMs). While previous methods aim to address overcorrection in Seq2Seq models,\nthey are difficult to adapt to decoder-only LLMs. In this paper, we propose an\nalignment-enhanced corrector for the overcorrection problem that applies to\nboth Seq2Seq models and decoder-only LLMs. Our method first trains a correction\nmodel to generate an initial correction of the source sentence. Then, we\ncombine the source sentence with the initial correction and feed it through an\nalignment model for another round of correction, aiming to enforce the\nalignment model to focus on potential overcorrection. Moreover, to enhance the\nmodel's ability to identify nuances, we further explore the reverse alignment\nof the source sentence and the initial correction. Finally, we transfer the\nalignment knowledge from two alignment models to the correction model,\ninstructing it on how to avoid overcorrection. Experimental results on three\nCGEC datasets demonstrate the effectiveness of our approach in alleviating\novercorrection and improving overall performance.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04601v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04601v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04601v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04599v1",
    "updated": "2024-02-07T05:47:31+00:00",
    "published": "2024-02-07T05:47:31+00:00",
    "title": "Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment",
    "authors": [
      {
        "name": "Lei Wang"
      },
      {
        "name": "Jun Liu"
      },
      {
        "name": "Liang Zheng"
      },
      {
        "name": "Tom Gedeon"
      },
      {
        "name": "Piotr Koniusz"
      }
    ],
    "summary": "Video sequences exhibit significant nuisance variations (undesired effects)\nof speed of actions, temporal locations, and subjects' poses, leading to\ntemporal-viewpoint misalignment when comparing two sets of frames or evaluating\nthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmera\nviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D\nskeleton sequences whose camera and subjects' poses can be easily manipulated\nin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where\nmatching well temporal blocks (temporal chunks that make up a sequence) of\nsupport-query sequence pairs (by factoring out nuisance variations) is\nessential due to limited samples of novel classes. Given a query sequence, we\ncreate its several views by simulating several camera locations. For a support\nsequence, we match it with view-simulated query sequences, as in the popular\nDynamic Time Warping (DTW). Specifically, each support temporal block can be\nmatched to the query temporal block with the same or adjacent (next) temporal\nindex, and adjacent camera views to achieve joint local temporal-viewpoint\nwarping. JEANIE selects the smallest distance among matching paths with\ndifferent temporal-viewpoint warping patterns, an advantage over DTW which only\nperforms temporal alignment. We also propose an unsupervised FSAR akin to\nclustering of sequences with JEANIE as a distance measure. JEANIE achieves\nstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D\nMultiview Activity II on supervised and unsupervised FSAR, and their\nmeta-learning inspired fusion.",
    "comment": "Under minor revision with IJCV. An extension of our ACCV'22 paper\n  [arXiv:arXiv:2210.16820] which was distinguished by the Sang Uk Lee Best\n  Student Paper Award. arXiv admin note: text overlap with arXiv:2112.12668",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04599v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04599v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04599v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04597v1",
    "updated": "2024-02-07T05:43:57+00:00",
    "published": "2024-02-07T05:43:57+00:00",
    "title": "CMSA algorithm for solving the prioritized pairwise test data generation problem in software product lines",
    "authors": [
      {
        "name": "Javier Ferrer"
      },
      {
        "name": "Francisco Chicano"
      },
      {
        "name": "Jos\u00e9 Antonio Ortega Toro"
      }
    ],
    "summary": "In Software Product Lines (SPLs) it may be difficult or even impossible to\ntest all the products of the family because of the large number of valid\nfeature combinations that may exist. Thus, we want to find a minimal subset of\nthe product family that allows us to test all these possible combinations\n(pairwise). Furthermore, when testing a single product is a great effort, it is\ndesirable to first test products composed of a set of priority features. This\nproblem is called Prioritized Pairwise Test Data Generation Problem.\n  State-of-the-art algorithms based on Integer Linear Programming for this\nproblema are faster enough for small and medium instances. However, there\nexists some real instances that are too large to be computed with these\nalgorithms in a reasonable time because of the exponential growth of the number\nof candidate solutions. Also, these heuristics not always lead us to the best\nsolutions. In this work we propose a new approach based on a hybrid\nmetaheuristic algorithm called Construct, Merge, Solve & Adapt. We compare this\nmatheuristic with four algorithms: a Hybrid algorithm based on Integer Linear\nProgramming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming\n(HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm\ncalled prioritized-ICPL. The analysis reveals that CMSA results in\nstatistically significantly better quality solutions in most instances and for\nmost levels of weighted coverage, although it requires more execution time.",
    "comment": "Preprint of the submitted version of the article in Journal of\n  Heuristics",
    "journal_ref": "J. Heuristics 27(1-2): 229-249 (2021)",
    "doi": "10.1007/s10732-020-09462-w",
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1007/s10732-020-09462-w",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.04597v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04597v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04597v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04596v1",
    "updated": "2024-02-07T05:38:53+00:00",
    "published": "2024-02-07T05:38:53+00:00",
    "title": "Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)",
    "authors": [
      {
        "name": "Sourav Mishra"
      },
      {
        "name": "Shirin Dora"
      },
      {
        "name": "Suresh Sundaram"
      }
    ],
    "summary": "Algorithms designed for addressing typical supervised classification problems\ncan only learn from a fixed set of samples and labels, making them unsuitable\nfor the real world, where data arrives as a stream of samples often associated\nwith multiple labels over time. This motivates the study of task-agnostic\ncontinual multi-label learning problems. While algorithms using deep learning\napproaches for continual multi-label learning have been proposed in the recent\nliterature, they tend to be computationally heavy. Although spiking neural\nnetworks (SNNs) offer a computationally efficient alternative to artificial\nneural networks, existing literature has not used SNNs for continual\nmulti-label learning. Also, accurately determining multiple labels with SNNs is\nstill an open research problem. This work proposes a dual output spiking\narchitecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss\nfunction is also proposed, improving the multi-label classification performance\nof the model by making it more robust to data imbalance. A modified F1 score is\npresented to evaluate the effectiveness of the proposed loss function in\nhandling imbalance. Experiments on several benchmark multi-label datasets show\nthat DOSA trained with the proposed loss function shows improved robustness to\ndata imbalance and obtains better continual multi-label learning performance\nthan CIFDM, a previous state-of-the-art algorithm.",
    "comment": "8 pages, 4 figures, 4 tables, 45 references. Submitted to IJCNN 2024",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04596v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04596v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04596v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04580v1",
    "updated": "2024-02-07T04:43:41+00:00",
    "published": "2024-02-07T04:43:41+00:00",
    "title": "A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents",
    "authors": [
      {
        "name": "Haoyi Niu"
      },
      {
        "name": "Jianming Hu"
      },
      {
        "name": "Guyue Zhou"
      },
      {
        "name": "Xianyuan Zhan"
      }
    ],
    "summary": "The burgeoning fields of robot learning and embodied AI have triggered an\nincreasing demand for large quantities of data. However, collecting sufficient\nunbiased data from the target domain remains a challenge due to costly data\ncollection processes and stringent safety requirements. Consequently,\nresearchers often resort to data from easily accessible source domains, such as\nsimulation and laboratory environments, for cost-effective data acquisition and\nrapid model iteration. Nevertheless, the environments and embodiments of these\nsource domains can be quite different from their target domain counterparts,\nunderscoring the need for effective cross-domain policy transfer approaches. In\nthis paper, we conduct a systematic review of existing cross-domain policy\ntransfer methods. Through a nuanced categorization of domain gaps, we\nencapsulate the overarching insights and design considerations of each problem\nsetting. We also provide a high-level discussion about the key methodologies\nused in cross-domain policy transfer problems. Lastly, we summarize the open\nchallenges that lie beyond the capabilities of current paradigms and discuss\npotential future directions in this field.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04580v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04580v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04580v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04579v1",
    "updated": "2024-02-07T04:39:23+00:00",
    "published": "2024-02-07T04:39:23+00:00",
    "title": "Collective Counterfactual Explanations via Optimal Transport",
    "authors": [
      {
        "name": "Ahmad-Reza Ehyaei"
      },
      {
        "name": "Ali Shirali"
      },
      {
        "name": "Samira Samadi"
      }
    ],
    "summary": "Counterfactual explanations provide individuals with cost-optimal actions\nthat can alter their labels to desired classes. However, if substantial\ninstances seek state modification, such individual-centric methods can lead to\nnew competitions and unanticipated costs. Furthermore, these recommendations,\ndisregarding the underlying data distribution, may suggest actions that users\nperceive as outliers. To address these issues, our work proposes a collective\napproach for formulating counterfactual explanations, with an emphasis on\nutilizing the current density of the individuals to inform the recommended\nactions. Our problem naturally casts as an optimal transport problem.\nLeveraging the extensive literature on optimal transport, we illustrate how\nthis collective method improves upon the desiderata of classical counterfactual\nexplanations. We support our proposal with numerical simulations, illustrating\nthe effectiveness of the proposed approach and its relation to classic methods.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04579v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04579v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04579v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04578v2",
    "updated": "2024-02-08T17:01:00+00:00",
    "published": "2024-02-07T04:36:31+00:00",
    "title": "S-Agents: self-organizing agents in open-ended environment",
    "authors": [
      {
        "name": "Jiaqi Chen"
      },
      {
        "name": "Yuxian Jiang"
      },
      {
        "name": "Jiachen Lu"
      },
      {
        "name": "Li Zhang"
      }
    ],
    "summary": "Leveraging large language models (LLMs), autonomous agents have significantly\nimproved, gaining the ability to handle a variety of tasks. In open-ended\nsettings, optimizing collaboration for efficiency and effectiveness demands\nflexible adjustments. Despite this, current research mainly emphasizes fixed,\ntask-oriented workflows and overlooks agent-centric organizational structures.\nDrawing inspiration from human organizational behavior, we introduce a\nself-organizing agent system (S-Agents) with a \"tree of agents\" structure for\ndynamic workflow, an \"hourglass agent architecture\" for balancing information\npriorities, and a \"non-obstructive collaboration\" method to allow asynchronous\ntask execution among agents. This structure can autonomously coordinate a group\nof agents, efficiently addressing the challenges of an open and dynamic\nenvironment without human intervention. Our experiments demonstrate that\nS-Agents proficiently execute collaborative building tasks and resource\ncollection in the Minecraft environment, validating their effectiveness.",
    "comment": "Preview, 23 pages, 12 figure",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04578v2",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04578v2",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04578v2"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04567v1",
    "updated": "2024-02-07T04:06:53+00:00",
    "published": "2024-02-07T04:06:53+00:00",
    "title": "OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences",
    "authors": [
      {
        "name": "Chen Wang"
      },
      {
        "name": "Sarah Erfani"
      },
      {
        "name": "Tansu Alpcan"
      },
      {
        "name": "Christopher Leckie"
      }
    ],
    "summary": "Anomaly detection in decision-making sequences is a challenging problem due\nto the complexity of normality representation learning and the sequential\nnature of the task. Most existing methods based on Reinforcement Learning (RL)\nare difficult to implement in the real world due to unrealistic assumptions,\nsuch as having access to environment dynamics, reward signals, and online\ninteractions with the environment. To address these limitations, we propose an\nunsupervised method named Offline Imitation Learning based Anomaly Detection\n(OIL-AD), which detects anomalies in decision-making sequences using two\nextracted behaviour features: action optimality and sequential association. Our\noffline learning model is an adaptation of behavioural cloning with a\ntransformer policy network, where we modify the training process to learn a Q\nfunction and a state value function from normal trajectories. We propose that\nthe Q function and the state value function can provide sufficient information\nabout agents' behavioural data, from which we derive two features for anomaly\ndetection. The intuition behind our method is that the action optimality\nfeature derived from the Q function can differentiate the optimal action from\nothers at each local state, and the sequential association feature derived from\nthe state value function has the potential to maintain the temporal\ncorrelations between decisions (state-action pairs). Our experiments show that\nOIL-AD can achieve outstanding online anomaly detection performance with up to\n34.8% improvement in F1 score over comparable baselines.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04567v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04567v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04567v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04563v1",
    "updated": "2024-02-07T03:43:56+00:00",
    "published": "2024-02-07T03:43:56+00:00",
    "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
    "authors": [
      {
        "name": "Saebom Leem"
      },
      {
        "name": "Hyunseok Seo"
      }
    ],
    "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer\nvision field with its great performance on various tasks. In order to fully\nutilize the ViT-based architecture in various applications, proper\nvisualization methods with a decent localization performance are necessary, but\nthese methods employed in CNN-based models are still not available in ViT due\nto its unique structure. In this work, we propose an attention-guided\nvisualization method applied to ViT that provides a high-level semantic\nexplanation for its decision. Our method selectively aggregates the gradients\ndirectly propagated from the classification output to each self-attention,\ncollecting the contribution of image features extracted from each location of\nthe input image. These gradients are additionally guided by the normalized\nself-attention scores, which are the pairwise patch correlation scores. They\nare used to supplement the gradients on the patch-level context information\nefficiently detected by the self-attention mechanism. This approach of our\nmethod provides elaborate high-level semantic explanations with great\nlocalization performance only with the class labels. As a result, our method\noutperforms the previous leading explainability methods of ViT in the\nweakly-supervised localization task and presents great capability in capturing\nthe full instances of the target class object. Meanwhile, our method provides a\nvisualization that faithfully explains the model, which is demonstrated in the\nperturbation comparison test.",
    "comment": "AAAI2024. Code available at\n  https://github.com/LeemSaebom/Attention-Guided-CAM-Visual-Explanations-of-Vision-Transformer-Guided-by-Self-Attention.git",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04563v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04563v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04563v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04559v1",
    "updated": "2024-02-07T03:37:19+00:00",
    "published": "2024-02-07T03:37:19+00:00",
    "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
    "authors": [
      {
        "name": "Chengxing Xie"
      },
      {
        "name": "Canyu Chen"
      },
      {
        "name": "Feiran Jia"
      },
      {
        "name": "Ziyu Ye"
      },
      {
        "name": "Kai Shu"
      },
      {
        "name": "Adel Bibi"
      },
      {
        "name": "Ziniu Hu"
      },
      {
        "name": "Philip Torr"
      },
      {
        "name": "Bernard Ghanem"
      },
      {
        "name": "Guohao Li"
      }
    ],
    "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in applications such as social science.\nHowever, one fundamental question remains: can LLM agents really simulate human\nbehaviors? In this paper, we focus on one of the most critical behaviors in\nhuman interactions, trust, and aim to investigate whether or not LLM agents can\nsimulate human trust behaviors. We first find that LLM agents generally exhibit\ntrust behaviors, referred to as agent trust, under the framework of Trust\nGames, which are widely recognized in behavioral economics. Then, we discover\nthat LLM agents can have high behavioral alignment with humans regarding trust\nbehaviors, indicating the feasibility to simulate human trust behaviors with\nLLM agents. In addition, we probe into the biases in agent trust and the\ndifferences in agent trust towards agents and humans. We also explore the\nintrinsic properties of agent trust under conditions including advanced\nreasoning strategies and external manipulations. We further offer important\nimplications for various scenarios where trust is paramount. Our study\nrepresents a significant step in understanding the behaviors of LLM agents and\nthe LLM-human analogy.",
    "comment": "The first two authors contributed equally. Project website:\n  https://www.camel-ai.org/research/agent-trust",
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "links": [
      {
        "href": "http://arxiv.org/abs/2402.04559v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04559v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04559v1"
  },
  {
    "entry_id": "http://arxiv.org/abs/2402.04557v1",
    "updated": "2024-02-07T03:25:08+00:00",
    "published": "2024-02-07T03:25:08+00:00",
    "title": "An Artificial Intelligence (AI) workflow for catalyst design and optimization",
    "authors": [
      {
        "name": "Nung Siong Lai"
      },
      {
        "name": "Yi Shen Tew"
      },
      {
        "name": "Xialin Zhong"
      },
      {
        "name": "Jun Yin"
      },
      {
        "name": "Jiali Li"
      },
      {
        "name": "Binhang Yan"
      },
      {
        "name": "Xiaonan Wang"
      }
    ],
    "summary": "In the pursuit of novel catalyst development to address pressing\nenvironmental concerns and energy demand, conventional design and optimization\nmethods often fall short due to the complexity and vastness of the catalyst\nparameter space. The advent of Machine Learning (ML) has ushered in a new era\nin the field of catalyst optimization, offering potential solutions to the\nshortcomings of traditional techniques. However, existing methods fail to\neffectively harness the wealth of information contained within the burgeoning\nbody of scientific literature on catalyst synthesis. To address this gap, this\nstudy proposes an innovative Artificial Intelligence (AI) workflow that\nintegrates Large Language Models (LLMs), Bayesian optimization, and an active\nlearning loop to expedite and enhance catalyst optimization. Our methodology\ncombines advanced language understanding with robust optimization strategies,\neffectively translating knowledge extracted from diverse literature into\nactionable parameters for practical experimentation and optimization. In this\narticle, we demonstrate the application of this AI workflow in the optimization\nof catalyst synthesis for ammonia production. The results underscore the\nworkflow's ability to streamline the catalyst development process, offering a\nswift, resource-efficient, and high-precision alternative to conventional\nmethods.",
    "comment": "31 pages, 7 figures",
    "journal_ref": "Ind. Eng. Chem. Res. 2023, 62, 43, 17835-17848",
    "doi": "10.1021/acs.iecr.3c02520",
    "primary_category": "physics.chem-ph",
    "categories": [
      "physics.chem-ph",
      "cs.LG"
    ],
    "links": [
      {
        "href": "http://dx.doi.org/10.1021/acs.iecr.3c02520",
        "title": "doi",
        "rel": "related",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/abs/2402.04557v1",
        "title": null,
        "rel": "alternate",
        "content_type": null
      },
      {
        "href": "http://arxiv.org/pdf/2402.04557v1",
        "title": "pdf",
        "rel": "related",
        "content_type": null
      }
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04557v1"
  }
]